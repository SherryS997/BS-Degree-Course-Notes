[
  {
    "objectID": "pages/RL/Week02.html",
    "href": "pages/RL/Week02.html",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The MAB problem conceptualizes actions as arms, each associated with a reward drawn from a probability distribution. The quest is to identify the arm with the highest mean reward, denoted as \\(\\mu^*\\), and consistently exploit it for maximum cumulative reward.\n\n\n\n\n\\(r_{i,k}\\): Reward obtained when selecting the \\(i\\)-th action for the \\(k\\)-th time.\n\\(Q(a_i)\\): Expected reward for selecting action \\(a_i\\) based on historical experiences.\n\\(Q(a^*)\\): The action maximizing the expected reward.\n\\(\\mu_i\\): True average reward for selecting action \\(a_i\\).\n\n\n\n\nThe estimation of \\(Q(a_i)\\) involves aggregating observed rewards for action \\(a_i\\) and dividing by the number of times the action is taken. Mathematically:\n\\[Q(a_i) = \\frac{\\sum_{k=1}^{n_i} r_{i,k}}{n_i}\\]\nHere, \\(n_i\\) represents the number of times action \\(a_i\\) is chosen.\n\n\n\nThe dynamic nature of the estimation process demands continuous updates. The formula for updating the estimate employs a learning rate (\\(\\alpha\\)) to adjust for new information:\n\\[Q_{k+1}(a_i) = Q_k(a_i) + \\alpha [r_{i,k} - Q_k(a_i)]\\]\nIn this formula, \\(Q_{k+1}(a_i)\\) is the updated estimate, \\(Q_k(a_i)\\) is the current estimate, \\(r_{i,k}\\) is the latest reward, and \\(\\alpha\\) governs the rate of adaptation.\n\n\n\nNavigating the exploration-exploitation dilemma requires a delicate balance. Striking the right equilibrium ensures optimal learning and maximizes cumulative rewards over time.\n\n\n\nAn analogy is drawn to a slot machine (one-arm bandit) with multiple levers (arms), each having distinct probabilities of payoff. The challenge mirrors that of identifying the lever (action) with the highest probability of payoff (mean reward).\n\n\n\nThe learning rate (\\(\\alpha\\)) serves as a crucial parameter influencing the rate at which the model adapts to new information. Choices of \\(\\alpha\\) lead to variations in the update rule, determining the emphasis on recent versus older rewards.\n\n\n\nConsider two actions, \\(A_1\\) and \\(A_2\\), each having distinct reward probabilities. For \\(A_1\\), the rewards are \\(+1\\) with a probability of \\(0.8\\) and \\(0\\) with a probability of \\(0.2\\). On the other hand, \\(A_2\\) yields \\(+1\\) with a probability of \\(0.6\\) and \\(0\\) with a probability of \\(0.4\\).\n\n\n\nA challenge arises when choosing actions based on initial rewards. If, for instance, \\(A_2\\) is selected first and a reward of \\(+1\\) is obtained, there is a risk of getting stuck with \\(A_2\\) due to its higher immediate reward probability. The same issue arises if starting with \\(A_1\\).\n\n\n\n\n\nThe Epsilon-Greedy strategy involves a balance between exploitation and exploration. It mainly consists of selecting the action with the highest estimated value most of the time (\\(1 - \\epsilon\\)), while occasionally exploring other actions with a probability of \\(\\epsilon\\). Here, \\(\\epsilon\\) is a small value, typically \\(0.1\\) or \\(0.01\\), determining the exploration rate. The strategy ensures asymptotic convergence, guaranteeing exploration of all actions in the long run.\n\n\n\nThe Softmax strategy employs a mathematical function to convert estimated action values into a probability distribution. The Softmax function is defined as:\n\\[P(A_i) = \\frac{e^{Q(A_i) / \\tau}}{\\sum_{j} e^{Q(A_j) / \\tau}}\\]\nWhere:\n\n\\(Q(A_i)\\) represents the estimated value of action \\(A_i\\),\n\\(\\tau\\) is the temperature parameter.\n\nThe temperature parameter (\\(\\tau\\)) controls the sensitivity to differences in estimated values. When \\(\\tau\\) is high, the probability distribution becomes more uniform, favoring exploration. Conversely, a low \\(\\tau\\) emphasizes exploiting the best-known action. Softmax also provides asymptotic convergence, ensuring exploration of all actions over time.\n\n\n\n\nThe temperature parameter, \\(\\tau\\), is a crucial factor in the Softmax strategy. A higher \\(\\tau\\) results in a more uniform probability distribution, making exploration more likely. Conversely, a lower \\(\\tau\\) amplifies differences in estimated values, making the strategy closer to a greedy approach."
  },
  {
    "objectID": "pages/RL/Week02.html#estimating-expected-reward",
    "href": "pages/RL/Week02.html#estimating-expected-reward",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The estimation of \\(Q(a_i)\\) involves aggregating observed rewards for action \\(a_i\\) and dividing by the number of times the action is taken. Mathematically:\n\\[Q(a_i) = \\frac{\\sum_{k=1}^{n_i} r_{i,k}}{n_i}\\]\nHere, \\(n_i\\) represents the number of times action \\(a_i\\) is chosen."
  },
  {
    "objectID": "pages/RL/Week02.html#updating-estimated-reward",
    "href": "pages/RL/Week02.html#updating-estimated-reward",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The dynamic nature of the estimation process demands continuous updates. The formula for updating the estimate employs a learning rate (\\(\\alpha\\)) to adjust for new information:\n\\[Q_{k+1}(a_i) = Q_k(a_i) + \\alpha [r_{i,k} - Q_k(a_i)]\\]\nIn this formula, \\(Q_{k+1}(a_i)\\) is the updated estimate, \\(Q_k(a_i)\\) is the current estimate, \\(r_{i,k}\\) is the latest reward, and \\(\\alpha\\) governs the rate of adaptation."
  },
  {
    "objectID": "pages/RL/Week02.html#challenges-and-considerations",
    "href": "pages/RL/Week02.html#challenges-and-considerations",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "Navigating the exploration-exploitation dilemma requires a delicate balance. Striking the right equilibrium ensures optimal learning and maximizes cumulative rewards over time."
  },
  {
    "objectID": "pages/RL/Week02.html#multi-arm-bandit-analogy",
    "href": "pages/RL/Week02.html#multi-arm-bandit-analogy",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "An analogy is drawn to a slot machine (one-arm bandit) with multiple levers (arms), each having distinct probabilities of payoff. The challenge mirrors that of identifying the lever (action) with the highest probability of payoff (mean reward)."
  },
  {
    "objectID": "pages/RL/Week02.html#learning-rate-alpha",
    "href": "pages/RL/Week02.html#learning-rate-alpha",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The learning rate (\\(\\alpha\\)) serves as a crucial parameter influencing the rate at which the model adapts to new information. Choices of \\(\\alpha\\) lead to variations in the update rule, determining the emphasis on recent versus older rewards."
  },
  {
    "objectID": "pages/RL/Week02.html#actions-and-reward-probabilities",
    "href": "pages/RL/Week02.html#actions-and-reward-probabilities",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "Consider two actions, \\(A_1\\) and \\(A_2\\), each having distinct reward probabilities. For \\(A_1\\), the rewards are \\(+1\\) with a probability of \\(0.8\\) and \\(0\\) with a probability of \\(0.2\\). On the other hand, \\(A_2\\) yields \\(+1\\) with a probability of \\(0.6\\) and \\(0\\) with a probability of \\(0.4\\)."
  },
  {
    "objectID": "pages/RL/Week02.html#exploitation-challenge",
    "href": "pages/RL/Week02.html#exploitation-challenge",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "A challenge arises when choosing actions based on initial rewards. If, for instance, \\(A_2\\) is selected first and a reward of \\(+1\\) is obtained, there is a risk of getting stuck with \\(A_2\\) due to its higher immediate reward probability. The same issue arises if starting with \\(A_1\\)."
  },
  {
    "objectID": "pages/RL/Week02.html#exploration-strategies",
    "href": "pages/RL/Week02.html#exploration-strategies",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The Epsilon-Greedy strategy involves a balance between exploitation and exploration. It mainly consists of selecting the action with the highest estimated value most of the time (\\(1 - \\epsilon\\)), while occasionally exploring other actions with a probability of \\(\\epsilon\\). Here, \\(\\epsilon\\) is a small value, typically \\(0.1\\) or \\(0.01\\), determining the exploration rate. The strategy ensures asymptotic convergence, guaranteeing exploration of all actions in the long run.\n\n\n\nThe Softmax strategy employs a mathematical function to convert estimated action values into a probability distribution. The Softmax function is defined as:\n\\[P(A_i) = \\frac{e^{Q(A_i) / \\tau}}{\\sum_{j} e^{Q(A_j) / \\tau}}\\]\nWhere:\n\n\\(Q(A_i)\\) represents the estimated value of action \\(A_i\\),\n\\(\\tau\\) is the temperature parameter.\n\nThe temperature parameter (\\(\\tau\\)) controls the sensitivity to differences in estimated values. When \\(\\tau\\) is high, the probability distribution becomes more uniform, favoring exploration. Conversely, a low \\(\\tau\\) emphasizes exploiting the best-known action. Softmax also provides asymptotic convergence, ensuring exploration of all actions over time."
  },
  {
    "objectID": "pages/RL/Week02.html#temperature-parameter-tau",
    "href": "pages/RL/Week02.html#temperature-parameter-tau",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The temperature parameter, \\(\\tau\\), is a crucial factor in the Softmax strategy. A higher \\(\\tau\\) results in a more uniform probability distribution, making exploration more likely. Conversely, a lower \\(\\tau\\) amplifies differences in estimated values, making the strategy closer to a greedy approach."
  },
  {
    "objectID": "pages/RL/Week02.html#regret-minimization",
    "href": "pages/RL/Week02.html#regret-minimization",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Regret Minimization",
    "text": "Regret Minimization\n\nDefinition of Regret\nRegret, denoted as \\(R_T\\), quantifies the total loss in rewards incurred due to the agent’s lack of knowledge about the optimal action during the initial \\(T\\) time steps. It is defined as the difference between the cumulative reward obtained by an optimal strategy and the cumulative reward obtained by the learning algorithm.\n\\[R_T = \\sum_{t=1}^T \\mu^* - \\mathbb{E}\\left[\\sum_{t=1}^T r_{a_t}(t)\\right]\\]\nwhere:\n\n\\(\\mu^*\\) is the expected reward of the optimal arm,\n\\(r_{a_t}(t)\\) is the reward obtained at time \\(t\\) from action \\(a_t\\),\n\\(a_t\\) is the action selected by the learning algorithm at time \\(t\\).\n\n\n\nObjective\nThe primary goal is to minimize regret by quickly identifying and exploiting the optimal arm. In dynamic scenarios, like news recommendation, where the optimal action may change frequently, minimizing regret becomes crucial for effective decision-making."
  },
  {
    "objectID": "pages/RL/Week02.html#total-rewards-maximization",
    "href": "pages/RL/Week02.html#total-rewards-maximization",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Total Rewards Maximization",
    "text": "Total Rewards Maximization\n\nLearning Curve\nIn the context of the multi-arm bandit problem, the learning curve represents the evolution of cumulative rewards over time. The objective is to minimize the area under this curve, signifying the loss incurred before reaching optimal performance.\n\\[R(t) = \\sum_{\\tau=1}^t \\mu^* - \\mathbb{E}\\left[\\sum_{\\tau=1}^t r_{a_\\tau}(\\tau)\\right]\\]\nHere, \\(R(t)\\) represents the cumulative regret up to time \\(t\\).\n\n\nQuick Learning\nIn scenarios like news recommendation, algorithms must adapt swiftly to changing optimal arms. The emphasis is on achieving quick learning to minimize the region under the learning curve and accelerate the convergence to optimal performance."
  },
  {
    "objectID": "pages/RL/Week02.html#pac-framework",
    "href": "pages/RL/Week02.html#pac-framework",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "PAC Framework",
    "text": "PAC Framework\n\nDefinition\nThe Probably Approximately Correct (PAC) framework aims to minimize the number of samples required to find an approximately correct solution. It introduces the concept of an \\(\\epsilon\\)-optimal arm, where an arm is considered approximately correct if its expected reward is within \\(\\epsilon\\) of the true optimal reward.\n\\[|\\hat{\\mu}_a - \\mu^*| \\leq \\epsilon\\]\nThe PAC framework also incorporates a confidence parameter \\(\\delta\\), representing the probability that the algorithm fails to provide an \\(\\epsilon\\)-optimal arm.\n\n\nTrade-off\nChoosing suitable values for \\(\\epsilon\\) and \\(\\delta\\) involves a trade-off between the acceptable performance loss (\\(\\epsilon\\)) and the confidence in achieving this performance (\\(\\delta\\)). This trade-off ensures robustness in the face of uncertainty."
  },
  {
    "objectID": "pages/RL/Week02.html#median-elimination-algorithm",
    "href": "pages/RL/Week02.html#median-elimination-algorithm",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Median Elimination Algorithm",
    "text": "Median Elimination Algorithm\n\nRound-Based Approach\nThe Median Elimination Algorithm divides the learning process into rounds. In each round, the algorithm samples each arm and eliminates those with estimated rewards below the median, reducing the set of candidate arms.\n\n\nSample Complexity\nThe total sample complexity is determined by the sum of samples drawn in each round. The algorithm guarantees that at least one arm remains \\(\\epsilon\\)-optimal with high probability."
  },
  {
    "objectID": "pages/RL/Week02.html#upper-confidence-bound-ucb-algorithm",
    "href": "pages/RL/Week02.html#upper-confidence-bound-ucb-algorithm",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Upper Confidence Bound (UCB) Algorithm",
    "text": "Upper Confidence Bound (UCB) Algorithm\n\nObjective\nThe UCB algorithm aims to achieve regret optimality by efficiently balancing exploration and exploitation. Unlike round-based approaches, UCB1 selects arms based on upper confidence bounds of estimated rewards.\n\n\nImplementation\nUCB1 is known for its simplicity and ease of implementation. It provides practical performance in scenarios like ad or news placement, where quick learning and adaptability are crucial."
  },
  {
    "objectID": "pages/RL/Week02.html#thompson-sampling",
    "href": "pages/RL/Week02.html#thompson-sampling",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Thompson Sampling",
    "text": "Thompson Sampling\n\nBayesian Approach\nThompson Sampling adopts a Bayesian approach, modeling uncertainty in the bandit problem through probability distributions. It leverages Bayesian inference to update beliefs about the reward distributions associated with each arm.\n\n\nRegret Optimality\nAgarwal and Goyal (2012) demonstrated that Thompson Sampling achieves regret optimality. This means that, asymptotically, the cumulative regret approaches the lower bound, signifying optimal learning performance.\n\n\nAdvantage over UCB\nThompson Sampling tends to have better constants than UCB-based methods, providing improved practical performance. It is particularly advantageous in scenarios where the underlying distribution of arms is uncertain."
  },
  {
    "objectID": "pages/RL/Week02.html#introduction",
    "href": "pages/RL/Week02.html#introduction",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of reinforcement learning, the Upper Confidence Bound (UCB) algorithm stands out as an effective strategy for addressing the multi-armed bandit problem. This algorithm offers a nuanced approach to the exploration-exploitation trade-off, mitigating the drawbacks associated with simpler strategies such as Epsilon-Greedy."
  },
  {
    "objectID": "pages/RL/Week02.html#challenges-with-epsilon-greedy",
    "href": "pages/RL/Week02.html#challenges-with-epsilon-greedy",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Challenges with Epsilon-Greedy",
    "text": "Challenges with Epsilon-Greedy\n\nExpected Value Maintenance\nIn the Epsilon-Greedy approach, the algorithm maintains the expected values (Q values) for each arm. However, a crucial limitation arises during exploration. The algorithm, guided by a fixed exploration probability (Epsilon), often expends valuable samples on suboptimal arms.\n\n\nWasted Samples and Regret Impact\nThe consequences of this exploration strategy are two-fold. Firstly, it results in wasted opportunities, as the algorithm neglects gathering valuable information about potentially optimal arms in favor of the suboptimal ones. Secondly, the impact on regret is substantial, particularly when selecting arms with low rewards."
  },
  {
    "objectID": "pages/RL/Week02.html#ucb-a-solution-to-exploration-challenges",
    "href": "pages/RL/Week02.html#ucb-a-solution-to-exploration-challenges",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "UCB: A Solution to Exploration Challenges",
    "text": "UCB: A Solution to Exploration Challenges\n\nIntroduction of Confidence Intervals\nUCB introduces a novel approach by not only maintaining mean estimates (Q values) for each arm but also incorporating confidence intervals. These intervals signify the algorithm’s confidence that the true value of Q for a particular arm lies within a specified range. \n\n\nAction Selection Mechanism\nThe key to UCB’s success lies in its action selection mechanism. Instead of relying solely on mean estimates, it considers an upper confidence bound for each arm. Mathematically, this can be expressed as:\n\\[\\text{UCB}_{j} = \\bar{X}_{j} + \\sqrt{\\frac{2 \\ln{N}}{n_{j}}}\\]\nHere,\n\n\\(\\bar{X}_{j}\\) is the mean estimate for arm j.\n\\(N\\) is the total number of actions taken.\n\\(n_{j}\\) represents the number of times arm j has been played.\n\nThis formulation balances exploration and exploitation, with the exploration term gradually diminishing as the number of plays (\\(n_{j}\\)) increases.\n\n\nRegret Minimization\nUCB is designed to minimize regret, a measure of the algorithm’s deviation from the optimal strategy. The regret for playing a suboptimal arm (arm J) is limited by:\n\\[\\text{Regret}_{J} \\leq 8 \\Delta_{J} \\ln{N}\\]\nHere,\n\n\\(\\Delta_{J}\\) represents the difference between the optimal arm’s expected reward and that of arm J."
  },
  {
    "objectID": "pages/RL/Week02.html#advantages-of-ucb",
    "href": "pages/RL/Week02.html#advantages-of-ucb",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Advantages of UCB",
    "text": "Advantages of UCB\n\nEfficient Exploration\nUCB efficiently focuses exploration efforts on arms with the potential for high rewards, reducing the occurrence of wasted samples on suboptimal choices.\n\n\nRegret Optimality\nBy limiting the number of plays for suboptimal arms, UCB minimizes regret and ensures that the algorithm converges towards optimal choices over time.\n\n\nSimplicity and Practical Performance\nUCB’s elegance lies in its simplicity of implementation, requiring no random number generation for exploration. This simplicity, coupled with its strong performance in practical scenarios, establishes UCB as a formidable algorithm for real-world applications."
  },
  {
    "objectID": "pages/RL/Week02.html#introduction-1",
    "href": "pages/RL/Week02.html#introduction-1",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Introduction",
    "text": "Introduction\nThe focus of this discussion is on addressing the challenge of customization in online platforms, specifically in the realms of ad selection and news story recommendations. The proposed solution is the utilization of contextual bandits, an extension of traditional bandit algorithms designed to incorporate user-specific attributes for a more personalized experience."
  },
  {
    "objectID": "pages/RL/Week02.html#contextual-bandits-a-conceptual-framework",
    "href": "pages/RL/Week02.html#contextual-bandits-a-conceptual-framework",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Contextual Bandits: A Conceptual Framework",
    "text": "Contextual Bandits: A Conceptual Framework\n\nTraditional Bandits vs. Contextual Bandits\nTraditional bandit algorithms involve the selection of actions without considering any contextual information. Contextual bandits, on the other hand, extend this paradigm by introducing the consideration of features related to both users and the available actions.\n\n\nMotivation for Contextual Bandits\nThe motivation behind introducing contextual bandits arises from the inherent challenge of tailoring recommendations for each user. In the context of ad selection and news story recommendations, a one-size-fits-all approach proves inadequate. Contextual bandits address this by accommodating user-specific features in the decision-making process."
  },
  {
    "objectID": "pages/RL/Week02.html#challenges-and-solutions",
    "href": "pages/RL/Week02.html#challenges-and-solutions",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Challenges and Solutions",
    "text": "Challenges and Solutions\n\nIndividual Bandits per User: Training Difficulties\nA significant challenge in implementing bandit algorithms for each user lies in the impracticality of training due to the extensive user base. Users’ infrequent visits to pages make it challenging to accumulate sufficient training data.\n\n\nGrouping Users Based on Features\nTo overcome the challenges of individual bandits per user, a strategy is proposed wherein users are grouped based on a set of parameters such as age, gender, and browsing behavior. This grouping allows for a more efficient handling of user features."
  },
  {
    "objectID": "pages/RL/Week02.html#mathematical-foundations",
    "href": "pages/RL/Week02.html#mathematical-foundations",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Mathematical Foundations",
    "text": "Mathematical Foundations\n\nLinear Parameterization of Features\nIn contextual bandits, the mean (\\(\\mu\\)) and variance (\\(\\sigma\\)) of the reward distribution associated with each action are influenced by user features. This relationship is commonly expressed through linear parameterization. Mathematically, this can be represented as:\n\\[\\mu_{a,s} = \\mathbf{w}_a \\cdot \\mathbf{X}_s\\]\nWhere:\n\n\\(\\mu_{a,s}\\) is the mean for action \\(a\\) and user features \\(s\\).\n\\(\\mathbf{w}_a\\) is the weight vector associated with action \\(a\\).\n\\(\\mathbf{X}_s\\) represents the feature vector for user \\(s\\).\n\n\n\nContextual Bandits for Actions and Context\nExtending the mathematical framework, features are considered not only for users but also for actions. This enhancement allows for a more nuanced approach, facilitating the reuse of information when actions change. The revised equation becomes:\n\\[Q_{s,a} = \\mathbf{w}_a \\cdot \\mathbf{X}_s\\]\nHere, \\(Q_{s,a}\\) represents the expected reward for action \\(a\\) given user features \\(s\\).\n\n\nLinUCB Algorithm\nThe LinUCB algorithm is introduced as a practical implementation of contextual bandits. It leverages ridge regression to predict expected rewards, creating a linear function of features. The ridge regression is expressed as:\n\\[\\hat{\\mathbf{w}}_a = \\arg \\min_{\\mathbf{w}_a} \\sum_{t=1}^{T} (r_{t,a} - \\mathbf{w}_a \\cdot \\mathbf{X}_{t,s})^2 + \\lambda \\|\\mathbf{w}_a\\|_2^2\\]\nWhere:\n\n\\(\\hat{\\mathbf{w}}_a\\) is the estimated weight vector for action \\(a\\).\n\\(r_{t,a}\\) is the observed reward for action \\(a\\) at time \\(t\\).\n\\(\\mathbf{X}_{t,s}\\) is the feature vector for user \\(s\\) at time \\(t\\).\n\\(\\lambda\\) is the regularization parameter.\n\n\n\nAdvantages of Contextual Bandits\nContextual bandits offer several advantages: - Personalized recommendations based on user features. - Efficient learning and adaptation even when the set of actions changes."
  },
  {
    "objectID": "pages/RL/Week02.html#contextual-bandits-in-the-reinforcement-learning-spectrum",
    "href": "pages/RL/Week02.html#contextual-bandits-in-the-reinforcement-learning-spectrum",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Contextual Bandits in the Reinforcement Learning Spectrum",
    "text": "Contextual Bandits in the Reinforcement Learning Spectrum\nContextual bandits serve as a crucial link between traditional bandits and full reinforcement learning. While considering both actions and context, they do not explicitly address the sequence, providing a bridge in the learning spectrum."
  },
  {
    "objectID": "pages/RL/Week02.html#full-reinforcement-learning-problem",
    "href": "pages/RL/Week02.html#full-reinforcement-learning-problem",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Full Reinforcement Learning Problem",
    "text": "Full Reinforcement Learning Problem\n\nSequence of Decisions\nIn contrast, the full RL problem involves a sequence of actions. Each decision influences subsequent situations, introducing complexity compared to the immediate and contextual Bandit problems.\n\n\nDelayed Rewards\nUnlike Bandits, the RL problem deals with delayed rewards. The consequences of an action may not manifest immediately but rather at the conclusion of a sequence of decisions. This delayed reward challenges the agent to associate distant outcomes with earlier choices.\n\n\nContext-Dependent Sequences\nMoreover, the sequence of problems in RL is context-dependent. The nature of the second problem depends on the action taken in the first, introducing an interdependence that was absent in contextual Bandit scenarios."
  },
  {
    "objectID": "pages/RL/Week02.html#temporal-distance-and-stochasticity",
    "href": "pages/RL/Week02.html#temporal-distance-and-stochasticity",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Temporal Distance and Stochasticity",
    "text": "Temporal Distance and Stochasticity\nThe concept of temporal distance in RL, where rewards are tied to actions in the past, is essential. Additionally, RL often involves stochastic environments, where variations or noise influence the outcomes.\n\nStochastic Environment\nStochasticity in the environment implies uncertainty in the response to an action. For instance, in a maze-running scenario, the mouse’s decision might lead to different outcomes due to environmental variability.\n\n\nNeed for Stochastic Models\nStochastic environments are employed in RL due to the impracticality of measuring or modeling every aspect precisely. For example, even in a simple coin toss, various unobservable factors contribute to the randomness observed."
  },
  {
    "objectID": "pages/RL/Week02.html#reinforcement-learning-framework",
    "href": "pages/RL/Week02.html#reinforcement-learning-framework",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Reinforcement Learning Framework",
    "text": "Reinforcement Learning Framework\n\nAgent-Environment Interaction\nThe RL framework comprises an agent and an environment in close interaction. The agent senses the environment’s state, takes actions, and receives rewards, leading to a continuous loop of interaction.\n\n\nStochasticity in State Transitions\nBoth state transitions and action selections can be stochastic, adding an element of unpredictability to the RL setting. The agent’s decisions are based on incomplete information and uncertain outcomes.\n\n\nEvaluation and Rewards\nCentral to RL is the concept of evaluation through rewards. The agent’s goal is to learn a mapping from states to actions, aiming to maximize cumulative rewards over the long term."
  },
  {
    "objectID": "pages/RL/Week02.html#temporal-difference-in-rewards",
    "href": "pages/RL/Week02.html#temporal-difference-in-rewards",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Temporal Difference in Rewards",
    "text": "Temporal Difference in Rewards\nThe delayed and noisy nature of rewards in RL introduces the need for temporal difference considerations. Agents must predict future rewards based on their current actions, leading to a more intricate decision-making process.\n\nExample: Tic-Tac-Toe\nIllustrating this, in a game of tic-tac-toe, a move made early in the game may strongly influence the eventual outcome, even though the final reward is received only at the game’s end."
  },
  {
    "objectID": "pages/RL/Week02.html#full-rl-problem-solving-approach",
    "href": "pages/RL/Week02.html#full-rl-problem-solving-approach",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Full RL Problem Solving Approach",
    "text": "Full RL Problem Solving Approach\n\nSequence of Bandit Problems\nTo solve the full RL problem, a sequence of Bandit problems is employed. Each state-action pair corresponds to a Bandit problem that the agent must solve, and the solutions cascade to form a comprehensive strategy.\n\n\nDynamic Programming\nThe solution approach aligns with dynamic programming, where the value derived from solving one Bandit problem serves as the reward for the preceding state-action pair. This recursive approach forms the basis for tackling the complexity of RL scenarios."
  },
  {
    "objectID": "pages/RL/Week02.html#points-to-remember",
    "href": "pages/RL/Week02.html#points-to-remember",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nMulti-Arm Bandit Problem: Conceptualizes actions as arms, each with a reward drawn from a probability distribution. Balancing exploration and exploitation is crucial for optimal learning and cumulative rewards.\nExploration Strategies:\n\nEpsilon-Greedy: Balances exploitation and exploration, with a small exploration rate (\\(\\epsilon\\)).\nSoftmax: Converts estimated action values into a probability distribution, controlled by a temperature parameter (\\(\\tau\\)).\n\nRegret Minimization Framework: Aims to minimize regret (\\(R_T\\)), the total loss in rewards compared to an optimal strategy. Efficient learning and quick adaptation are essential in dynamic scenarios.\nPAC Framework: Probably Approximately Correct framework introduces the concept of an \\(\\epsilon\\)-optimal arm, balancing performance loss (\\(\\epsilon\\)) and confidence (\\(\\delta\\)).\nUCB Algorithm: Upper Confidence Bound algorithm efficiently balances exploration and exploitation by considering confidence intervals. It minimizes regret and converges towards optimal choices over time.\nContextual Bandits: Extend traditional bandits by incorporating user-specific features for personalized recommendations. The LinUCB algorithm is a practical implementation.\nFull Reinforcement Learning (RL): Involves sequences of decisions, delayed rewards, and stochastic environments. Temporal difference considerations become crucial in predicting future rewards.\nDynamic Programming in RL: Solving the Full RL Problem involves treating it as a sequence of Bandit problems, employing dynamic programming for recursive learning and optimization.\n\nThe journey through Multi-Arm Bandit Problems, regret minimization, contextual bandits, and Full RL has equipped us with a comprehensive understanding of decision-making in uncertain and dynamic environments. These concepts provide a solid foundation for addressing challenges in various real-world scenarios."
  },
  {
    "objectID": "pages/RL/Week01_1.html",
    "href": "pages/RL/Week01_1.html",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "In the realm of machine learning, a predominant paradigm involves the acquisition of knowledge through the learning of functions that map input features to specific outputs. This conventional approach, known as supervised learning, relies on the provision of explicit instructions and training data to inform the learning process. Typically, the model generalizes from examples presented during training to make predictions or classifications on new, unseen data.\n\n\n\nReinforcement Learning (RL), in contrast, embodies a distinctive methodology centered around trial-and-error learning within systems characterized by intricate and challenging control dynamics. In the RL framework, explicit instructions are eschewed in favor of evaluating actions based on received rewards and punishments. This departure from prescriptive learning mirrors the way humans acquire skills such as cycling or walking, where trial and error, coupled with feedback, plays a pivotal role.\n\n\n\nTrial and Error Approach: RL stands out for its reliance on the iterative process of trying various actions and subsequently gauging their efficacy through outcomes, be they positive rewards or negative consequences.\nAbsence of Upfront Instructions: Unlike supervised learning, RL lacks a predetermined set of instructions provided beforehand. Instead, the system learns by interacting with its environment and adapting based on the consequences of its actions.\n\n\n\n\n\nThe dichotomy between supervised learning and reinforcement learning can be elucidated by highlighting their fundamental disparities.\n\n\nIn supervised learning, explicit instructions are imparted to the learning algorithm upfront. The model is trained to generate outputs conforming to the provided instructions, drawing insights from labeled examples.\n\n\n\nConversely, reinforcement learning refrains from pre-established instructions. Actions are executed, and their merit is subsequently appraised through a feedback mechanism of rewards and punishments. The system learns to optimize its behavior based on experiential outcomes.\n\n\n\n\nDrawing parallels with how humans assimilate complex skills, reinforcement learning aligns with a trial-and-error learning paradigm. Consider the analogy of a child learning to cycle; the process involves attempts, feedback (both positive and negative), and an eventual refinement of the skill through repeated iterations.\n\n\nRooted in behavioral psychology, reinforcement learning embodies a system’s interaction with its environment, learning through the consequences of its actions. The classical example of Pavlov’s dog underscores the association of stimuli (bell ringing) with rewards (food), illustrating the behavioral conditioning inherent in RL principles.\n\n\n\n\nReinforcement learning finds diverse applications across various domains, demonstrating its efficacy in addressing complex challenges.\n\n\nIn domains like autonomous driving or the control of a helicopter, reinforcement learning proves invaluable. The ability to navigate complex environments and execute intricate maneuvers showcases the adaptability of RL in real-world scenarios.\n\n\n\nHumanoid robots, engaged in tasks like playing soccer, leverage reinforcement learning to master complex movements, such as kicking a ball. This exemplifies the adaptability of RL in training systems to perform dynamic and agile actions.\n\n\n\nReinforcement learning excels in navigating cluttered and intricate spaces, offering a more pragmatic approach compared to conventional control methods. Examples range from traffic scenarios to multi-roomed buildings.\n\n\n\nWhen dealing with stochastic systems and probabilistic outcomes, reinforcement learning provides an effective solution. Its application in scenarios where precise control or prediction is challenging due to inherent uncertainty demonstrates its versatility.\n\n\n\nIn tasks requiring human-like cognitive processes, such as determining where to focus attention in a complex environment, reinforcement learning, under the banner of cognitively motivated learning, strives to emulate human decision-making patterns.\n\n\n\nReinforcement learning extends its utility to customization and personalization tasks in various industries. Tailoring products or services based on individual preferences underscores its role in enhancing user experiences.\n\n\n\n\nThe success of reinforcement learning in solving real-world challenges is underscored by notable achievements such as ChatGPT. Ongoing advancements contribute to its widespread adoption, positioning RL as a potent tool for addressing intricate problems characterized by complexity, uncertainty, and human-like decision-making processes."
  },
  {
    "objectID": "pages/RL/Week01_1.html#introduction-to-machine-learning",
    "href": "pages/RL/Week01_1.html#introduction-to-machine-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "In the realm of machine learning, a predominant paradigm involves the acquisition of knowledge through the learning of functions that map input features to specific outputs. This conventional approach, known as supervised learning, relies on the provision of explicit instructions and training data to inform the learning process. Typically, the model generalizes from examples presented during training to make predictions or classifications on new, unseen data."
  },
  {
    "objectID": "pages/RL/Week01_1.html#fundamentals-of-reinforcement-learning",
    "href": "pages/RL/Week01_1.html#fundamentals-of-reinforcement-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "Reinforcement Learning (RL), in contrast, embodies a distinctive methodology centered around trial-and-error learning within systems characterized by intricate and challenging control dynamics. In the RL framework, explicit instructions are eschewed in favor of evaluating actions based on received rewards and punishments. This departure from prescriptive learning mirrors the way humans acquire skills such as cycling or walking, where trial and error, coupled with feedback, plays a pivotal role.\n\n\n\nTrial and Error Approach: RL stands out for its reliance on the iterative process of trying various actions and subsequently gauging their efficacy through outcomes, be they positive rewards or negative consequences.\nAbsence of Upfront Instructions: Unlike supervised learning, RL lacks a predetermined set of instructions provided beforehand. Instead, the system learns by interacting with its environment and adapting based on the consequences of its actions."
  },
  {
    "objectID": "pages/RL/Week01_1.html#contrast-with-supervised-learning",
    "href": "pages/RL/Week01_1.html#contrast-with-supervised-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "The dichotomy between supervised learning and reinforcement learning can be elucidated by highlighting their fundamental disparities.\n\n\nIn supervised learning, explicit instructions are imparted to the learning algorithm upfront. The model is trained to generate outputs conforming to the provided instructions, drawing insights from labeled examples.\n\n\n\nConversely, reinforcement learning refrains from pre-established instructions. Actions are executed, and their merit is subsequently appraised through a feedback mechanism of rewards and punishments. The system learns to optimize its behavior based on experiential outcomes."
  },
  {
    "objectID": "pages/RL/Week01_1.html#learning-paradigm-in-reinforcement-learning",
    "href": "pages/RL/Week01_1.html#learning-paradigm-in-reinforcement-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "Drawing parallels with how humans assimilate complex skills, reinforcement learning aligns with a trial-and-error learning paradigm. Consider the analogy of a child learning to cycle; the process involves attempts, feedback (both positive and negative), and an eventual refinement of the skill through repeated iterations.\n\n\nRooted in behavioral psychology, reinforcement learning embodies a system’s interaction with its environment, learning through the consequences of its actions. The classical example of Pavlov’s dog underscores the association of stimuli (bell ringing) with rewards (food), illustrating the behavioral conditioning inherent in RL principles."
  },
  {
    "objectID": "pages/RL/Week01_1.html#applications-of-reinforcement-learning",
    "href": "pages/RL/Week01_1.html#applications-of-reinforcement-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "Reinforcement learning finds diverse applications across various domains, demonstrating its efficacy in addressing complex challenges.\n\n\nIn domains like autonomous driving or the control of a helicopter, reinforcement learning proves invaluable. The ability to navigate complex environments and execute intricate maneuvers showcases the adaptability of RL in real-world scenarios.\n\n\n\nHumanoid robots, engaged in tasks like playing soccer, leverage reinforcement learning to master complex movements, such as kicking a ball. This exemplifies the adaptability of RL in training systems to perform dynamic and agile actions.\n\n\n\nReinforcement learning excels in navigating cluttered and intricate spaces, offering a more pragmatic approach compared to conventional control methods. Examples range from traffic scenarios to multi-roomed buildings.\n\n\n\nWhen dealing with stochastic systems and probabilistic outcomes, reinforcement learning provides an effective solution. Its application in scenarios where precise control or prediction is challenging due to inherent uncertainty demonstrates its versatility.\n\n\n\nIn tasks requiring human-like cognitive processes, such as determining where to focus attention in a complex environment, reinforcement learning, under the banner of cognitively motivated learning, strives to emulate human decision-making patterns.\n\n\n\nReinforcement learning extends its utility to customization and personalization tasks in various industries. Tailoring products or services based on individual preferences underscores its role in enhancing user experiences."
  },
  {
    "objectID": "pages/RL/Week01_1.html#success-stories-and-advancements",
    "href": "pages/RL/Week01_1.html#success-stories-and-advancements",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "The success of reinforcement learning in solving real-world challenges is underscored by notable achievements such as ChatGPT. Ongoing advancements contribute to its widespread adoption, positioning RL as a potent tool for addressing intricate problems characterized by complexity, uncertainty, and human-like decision-making processes."
  },
  {
    "objectID": "pages/RL/Week01_1.html#customization-on-yahoo-news",
    "href": "pages/RL/Week01_1.html#customization-on-yahoo-news",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Customization on Yahoo News",
    "text": "Customization on Yahoo News\n\nOverview\nCustomization involves tailoring content based on user preferences and behavior. Yahoo News, for example, uses a personalized approach in presenting news stories to users.\n\n\nManual Labeling Challenges\nDue to the dynamic nature of news content and the vast user diversity, manual labeling of stories for individual users is impractical. It is neither feasible nor efficient to have editors constantly labeling stories for the millions of users who access the platform.\n\n\nReinforcement Learning Solution\nTo address this challenge, a reinforcement learning (RL) approach is employed. Instead of explicit human instructions, RL utilizes user interactions as feedback for personalization. Editors initially select a set of stories, and based on user actions (clicks or dislikes), the system learns to predict the likelihood of future user interactions. This way, the content presented to users becomes customized based on their preferences."
  },
  {
    "objectID": "pages/RL/Week01_1.html#ad-selection-in-computational-advertising",
    "href": "pages/RL/Week01_1.html#ad-selection-in-computational-advertising",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Ad Selection in Computational Advertising",
    "text": "Ad Selection in Computational Advertising\n\nComputational Advertising\nComputational advertising is a field that involves the automated selection of relevant advertisements for users, a process crucial for revenue generation, especially for platforms like Google.\n\n\nReinforcement Learning for Ad Selection\nIn ad selection, RL plays a key role in determining the probability of a user clicking on a specific ad. User interactions, such as clicks or dislikes, serve as positive or negative feedback. This information refines the ad selection process, making it more effective in presenting ads that are likely to engage users."
  },
  {
    "objectID": "pages/RL/Week01_1.html#reinforcement-learning-in-recommendation-engines",
    "href": "pages/RL/Week01_1.html#reinforcement-learning-in-recommendation-engines",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Reinforcement Learning in Recommendation Engines",
    "text": "Reinforcement Learning in Recommendation Engines\n\nTraditional Recommendation Systems\nRecommendation engines traditionally employ collaborative filtering, using methods like “customers who bought this item also bought.” However, this approach has limitations in handling a vast pool of potential recommendations.\n\n\nTrial-and-Error with Reinforcement Learning\nReinforcement learning complements traditional methods by introducing a trial-and-error layer. Users’ feedback becomes a crucial component in refining recommendations over time. This allows the system to adapt to changing user preferences dynamically."
  },
  {
    "objectID": "pages/RL/Week01_1.html#content-and-comment-recommendations",
    "href": "pages/RL/Week01_1.html#content-and-comment-recommendations",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Content and Comment Recommendations",
    "text": "Content and Comment Recommendations\n\nContent Recommendation Systems\nRL is applied in content recommendation systems, leveraging user history and feedback to personalize suggestions. This goes beyond conventional methods and incorporates a trial-and-error approach for more accurate predictions.\n\n\nComment Recommendations\nIn websites where comments are displayed, RL is utilized to reorder and present comments based on user feedback. Thumbs up or thumbs down serve as positive and negative rewards, influencing the order in which comments are displayed."
  },
  {
    "objectID": "pages/RL/Week01_1.html#beyond-human-knowledge",
    "href": "pages/RL/Week01_1.html#beyond-human-knowledge",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Beyond Human Knowledge",
    "text": "Beyond Human Knowledge\n\nReinforcement Learning Autonomy\nReinforcement learning demonstrates the capability to operate autonomously without explicit human guidance. Early successes, such as Jerry Tesauro’s TD Gammon in backgammon, exemplify RL’s ability to learn from self-play.\n\n\nBreakthrough in 2014\nA pivotal moment occurred in 2014 when DeepMind trained RL agents to play Atari games. This breakthrough showcased RL’s capacity to learn complex tasks with minimal input, opening the door to widespread replication of success stories."
  },
  {
    "objectID": "pages/RL/Week01_1.html#success-in-strategic-games",
    "href": "pages/RL/Week01_1.html#success-in-strategic-games",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Success in Strategic Games",
    "text": "Success in Strategic Games\n\nAlphaGo’s Triumph\nDeepMind’s AlphaGo achieved unprecedented success by defeating the world champion in the ancient game of Go. RL’s application extended to mastering various strategy games, surpassing human-level performance in competitive scenarios.\n\n\nAlphaZero’s Versatility\nAlphaZero demonstrated versatility by playing and excelling in multiple games, including chess and shogi. Its success showcased RL’s ability to adapt and learn across diverse gaming environments without relying on human data."
  },
  {
    "objectID": "pages/RL/Week01_1.html#applications-beyond-gaming",
    "href": "pages/RL/Week01_1.html#applications-beyond-gaming",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Applications Beyond Gaming",
    "text": "Applications Beyond Gaming\n\nRL in Real-world Challenges\nReinforcement learning’s success in gaming applications paved the way for its adoption in solving real-world challenges. RL is utilized in optimizing data center cooling, controlling chemical plants, and even improving airport conveyor belt efficiency.\n\n\nImpact on Combinatorial Optimization\nRL has played a crucial role in solving combinatorial optimization problems, including scheduling, routing, and call admission control. Its application extends to diverse domains, showcasing its versatility in addressing complex decision-making challenges."
  },
  {
    "objectID": "pages/RL/Week01_1.html#control-systems-and-optimization",
    "href": "pages/RL/Week01_1.html#control-systems-and-optimization",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Control Systems and Optimization",
    "text": "Control Systems and Optimization\n\nRL in Control Systems\nReinforcement learning finds applications in controlling various systems, from chemical plants to robot navigation. Its adaptability and ability to optimize processes make it a valuable tool in real-world applications.\n\n\nAirport Conveyor Belt Control\nAn interesting application involves using RL to control conveyor belts in airports. RL-based controllers aim to ensure timely package delivery, showcasing the technology’s potential in optimizing large-scale logistical systems."
  },
  {
    "objectID": "pages/RL/Week01_1.html#connections-with-neuroscience-and-psychology",
    "href": "pages/RL/Week01_1.html#connections-with-neuroscience-and-psychology",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Connections with Neuroscience and Psychology",
    "text": "Connections with Neuroscience and Psychology\n\nRoots in Behavioral Psychology\nReinforcement learning has roots in behavioral psychology, emphasizing learning through trial and error. This connection provides insights into human decision-making processes.\n\n\nInteraction with Neuroscience\nRL’s impact extends to neuroscience, with some suggesting that RL could be a primary mechanism of learning in certain brain regions. This reciprocal interaction enriches both the computational neuroscience and RL fields."
  },
  {
    "objectID": "pages/RL/Week01_1.html#real-world-applications-of-rl",
    "href": "pages/RL/Week01_1.html#real-world-applications-of-rl",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Real-world Applications of RL",
    "text": "Real-world Applications of RL\n\nIntelligent Tutoring Systems\nReinforcement learning contributes to the development of intelligent tutoring systems, providing personalized and adaptive learning experiences for students based on their interactions.\n\n\nRL in Dialogue Systems and Chatbots\nDialogue systems and chatbots benefit from RL, enabling more natural and context-aware interactions. RL’s trial-and-error learning enhances these systems’ ability to understand and respond to user inputs effectively."
  },
  {
    "objectID": "pages/RL/Week01_1.html#points-to-remember",
    "href": "pages/RL/Week01_1.html#points-to-remember",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nFundamentals of RL\n\nRL is distinguished by a trial-and-error approach, relying on received rewards and punishments for learning.\nUnlike supervised learning, RL lacks upfront instructions, allowing the system to adapt through interaction with its environment. \n\nContrast with Supervised Learning\n\nSupervised learning relies on explicit instructions provided beforehand, while RL refrains from pre-established instructions.\n\nLearning Paradigm in RL\n\nRL aligns with a trial-and-error learning paradigm, resembling how humans acquire complex skills through attempts, feedback, and refinement.\n\nApplications of RL\n\nRL excels in domains such as autonomous systems, humanoid control, complex environments, uncertain environments, cognitively motivated learning, and customization/personalization tasks.\nSuccess stories like ChatGPT showcase RL’s efficacy in addressing intricate problems.\n\nRL in Personalization and Customization\n\nRL is applied in platforms like Yahoo News for content customization, utilizing user interactions as feedback.\nComputational advertising and recommendation engines leverage RL for ad selection and dynamic adaptation to changing user preferences.\n\nAdvancements in RL\n\nRL demonstrates autonomy in learning, as seen in early successes like TD Gammon and breakthroughs in gaming applications.\nSuccess in strategic games, such as AlphaGo and AlphaZero, highlights RL’s adaptability and versatility.\n\nRL’s Impact on Problem Solving\n\nRL contributes to solving real-world challenges in areas like data center cooling, chemical plant control, and combinatorial optimization.\nApplications in control systems, airport conveyor belt control, and connections with neuroscience showcase RL’s broad impact.\n\nReal-world Applications of RL\n\nRL is instrumental in intelligent tutoring systems, providing personalized learning experiences.\nDialogue systems and chatbots benefit from RL, enhancing natural and context-aware interactions."
  },
  {
    "objectID": "pages/SE/Week01.html",
    "href": "pages/SE/Week01.html",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "In the contemporary landscape of online platforms, exemplified by industry leader Amazon, the intricate systems governing processes such as ordering and delivery are constructed incrementally. In contrast to a monolithic approach, these systems evolve feature by feature. This incremental strategy arises from the inherent uncertainty surrounding the complete set of required features at the project’s inception.\n\n\n\nWithin the domain of software engineering, the concept of components assumes a pivotal role. These components serve as manageable units that facilitate collaborative efforts by different teams, each working on distinct facets of the system. These individual aspects are later integrated into a coherent whole. Importantly, effective collaboration is achieved by understanding a component’s interface, which shields the intricacies of its internal workings.\n\n\nPurpose: The Inventory Management System is designed to intelligently track and manage inventory.\nDefinition: This involves measuring quantity, location, pricing, and the composition of products available on platforms like Amazon.\nCustomization: Amazon’s homepage dynamically updates based on factors such as purchasing trends, seasonal variations, customer demand, and logistical and analytical considerations.\n\n\n\nPurpose: The Payment Gateway facilitates electronic payments, ensuring a seamless experience for buyers and sellers.\nDefinition: Serving as a service authorizing electronic payments (e.g., online banking, debit cards), the Payment Gateway acts as an intermediary between the bank and the merchant’s platform.\nProcess: The gateway validates payment details, confirming their legitimacy with the bank before transferring the specified amount from the user’s account to the platform.\n\n\n\n\nLarge-scale systems, exemplified by the infrastructure of industry leaders like Amazon, do not materialize in a single endeavor. Instead, they are deconstructed into components or modules that can be independently developed before harmonious integration. This integration phase involves establishing communication pathways between the modules."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction",
    "href": "pages/SE/Week01.html#introduction",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "In the contemporary landscape of online platforms, exemplified by industry leader Amazon, the intricate systems governing processes such as ordering and delivery are constructed incrementally. In contrast to a monolithic approach, these systems evolve feature by feature. This incremental strategy arises from the inherent uncertainty surrounding the complete set of required features at the project’s inception."
  },
  {
    "objectID": "pages/SE/Week01.html#components-in-software-systems",
    "href": "pages/SE/Week01.html#components-in-software-systems",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "Within the domain of software engineering, the concept of components assumes a pivotal role. These components serve as manageable units that facilitate collaborative efforts by different teams, each working on distinct facets of the system. These individual aspects are later integrated into a coherent whole. Importantly, effective collaboration is achieved by understanding a component’s interface, which shields the intricacies of its internal workings.\n\n\nPurpose: The Inventory Management System is designed to intelligently track and manage inventory.\nDefinition: This involves measuring quantity, location, pricing, and the composition of products available on platforms like Amazon.\nCustomization: Amazon’s homepage dynamically updates based on factors such as purchasing trends, seasonal variations, customer demand, and logistical and analytical considerations.\n\n\n\nPurpose: The Payment Gateway facilitates electronic payments, ensuring a seamless experience for buyers and sellers.\nDefinition: Serving as a service authorizing electronic payments (e.g., online banking, debit cards), the Payment Gateway acts as an intermediary between the bank and the merchant’s platform.\nProcess: The gateway validates payment details, confirming their legitimacy with the bank before transferring the specified amount from the user’s account to the platform."
  },
  {
    "objectID": "pages/SE/Week01.html#incremental-system-development",
    "href": "pages/SE/Week01.html#incremental-system-development",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "Large-scale systems, exemplified by the infrastructure of industry leaders like Amazon, do not materialize in a single endeavor. Instead, they are deconstructed into components or modules that can be independently developed before harmonious integration. This integration phase involves establishing communication pathways between the modules."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-1",
    "href": "pages/SE/Week01.html#introduction-1",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software engineering, a comprehensive understanding of a software system’s components and their interactions is fundamental. This lecture explores the intricacies of the software development process, using the example of Amazon Pay, a mobile wallet, to elucidate key concepts."
  },
  {
    "objectID": "pages/SE/Week01.html#amazon-pay-overview",
    "href": "pages/SE/Week01.html#amazon-pay-overview",
    "title": "Thinking of Software in terms of Components",
    "section": "Amazon Pay Overview",
    "text": "Amazon Pay Overview\n\nFeatures\nAmazon Pay, a mobile wallet, facilitates digital cash transactions by offering a spectrum of features. Users can link credit/debit cards, bank accounts, and engage in various transactions, including recharges, bill payments, travel bookings, insurance, and redemption of rewards and gift vouchers. Two notable functionalities include the ability to add money and an auto-reload feature."
  },
  {
    "objectID": "pages/SE/Week01.html#software-development-process",
    "href": "pages/SE/Week01.html#software-development-process",
    "title": "Thinking of Software in terms of Components",
    "section": "Software Development Process",
    "text": "Software Development Process\n\n1. Identifying the Problem\nBefore delving into programming languages, the foremost step in the software development process involves a profound understanding of the problem at hand. This recognition sets the stage for subsequent development efforts.\n\n\n2. Studying Existing Components\nTo gain insights into the intricacies of system components, a meticulous examination of existing elements, such as inventory management and payment gateways, is crucial. Additionally, studying analogous systems, like Paytm and PhonePe, aids in identifying essential features.\n\n\n3. Defining System Requirements\nThe foundation of the development process lies in explicitly defining system requirements. These requirements, derived from a thorough analysis of existing systems, serve as the guiding principles throughout the software development lifecycle."
  },
  {
    "objectID": "pages/SE/Week01.html#clients-in-software-systems",
    "href": "pages/SE/Week01.html#clients-in-software-systems",
    "title": "Thinking of Software in terms of Components",
    "section": "Clients in Software Systems",
    "text": "Clients in Software Systems\n\nDefinition of Client\nClients, referring to users of the software system, can be categorized as either external or internal entities. External clients are end-users or buyers, while internal clients encompass components within the system itself.\n\n\nTypes of Clients\n\nExternal Clients\nFor instance, in mobile banking software, external clients are bank customers utilizing features like account balance checks and money transfers.\n\n\nInternal Clients\nInternal clients may include teams within a company, such as an internal products team constructing an employee resources portal by collaborating with various departments.\n\n\n\nSoftware-to-Software Clients\nIn certain scenarios, software components, like payment gateways (e.g., Razer Pay), act as clients, facilitating communication between an e-commerce website and customers’ banks."
  },
  {
    "objectID": "pages/SE/Week01.html#importance-of-gathering-requirements",
    "href": "pages/SE/Week01.html#importance-of-gathering-requirements",
    "title": "Thinking of Software in terms of Components",
    "section": "Importance of Gathering Requirements",
    "text": "Importance of Gathering Requirements\n\nSignificance of the First Step\nGathering requirements stands as the initial and crucial step in the software development process. This process ensures a holistic understanding of users or clients, and adherence to requirements at every stage is imperative for meeting end-user needs."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-2",
    "href": "pages/SE/Week01.html#introduction-2",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we delved into the initial steps of the software development cycle, primarily focusing on the gathering of requirements. However, a common misconception arises at this juncture—many individuals are inclined to proceed directly to coding. This session aims to dispel this notion through a practical example."
  },
  {
    "objectID": "pages/SE/Week01.html#example-implementation-of-amazon-pay-feature",
    "href": "pages/SE/Week01.html#example-implementation-of-amazon-pay-feature",
    "title": "Thinking of Software in terms of Components",
    "section": "Example: Implementation of Amazon Pay Feature",
    "text": "Example: Implementation of Amazon Pay Feature\nConsider a scenario where a small team is eager to implement the Amazon Pay feature based on gathered requirements. The tendency to immediately engage in coding poses several challenges that warrant careful consideration.\n\nPitfalls of Skipping Design Phase\n\nDivergent Implementation Ideas:\n\nDevelopers may harbor disparate concepts regarding the feature’s implementation.\nChanges made by one developer could inadvertently impact others.\n\nInterconnected Components Challenge:\n\nComponents developed by different individuals may intertwine, resulting in complications.\nLack of a holistic view impedes the seamless integration of features."
  },
  {
    "objectID": "pages/SE/Week01.html#the-significance-of-the-design-phase",
    "href": "pages/SE/Week01.html#the-significance-of-the-design-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "The Significance of the Design Phase",
    "text": "The Significance of the Design Phase\nThe design phase serves as a crucial precursor to the coding phase, offering distinct advantages in the software development process.\n\nCreating a System Overview\nThe primary goal is to construct a comprehensive overview of the entire system. This macroscopic perspective aids in organizing the subsequent coding phase efficiently.\n\n\nBenefits of a Well-Executed Design Phase\n\nConsistency:\n\nMitigates conflicts stemming from diverse developer perspectives.\nEnsures a uniform comprehension of the codebase.\n\nEfficiency Enhancement:\n\nPrecludes unnecessary alterations and errors during the implementation phase.\nFacilitates punctual product delivery.\n\nFuture-Proofing:\n\nStreamlines the addition of new features in subsequent phases.\nEnables seamless integration into the existing system."
  },
  {
    "objectID": "pages/SE/Week01.html#the-development-phase",
    "href": "pages/SE/Week01.html#the-development-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "The Development Phase",
    "text": "The Development Phase\nFollowing the design phase, the development phase entails collaborative coding efforts involving multiple developers. This phase often unfolds in a distributed manner, with team members situated in diverse locations and time zones. Collaboration tools such as GitHub play a pivotal role in this collective coding endeavor."
  },
  {
    "objectID": "pages/SE/Week01.html#imperative-role-of-documentation",
    "href": "pages/SE/Week01.html#imperative-role-of-documentation",
    "title": "Thinking of Software in terms of Components",
    "section": "Imperative Role of Documentation",
    "text": "Imperative Role of Documentation\nGiven the dispersed nature of development efforts, comprehensive documentation becomes imperative. This documentation, encompassing precise interface definitions, ensures a consistent understanding of code functionality among developers.\n\nInterface Definitions\nInterface definitions are foundational descriptions outlining the actions that functions can perform. Distinctively, the focus is on delineating actions rather than delving into intricate implementation details. Such definitions stipulate the types of requests accepted and the corresponding format of responses. The flexibility for code modifications exists, provided the interface remains consistent."
  },
  {
    "objectID": "pages/SE/Week01.html#collaborative-dynamics-in-development",
    "href": "pages/SE/Week01.html#collaborative-dynamics-in-development",
    "title": "Thinking of Software in terms of Components",
    "section": "Collaborative Dynamics in Development",
    "text": "Collaborative Dynamics in Development\nCollaboration during the development phase entails the coordinated efforts of multiple developers, often located in different time zones. Effective communication, facilitated through clear and concise interface definitions, is paramount to achieving seamless integration of components."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-3",
    "href": "pages/SE/Week01.html#introduction-3",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn the preceding video, we explored the design and development phases crucial to software development. However, two additional pivotal phases demand our attention: Testing and Maintenance."
  },
  {
    "objectID": "pages/SE/Week01.html#importance-of-testing",
    "href": "pages/SE/Week01.html#importance-of-testing",
    "title": "Thinking of Software in terms of Components",
    "section": "Importance of Testing",
    "text": "Importance of Testing\nTesting serves as a critical measure to ensure the alignment of software behavior with specified requirements. The existence of bugs and defects, if left unaddressed, may lead\nto substantial financial losses. For instance, a noteworthy study indicates that in 2002, software bugs resulted in a $60 billion loss in the U.S. economy, a figure that surged to $1.1 trillion in 2016. The failure to rectify such bugs can potentially precipitate severe catastrophes."
  },
  {
    "objectID": "pages/SE/Week01.html#testing-granularities",
    "href": "pages/SE/Week01.html#testing-granularities",
    "title": "Thinking of Software in terms of Components",
    "section": "Testing Granularities",
    "text": "Testing Granularities\n\n1. Unit Testing\nUnit testing directs its focus toward a singular component, often a class or function, examined in complete isolation.\n\n\n2. Integration Testing\nIntegration testing scrutinizes the interaction and collaboration of different parts within the application, ensuring seamless functionality as a unified whole.\n\n\n3. Acceptance Testing\nAcceptance testing verifies the fulfillment of user requirements. This testing stage bifurcates into:\n\nAlpha Testing\nInternal employees conduct alpha testing within a controlled environment, such as a lab or staging area.\n\n\nBeta Testing\nActual users undertake beta testing in real-world scenarios, providing valuable insights into the software’s performance."
  },
  {
    "objectID": "pages/SE/Week01.html#maintenance-phase",
    "href": "pages/SE/Week01.html#maintenance-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "Maintenance Phase",
    "text": "Maintenance Phase\n\nPurpose\n\nUser Monitoring:\n\nContinuous observation of user activities and software usage.\n\nCode Changes:\n\nImplementation of code modifications for upgrades, including patch releases.\n\nFeature Addition:\n\nIntroduction of new features to enhance software functionality."
  },
  {
    "objectID": "pages/SE/Week01.html#example---amazon-pay",
    "href": "pages/SE/Week01.html#example---amazon-pay",
    "title": "Thinking of Software in terms of Components",
    "section": "Example - Amazon Pay",
    "text": "Example - Amazon Pay\n\nPost-Release Issues\nAfter the release of a feature like Amazon Pay, potential difficulties or errors that users may encounter must be anticipated. Examples include missed conditions, failures, and UI issues specific to certain browsers.\n\n\nMaintenance Process\nThe maintenance phase involves a systematic approach where the development team identifies issues and engages in a continuous process of rectification to ensure optimal software performance."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-4",
    "href": "pages/SE/Week01.html#introduction-4",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nSoftware engineering is a discipline that advocates a systematic approach to the development of software through a well-defined and structured set of activities. These activities are commonly denoted as the software lifecycle model, software development lifecycle (SDLC), or the software development process model."
  },
  {
    "objectID": "pages/SE/Week01.html#waterfall-model",
    "href": "pages/SE/Week01.html#waterfall-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Waterfall Model",
    "text": "Waterfall Model\n\nSequential Phases\nThe waterfall model entails a linear progression of phases, with each phase following the completion of the previous one. These phases encompass gathering requirements, design, coding, testing, and maintenance. The approach is also recognized as the plan and document perspective.\n\n\nDrawbacks\nDespite its structured nature, the waterfall model has notable drawbacks:\n\nIncreased Cost and Time: Modifications later in the process lead to elevated costs and time consumption.\nClient Understanding: Clients may not fully comprehend their needs initially.\nDesign Challenges: Developers may face challenges in determining the most feasible design.\nLengthy Iterations: Each phase or iteration can span from 6 to 18 months."
  },
  {
    "objectID": "pages/SE/Week01.html#prototype-model",
    "href": "pages/SE/Week01.html#prototype-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Prototype Model",
    "text": "Prototype Model\n\nConcept and Execution\nTo address the drawbacks of the waterfall model, the prototype model advocates the creation of a working prototype of the system before the actual software development begins. The prototype, possessing limited functionality, is subsequently discarded or replaced with the final product.\n\n\nAdvantages and Disadvantages\nAdvantages: - Enhanced understanding for both clients and developers regarding project requirements.\nDisadvantages: - Augmented development costs. - Inability to anticipate risks and bugs emerging later in the development cycle."
  },
  {
    "objectID": "pages/SE/Week01.html#spiral-model",
    "href": "pages/SE/Week01.html#spiral-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Spiral Model",
    "text": "Spiral Model\n\nIntegration of Approaches\nThe spiral model amalgamates features from both the waterfall and prototype models. It unfolds in four distinct phases: determining objectives, evaluating alternatives, developing and testing, and planning for the subsequent phase. Each iteration involves a refinement of the prototype.\n\n\nIterative Process\nThis model fosters an iterative process, where the refinement of the prototype occurs at each iteration. Unlike the waterfall model, requirement documents are progressively developed across iterations. Client involvement at the end of each iteration mitigates misunderstandings.\n\n\nDrawback\nDespite its advantages, the spiral model still encounters a drawback: each iteration may extend from 6 to 24 months."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-5",
    "href": "pages/SE/Week01.html#introduction-5",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn our previous lectures, we navigated through the intricacies of the software development lifecycle, concentrating particularly on established models like the waterfall model. While these models, falling under the plan and document process category, brought structure to software development, they faced considerable challenges in meeting deadlines and adhering to specified budgets. Surprisingly, studies conducted from 1995 to 2013 indicated that around 80 to 90 percent of software projects encountered issues such as overdue timelines, exceeding budgetary limits, or even abandonment. This realization triggered a significant shift in software development methodologies."
  },
  {
    "objectID": "pages/SE/Week01.html#emergence-of-the-agile-manifesto",
    "href": "pages/SE/Week01.html#emergence-of-the-agile-manifesto",
    "title": "Thinking of Software in terms of Components",
    "section": "Emergence of the Agile Manifesto",
    "text": "Emergence of the Agile Manifesto\nApproximately two decades ago, in February 2001, a coalition of software developers convened to devise a more flexible software development lifecycle. This effort culminated in the creation of the Agile Manifesto, a document founded on four key principles. The manifesto aimed to address the shortcomings of traditional approaches, laying the groundwork for a more lightweight and adaptive software development process.\n\nAgile Manifesto Principles\n\nIndividuals and Interactions over Processes and Tools:\n\nEmphasizes the importance of interpersonal dynamics within the development team and effective communication with clients.\n\nWorking Software over Comprehensive Documentation:\n\nPrioritizes the delivery of functional software in increments over exhaustive documentation.\n\nCustomer Collaboration over Contract Negotiation:\n\nAdvocates for active collaboration with customers to understand their needs rather than fixating on contractual minutiae.\n\nResponding to Change over Following a Plan:\n\nEncourages adaptability to change during the development process, emphasizing responsiveness."
  },
  {
    "objectID": "pages/SE/Week01.html#agile-development-approach",
    "href": "pages/SE/Week01.html#agile-development-approach",
    "title": "Thinking of Software in terms of Components",
    "section": "Agile Development Approach",
    "text": "Agile Development Approach\nThe Agile development approach is characterized by its iterative and incremental model. Teams adopting Agile construct the software product in small, manageable increments through multiple iterations. This process involves developing prototypes for key features, promptly releasing them for feedback. Noteworthy Agile methodologies include Extreme Programming (XP), Scrum, and Kanban.\n\nExtreme Programming (XP)\nExtreme Programming incorporates key practices such as behavior-driven design, test-driven development, and pair programming. These practices contribute to a development environment centered around quick iterations and continuous feedback.\n\n\nScrum\nScrum, another Agile methodology, divides the product development into iterations known as sprints, typically lasting one to two weeks. This approach facilitates breaking down complex projects into more manageable and actionable components.\n\n\nKanban\nIn Kanban, the software is segmented into small work items, visually represented on a Kanban board. This visual aid enables team members to monitor the status of each work item in real-time."
  },
  {
    "objectID": "pages/SE/Week01.html#choosing-the-development-perspective",
    "href": "pages/SE/Week01.html#choosing-the-development-perspective",
    "title": "Thinking of Software in terms of Components",
    "section": "Choosing the Development Perspective",
    "text": "Choosing the Development Perspective\nSelecting between the plan and document perspective and the Agile perspective depends on various factors. Key considerations include the fixity of requirements, client availability, system characteristics, team distribution, team familiarity with documentation models, and the presence of regulatory constraints.\n\nFactors Influencing Choice\n\nRequirements/Specifications Fixity:\n\nAre requirements/specifications mandated to be fixed upfront?\n\nClient Availability:\n\nIs the client or customer consistently available for collaboration?\n\nSystem Characteristics:\n\nDoes the system possess characteristics like size and complexity that warrant extensive planning and documentation?\n\nTeam Distribution:\n\nIs the software team geographically dispersed?\n\nTeam Familiarity:\n\nIs the team already acquainted with the plan and document model?\n\nRegulatory Constraints:\n\nIs the system subject to numerous regulatory requirements?"
  },
  {
    "objectID": "pages/SE/Week01.html#points-to-remember",
    "href": "pages/SE/Week01.html#points-to-remember",
    "title": "Thinking of Software in terms of Components",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nIncremental System Development: Large-scale systems, like those employed by industry leaders such as Amazon, evolve incrementally, emphasizing the construction of components or modules before their integration.\nRequirement Specification: The software development process begins with a deep understanding of the problem, studying existing components, and defining system requirements derived from thorough analyses.\nSoftware Design and Development: The design phase is crucial, providing a macroscopic perspective of the entire system and offering advantages such as consistency, efficiency enhancement, and future-proofing. Effective documentation and collaborative coding are imperative during the development phase.\nTesting and Maintenance: Testing is critical to ensure software behavior aligns with requirements, and maintenance involves continuous monitoring, code changes, and feature additions to enhance software functionality.\nWaterfall Model: A structured, sequential model with phases like gathering requirements, design, coding, testing, and maintenance. However, it has drawbacks, including increased cost and time.\nPrototype Model: Advocates creating a working prototype before actual development to enhance understanding but may incur augmented development costs.\nSpiral Model: Integrates features from both waterfall and prototype models, fostering an iterative process, but each iteration may extend over a considerable duration.\nAgile Development: A response to challenges faced by traditional approaches, characterized by an iterative and incremental model. Agile methodologies include Extreme Programming (XP), Scrum, and Kanban.\nAgile Manifesto Principles:\n\nIndividuals and interactions over processes and tools.\nWorking software over comprehensive documentation.\nCustomer collaboration over contract negotiation.\nResponding to change over following a plan.\n\nChoosing the Development Perspective: Factors influencing the choice between plan and document perspective and Agile perspective include requirements fixity, client availability, system characteristics, team distribution, team familiarity, and regulatory constraints."
  },
  {
    "objectID": "pages/AI/Week01.html",
    "href": "pages/AI/Week01.html",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning.\n\n\n\n\nData Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward.\n\n\n\n\n\n\nThe foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers.\n\n\n\n\n\n\nThe training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology.\n\n\n\n\n\n\n\nThe dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification.\n\n\n\n\n\n\n\nNeural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains.\n\n\n\n\n\n\nThe triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously."
  },
  {
    "objectID": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "href": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning."
  },
  {
    "objectID": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "href": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Data Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward."
  },
  {
    "objectID": "pages/AI/Week01.html#evolution-of-neural-networks",
    "href": "pages/AI/Week01.html#evolution-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers."
  },
  {
    "objectID": "pages/AI/Week01.html#training-process-of-neural-networks",
    "href": "pages/AI/Week01.html#training-process-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology."
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "href": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification."
  },
  {
    "objectID": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "href": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Neural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains."
  },
  {
    "objectID": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "href": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously."
  },
  {
    "objectID": "pages/AI/Week01.html#human-cognition-and-ai",
    "href": "pages/AI/Week01.html#human-cognition-and-ai",
    "title": "A Decade of Machine Learning",
    "section": "Human Cognition and AI",
    "text": "Human Cognition and AI\n\nCognitive Landscape\nHuman intelligence engages in a myriad of activities such as logic, representation, planning, reasoning, and search. The crux of these cognitive endeavors lies in symbolic reasoning, a substantial facet of the human cognitive load.\n\n\nSymbolic Reasoning\nSymbolic reasoning, integral to human cognition, involves the management of symbolic knowledge representation and intricate problem-solving processes."
  },
  {
    "objectID": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "href": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "Distinguishing AI from Machine Learning",
    "text": "Distinguishing AI from Machine Learning\n\nAI Emphasis\nWithin the domain of Artificial Intelligence (AI), the spotlight is on symbolic knowledge representation and advanced problem-solving methodologies.\n\n\nMachine Learning Focus\nIn contrast, Machine Learning (ML) gravitates towards interpreting data, with applications ranging from recommender systems to predictive analytics and classification."
  },
  {
    "objectID": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "href": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Knowledge Representation in AI",
    "text": "Knowledge Representation in AI\n\nDeclarative Knowledge\nAligning with the cognitive domain of humans, explicit symbolic representation, known as declarative knowledge, assumes a pivotal role. It encompasses the representation of the world and engages in reasoned deductions.\n\n\nInferences in AI\nAI agents showcase a spectrum of inferences, ranging from deductive reasoning based on logic to plausible or probabilistic inferences that incorporate an element of likelihood."
  },
  {
    "objectID": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "href": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Symbolic Representation in AI",
    "text": "Symbolic Representation in AI\n\nDefining Symbols\nSymbols, representing abstract concepts, manifest in diverse forms. For example, the number (7) can be expressed in various ways, illustrating the distinction between the conceptualization of numbers and their symbolic representations.\n\n\nMeaning of Symbols\nThe significance of symbols is socially agreed upon, forming the foundation for semiotic systems. Whether in road signs or linguistic characters, symbols encapsulate shared meanings."
  },
  {
    "objectID": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "href": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "title": "A Decade of Machine Learning",
    "section": "Semiotics and Biosemiotics",
    "text": "Semiotics and Biosemiotics\n\nSemiotics\nSemiotics, the scientific study of symbols in spoken and written languages, lays the groundwork for comprehending human communication and representation.\n\n\nBiosemiotics\nDelving deeper, Biosemiotics explores the emergence of complex behavior when simple systems engage in symbolic communication. This is exemplified by phenomena such as ant trails utilizing pheromones."
  },
  {
    "objectID": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "href": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Reasoning Mechanisms in AI",
    "text": "Reasoning Mechanisms in AI\n\nFormal Reasoning\nIn the context of AI, reasoning involves the systematic manipulation of symbols in a meaningful manner. This encompasses algorithms for fundamental operations like addition and multiplication, extending to more intricate processes like the Fourier transform.\n\n\nConceptualizing Algorithms\nUnderstanding AI algorithms necessitates a conceptual grasp of symbolic manipulations. For instance, multiplication algorithms entail conceptualizing the multiplication of unit digits and the subsequent shifting of results."
  },
  {
    "objectID": "pages/AI/Week01.html#automation-vs.-ai",
    "href": "pages/AI/Week01.html#automation-vs.-ai",
    "title": "A Decade of Machine Learning",
    "section": "Automation vs. AI",
    "text": "Automation vs. AI\n\nNavigating Overlap\nWhile Automation and AI share common ground, not all automated systems integrate AI. For instance, implementations such as train reservation systems or basic online shopping may lack substantial AI components."
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "href": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Machine Learning’s Role in AI",
    "text": "Machine Learning’s Role in AI\n\nML as a Component\nMachine Learning constitutes one facet of the multifaceted field of AI. Examples such as self-driving cars leverage ML for tasks including pattern recognition, speech processing, and object classification."
  },
  {
    "objectID": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "href": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Clarifying Data Science in AI",
    "text": "Clarifying Data Science in AI\n\nData’s Multifaceted Role\nData science, encompassing elements of statistics, AI, and machine learning, plays a crucial role in the broader field of AI. It serves as a foundational component but does not encapsulate the entirety of AI."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "href": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to AI and Definitions:",
    "text": "Introduction to AI and Definitions:\n\n\nDefinitions of AI:\n\nHerbert Simon: Programs are considered intelligent if they display behaviors regarded as intelligent in humans.\nBar and Feigenbaum: AI seeks to comprehend the systematic behavior of information processing systems, analogous to physicists and biologists in their respective domains.\nElaine Rich: AI involves solving exponentially hard problems in polynomial time, leveraging domain-specific knowledge.\nJohn Hoagland: AI’s goal is to create machines with minds of their own, treating thinking and computing as fundamentally interconnected."
  },
  {
    "objectID": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "href": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Fundamental Questions in AI:",
    "text": "Fundamental Questions in AI:\n\nQuestions about Intelligence:\nVarious perspectives exist on what constitutes intelligence, encompassing language use, reasoning, and learning. Ongoing debates revolve around whether machines can genuinely exhibit thinking, with insights from philosophers like Roger Penrose exploring quantum mechanics in the human brain."
  },
  {
    "objectID": "pages/AI/Week01.html#turing-test-and-challenges",
    "href": "pages/AI/Week01.html#turing-test-and-challenges",
    "title": "A Decade of Machine Learning",
    "section": "Turing Test and Challenges:",
    "text": "Turing Test and Challenges:\n\nAlan Turing’s Turing Test:\nThe evaluation of machine intelligence through its ability to convincingly engage in natural language conversations with a human judge forms the essence of the Turing Test. Associated challenges include situations where chatbots may impress but lack genuine intelligence. The Löbner Prize Competition attempts a similar test.\n\n\nHector Levesque’s Alternative: Vinograd Schemas\nAn alternate test proposed by Hector Levesque challenges a machine’s understanding through multiple-choice questions that require subject matter knowledge."
  },
  {
    "objectID": "pages/AI/Week01.html#vinograd-schemas-examples",
    "href": "pages/AI/Week01.html#vinograd-schemas-examples",
    "title": "A Decade of Machine Learning",
    "section": "Vinograd Schemas Examples:",
    "text": "Vinograd Schemas Examples:\n\nExample 1:\n\nOriginal Sentence: “The city council refused the demonstrators a permit because they feared violence.”\nAlternate Sentence: “The city council refused the demonstrators a permit because they advocated violence.”\nQuestion: What does “they” refer to? Options: Council, Demonstrators.\n\nExample 2:\n\nOriginal Sentence: “John took the water bottle out of the backpack so that it would be lighter.”\nAlternate Sentence: “John took the water bottle out of the backpack so that it would be handy.”\nQuestion: What does “it” refer to? Options: Backpack, Water Bottle.\n\nExample 3:\n\nOriginal Sentence: “The trophy would not fit into the brown suitcase because it was too small.”\nAlternate Sentence: “The trophy would not fit into the brown suitcase because it was too big.”\nQuestion: What does “it” refer to? Options: Trophy, Brown Suitcase.\n\nExample 4:\n\nOriginal Sentence: “The lawyer asked the witness a question but he was reluctant to repeat it.”\nAlternate Sentence: “The lawyer asked the witness a question but he was reluctant to answer it.”\nQuestion: Who was reluctant? Options: Lawyer, Witness."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "href": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Intelligence and AI Goals",
    "text": "Introduction to Intelligence and AI Goals\nIn the exploration of artificial intelligence (AI), the concept of intelligence takes center stage. AI endeavors to construct intelligent agents capable of complex problem-solving. A historical glimpse into European thinkers sheds light on the roots of AI ideologies.\n\nGalileo Galilei (1623)\nIn his 1623 publication, Galileo Galilei delves into the subjective nature of sensory experiences. He contends that taste, odors, and colors are subjective perceptions residing in consciousness. Galileo challenges the idea that these qualities exist inherently in external objects. Moreover, he posits that philosophy is expressed through the language of mathematics.\n\n\nThomas Hobbs\nThomas Hobbs, often referred to as the grandfather of AI, introduces the notion that thinking involves the manipulation of symbols. He associates reasoning with computation, not in the contemporary sense of computers, but as a form of mathematical operations. Hobbs views computation as the sum of many things added together or the determination of the remainder when one thing is subtracted from another.\n\n\nRené Descartes\nBuilding on Galileo’s ideas, Descartes extends the concept that animals are intricate machines, reserving acknowledgment of a mind solely for humans. He aligns thought with symbols and introduces the mind-body dualism, raising questions about the interaction between the mental world and the physical body."
  },
  {
    "objectID": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "href": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "title": "A Decade of Machine Learning",
    "section": "Early Concepts of Thinking Machines",
    "text": "Early Concepts of Thinking Machines\nThe early stages of envisioning thinking machines were influenced by the use of punch cards in the textile industry’s Jacquard looms.\n\nJacquard Looms\nPunch cards were employed to control patterns in textile looms. This concept of punched cards was later adapted for programming early computers, emphasizing a transition from controlling patterns to controlling programs.\n\n\nCharles Babbage and Augusta Ada Byron\nCharles Babbage, a mathematician and inventor, conceptualized the Difference Engine and the Analytic Engine. Augusta Ada Byron, daughter of Lord Byron, collaborated with Babbage and is recognized as the world’s first programmer. She envisioned computers going beyond mere number crunching, foreseeing applications in music composition and AI-like capabilities."
  },
  {
    "objectID": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "href": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "title": "A Decade of Machine Learning",
    "section": "Mechanical Calculators and Early Computers",
    "text": "Mechanical Calculators and Early Computers\nThe evolution of mechanical calculators and the emergence of early electronic computers marked significant progress in computational capabilities.\n\nPascal’s Calculator and Leibniz’s Step Drum\nPascal’s mechanical calculator incorporated Latin Lantern gears, performing basic arithmetic operations. Leibniz introduced the step drum, a mechanism for counting and representing numbers. Both contributed to the development of early calculating machines.\n\n\nENIAC (Electronic Numerical Integrator and Computer)\nENIAC, the first electronic computer, boasted over 17,000 vacuum tubes. Despite its immense size and weight, it laid the foundation for electronic computing. Augusta Ada Byron’s visionary insights into the potential of computers started to materialize with the advent of ENIAC."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction",
    "href": "pages/AI/Week01.html#introduction",
    "title": "A Decade of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nThe course provides a comprehensive exploration of the evolution and fundamental principles of Artificial Intelligence (AI). With historical roots reaching back to the 1300s, early attempts by figures like Jazari and Ramon Llull set the stage for the development of AI.\n\nCoined Terminology\nThe term “Artificial Intelligence” was officially coined by John McCarthy during the Dartmouth Conference in 1956. This landmark event, organized alongside Marvin Minsky and Claude Shannon, aimed to investigate the potential for machines to simulate human intelligence through precise descriptions."
  },
  {
    "objectID": "pages/AI/Week01.html#key-figures",
    "href": "pages/AI/Week01.html#key-figures",
    "title": "A Decade of Machine Learning",
    "section": "Key Figures",
    "text": "Key Figures\n\n1. John McCarthy\n\nCredited with Naming AI\nAssistant Professor at Dartmouth\nDesigner of Lisp Programming Language\nContributions to Logic and Common Sense Reasoning\n\n\n\n2. Marvin Minsky\n\nCo-founder of MIT AI Lab\nNotable for Frame Systems (Foundation of OOP)\nAuthor of “The Society of the Mind” and “The Emotional Machine”\n\n\n\n3. Nathaniel Rochester\n\nIBM Engineer\nDesigner of IBM 701\nSupervised Arthur Samuel and the Checkers-playing Program\n\n\n\n4. Claude Shannon\n\nFather of Information Theory\nMathematician at Bell Labs\n\n\n\n5. Herbert Simon and Allen Newell\n\nDevelopers of Logic Theorist (LT) Program\nPioneers in Symbolic AI\nIntroduction of Physical Symbol Systems\nSimon’s Diverse Scholarship (Nobel Prize in Economics)"
  },
  {
    "objectID": "pages/AI/Week01.html#physical-symbol-systems",
    "href": "pages/AI/Week01.html#physical-symbol-systems",
    "title": "A Decade of Machine Learning",
    "section": "Physical Symbol Systems",
    "text": "Physical Symbol Systems\n\nSymbolic Representation\nSymbol systems represent perceptible entities and adhere to formal laws, mirroring the structure of the physical world. Simon and Newell’s hypothesis posits that a Physical Symbol System is both necessary and sufficient for general intelligent action, distinguishing it from sub-symbolic AI, where information is stored in weights without explicit symbols."
  },
  {
    "objectID": "pages/AI/Week01.html#philosophical-considerations",
    "href": "pages/AI/Week01.html#philosophical-considerations",
    "title": "A Decade of Machine Learning",
    "section": "Philosophical Considerations",
    "text": "Philosophical Considerations\n\nCopernican Shift\n\nGalileo’s Distinction Between Thought and Reality\nGalileo Galilei’s intellectual endeavors were marked by a profound separation between the realm of thought and the objective reality. This conceptual wedge laid the foundation for a nuanced understanding of how human cognition interfaces with the external world.\n\n\nCopernicus’ Challenge to the Geocentric Model, Emphasizing Subjectivity\nCopernicus, through his revolutionary heliocentric model, not only challenged the prevailing geocentric view but also underscored the subjectivity inherent in our interpretations of celestial motions. This shift forced a reconsideration of humanity’s position in the cosmos.\n\n\nHuman Creation of Mental Models; Reality Comprising Fundamental Particles\nHumans engage in the active creation of mental models to comprehend the intricacies of reality. The Copernican Shift extends to the microscopic realm, where the abundant nature of fundamental particles renders them unsuitable as standalone elements of representation. Instead, reality is approached through disciplined ontologies, focusing on entities like atoms, molecules, or cells based on the context of study.\n\n\nIllustration through the Powers of Ten Film\nThe Powers of Ten film serves as a captivating medium to illustrate the Copernican Shift, visually portraying the vastness and intricacies of the universe at different scales. This cinematic exploration emphasizes the dynamic interplay between our mental representations and the expansive reality they seek to capture."
  },
  {
    "objectID": "pages/AI/Week01.html#representation-and-reasoning",
    "href": "pages/AI/Week01.html#representation-and-reasoning",
    "title": "A Decade of Machine Learning",
    "section": "Representation and Reasoning",
    "text": "Representation and Reasoning\n\nHuman Reasoning\n\nHuman Reasoning Involves Symbolic Representations\nIn the realm of human cognition, symbolic representations play a pivotal role in the process of reasoning. These symbols serve as cognitive tools that humans manipulate to make sense of the world around them.\n\n\nFundamental Particles Unsuitable as Elements of Representation due to Abundance\nDespite the fundamental nature of particles, their sheer abundance makes them impractical as elemental units of representation. Human cognition necessitates a selective focus, leading to the adoption of more manageable entities like atoms, molecules, or cells, depending on the specific domain of study.\n\n\nRepresentation Depends on the Focus of Study (e.g., Atoms, Molecules, Cells)\nThe choice of representation is intricately tied to the focus of study. Whether delving into the microscopic realm of atoms or exploring the complexity of biological systems at the cellular level, the selection of representational units is driven by the demands of the specific discipline.\n\n\nDiscipline-specific Ontologies Define Level of Detail in Representations\nDiscipline-specific ontologies play a crucial role in determining the level of detail embedded in representations. These structured frameworks provide a systematic approach to capturing and organizing knowledge within distinct domains."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-problem-solving",
    "href": "pages/AI/Week01.html#introduction-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Problem Solving",
    "text": "Introduction to Problem Solving\nIn the expansive domain of Artificial Intelligence (AI), problem-solving emerges as the orchestrated actions of autonomous agents navigating predefined objectives within dynamic environments. This course delves into the intricacies of problem-solving, elucidating the diverse methodologies encapsulated within search methods.\n\nProblem-Solving Framework\n\nAgent and Environment:\n\nAutonomous agents operate within a world defined by specific objectives and a repertoire of actions. Decision-making unfolds in real-time, navigating challenges posed by incomplete knowledge and the concurrent activities of other agents.\n\nSimplifying Assumptions:\n\nInitial simplifications envision a static world with a solitary agent making decisions, providing foundational insights into fundamental problem-solving principles."
  },
  {
    "objectID": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "href": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Two Approaches to Problem Solving",
    "text": "Two Approaches to Problem Solving\n\n1. Model-Based Reasoning (Search Methods)\n\nDefinition:\nModel-Based Reasoning involves grounded reasoning in first principles or search approaches, wherein agents experiment with diverse actions to discern their efficacy.\n\n\nAssumptions:\nThis approach assumes a static world, complete knowledge, and actions that never fail, forming the foundational basis for problem-solving methodologies.\n\n\n\n2. Knowledge-Based Approach\n\nCharacteristics:\nThe Knowledge-Based Approach draws upon a societal structure rich in stored experiences, leveraging accumulated knowledge for effective problem-solving. It encompasses memory-based reasoning, case-based reasoning, and machine learning paradigms."
  },
  {
    "objectID": "pages/AI/Week01.html#rubiks-cube-example",
    "href": "pages/AI/Week01.html#rubiks-cube-example",
    "title": "A Decade of Machine Learning",
    "section": "Rubik’s Cube Example",
    "text": "Rubik’s Cube Example\nThe Rubik’s Cube serves as an illustrative example, elucidating the dichotomy between knowledge-based and search-based problem-solving approaches.\n\nLearning Dynamics\n\nInitial Challenge:\n\nThe Rubik’s Cube presents an initial challenge devoid of a known solution, necessitating exploratory actions.\n\nEvolution of Knowledge:\n\nOver time, individuals develop efficient solving methods through experiential learning, showcasing the adaptive nature of human problem-solving.\n\nDeep Reinforcement Learning:\n\nThe introduction of deep reinforcement learning emphasizes autonomous learning without human guidance, mirroring aspects of artificial intelligence."
  },
  {
    "objectID": "pages/AI/Week01.html#sudoku-example",
    "href": "pages/AI/Week01.html#sudoku-example",
    "title": "A Decade of Machine Learning",
    "section": "Sudoku Example",
    "text": "Sudoku Example\nThe Sudoku puzzle exemplifies the synergy between search and reasoning in problem-solving, offering insights into the nuanced interplay of diverse problem-solving methodologies.\n\nCombined Approach\n\nSearch Methods:\n\nBasic search algorithms, such as depth-first search and breadth-first search, lay the foundation for problem-solving endeavors.\n\nReasoning:\n\nReasoning techniques refine available options, harmonizing search methodologies with informed decision-making for a holistic problem-solving strategy."
  },
  {
    "objectID": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "href": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Role of Logic in Problem Solving",
    "text": "Role of Logic in Problem Solving\nLogic, particularly first-order logic, assumes a pivotal role in representing knowledge and facilitating deductive reasoning within the problem-solving paradigm.\n\nLogical Components\n\nDeductive Reasoning:\n\nLogic functions as a tool for deductive reasoning, employing principles such as deduction, induction, abduction, and plausible reasoning to navigate complex problem spaces.\n\nConstraint Processing:\n\nLogic, search methods, and other reasoning approaches converge under the umbrella of constraint processing, offering a comprehensive framework for addressing intricate problem scenarios."
  },
  {
    "objectID": "pages/AI/Week01.html#map-coloring-problem",
    "href": "pages/AI/Week01.html#map-coloring-problem",
    "title": "A Decade of Machine Learning",
    "section": "Map Coloring Problem",
    "text": "Map Coloring Problem\nThe Map Coloring Problem stands as an exemplary challenge within AI, involving the assignment of colors to regions while adhering to specific constraints.\n\nConstraint Graph Representation\n\nGraph Transformation:\n\nRegions and their color preferences undergo a transformative process, manifesting as a constraint graph that encapsulates the intricacies of the problem.\n\nAlgorithmic Solutions:\n\nConstraint processing algorithms come to the forefront as viable solutions to graph-related problems, showcasing the practical application of logical problem-solving methodologies."
  },
  {
    "objectID": "pages/AI/Week01.html#key-takeaways",
    "href": "pages/AI/Week01.html#key-takeaways",
    "title": "A Decade of Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDecade of Machine Learning:\n\nThe surge in machine learning over the past decade is attributed to increased data availability, enhanced computing power, and advancements in training algorithms, particularly within the domain of deep learning. \n\nNeural Network Evolution:\n\nFrom the foundational perceptron era to the transformative deep neural networks in computer vision, the evolution of neural networks has played a pivotal role in shaping the landscape of AI.\n\nTraining Process:\n\nSupervised training, especially in medical diagnosis and face recognition, showcases the practical applications of deep neural networks in real-world scenarios.\n\nMachine Learning in Internet Interaction:\n\nUsers’ dynamic interaction with the internet transforms them into valuable data points, shaping the customization of ads and optimizing user experiences.\n\nPerformance vs. Competence:\n\nNeural networks exhibit proficiency in specific tasks but lack a holistic understanding of the world, highlighting the need for specialized training.\n\nGame of Go and Reinforcement Learning:\n\nThe triumph of AlphaGo exemplifies the success achievable through reinforcement learning, showcasing the capacity for autonomous learning without human intervention.\n\nHuman Cognitive Architecture:\n\nUnderstanding human cognitive abilities, symbolic reasoning, and the distinction between AI and machine learning provides insights into the complex realm of intelligence.\n\nHistory and Philosophy:\n\nThe historical roots of AI, key figures in AI development, and philosophical considerations underscore the interdisciplinary nature of artificial intelligence.\n\nPhysical Symbol Systems:\n\nSymbolic representation, as proposed by Simon and Newell, forms the basis for general intelligent action, distinguishing it from sub-symbolic AI.\n\nProblem Solving:\n\nTwo approaches, model-based reasoning and knowledge-based approaches, along with the role of logic, contribute to nuanced problem-solving methodologies.\n\nMap Coloring Problem:\n\nThe Map Coloring Problem serves as a concrete example, highlighting the integration of graph theory, constraint processing, and algorithmic solutions in logical problem-solving."
  },
  {
    "objectID": "pages/DL/Week01_1.html",
    "href": "pages/DL/Week01_1.html",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain.\n\n\n\nIn 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969.\n\n\n\nThe period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape.\n\n\n\nThe Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation.\n\n\n\nA disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy."
  },
  {
    "objectID": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "href": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain."
  },
  {
    "objectID": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "href": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969."
  },
  {
    "objectID": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "href": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape."
  },
  {
    "objectID": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "href": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation."
  },
  {
    "objectID": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "href": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "A disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy."
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction",
    "href": "pages/DL/Week01_1.html#introduction",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\n\nHistorical Perspective\nDeep learning encountered challenges in training via backpropagation. Jeff Hinton’s group proposed a crucial weight initialization idea in 2016, fostering stable training. The improved availability of computing power and data around 2006 laid the foundation for success."
  },
  {
    "objectID": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "href": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Early Challenges and Solutions",
    "text": "Early Challenges and Solutions\n\nUnsupervised Pre-training\nBetween 2007 and 2009, investigations into the effectiveness of unsupervised pre-training led to insights that shaped optimization and regularization algorithms. The course will delve into topics such as initializations, regularizations, and optimizations."
  },
  {
    "objectID": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "href": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Emergence of Deep Learning",
    "text": "Emergence of Deep Learning\n\nPractical Utility\nDeep learning applications started winning competitions, including handwriting recognition on the MNIST dataset, speech recognition, and visual pattern recognition like traffic sign data.\n\n\nImageNet Challenge (2012-2016)\nThe ImageNet challenge, a pivotal turning point, witnessed the evolution from ZFNet to ResNet (152 layers), achieving a remarkable 3.6% error rate in 2016, surpassing human performance."
  },
  {
    "objectID": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "href": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transition Period (2012-2016)",
    "text": "Transition Period (2012-2016)\n\nGolden Period of Deep Learning\nThe universal acceptance of deep learning marked its golden period, with convolutional neural networks dominating image-related problems. Similar trends were observed in natural language processing (NLP) and speech processing."
  },
  {
    "objectID": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "href": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "title": "A Historical Overview of Deep Learning",
    "section": "From Cats to Convolutional Neural Networks",
    "text": "From Cats to Convolutional Neural Networks\n\nMotivation from Neural Science (1959)\nAn experiment with a cat’s brain in 1959 revealed different parts activated for different stick positions, motivating the concept of receptive fields in convolutional neural networks (CNNs).\n\n\nNeocognitron Model (1980)\nInspired by distributed processing observed in the cat experiment, the Neocognitron model utilized receptive fields for different parts of the network.\n\n\nLeNet Model (1989)\nJan Lecun’s contribution to deep learning, the LeNet model, was employed for recognizing handwritten digits, finding applications in postal services for automated sorting of letters.\n\n\nLeNet-5 Model (1998)\nFurther improvements on the LeNet model, introducing the MNIST dataset for testing CNNs."
  },
  {
    "objectID": "pages/DL/Week01_1.html#history-of-deep-learning",
    "href": "pages/DL/Week01_1.html#history-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "History of Deep Learning",
    "text": "History of Deep Learning\n\n1950s: Enthusiasm in AI.\n1990s: Convolutional Neural Networks (CNNs) used for real-world problems, challenges with large networks and training.\n2006-2012: Advances in deep learning, successful training for ImageNet challenges.\n2016 onwards: Acceleration with better optimization methods (Nesterov’s method), leading to faster convergence.\nOptimization Algorithms: Adagrad, RMSprop, Adam, AdamW, etc., focus on faster and better convergence."
  },
  {
    "objectID": "pages/DL/Week01_1.html#activation-functions",
    "href": "pages/DL/Week01_1.html#activation-functions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Activation Functions",
    "text": "Activation Functions\nThe evolution from the logistic function to various activation functions (ReLU, Leaky ReLU, Parametric ReLU, Tanh, etc.) aimed at stabilizing training, achieving better performance, and faster convergence. The use of improved activation functions contributed to enhanced stability and performance."
  },
  {
    "objectID": "pages/DL/Week01_1.html#sequence-processing",
    "href": "pages/DL/Week01_1.html#sequence-processing",
    "title": "A Historical Overview of Deep Learning",
    "section": "Sequence Processing",
    "text": "Sequence Processing\nIntroduction to problems involving sequences in deep learning, featuring Recurrent Neural Networks (RNNs) proposed in 1982 for sequence processing. Long Short-Term Memory Cells (LSTMs) were introduced in 1997 to address the vanishing gradient problem. By 2014, RNNs and LSTMs dominated natural language processing (NLP) and speech applications. In 2017, Transformer networks started replacing RNNs and LSTMs in sequence learning."
  },
  {
    "objectID": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "href": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Game Playing with Deep Learning",
    "text": "Game Playing with Deep Learning\n\n2015: Deep Reinforcement Learning (DRL) agents beat humans in Atari games.\nBreakthrough in Go game playing using DRL in 2015.\n2016: DRL-based agents beat professional poker players.\nComplex strategy games like Dota 2 mastered by DRL agents.\nIntroduction of OpenAI Gym as a toolkit for developing and comparing reinforcement learning algorithms.\nEmergence of AlphaStar and MuZero for mastering multiple games and tasks."
  },
  {
    "objectID": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "href": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "General Trends in Deep Reinforcement Learning",
    "text": "General Trends in Deep Reinforcement Learning\nDeep RL agents consistently outperforming humans in various complex games, progressing from simple environments to mastering complex strategy games. The trend is towards developing “master of all” models (e.g., MuZero) for general intelligence in multiple tasks."
  },
  {
    "objectID": "pages/DL/Week01_1.html#overview",
    "href": "pages/DL/Week01_1.html#overview",
    "title": "A Historical Overview of Deep Learning",
    "section": "Overview",
    "text": "Overview\n\nRevival and Advances\nA recap of deep learning’s revival and recent advances, reflecting an increasing interest in real-world problem-solving and challenges.\n\n\nAI Publications Growth\nThe Stanford AI Index Report highlights a significant increase in AI publications, indicating exponential growth across machine learning, computer vision, and NLP.\n\n\nFunding and Startups\nThe rise of AI startups, coupled with the interest from major tech companies, has led to exponential growth in AI-related patent filings."
  },
  {
    "objectID": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "href": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Evolution of Neural Network Models",
    "text": "Evolution of Neural Network Models\n\nIntroduction of Transformers\nIn 2017, transformers were introduced, revolutionizing AI and finding success in NLP, subsequently adopted in other domains.\n\n\nMachine Translation and Transformers\nA historical overview of machine translation, emphasizing the shift from IBM models to neural machine translation. The impact of sequence-to-sequence models (2014) and transformers (2017) is discussed.\n\n\nTransformer-based Models\nThe BERT model (2018) with a focus on pre-training, the evolution of models with increasing parameters from GPT-3 (175 billion) to 1.6 trillion parameters, and a comparison with human brain synapses provide perspective."
  },
  {
    "objectID": "pages/DL/Week01_1.html#transformers-in-vision",
    "href": "pages/DL/Week01_1.html#transformers-in-vision",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transformers in Vision",
    "text": "Transformers in Vision\n\nAdoption in Image\nClassification The evolution of image classification models, starting with AlexNet (2012), is traced. Transformers entered image classification and object detection in 2019, marking a paradigm shift towards transformers in state-of-the-art models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#generative-models",
    "href": "pages/DL/Week01_1.html#generative-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Generative Models",
    "text": "Generative Models\n\nOverview\nAn introduction to generative models for image synthesis, covering the evolution from variation autoencoders to GANs (Generative Adversarial Networks). Recent developments in diffusion-based models overcoming GAN drawbacks are discussed.\n\n\nDALL-E and DALL-E 2\nDALL-E’s capability to generate realistic images based on text prompts is explored. The introduction of DALL-E 2, a diffusion-based model, exceeding expectations, is highlighted with examples of generated images showcasing photorealistic results.\n\n\nExciting Times in Generative Models\nThe exploration of generative models for realistic image generation is showcased, with examples of prompts generating photorealistic images illustrating field advancements."
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction-1",
    "href": "pages/DL/Week01_1.html#introduction-1",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\nRapid advancements in deep learning have yielded powerful models trained on large datasets, showcasing impressive results. However, there is a growing need for sanity, interpretability, fairness, and responsibility in deploying these models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "href": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Paradox of Deep Learning",
    "text": "Paradox of Deep Learning\nDespite the high capacity of deep learning models, they exhibit remarkable performance. Challenges include numerical instability, sharp minima, and susceptibility to adversarial examples."
  },
  {
    "objectID": "pages/DL/Week01_1.html#calls-for-sanity",
    "href": "pages/DL/Week01_1.html#calls-for-sanity",
    "title": "A Historical Overview of Deep Learning",
    "section": "Calls for Sanity",
    "text": "Calls for Sanity\nEmphasis is placed on explainability and interpretability to comprehend model decisions. Advances include workshops on human interpretability, tools like the Clever Hans toolkit to identify model reliance on cues, and benchmarking on adversarial examples."
  },
  {
    "objectID": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "href": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "title": "A Historical Overview of Deep Learning",
    "section": "Fairness and Responsibility",
    "text": "Fairness and Responsibility\nIncreasing awareness of biases in AI models, particularly in facial recognition and criminal risk predictions, has led to concerns about fairness. Efforts such as the AI audit challenge at Stanford focus on building non-discriminatory models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#green-ai",
    "href": "pages/DL/Week01_1.html#green-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Green AI",
    "text": "Green AI\nRising environmental concerns due to the high computational power and energy consumption of deep learning models have spurred calls for responsible AI, extending to the environmental impact. There is a push for more energy-efficient models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "href": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Exciting Times in AI",
    "text": "Exciting Times in AI\nThe AI revolution is influencing scientific research, evident in DeepMind’s AlphaFold predicting protein folding. Applications in astronomy, predicting galaxy aging, and generating images for fundamental variables in experimental data are emerging. There is an emphasis on efficient deep learning for mobile devices, edge computing, and addressing constraints of power, storage, and real-time processing."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BS Degree Notes",
    "section": "",
    "text": "I put the course notes for my own easy perusal."
  },
  {
    "objectID": "pages/DL/Week01_2.html",
    "href": "pages/DL/Week01_2.html",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units.\n\n\n\n\n\n\nDendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter.\n\n\n\n\n\nLayered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli.\n\n\n\n\n\n\nNeural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction",
    "href": "pages/DL/Week01_2.html#introduction",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units."
  },
  {
    "objectID": "pages/DL/Week01_2.html#biological-neurons",
    "href": "pages/DL/Week01_2.html#biological-neurons",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Dendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter."
  },
  {
    "objectID": "pages/DL/Week01_2.html#neural-network-architecture",
    "href": "pages/DL/Week01_2.html#neural-network-architecture",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Layered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli."
  },
  {
    "objectID": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "href": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Neural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-1",
    "href": "pages/DL/Week01_2.html#introduction-1",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\n\nObjective: Comprehend the McCulloch-Pitts neuron, a simplified computational model inspired by biological neurons.\nHistorical Context: Proposed in 1943 by McCulloch (neuroscientist) and Pitts (logician).\nPurpose: Emulate the brain’s complex processing for decision-making."
  },
  {
    "objectID": "pages/DL/Week01_2.html#neuron-structure",
    "href": "pages/DL/Week01_2.html#neuron-structure",
    "title": "Motivation from Biological Neuron",
    "section": "Neuron Structure",
    "text": "Neuron Structure\n\nComponents: Divided into two parts - g and f.\ng (Aggregation): Aggregates binary inputs via a simple summation process.\nf (Decision): Makes a binary decision based on the aggregation.\nExcitatory and Inhibitory Inputs: Inputs can be either excitatory (positive) or inhibitory (negative)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#functionality",
    "href": "pages/DL/Week01_2.html#functionality",
    "title": "Motivation from Biological Neuron",
    "section": "Functionality",
    "text": "Functionality\n\nAggregation Function g(x):\n\nRepresents the sum of all inputs using the formula \\(g(x) = \\sum_{i=1}^{n} x_i\\), where \\(x_i\\) is a binary input (0 or 1).\n\nDecision Function f(g(x)):\n\nUtilizes a threshold parameter \\(\\theta\\) to determine firing.\nDecision is \\(f(g(x)) = \\begin{cases} 1 & \\text{if } g(x) \\geq \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#boolean-function-implementation",
    "href": "pages/DL/Week01_2.html#boolean-function-implementation",
    "title": "Motivation from Biological Neuron",
    "section": "Boolean Function Implementation",
    "text": "Boolean Function Implementation\n\nExamples:\n\nImplemented using McCulloch-Pitts neuron for boolean functions like AND, OR, NOR, and NOT.\nExcitatory and inhibitory inputs utilized based on boolean function logic."
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation",
    "href": "pages/DL/Week01_2.html#geometric-interpretation",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\nIn 2D:\n\nDraws a line to separate input space into two halves.\n\nIn 3D:\n\nUses a plane for separation.\n\nFor n Inputs:\n\nUtilizes a hyperplane for linear separation."
  },
  {
    "objectID": "pages/DL/Week01_2.html#linear-separability",
    "href": "pages/DL/Week01_2.html#linear-separability",
    "title": "Motivation from Biological Neuron",
    "section": "Linear Separability",
    "text": "Linear Separability\n\nDefinition: Boolean functions representable by a single McCulloch-Pitts neuron are linearly separable.\nImplication: Implies the existence of a plane (or hyperplane) separating points with output 0 and 1."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-2",
    "href": "pages/DL/Week01_2.html#introduction-2",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nPerceptrons, introduced by Frank Rosenblatt circa 1958, extend the concept of McCulloch-Pitts neurons with non-Boolean inputs, input weights, and a learning algorithm for weight adjustment."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-model",
    "href": "pages/DL/Week01_2.html#perceptron-model",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\n\nMathematical Representation\nThe perceptron is represented as \\(y = 1\\) if \\(\\sum_{i=1}^{n} w_i x_i \\geq \\text{threshold}\\); otherwise, \\(y = 0\\).\n\nNotable Differences\n\nInputs can be real, not just Boolean.\nIntroduction of weights, denoted by \\(w_i\\), indicating input importance.\nLearning algorithm to adapt weights based on data.\n\n\n\n\nNeater Formulation\nThe equation is rearranged for simplicity: \\(\\sum_{i=0}^{n} w_i x_i \\geq 0\\), where \\(x_0 = 1\\) and \\(w_0 = -\\text{threshold}\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "href": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation for Boolean Functions",
    "text": "Motivation for Boolean Functions\nBoolean functions provide a foundation for understanding perceptrons. For instance, predicting movie preferences using Boolean inputs such as actor, director, and genre."
  },
  {
    "objectID": "pages/DL/Week01_2.html#importance-of-weights",
    "href": "pages/DL/Week01_2.html#importance-of-weights",
    "title": "Motivation from Biological Neuron",
    "section": "Importance of Weights",
    "text": "Importance of Weights\nWeights signify the importance of specific inputs in decision-making. Learning from data helps adjust weights, reflecting user preferences. For example, assigning a high weight to the director may heavily influence the decision to watch a movie."
  },
  {
    "objectID": "pages/DL/Week01_2.html#bias-w_0",
    "href": "pages/DL/Week01_2.html#bias-w_0",
    "title": "Motivation from Biological Neuron",
    "section": "Bias (\\(w_0\\))",
    "text": "Bias (\\(w_0\\))\n\\(w_0\\) acts as a bias or prior, influencing decision-making. It represents the initial bias or prejudice in decision-making. Adjusting \\(w_0\\) alters the decision threshold, accommodating user preferences."
  },
  {
    "objectID": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "href": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Implementing Boolean Functions",
    "text": "Implementing Boolean Functions\nPerceptrons can implement Boolean functions with linear decision boundaries. For instance, implementing the OR function with a perceptron involves a geometric interpretation where a line separates positive and negative regions based on inputs."
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-adjustments",
    "href": "pages/DL/Week01_2.html#errors-and-adjustments",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Adjustments",
    "text": "Errors and Adjustments\nErrors arise when the decision boundary misclassifies inputs. The learning algorithm adjusts weights iteratively to minimize errors and enhance accuracy. It’s an iterative process where weights are modified until the desired decision boundary is achieved."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-3",
    "href": "pages/DL/Week01_2.html#introduction-3",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThis section delves into errors within the context of perceptrons and introduces error surfaces as a recurring theme in the course, with a focus on understanding errors related to linear separability."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "href": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron for AND Function",
    "text": "Perceptron for AND Function\nConsideration of the AND function showcases an output of 1 for a specific input (green) and 0 for others (red). The decision is based on \\(w_0 + w_1x_1 + w_2x_2 \\geq 0\\), with \\(w_0\\) fixed at -1. Exploration of the impact of \\(w_1\\) and \\(w_2\\) on the decision boundary is undertaken."
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "href": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Decision Boundaries",
    "text": "Errors and Decision Boundaries\nDemonstration of errors occurs with specific \\(w_1\\) and \\(w_2\\) values, showcasing misclassified points due to incorrect decision boundaries. Variability in errors is noted based on different weight values."
  },
  {
    "objectID": "pages/DL/Week01_2.html#error-function",
    "href": "pages/DL/Week01_2.html#error-function",
    "title": "Motivation from Biological Neuron",
    "section": "Error Function",
    "text": "Error Function\nViewing error as a function of \\(w_1\\) and \\(w_2\\) is introduced. The concept of error surfaces is brought in, where error is plotted against \\(w_1\\) and \\(w_2\\) values, each region on the surface corresponding to a distinct error level."
  },
  {
    "objectID": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "href": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "title": "Motivation from Biological Neuron",
    "section": "Visualizing the Error Surface",
    "text": "Visualizing the Error Surface\nThe error surface is plotted for \\(w_1\\) and \\(w_2\\) values in the range -4 to +4. Each region on the surface corresponds to a distinct error level, highlighting the utility of visualizations in comprehending perceptron behavior."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "href": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Learning Algorithm",
    "text": "Perceptron Learning Algorithm\nExploration of the necessity for an algorithmic approach to finding optimal \\(w_1\\) and \\(w_2\\) values is undertaken. Limitations in visual inspection, especially in higher dimensions, are acknowledged. A teaser for the upcoming module on the perceptron learning algorithm is provided as a solution for finding suitable weight values algorithmically."
  },
  {
    "objectID": "pages/DL/Week01_2.html#overview",
    "href": "pages/DL/Week01_2.html#overview",
    "title": "Motivation from Biological Neuron",
    "section": "Overview",
    "text": "Overview\nThis module focuses on the Perceptron Learning Algorithm, building upon the perceptron’s concept and introducing a method to iteratively adjust weights for accurate binary classification."
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation",
    "href": "pages/DL/Week01_2.html#motivation",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation",
    "text": "Motivation\nThe perceptron, initially designed for boolean functions, finds practical application in real-world scenarios. Consider a movie recommendation system based on past preferences, where features include both boolean and real-valued inputs. The goal is to learn weights that enable accurate predictions for new inputs."
  },
  {
    "objectID": "pages/DL/Week01_2.html#algorithm",
    "href": "pages/DL/Week01_2.html#algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Algorithm",
    "text": "Algorithm\n\nNotations\n\n\\(p\\): Inputs with label 1 (positive points)\n\\(n\\): Inputs with label 0 (negative points)\n\n\n\nConvergence\nConvergence is achieved when all positive points satisfy \\(\\sum w_i x_i &gt; 0\\) and all negative points satisfy \\(\\sum w_i x_i &lt; 0\\).\n\n\nSteps\n\nInitialization: Randomly initialize weights \\(w\\).\nIterative Update:\n\nWhile not converged:\n\nPick a random point \\(x\\) from \\(p \\cup n\\).\nIf \\(x\\) is in \\(p\\) and \\(w^T x &lt; 0\\), update \\(w = w + x\\).\nIf \\(x\\) is in \\(n\\) and \\(w^T x \\geq 0\\), update \\(w = w - x\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "href": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nUnderstanding the geometric relationship involves recognizing that the angle between \\(w\\) and a point on the decision boundary is 90 degrees. Positive points’ angles should be acute (&lt; 90 degrees), and negative points’ angles should be obtuse (&gt; 90 degrees). Iteratively adjusting \\(w\\) aligns it better with correctly classified points."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-4",
    "href": "pages/DL/Week01_2.html#introduction-4",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this lecture is to present a formal proof establishing the convergence of the perceptron learning algorithm. The primary focus is to rigorously determine whether the algorithm exhibits convergence or continues weight updates indefinitely."
  },
  {
    "objectID": "pages/DL/Week01_2.html#definitions",
    "href": "pages/DL/Week01_2.html#definitions",
    "title": "Motivation from Biological Neuron",
    "section": "Definitions",
    "text": "Definitions\n\nAbsolutely Linearly Separable Sets\n\nConsider two sets, \\(P\\) and \\(N\\), in an \\(n\\)-dimensional space. They are deemed absolutely linearly separable if there exist \\(n + 1\\) real numbers \\(w_0\\) to \\(w_n\\) such that the following conditions hold: \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n \\geq 0 \\quad \\text{for every } \\mathbf{x} \\in P\n\\] \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n &lt; 0 \\quad \\text{for every } \\mathbf{x} \\in N\n\\]\n\nPerceptron Learning Algorithm Convergence Theorem\n\nIf sets \\(P\\) and \\(N\\) are finite and linearly separable, the perceptron learning algorithm will update the weight vector a finite number of times. This implies that after a finite number of steps, the algorithm will find a weight vector \\(\\mathbf{w}\\) capable of separating sets \\(P\\) and \\(N\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#proof",
    "href": "pages/DL/Week01_2.html#proof",
    "title": "Motivation from Biological Neuron",
    "section": "Proof",
    "text": "Proof\n\nSetup\nDefine \\(P'\\) as the union of \\(P\\) and the negation of \\(N\\). Normalize all inputs for convenience.\n\n\nAssumptions and Definitions\nAssume the existence of a normalized solution vector \\(\\mathbf{w^*}\\). Define the minimum dot product, \\(\\delta\\), as the minimum value obtained by dot products between \\(\\mathbf{w^*}\\) and points in \\(P'\\).\n\n\nPerceptron Learning Algorithm\nThe perceptron learning algorithm can be expressed as follows:\n\nInitialization:\n\nInitialize weight vector \\(\\mathbf{w}\\) randomly.\n\nIteration:\n\nAt each iteration, randomly select a point \\(\\mathbf{p}\\) from \\(P'\\).\nIf the condition \\(\\mathbf{w}^T\\mathbf{p} \\geq 0\\) is not satisfied, update \\(\\mathbf{w}\\) by \\(\\mathbf{w} = \\mathbf{w} + \\mathbf{p}\\).\n\n\n\n\nNormalization and Definitions\nNormalize all inputs, ensuring the norm of \\(\\mathbf{p}\\) is 1. Define the numerator of \\(\\cos \\beta\\) as the dot product between \\(\\mathbf{w^*}\\) and the updated weight vector at each iteration.\n\n\nNumerator Analysis\nShow that the numerator is greater than or equal to \\(\\delta\\) for each iteration.\nFor a randomly selected \\(\\mathbf{p}\\), if \\(\\mathbf{w}^T\\mathbf{p} &lt; 0\\) and an update is performed, the numerator is:\n\\[\n\\mathbf{w^*} \\cdot (\\mathbf{w} + \\mathbf{p}) \\geq \\delta\n\\]\n\n\nDenominator Analysis\nExpand the denominator, the square of the norm of the updated weight vector:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 = \\|\\mathbf{w}\\|^2 + 2\\mathbf{w}^T\\mathbf{p} + \\|\\mathbf{p}\\|^2\n\\]\nShow that the denominator is less than or equal to a value involving \\(k\\), the number of updates made:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 \\leq \\|\\mathbf{w^*}\\|^2 + k\n\\]\n\n\nCombining Numerator and Denominator\nUse the definition of \\(\\cos \\beta\\) to conclude that \\(\\cos \\beta\\) is greater than or equal to a certain quantity involving the square root of \\(k\\):\n\\[\n\\cos \\beta \\geq \\frac{\\delta}{\\sqrt{k}}\n\\]"
  },
  {
    "objectID": "pages/DL/Week02.html",
    "href": "pages/DL/Week02.html",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Boolean functions, fundamental to computational logic, pose challenges when it comes to their linear separability. The perceptron learning algorithm, known for its guarantees with linearly separable data, encounters limitations when dealing with certain boolean functions. This module delves into the intricacies of these functions and explores the concept of linear separability.\n\n\n\n\n\nThe XOR function, denoted as \\(f(x_1, x_2)\\), outputs 1 when exactly one of its inputs is 1. It follows the logic: \\[f(0,0) \\rightarrow 0, \\, f(0,1) \\rightarrow 1, \\, f(1,0) \\rightarrow 1, \\, f(1,1) \\rightarrow 0\\]\n\n\n\nAttempting to implement XOR using a perceptron leads to a set of four inequalities. These conditions, when applied to weights (\\(w_0, w_1, w_2\\)), cannot be simultaneously satisfied. Geometrically, this signifies the inability to draw a line that separates positive and negative points in the XOR function.\n\n\n\n\nReal-world data often deviates from the assumption of linear separability. For instance, individuals with similar characteristics may exhibit diverse preferences, challenging the effectiveness of linear decision boundaries.\n\n\n\nRecognizing the limitations of a single perceptron in handling non-linearly separable data, a proposed solution involves using a network of perceptrons. This approach aims to extend the capability of handling complex, non-linearly separable boolean functions.\n\n\n\nBoolean functions with \\(n\\) inputs offer a wide range of possibilities, such as AND, OR, and others. The total number of boolean functions from \\(n\\) inputs is given by \\(2^{2^n}\\). The discussion extends to the linear separability of these boolean functions.\n\n\n\nOut of the \\(2^{2^n}\\) boolean functions, some are not linearly separable. The precise count of non-linearly separable functions remains an unsolved problem, highlighting the need for robust methods capable of handling such cases."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction",
    "href": "pages/DL/Week02.html#introduction",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Boolean functions, fundamental to computational logic, pose challenges when it comes to their linear separability. The perceptron learning algorithm, known for its guarantees with linearly separable data, encounters limitations when dealing with certain boolean functions. This module delves into the intricacies of these functions and explores the concept of linear separability."
  },
  {
    "objectID": "pages/DL/Week02.html#xor-function-analysis",
    "href": "pages/DL/Week02.html#xor-function-analysis",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "The XOR function, denoted as \\(f(x_1, x_2)\\), outputs 1 when exactly one of its inputs is 1. It follows the logic: \\[f(0,0) \\rightarrow 0, \\, f(0,1) \\rightarrow 1, \\, f(1,0) \\rightarrow 1, \\, f(1,1) \\rightarrow 0\\]\n\n\n\nAttempting to implement XOR using a perceptron leads to a set of four inequalities. These conditions, when applied to weights (\\(w_0, w_1, w_2\\)), cannot be simultaneously satisfied. Geometrically, this signifies the inability to draw a line that separates positive and negative points in the XOR function."
  },
  {
    "objectID": "pages/DL/Week02.html#implications-for-real-world-data",
    "href": "pages/DL/Week02.html#implications-for-real-world-data",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Real-world data often deviates from the assumption of linear separability. For instance, individuals with similar characteristics may exhibit diverse preferences, challenging the effectiveness of linear decision boundaries."
  },
  {
    "objectID": "pages/DL/Week02.html#network-of-perceptrons",
    "href": "pages/DL/Week02.html#network-of-perceptrons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Recognizing the limitations of a single perceptron in handling non-linearly separable data, a proposed solution involves using a network of perceptrons. This approach aims to extend the capability of handling complex, non-linearly separable boolean functions."
  },
  {
    "objectID": "pages/DL/Week02.html#boolean-functions-from-n-inputs",
    "href": "pages/DL/Week02.html#boolean-functions-from-n-inputs",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Boolean functions with \\(n\\) inputs offer a wide range of possibilities, such as AND, OR, and others. The total number of boolean functions from \\(n\\) inputs is given by \\(2^{2^n}\\). The discussion extends to the linear separability of these boolean functions."
  },
  {
    "objectID": "pages/DL/Week02.html#challenge-of-non-linear-separability",
    "href": "pages/DL/Week02.html#challenge-of-non-linear-separability",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Out of the \\(2^{2^n}\\) boolean functions, some are not linearly separable. The precise count of non-linearly separable functions remains an unsolved problem, highlighting the need for robust methods capable of handling such cases."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-to-multi-layer-perceptrons",
    "href": "pages/DL/Week02.html#introduction-to-multi-layer-perceptrons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction to Multi-Layer Perceptrons",
    "text": "Introduction to Multi-Layer Perceptrons\nMulti-Layer Perceptrons (MLPs) constitute a pivotal advancement in artificial neural networks. These networks boast a layered architecture, each layer serving a distinct role in processing information.\n\nLayers in an MLP\n\nInput Layer:\n\nComprising nodes representing input features (\\(x_1, x_2, ..., x_n\\)).\n\nHidden Layer:\n\nFeatures multiple perceptrons introducing non-linearities to the network.\n\nOutput Layer:\n\nHouses a single perceptron providing the final network output.\n\n\n\n\nWeights and Bias\n\nConnection Characteristics:\n\nWeights (\\(w\\)) and a bias term (\\(w_0\\)) define the connections between nodes.\n\nWeighted Sum and Activation:\n\nThe weighted sum of inputs, combined with the bias, influences perceptron activation."
  },
  {
    "objectID": "pages/DL/Week02.html#representation-of-boolean-functions-in-mlps",
    "href": "pages/DL/Week02.html#representation-of-boolean-functions-in-mlps",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Representation of Boolean Functions in MLPs",
    "text": "Representation of Boolean Functions in MLPs\n\nNetwork Structure for Boolean Functions\n\nHidden Layer Configuration:\n\nFor a boolean function with \\(n\\) inputs, the hidden layer consists of \\(2^n\\) perceptrons.\n\nWeight and Bias Adjustment:\n\nWeights and biases are adjusted to meet boolean logic conditions for accurate function representation.\n\n\n\n\nBoolean Function Implementation\n\nPerceptron Activation Conditions:\n\nEach perceptron in the hidden layer selectively fires based on specific input combinations.\n\nXOR Function Illustration:\n\nUsing the XOR function as an example, conditions on weights (\\(w_1, w_2, w_3, w_4\\)) are established for faithful representation.\n\nExtension to \\(n\\) Inputs:\n\nGeneralizing the approach to \\(n\\) inputs involves \\(2^n\\) perceptrons in the hidden layer.\nConditions for output layer weights are derived to ensure accurate representation."
  },
  {
    "objectID": "pages/DL/Week02.html#representation-power-and-implications",
    "href": "pages/DL/Week02.html#representation-power-and-implications",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Representation Power and Implications",
    "text": "Representation Power and Implications\n\nRepresentation Power Theorem\n\nTheorem Statement:\n\nAny boolean function of \\(n\\) inputs can be precisely represented by an MLP.\n\nSuggested MLP Structure:\n\nAn MLP with \\(2^n\\) perceptrons in the hidden layer and 1 perceptron in the output layer is deemed sufficient.\n\n\n\n\nPractical Considerations\n\nChallenges with Growing \\(n\\):\n\nThe exponential increase in perceptrons as \\(n\\) grows poses practical challenges.\n\nReal-World Applications:\n\nManaging and computing with a large number of perceptrons may be challenging in practical applications."
  },
  {
    "objectID": "pages/DL/Week02.html#transition-from-perceptrons-to-sigmoid-neurons",
    "href": "pages/DL/Week02.html#transition-from-perceptrons-to-sigmoid-neurons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Transition from Perceptrons to Sigmoid Neurons",
    "text": "Transition from Perceptrons to Sigmoid Neurons\n\nBinary Output Limitation\nPerceptrons, governed by binary output based on the weighted sum of inputs exceeding a threshold, exhibit a binary decision boundary. This rigid characteristic proves restrictive in scenarios where a more gradual decision-making process is preferred.\n\n\nReal-Valued Inputs and Outputs\nThe shift towards sigmoid neurons arises in the context of addressing arbitrary functions \\(Y = f(X)\\), wherein \\(X \\in \\mathbb{R}^n\\) and \\(Y \\in \\mathbb{R}\\). This entails the consideration of real numbers for both inputs and outputs. Examples include predicting oil quantity based on salinity, density, pressure, temperature, and marine diversity, as well as determining bank interest rates considering factors like salary, family size, previous loans, and defaults."
  },
  {
    "objectID": "pages/DL/Week02.html#objective",
    "href": "pages/DL/Week02.html#objective",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Objective",
    "text": "Objective\nThe primary objective is to construct a neural network capable of accurately approximating or representing real-valued functions, ensuring the proximity of the network’s output to actual values present in the training data."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-to-sigmoid-neurons",
    "href": "pages/DL/Week02.html#introduction-to-sigmoid-neurons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction to Sigmoid Neurons",
    "text": "Introduction to Sigmoid Neurons\n\nSigmoid Function\nSigmoid neurons employ the sigmoid function (logistic function) to introduce smoothness in decision-making. Mathematically represented as: \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] where \\(z\\) denotes the weighted sum of inputs.\n\n\nSigmoid Function Properties\n\nAs \\(z\\) tends to positive infinity: \\(\\lim_{{z \\to \\infty}} \\sigma(z) = 1\\)\nAs \\(z\\) tends to negative infinity: \\(\\lim_{{z \\to -\\infty}} \\sigma(z) = 0\\)\nAt \\(W^T X = 0\\): \\(\\sigma(0) = \\frac{1}{2}\\)\n\nThe sigmoid function transforms outputs into the range [0, 1], facilitating a probabilistic interpretation.\n\n\nComparison with Perceptron\nContrasting with the perceptron function, the sigmoid function exhibits smoothness and continuity. The perceptron function lacks differentiability at the abrupt change in value, whereas the sigmoid function is differentiable."
  },
  {
    "objectID": "pages/DL/Week02.html#importance-of-differentiability",
    "href": "pages/DL/Week02.html#importance-of-differentiability",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Importance of Differentiability",
    "text": "Importance of Differentiability\nDifferentiability holds paramount importance for various machine learning algorithms, particularly in derivative-related operations. The application of calculus in neural network training and optimization is streamlined by the differentiability of the sigmoid neuron’s activation function."
  },
  {
    "objectID": "pages/DL/Week02.html#overview",
    "href": "pages/DL/Week02.html#overview",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Overview",
    "text": "Overview\nIn the realm of supervised machine learning, the fundamental objective is to comprehend the intricate structure of the setup, which encompasses various components crucial for effective model training. These components include the dataset, model representation, the learning algorithm, and the definition of an objective function."
  },
  {
    "objectID": "pages/DL/Week02.html#components",
    "href": "pages/DL/Week02.html#components",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Components",
    "text": "Components\n\nData Representation\nThe dataset, denoted as \\((x_i, y_i)\\), is pivotal to the learning process. Here, \\(x_i\\) signifies an \\(m\\)-dimensional input vector, while \\(y_i\\) represents a real-valued output associated with the given input. The dataset essentially comprises a collection of such input-output pairs.\n\n\nModel Assumption\nA critical assumption in this paradigm is that the output \\(y\\) is contingent upon the input \\(x\\), expressed as \\(y = f(x)\\). However, the specific form of the function \\(f\\) remains elusive, prompting the need for learning algorithms to discern it from the provided data.\n\nLearning Algorithm\nThe learning algorithm employed in this context is the Gradient Descent algorithm. This iterative approach facilitates the adjustment of model parameters, ensuring a continuous refinement of the model’s approximation.\n\n\n\nObjective Function (Loss Function)\nCentral to the learning process is the formulation of an objective function, commonly referred to as the Loss Function. Mathematically, it is defined as follows:\n\\[\\mathcal{L}(\\theta) = \\sum_{i=1}^{n} \\text{Difference}(y_{\\hat{i}}, y_i)\\]\nHere, \\(\\theta\\) denotes the parameters of the model, and \\(\\text{Difference}(y_{\\hat{i}}, y_i)\\) quantifies the dissimilarity between the predicted (\\(y_{\\hat{i}}\\)) and actual (\\(y_i\\)) values."
  },
  {
    "objectID": "pages/DL/Week02.html#objective-function-details",
    "href": "pages/DL/Week02.html#objective-function-details",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Objective Function Details",
    "text": "Objective Function Details\n\nDifference Function (Squared Error Loss)\nThe Difference Function, an integral component of the Loss Function, is expressed as:\n\\[\\text{Difference}(\\hat{y}, y) = (\\hat{y} - y)^2\\]\nThe squaring operation is implemented to ensure that both positive and negative errors contribute to the overall loss without canceling each other out."
  },
  {
    "objectID": "pages/DL/Week02.html#analogy-with-learning-trigonometry",
    "href": "pages/DL/Week02.html#analogy-with-learning-trigonometry",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Analogy with Learning Trigonometry",
    "text": "Analogy with Learning Trigonometry\n\nTraining Phase\nAnalogous to mastering a chapter in a textbook, the training phase strives for zero or minimal errors on the content encapsulated within the training dataset.\n\n\nValidation Phase\nResembling the solving of exercises at the end of a chapter, the validation phase allows for revisiting and enhancing comprehension based on additional exercises.\n\n\nTest Phase (Exam)\nThe test phase simulates a real-world scenario where the model encounters new data. Unlike the training and validation phases, there is no opportunity for revisiting and refining the learned information."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-1",
    "href": "pages/DL/Week02.html#introduction-1",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nSupervised machine learning involves the development of algorithms to learn parameters for a given model. This process aims to minimize the difference between predicted and actual values using a defined objective function. In this context, we explore a simplified model with one input, connected by weight (\\(w\\)), and a bias (\\(b\\)).\n\nModel Representation\nThe model is represented as \\(f(\\mathbf{x}) = -w \\mathbf{x} + b\\), where \\(\\mathbf{x}\\) is the input vector. The task is to determine an algorithm that learns the optimal values for \\(w\\) and \\(b\\) using training data.\n\n\nTraining Objective\nThe training objective involves minimizing the average difference between predicted values (\\(f(\\mathbf{x})\\)) and actual values (\\(y\\)) over all training points. The process requires finding the optimal \\(w\\) and \\(b\\) values that achieve this minimum loss."
  },
  {
    "objectID": "pages/DL/Week02.html#training-data",
    "href": "pages/DL/Week02.html#training-data",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Training Data",
    "text": "Training Data\nThe training data consists of pairs \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input, and \\(y\\) corresponds to the output. The loss function is defined as the average difference between predicted and actual values across all training points."
  },
  {
    "objectID": "pages/DL/Week02.html#loss-function",
    "href": "pages/DL/Week02.html#loss-function",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Loss Function",
    "text": "Loss Function\nThe loss function is expressed as:\n\\[\\mathcal{L}(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\left| f(\\mathbf{x}_i) - y_i \\right|\\]\nHere, \\(N\\) is the number of training points, \\(\\mathbf{x}_i\\) is the input for the \\(i\\)-th point, and \\(y_i\\) is the corresponding actual output."
  },
  {
    "objectID": "pages/DL/Week02.html#trial-and-error-approach",
    "href": "pages/DL/Week02.html#trial-and-error-approach",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Trial-and-Error Approach",
    "text": "Trial-and-Error Approach\nTo illustrate the concept, a trial-and-error approach is employed initially. Random values for \\(w\\) and \\(b\\) are chosen, and the loss is calculated. Adjustments are made iteratively to minimize the loss. This process involves systematically changing \\(w\\) and \\(b\\) values until an optimal solution is found.\n\nVisualization with Error Surface\nA 3D surface plot is used to visualize the loss in the \\(w-b\\) plane. This plot aids in identifying regions of low and high loss. However, the impracticality of exhaustively exploring this surface for large datasets is acknowledged due to computational constraints."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-2",
    "href": "pages/DL/Week02.html#introduction-2",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nThe transcript delves into the intricacies of parameter optimization, focusing on the goal of efficiently traversing the error surface to reach the minimum error. The parameters of interest, denoted as \\(\\theta\\), are expressed as vectors, specifically encompassing \\(W\\) and \\(B\\) in the context of a toy network."
  },
  {
    "objectID": "pages/DL/Week02.html#update-rule-with-conservative-movement",
    "href": "pages/DL/Week02.html#update-rule-with-conservative-movement",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Update Rule with Conservative Movement",
    "text": "Update Rule with Conservative Movement\nThe update rule for altering \\(\\theta\\) entails a meticulous adjustment of the parameters. The process involves taking a measured step, determined by a scalar \\(\\eta\\), in the direction of \\(\\Delta\\theta\\), which encapsulates the parameter changes. This introduces a level of conservatism in the parameter adjustments, promoting stability in the optimization process."
  },
  {
    "objectID": "pages/DL/Week02.html#taylor-series-for-function-approximation",
    "href": "pages/DL/Week02.html#taylor-series-for-function-approximation",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Taylor Series for Function Approximation",
    "text": "Taylor Series for Function Approximation\n\nOverview\nThe lecture introduces the Taylor series, a powerful mathematical tool for approximating functions that exhibit continuous differentiability. This method enables the representation of a function through polynomials, allowing for varying degrees of precision in the approximation.\n\n\nLinear Approximation\nLinear approximation entails the establishment of a tangent line at a specific point on the function. This approach provides an initial approximation, and the accuracy is contingent on the chosen neighborhood size, denoted as \\(\\varepsilon\\).\n\n\nQuadratic and Higher-Order Approximations\nQuadratic and higher-order approximations extend the accuracy of the approximation by incorporating additional terms. The lecture underscores the importance of selecting a small neighborhood for these approximations to maintain efficacy."
  },
  {
    "objectID": "pages/DL/Week02.html#extending-concepts-to-multiple-dimensions",
    "href": "pages/DL/Week02.html#extending-concepts-to-multiple-dimensions",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Extending Concepts to Multiple Dimensions",
    "text": "Extending Concepts to Multiple Dimensions\nThe discussion expands to functions with two variables, exemplifying how linear and quadratic approximations operate in multidimensional spaces. The lecture underscores the critical role of confined neighborhoods (\\(\\varepsilon\\)) in ensuring the precision of the Taylor series method across varying dimensions."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-3",
    "href": "pages/DL/Week02.html#introduction-3",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of optimization for machine learning models, the process of iteratively updating parameters to minimize a loss function is a fundamental concept. One key technique employed in this context is gradient descent. This discussion delves into the intricate mathematical foundations underpinning gradient descent, focusing on the decision criteria for parameter updates and the optimization of the update vector."
  },
  {
    "objectID": "pages/DL/Week02.html#taylor-series-expansion",
    "href": "pages/DL/Week02.html#taylor-series-expansion",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Taylor Series Expansion",
    "text": "Taylor Series Expansion\n\nObjective\nThe overarching objective is to determine an optimal change in parameters, denoted as \\(\\Delta\\theta\\) (represented as \\(\\mathbf{U}\\)), to minimize the loss function \\(\\mathcal{L}(\\theta)\\).\n\n\nLinear Approximation\nUtilizing the Taylor series, the loss function at a nearby point \\(\\theta + \\Delta\\theta\\) is approximated linearly as: \\[\\mathcal{L}(\\theta + \\Delta\\theta) \\approx \\mathcal{L}(\\theta) + \\eta\\mathbf{U}^T\\nabla \\mathcal{L}(\\theta)\\] Here, \\(\\eta\\) is a small positive scalar, ensuring a negligible difference."
  },
  {
    "objectID": "pages/DL/Week02.html#mathematical-aspects-of-gradient-descent",
    "href": "pages/DL/Week02.html#mathematical-aspects-of-gradient-descent",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Mathematical Aspects of Gradient Descent",
    "text": "Mathematical Aspects of Gradient Descent\n\nGradient\nThe gradient \\(\\nabla \\mathcal{L}(\\theta)\\) is introduced as a vector comprising partial derivatives of the loss function with respect to its parameters. For a function \\(y = W^2 + B^2\\) with two variables, the gradient is expressed as \\([2W, 2B]\\).\n\n\nSecond Order Derivative (Hessian)\nThe concept of the Hessian matrix, representing the second-order derivative, is introduced. This matrix provides insights into the curvature of the loss function. In the case of a two-variable function, the Hessian is illustrated as a \\(2\\times2\\) matrix."
  },
  {
    "objectID": "pages/DL/Week02.html#decision-criteria-for-parameter-updates",
    "href": "pages/DL/Week02.html#decision-criteria-for-parameter-updates",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Decision Criteria for Parameter Updates",
    "text": "Decision Criteria for Parameter Updates\n\nLinear Approximation and Criteria\nThe focus shifts to linear approximation, with higher-order terms neglected when \\(\\eta\\) is small. The decision criteria for a favorable parameter update is based on the condition: \\[\\eta\\mathbf{U}^T\\nabla \\mathcal{L}(\\theta) &lt; 0\\]"
  },
  {
    "objectID": "pages/DL/Week02.html#optimization-of-update-vector-mathbfu",
    "href": "pages/DL/Week02.html#optimization-of-update-vector-mathbfu",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Optimization of Update Vector \\(\\mathbf{U}\\)",
    "text": "Optimization of Update Vector \\(\\mathbf{U}\\)\n\nAngle \\(\\beta\\) and Cosine\nOptimizing the update vector involves considering the angle \\(\\beta\\) between \\(\\mathbf{U}\\) and the gradient vector. The cosine of \\(\\beta\\), denoted as \\(\\cos(\\beta)\\), is explored, and its range is discussed.\n\n\nOptimal Update for Maximum Descent\nIn the pursuit of maximum descent, the optimal scenario arises when \\(\\cos(\\beta) = -1\\), indicating that the angle \\(\\beta\\) is 180 degrees, signifying movement in the direction opposite to the gradient vector. This aligns with the well-known rule in gradient descent: “Move in the direction opposite to the gradient.”"
  },
  {
    "objectID": "pages/DL/Week02.html#overview-2",
    "href": "pages/DL/Week02.html#overview-2",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Overview",
    "text": "Overview\nIn the pursuit of optimizing the parameters of a sigmoid neuron, the lecture primarily delves into the application of the gradient descent algorithm. The primary objective is to minimize the associated loss function, thereby identifying optimal values for the neuron’s weights (\\(W\\)) and bias (\\(B\\))."
  },
  {
    "objectID": "pages/DL/Week02.html#key-concepts",
    "href": "pages/DL/Week02.html#key-concepts",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Key Concepts",
    "text": "Key Concepts\n\n1. Gradient Descent Rule\nThe gradient descent rule serves as an iterative optimization technique employed to minimize the loss function. The core update rule is defined as follows:\n\\[\nW = W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}, \\quad B = B - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial B}\n\\]\nThis iterative process aims to iteratively refine the parameters (\\(W\\) and \\(B\\)) based on the computed partial derivatives of the loss function.\n\n\n2. Derivative Computation\n\n2.1 Derivative of Loss with Respect to \\(W\\)\nThe partial derivative of the loss function with respect to weights (\\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\)) is computed through the application of the chain rule. In the context of the sigmoid function, the derivative is obtained as follows:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_i \\left(f(x_i) - y_i\\right) \\cdot f(x_i) \\cdot \\left(1 - f(x_i)\\right) \\cdot X_i\n\\]\nHere, \\(f(x_i)\\) represents the sigmoid function applied to the input \\(x_i\\) associated with data point \\(i\\).\n\n\n2.2 Derivative of Loss with Respect to \\(B\\)\nSimilarly, the partial derivative of the loss function with respect to bias (\\(\\frac{\\partial \\mathcal{L}}{\\partial B}\\)) is derived as:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial B} = \\sum_i \\left(f(x_i) - y_i\\right) \\cdot f(x_i) \\cdot \\left(1 - f(x_i)\\right)\n\\]\nThe introduction of \\(X_i\\) is omitted in this case, as it pertains to the bias term.\n\n\n\n3. Algorithm Execution\nThe algorithmic execution involves several key steps:\n\nInitialization:\n\nRandom initialization of weights (\\(W\\)) and bias (\\(B\\)).\nSetting the learning rate (\\(\\eta\\)) and maximum iterations.\n\nGradient Computation:\n\nIterating over all data points, computing the partial derivatives for \\(W\\) and \\(B\\) using the derived formulas.\n\nParameter Update:\n\nApplying the gradient descent update rule to iteratively adjust the weights and bias.\n\n\n\n\n4. Loss Surface Visualization\nThe lecture introduces the concept of visualizing the loss function surface in the \\(W-B\\) plane. This visual aid illustrates the algorithm’s movement along the surface, consistently reducing the loss.\n\n\n5. Observations\nThe lecture emphasizes crucial observations:\n\nLoss Reduction:\n\nEnsuring that at each iteration, the algorithm systematically decreases the loss.\n\nHyperparameter Impact:\n\nAcknowledging the influence of the learning rate (\\(\\eta\\)) on convergence and potential overshooting.\n\nExperimentation:\n\nEncouraging experimentation with diverse initializations and learning rates for a comprehensive understanding."
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-4",
    "href": "pages/DL/Week02.html#introduction-4",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nThe representation power of a multi-layer network, particularly employing sigmoid neurons, is the focal point of this discussion. The objective is to establish a theorem analogous to the one developed for perceptrons, specifically emphasizing the network’s capability to approximate any continuous function."
  },
  {
    "objectID": "pages/DL/Week02.html#universal-approximation-theorem",
    "href": "pages/DL/Week02.html#universal-approximation-theorem",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\nThe Universal Approximation Theorem posits that a multi-layer network with a single hidden layer possesses the capacity to approximate any continuous function with precision. This approximation is achieved by manipulating the weights and biases associated with the sigmoid neurons within the hidden layer."
  },
  {
    "objectID": "pages/DL/Week02.html#tower-functions-illustration",
    "href": "pages/DL/Week02.html#tower-functions-illustration",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Tower Functions Illustration",
    "text": "Tower Functions Illustration\nTo illustrate the approximation process, the concept of towers of functions is introduced. This entails deconstructing an arbitrary function into a summation of tower functions, wherein each tower is represented by sigmoid neurons. The amalgamation of these towers serves to approximate the original function."
  },
  {
    "objectID": "pages/DL/Week02.html#tower-construction-process",
    "href": "pages/DL/Week02.html#tower-construction-process",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Tower Construction Process",
    "text": "Tower Construction Process\nThe construction of towers involves the utilization of sigmoid neurons with exceptionally high weights, approaching infinity. This strategic choice mimics step functions. By subtracting these step functions, a tower-like structure is formed. Notably, the width and position of the tower are modulated by adjusting the biases of the sigmoid neurons."
  },
  {
    "objectID": "pages/DL/Week02.html#tower-maker-neural-network",
    "href": "pages/DL/Week02.html#tower-maker-neural-network",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Tower Maker Neural Network",
    "text": "Tower Maker Neural Network\n\nArchitecture\nThe lecture introduces a neural network architecture termed the “Tower Maker.” This architecture comprises two sigmoid neurons characterized by high weights. The subtraction of their outputs yields a function resembling a tower.\n\n\nSigmoid Neuron Configuration\nThe sigmoid neurons within the Tower Maker are configured with exceedingly high weights, akin to infinity. This configuration transforms the sigmoid functions into step functions, pivotal in constructing tower-like shapes.\n\n\nBias Adjustment\nControl over the width and position of the tower is exercised through the manipulation of biases associated with the sigmoid neurons. Adjusting these biases ensures the customization of the tower function according to specific requirements."
  },
  {
    "objectID": "pages/DL/Week02.html#linear-function-integration",
    "href": "pages/DL/Week02.html#linear-function-integration",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Linear Function Integration",
    "text": "Linear Function Integration\nAn additional layer is incorporated into the Tower Maker architecture to integrate linear functions. This augmentation enhances the network’s ability to generate tower functions based on the input parameters."
  },
  {
    "objectID": "pages/DL/Week02.html#network-adjustment-for-precision",
    "href": "pages/DL/Week02.html#network-adjustment-for-precision",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Network Adjustment for Precision",
    "text": "Network Adjustment for Precision\nThe lecture underscores the correlation between the desired precision (represented by epsilon) and the network’s complexity. As the precision requirement increases, a more intricate network with an augmented number of neurons in the hidden layer becomes imperative. However, it is acknowledged that practical implementation may encounter challenges as the network’s size expands."
  },
  {
    "objectID": "pages/DL/Week02.html#single-input-function",
    "href": "pages/DL/Week02.html#single-input-function",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Single Input Function",
    "text": "Single Input Function\nConsider a function with a single input (\\(x\\)) plotted on the x-axis and corresponding output (\\(y\\)) on the y-axis. This introductory scenario involves the use of a sigmoid neuron function, denoted by:\n\\[f(x) = \\frac{1}{1 + e^{-(wx + b)}}\\]\nwhere:\n\n\\(w\\) represents the weight associated with the input.\n\\(b\\) is the bias term.\nThe sigmoid function smoothly transitions between 0 and 1."
  },
  {
    "objectID": "pages/DL/Week02.html#two-input-function",
    "href": "pages/DL/Week02.html#two-input-function",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Two Input Function",
    "text": "Two Input Function\nExpanding the scope to a two-input function, let’s consider an example related to oil mining, where salinity (\\(x_1\\)) and pressure (\\(x_2\\)) serve as inputs. The challenge is to establish a decision boundary separating points indicating the presence (orange) and absence (blue) of oil.\nA linear decision boundary proves inadequate, prompting the need for a more complex function."
  },
  {
    "objectID": "pages/DL/Week02.html#building-a-tower-in-2d",
    "href": "pages/DL/Week02.html#building-a-tower-in-2d",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Building a Tower in 2D",
    "text": "Building a Tower in 2D\nTo construct a tower-like structure, two sigmoid neurons are introduced, each handling one input (\\(x_1\\) and \\(x_2\\)). The sigmoid function takes the form:\n\\[f(x) = \\frac{1}{1 + e^{-(w_ix_i + b)}}\\]\nHere, \\(i\\) denotes the input index (1 or 2), \\(w_i\\) is the associated weight, and \\(b\\) is the bias term. Adjusting weights (\\(w_1\\) and \\(w_2\\)) results in step functions, dictating the slope of the tower in different directions.\nCombining these sigmoid neurons produces an open tower structure in one direction."
  },
  {
    "objectID": "pages/DL/Week02.html#closing-the-tower",
    "href": "pages/DL/Week02.html#closing-the-tower",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Closing the Tower",
    "text": "Closing the Tower\nTo enclose the tower from all sides, two additional sigmoid neurons (h13 and h14) are introduced. These neurons, with specific weight configurations, contribute to the formation of walls in different directions. Subtracting the outputs of these sigmoid neurons results in a structure with walls on all four sides but an open top."
  },
  {
    "objectID": "pages/DL/Week02.html#thresholding-to-get-a-closed-tower",
    "href": "pages/DL/Week02.html#thresholding-to-get-a-closed-tower",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Thresholding to Get a Closed Tower",
    "text": "Thresholding to Get a Closed Tower\nTo address the open top issue, thresholding is introduced. A sigmoid function with a switch-over point at 1 is applied to the structure’s output. This process retains only the portion of the structure above level 1, effectively closing the tower."
  },
  {
    "objectID": "pages/DL/Week02.html#extending-to-higher-dimensions",
    "href": "pages/DL/Week02.html#extending-to-higher-dimensions",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Extending to Higher Dimensions",
    "text": "Extending to Higher Dimensions\nGeneralizing this approach to n-dimensional inputs, the methodology remains consistent. For a single input (\\(x\\)), two neurons suffice; for two inputs (\\(x_1\\) and \\(x_2\\)), four neurons are necessary. The number of neurons in the middle layer increases with higher dimensions, extending the method to handle arbitrary functions."
  },
  {
    "objectID": "pages/DL/Week02.html#universal-approximation-theorem-1",
    "href": "pages/DL/Week02.html#universal-approximation-theorem-1",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\nThis construction aligns with the Universal Approximation Theorem, asserting that a neural network, given a sufficient number of neurons, can approximate any arbitrary function to a desired precision."
  },
  {
    "objectID": "pages/DL/Week02.html#implications-for-deep-learning",
    "href": "pages/DL/Week02.html#implications-for-deep-learning",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Implications for Deep Learning",
    "text": "Implications for Deep Learning\nThis methodology underscores the flexibility of deep neural networks in approximating complex functions encountered in real-world applications. The ability to systematically construct networks capable of representing intricate relationships contributes to the effectiveness of deep learning models."
  },
  {
    "objectID": "pages/DL/Week02.html#points-to-remember",
    "href": "pages/DL/Week02.html#points-to-remember",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nBoolean Functions and Linear Separability:\n\nPerceptrons face challenges with non-linearly separable boolean functions.\nMulti-layer perceptrons (MLPs) extend capabilities for handling complex functions.\n\nSigmoid Neurons and Differentiability:\n\nSigmoid neurons introduce smoothness in decision-making.\nDifferentiability is crucial for optimization in neural network training.\n\nGradient Descent: Mathematical Foundation:\n\nTaylor series expansion facilitates linear approximation in gradient descent.\nDecision criteria for parameter updates involve linear approximation conditions.\n\nTower Maker and Universal Approximation Theorem:\n\nThe Universal Approximation Theorem states that a single hidden layer in a neural network can approximate any continuous function.\nTower Maker architecture showcases the construction of towers using sigmoid neurons.\n\nDeep Learning Flexibility:\n\nDeep neural networks are flexible in approximating complex functions.\nThe Tower Maker architecture demonstrates the power of neural networks in constructing intricate representations.\n\n\nThis week’s exploration laid the groundwork for understanding the core principles and capabilities of neural networks, setting the stage for further exploration into advanced topics in deep learning."
  },
  {
    "objectID": "pages/AI/Week02.html",
    "href": "pages/AI/Week02.html",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "In the realm of Artificial Intelligence, the study of search algorithms plays a pivotal role in problem-solving strategies. These algorithms, designed to explore the state space of a given problem, can be categorized into brute force search, informed search, and a general algorithmic approach. To illustrate these concepts, we delve into the map coloring problem, showcasing various problem-solving strategies.\n\n\n\n\n\nState space search involves representing a problem as a graph, where each node represents a unique state, and edges denote possible moves between states. The primary goal of this course is to explore general search methods, incorporating heuristic techniques for improved efficiency. The methods under consideration include state space search and constraint processing.\n\n\n\n\n\nStates are representations of specific situations in a given problem. These states are treated as nodes within the search space graph, with each node denoted by a symbol, such as \\(s\\). The state space, essentially an implicit graph, is defined by a move generation function.\n\n\n\nA move generation function is critical in navigating the state space. It determines the possible moves from a given state, producing a set of neighboring states. In functional terms, this function, denoted as \\(MoveGen(s)\\), takes a state \\(s\\) as input and returns a set of states, or neighbors, achievable from the current state.\n\n\n\nThe exploration of the state space is facilitated by a search algorithm, which employs the move generation function to navigate through the graph. The algorithm terminates based on the results of a goal test function.\n\n\n\nThe goal test function, denoted as \\(GoalTest(s)\\), checks whether a given state \\(s\\) is the desired goal state. It serves as the criterion for terminating the search algorithm.\n\n\n\n\n\nGeneral search methods are designed to create adaptable algorithms capable of addressing a variety of problems. The two primary approaches discussed in this course are state space search and constraint processing, with a primary focus on the former.\n\n\n\n\n\n\n\nThe water jug problem involves three jugs with different capacities, requiring the measurement of a specific amount of water.\n\n\n\nStates are described as a list of three numbers, representing the water levels in each jug.\n\n\n\nMoves involve pouring water between jugs, and the goal test function is contingent on achieving the desired water measurement.\n\n\n\n\n\n\nThe eight puzzle, a two-dimensional puzzle, requires rearranging tiles to achieve a specific configuration.\n\n\n\nStates are represented by an 8-puzzle configuration, and moves involve sliding tiles into the blank space.\n\n\n\nThe goal test function checks whether the configuration matches the desired goal configuration.\n\n\n\n\n\n\nThis classic problem involves transporting individuals across a river without violating specific constraints.\n\n\n\nVarious representations are discussed, including objects on the left bank or based on the boat’s location.\n\n\n\nThe goal test function checks for a specific configuration that adheres to the constraints.\n\n\n\n\n\n\nThe N-Queens problem requires placing N queens on an N×N chessboard with no mutual attacks.\n\n\n\nSolving involves finding a valid arrangement of queens on the chessboard.\n\n\n\n\n\n\nThe traveling salesman problem involves finding the optimal tour, with the lowest cost, visiting each city exactly once and returning to the starting city.\n\n\n\nThe objective is to discover the tour with the lowest cost, a challenging problem with factorial time complexity.\n\n\n\n\n\n\nMaze solving requires finding a path through a maze from the entrance to the exit.\n\n\n\nThe maze can be represented as a graph, with each node representing a choice point.\n\n\n\nThe goal is to find a path through the maze to reach the exit."
  },
  {
    "objectID": "pages/AI/Week02.html#introduction-to-search-algorithms-in-ai",
    "href": "pages/AI/Week02.html#introduction-to-search-algorithms-in-ai",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "In the realm of Artificial Intelligence, the study of search algorithms plays a pivotal role in problem-solving strategies. These algorithms, designed to explore the state space of a given problem, can be categorized into brute force search, informed search, and a general algorithmic approach. To illustrate these concepts, we delve into the map coloring problem, showcasing various problem-solving strategies."
  },
  {
    "objectID": "pages/AI/Week02.html#state-space-search-1",
    "href": "pages/AI/Week02.html#state-space-search-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "State space search involves representing a problem as a graph, where each node represents a unique state, and edges denote possible moves between states. The primary goal of this course is to explore general search methods, incorporating heuristic techniques for improved efficiency. The methods under consideration include state space search and constraint processing.\n\n\n\n\n\nStates are representations of specific situations in a given problem. These states are treated as nodes within the search space graph, with each node denoted by a symbol, such as \\(s\\). The state space, essentially an implicit graph, is defined by a move generation function.\n\n\n\nA move generation function is critical in navigating the state space. It determines the possible moves from a given state, producing a set of neighboring states. In functional terms, this function, denoted as \\(MoveGen(s)\\), takes a state \\(s\\) as input and returns a set of states, or neighbors, achievable from the current state.\n\n\n\nThe exploration of the state space is facilitated by a search algorithm, which employs the move generation function to navigate through the graph. The algorithm terminates based on the results of a goal test function.\n\n\n\nThe goal test function, denoted as \\(GoalTest(s)\\), checks whether a given state \\(s\\) is the desired goal state. It serves as the criterion for terminating the search algorithm."
  },
  {
    "objectID": "pages/AI/Week02.html#search-algorithms-overview",
    "href": "pages/AI/Week02.html#search-algorithms-overview",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "General search methods are designed to create adaptable algorithms capable of addressing a variety of problems. The two primary approaches discussed in this course are state space search and constraint processing, with a primary focus on the former."
  },
  {
    "objectID": "pages/AI/Week02.html#sample-problems-in-state-space-search",
    "href": "pages/AI/Week02.html#sample-problems-in-state-space-search",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "The water jug problem involves three jugs with different capacities, requiring the measurement of a specific amount of water.\n\n\n\nStates are described as a list of three numbers, representing the water levels in each jug.\n\n\n\nMoves involve pouring water between jugs, and the goal test function is contingent on achieving the desired water measurement.\n\n\n\n\n\n\nThe eight puzzle, a two-dimensional puzzle, requires rearranging tiles to achieve a specific configuration.\n\n\n\nStates are represented by an 8-puzzle configuration, and moves involve sliding tiles into the blank space.\n\n\n\nThe goal test function checks whether the configuration matches the desired goal configuration.\n\n\n\n\n\n\nThis classic problem involves transporting individuals across a river without violating specific constraints.\n\n\n\nVarious representations are discussed, including objects on the left bank or based on the boat’s location.\n\n\n\nThe goal test function checks for a specific configuration that adheres to the constraints.\n\n\n\n\n\n\nThe N-Queens problem requires placing N queens on an N×N chessboard with no mutual attacks.\n\n\n\nSolving involves finding a valid arrangement of queens on the chessboard.\n\n\n\n\n\n\nThe traveling salesman problem involves finding the optimal tour, with the lowest cost, visiting each city exactly once and returning to the starting city.\n\n\n\nThe objective is to discover the tour with the lowest cost, a challenging problem with factorial time complexity.\n\n\n\n\n\n\nMaze solving requires finding a path through a maze from the entrance to the exit.\n\n\n\nThe maze can be represented as a graph, with each node representing a choice point.\n\n\n\nThe goal is to find a path through the maze to reach the exit."
  },
  {
    "objectID": "pages/AI/Week02.html#introduction",
    "href": "pages/AI/Week02.html#introduction",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Introduction",
    "text": "Introduction\nIn the pursuit of developing domain-independent problem-solving algorithms within the realm of artificial intelligence (AI), the focus is on general search algorithms. These algorithms aim to provide solutions to diverse problems in a domain-independent form. This discussion revolves around two key algorithms: “Simple Search 1” and its modification, “Simple Search 2.”"
  },
  {
    "objectID": "pages/AI/Week02.html#components-of-state-space-search-1",
    "href": "pages/AI/Week02.html#components-of-state-space-search-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Components of State Space Search",
    "text": "Components of State Space Search\n\n1. Start and Goal States\nThe state space comprises a set of states, a defined start state, and a specified goal state. These elements form the foundational framework for problem-solving in AI.\n\n\n2. Move Gen Function\nThe move generation function, denoted as $ (n) $, serves as a domain-specific function responsible for generating the neighbors of a given node $ n $. Importantly, it dynamically constructs the graph as the algorithm progresses.\n\n\n3. Goal Test Function\nThe goal test function, $ (n) $, determines whether a given state $ n $ aligns with the defined goal state. This function plays a crucial role in assessing the success of the search algorithm.\n\n\n4. Scene Nodes and Candidate Nodes\nIn the process of state space search, two categories of nodes emerge: scene nodes and candidate nodes. - Scene Nodes: These nodes represent states that have been visited and tested for the goal. They are stored in a set or list termed “closed.” - Candidate Nodes: Generated by the move gen function, these nodes are candidates for exploration but have not yet been visited. They are stored in a set or list referred to as “open.”"
  },
  {
    "objectID": "pages/AI/Week02.html#simple-search-1-algorithm",
    "href": "pages/AI/Week02.html#simple-search-1-algorithm",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Simple Search 1 Algorithm",
    "text": "Simple Search 1 Algorithm\n\nAlgorithm Overview\nThe “Simple Search 1” algorithm adheres to the generate-and-test approach, a fundamental strategy in AI problem-solving. The algorithm iteratively generates nodes, tests them for the goal, and continues until either the goal is found or the open set becomes empty.\n\n\nNode Selection\nA node is selected from the open set. If this node corresponds to the goal state, the algorithm terminates successfully. Otherwise, the process continues.\n\n\nGraph Exploration\nThe algorithm leverages the move gen function to generate neighbors of the selected node. These generated nodes are then added to the open set for further exploration.\n\n\nPseudocode for Simple Search 1\nOPEN ← {S}\nwhile OPEN is not empty\n   Pick some node N from OPEN\n   OPEN ← OPEN - {N}\n   if GoalTest(N) = TRUE\n     return N\n   else \n     OPEN ← OPEN ∪ MoveGen(N)\nreturn null\n\n\nChallenge - Cyclic Exploration\nA notable challenge with “Simple Search 1” is its susceptibility to entering cycles, leading to the revisiting of nodes without making progress. This cyclic exploration issue poses a potential impediment to the algorithm’s effectiveness."
  },
  {
    "objectID": "pages/AI/Week02.html#simple-search-2-algorithm",
    "href": "pages/AI/Week02.html#simple-search-2-algorithm",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Simple Search 2 Algorithm",
    "text": "Simple Search 2 Algorithm\n\nIntroduction of Closed Set\nTo address the cyclic exploration problem, “Simple Search 2” introduces a new set named “closed.” This set serves as a repository for scene nodes, preventing their reevaluation during the search process.\n\n\nPurpose of Closed Set\nThe closed set’s primary function is to avoid revisiting nodes already assessed for the goal. By maintaining a record of scene nodes, the algorithm reduces the search space and mitigates the cyclic exploration challenge.\n\n\nAlgorithm Adjustment\nThe node selected from the open set is now moved to the closed set before testing for the goal. Additionally, during the generation of neighbors, nodes already present in the closed set are excluded from being added to the open set.\n\n\nPseudocode for Simple Search 2\nOPEN ← {S}\nCLOSED ← empty set\nwhile OPEN is not empty\n   Pick some node N from OPEN\n   OPEN ← OPEN – {N}\n   CLOSED ← CLOSED ∪ {N}\n   if GoalTest(N) = TRUE \n     return N \n   else \n     OPEN ← OPEN ∪ (MoveGen(N) – CLOSED)\nreturn null  \n\n\nImproved Exploration\n“Simple Search 2” demonstrates enhanced efficiency by minimizing the search space. The exclusion of nodes already visited contributes to a more focused exploration, addressing the cyclic exploration issue encountered in “Simple Search 1.”"
  },
  {
    "objectID": "pages/AI/Week02.html#consideration---solution-path",
    "href": "pages/AI/Week02.html#consideration---solution-path",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Consideration - Solution Path",
    "text": "Consideration - Solution Path\nWhile both algorithms aim to find the goal node, it’s essential to note that they do not provide the solution path. The goal test confirms the existence of a solution without specifying the sequence of states leading to it. Further considerations may be necessary to obtain the complete solution path."
  },
  {
    "objectID": "pages/AI/Week02.html#impact-of-algorithm-choice",
    "href": "pages/AI/Week02.html#impact-of-algorithm-choice",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Impact of Algorithm Choice",
    "text": "Impact of Algorithm Choice\nThe choice of algorithm significantly influences the exploration of the search space. Different algorithms may yield distinct search spaces for the same state space. The efficiency and effectiveness of the search process hinge on the algorithm’s ability to circumvent cyclic exploration and avoid unnecessary node revisits."
  },
  {
    "objectID": "pages/AI/Week02.html#problem-classification",
    "href": "pages/AI/Week02.html#problem-classification",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Problem Classification",
    "text": "Problem Classification\nIn the realm of state space search, two distinctive problem types emerge: Configuration Problems and Planning Problems.\n\nConfiguration Problems\nConfiguration problems involve seeking a state that satisfies a given description. For instance, classic problems like the N-Queens puzzle, Sudoku, Map Coloring, and others fall into this category. The primary objective is to identify a state that adheres to the specified criteria.\n\n\nPlanning Problems\nContrarily, planning problems revolve around scenarios where the goal is either explicitly known or described, and the pursuit is directed towards determining the optimal path to that goal. This type includes real-world situations such as finding a suitable restaurant, where the algorithm must discern both the destination and the most efficient route."
  },
  {
    "objectID": "pages/AI/Week02.html#graph-representation",
    "href": "pages/AI/Week02.html#graph-representation",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Graph Representation",
    "text": "Graph Representation\nIn the context of state space search, the graph serves as the fundamental model. Each node within this graph represents a unique state. However, in planning problems, the goal extends beyond merely reaching the final state; it includes the necessity to ascertain the path leading to that state.\n\nNode Pairs\nTo address this, the concept of node pairs is introduced. In this representation, every node is accompanied by information about its parent node. This augmentation proves pivotal when reconstructing the path to the goal."
  },
  {
    "objectID": "pages/AI/Week02.html#path-reconstruction",
    "href": "pages/AI/Week02.html#path-reconstruction",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Path Reconstruction",
    "text": "Path Reconstruction\nEfficient path reconstruction relies on the inclusion of node pairs within the search space. As the algorithm traverses the search space and identifies the goal state, the closed list—housing node pairs—facilitates the backward tracing of the path. Each node pair encapsulates information about the current node and its parent, enabling a step-by-step reconstruction."
  },
  {
    "objectID": "pages/AI/Week02.html#search-algorithm-overview",
    "href": "pages/AI/Week02.html#search-algorithm-overview",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Search Algorithm Overview",
    "text": "Search Algorithm Overview\nThe overarching search algorithm is designed to systematically explore the search space, attempting different paths until a viable route to the goal state is discovered.\n\nDeterministic Approach\nIn contrast to the initial non-deterministic approach of picking any node from the open set, the algorithm undergoes a modification. It transitions to a deterministic strategy, consistently selecting the node positioned at the head of the open list.\n\n\nList Structure\nThe traditional use of sets for open and closed is superseded by the adoption of lists. This shift is accompanied by a preference for adding new nodes to a specified location in the list, influencing their order and impact on the search algorithm."
  },
  {
    "objectID": "pages/AI/Week02.html#notational-conventions",
    "href": "pages/AI/Week02.html#notational-conventions",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Notational Conventions",
    "text": "Notational Conventions\n\nList Notation\n\nThe empty list is represented as square brackets: \\([]\\).\nOperations include the colon operator for adding an element to the head of a list and the plus plus operator for appending two lists.\nEssential functions, such as head and tail, serve in extracting elements and conducting tests.\n\n\n\nTuple Notation\nTuples, denoted by parentheses, accommodate ordered elements. Accessing tuple elements involves positional identification or leveraging built-in functions like first and second.\nFor further reference on operations and functions, refer to this pdf."
  },
  {
    "objectID": "pages/AI/Week02.html#algorithm-refinement",
    "href": "pages/AI/Week02.html#algorithm-refinement",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Algorithm Refinement",
    "text": "Algorithm Refinement\nThe transition from non-deterministic node selection to a deterministic strategy represents a pivotal refinement. This evolution ensures the consistent selection of the node residing at the forefront of the open list. Additionally, the determination of where new nodes are inserted in the list assumes significance, shaping their influence on the algorithm’s behavior."
  },
  {
    "objectID": "pages/AI/Week02.html#initialization",
    "href": "pages/AI/Week02.html#initialization",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Initialization",
    "text": "Initialization\n- OPEN ← (S, null) : []\n- CLOSED ← empty list\nThe algorithm starts with an open list containing the start node (S, null) where S is the start node, and null represents the absence of a parent. The CLOSED list is initially empty."
  },
  {
    "objectID": "pages/AI/Week02.html#main-algorithm",
    "href": "pages/AI/Week02.html#main-algorithm",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Main Algorithm",
    "text": "Main Algorithm\n- while OPEN is not empty\n  - nodePair ← head OPEN\n  - (N, _) ← nodePair\n  - if GoalTest(N) = TRUE\n    - return RECONSTRUCTPATH(nodePair, CLOSED)\n  - else CLOSED ← nodePair : CLOSED\n    - neighbours ← MoveGen(N)\n    - newNodes ← REMOVESEEN(neighbours, OPEN, CLOSED)\n    - newPairs ← MAKEPAIRS(newNodes, N)\n    - OPEN ← newPairs ++ (tail OPEN)\n- return empty list\nThe algorithm iteratively selects the first element from the open list and explores the node (N, _). If the goal test is satisfied, it calls the RECONSTRUCTPATH function. Otherwise, it adds the node pair to the closed list, generates and filters the children using REMOVESEEN, creates pairs with parents using MAKEPAIRS, and appends them to the front of the open list."
  },
  {
    "objectID": "pages/AI/Week02.html#ancillary-functions",
    "href": "pages/AI/Week02.html#ancillary-functions",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Ancillary Functions",
    "text": "Ancillary Functions\n\nRECONSTRUCTPATH Function\n- RECONSTRUCTPATH(nodePair, CLOSED)\n  - SKIPTO(parent, nodePairs)\n    - if parent = first head nodePairs\n      - return nodePairs\n    - else return SKIPTO(parent, tail nodePairs)\n  - (node, parent) ← nodePair\n  - path ← node : []\n  - while parent is not null\n    - path ← parent : path\n    - CLOSED ← SKIPTO(parent, CLOSED)\n    - (_, parent) ← head CLOSED\n  - return path\nThe RECONSTRUCTPATH function traces back from the goal node to the start node using parent pointers stored in the CLOSED list.\n\n\nMAKEPAIRS Function\n- MAKEPAIRS(nodeList, parent)\n  - if nodeList is empty\n    - return empty list\n  - else return (head nodeList, parent) : MAKEPAIRS(tail nodeList, parent)\nThe MAKEPAIRS function takes a list of nodes and a parent, creating pairs with each node and the given parent.\n\n\nREMOVESEEN Function\n- REMOVESEEN(nodeList, OPEN, CLOSED)\n  - if nodeList is empty\n    - return empty list\n  - else node ← head nodeList\n    - if OCCURSIN(node, OPEN) or OCCURSIN(node, CLOSED)\n      - return REMOVESEEN(tail nodeList, OPEN, CLOSED)\n    - else return node : REMOVESEEN(tail nodeList, OPEN, CLOSED)\nThe REMOVESEEN function filters out nodes already present in the OPEN or CLOSED lists."
  },
  {
    "objectID": "pages/AI/Week02.html#initialization-1",
    "href": "pages/AI/Week02.html#initialization-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Initialization",
    "text": "Initialization\n- OPEN ← (S, null) : []\n- CLOSED ← empty list\nSimilar to DFS, BFS starts with an open list containing the start node (S, null) and an empty CLOSED list."
  },
  {
    "objectID": "pages/AI/Week02.html#main-algorithm-1",
    "href": "pages/AI/Week02.html#main-algorithm-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Main Algorithm",
    "text": "Main Algorithm\n- while OPEN is not empty\n  - nodePair ← head OPEN\n  - (N, _) ← nodePair\n  - if GoalTest(N) = TRUE\n    - return RECONSTRUCTPATH(nodePair, CLOSED)\n  - else CLOSED ← nodePair : CLOSED\n    - neighbours ← MoveGen(N)\n    - newNodes ← REMOVESEEN(neighbours, OPEN, CLOSED)\n    - newPairs ← MAKEPAIRS(newNodes, N)\n    - OPEN ← (tail OPEN) ++ newPairs\n- return empty list\nThe main algorithm for BFS is identical to DFS, except for the addition of new nodes to the end of the OPEN list."
  },
  {
    "objectID": "pages/AI/Week02.html#analysis-of-dfs",
    "href": "pages/AI/Week02.html#analysis-of-dfs",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Analysis of DFS",
    "text": "Analysis of DFS\n\nOverview\nDepth First Search (DFS) is a search algorithm employed in problem-solving within the field of Artificial Intelligence. It is characterized by its treatment of the open set as a stack, following the Last In, First Out (LIFO) principle.\n\n\nExploration Strategy\nDFS explores the search tree in a deep-first manner, descending into the tree until it reaches a dead end. Upon encountering a dead end, the algorithm backtracks to explore alternative paths.\n\n\nBehavior\nDFS tends to find paths that are farther from the source node, emphasizing deep exploration rather than a systematic examination of all possibilities. It exhibits a distinct behavior of diving deep into the search tree.\n\n\nTime Complexity\nThe time complexity of DFS is exponential and can be expressed as \\(O(b^d)\\), where \\(b\\) represents the branching factor of the search tree, and \\(d\\) is the depth. This exponential growth can lead to infinite loops in scenarios with infinite search spaces.\n\n\nSpace Complexity\nDFS demonstrates linear space complexity. The space required is proportional to the depth of the search tree, making it more space-efficient compared to other algorithms with exponential space growth."
  },
  {
    "objectID": "pages/AI/Week02.html#analysis-of-bfs",
    "href": "pages/AI/Week02.html#analysis-of-bfs",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Analysis of BFS",
    "text": "Analysis of BFS\n\nOverview\nBreadth First Search (BFS) is another search algorithm used in problem-solving for Artificial Intelligence. Unlike DFS, BFS treats the open set as a queue, adhering to the First In, First Out (FIFO) principle.\n\n\nExploration Strategy\nBFS explores the search tree level by level, starting from the source node and moving outward systematically. It ensures a conservative approach by prioritizing paths closer to the source.\n\n\nBehavior\nBFS is designed to find paths that are closer to the source node, ensuring a more methodical exploration of the search tree. It guarantees the discovery of the shortest path due to its systematic approach.\n\n\nTime Complexity\nSimilar to DFS, BFS exhibits exponential time complexity, expressed as \\(O(b^d)\\), where \\(b\\) is the branching factor, and \\(d\\) is the depth. However, BFS explores paths of increasing length systematically, ensuring the identification of the shortest path.\n\n\nSpace Complexity\nBFS has exponential space complexity, with the size of the open set growing exponentially. This makes BFS less space-efficient compared to DFS, but it guarantees finding the shortest path."
  },
  {
    "objectID": "pages/AI/Week02.html#comparison",
    "href": "pages/AI/Week02.html#comparison",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Comparison",
    "text": "Comparison\n\nTime Complexity\nBoth DFS and BFS share exponential time complexity, posing challenges in scenarios with large search trees.\n\n\nSpace Complexity\nDFS outperforms BFS in terms of space efficiency, having linear space complexity compared to BFS’s exponential growth.\n\n\nQuality of Solution\nDFS does not guarantee the shortest path, while BFS ensures the identification of the shortest path due to its systematic exploration.\n\n\nCompleteness\nDFS may not be complete, especially in infinite search spaces, where it can get lost in infinite paths. On the other hand, BFS is complete, provided there exists a path of finite length from the source to the goal."
  },
  {
    "objectID": "pages/AI/Week02.html#search-space-characteristics-and-solution-strategies",
    "href": "pages/AI/Week02.html#search-space-characteristics-and-solution-strategies",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Search Space Characteristics and Solution Strategies",
    "text": "Search Space Characteristics and Solution Strategies\n\nInfinite Search Space Dilemma\nWhen confronted with an infinite search space, the choice between Depth-First Search (DFS) and Breadth-First Search (BFS) becomes contingent upon the problem’s specifics. BFS is the preferred option if the search space is infinite but a solution is known to exist. Conversely, DFS might be more suitable if the search space is finite, albeit without guaranteeing the shortest path."
  },
  {
    "objectID": "pages/AI/Week02.html#depth-bounded-depth-first-search",
    "href": "pages/AI/Week02.html#depth-bounded-depth-first-search",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Depth-Bounded Depth-First Search",
    "text": "Depth-Bounded Depth-First Search\n\nStrategy Overview\nDepth-Bounded Depth-First Search strikes a balance between the characteristics of DFS and BFS. It limits the exploration depth, ensuring linear space complexity while compromising on completeness and the guarantee of finding the shortest path. The algorithm delves into the search space up to a specified depth, potentially missing the goal if it exceeds this depth."
  },
  {
    "objectID": "pages/AI/Week02.html#depth-bounded-dfs-with-node-counting",
    "href": "pages/AI/Week02.html#depth-bounded-dfs-with-node-counting",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Depth-Bounded DFS with Node Counting",
    "text": "Depth-Bounded DFS with Node Counting\n\nEnhanced Exploration\nAn augmentation to Depth-Bounded DFS involves incorporating node counting during the search process. This count of visited nodes provides additional insights, proving advantageous in certain problem scenarios and facilitating subsequent analysis."
  },
  {
    "objectID": "pages/AI/Week02.html#depth-first-iterative-deepening-dfid",
    "href": "pages/AI/Week02.html#depth-first-iterative-deepening-dfid",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Depth-First Iterative Deepening (DFID)",
    "text": "Depth-First Iterative Deepening (DFID)\n\nIterative Depth Expansion\nDFID emerges as a solution that combines the strengths of DFS and BFS. It iteratively increases the depth limit for DFS until a solution is encountered. The algorithm mitigates the risk of failing to find a path due to depth constraints but introduces the challenge of revisiting nodes multiple times. The careful tracking of node counts prevents infinite loops and enhances overall efficiency."
  },
  {
    "objectID": "pages/AI/Week02.html#path-reconstruction-challenges",
    "href": "pages/AI/Week02.html#path-reconstruction-challenges",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Path Reconstruction Challenges",
    "text": "Path Reconstruction Challenges\n\nDilemma Overview\nPath reconstruction poses challenges, particularly when multiple paths to the goal exist. The lecture delves into the complexities of maintaining closed lists and the importance of judiciously selecting parents during the path reconstruction process."
  },
  {
    "objectID": "pages/AI/Week02.html#dfid-in-chess-programming",
    "href": "pages/AI/Week02.html#dfid-in-chess-programming",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "DFID in Chess Programming",
    "text": "DFID in Chess Programming\n\nTactical Application\nDFID finds practical application in chess programming, particularly in scenarios where players face time constraints. The algorithm’s iterative deepening approach accommodates the limited time available for move selection."
  },
  {
    "objectID": "pages/AI/Week02.html#combinatorial-explosion-and-dfid",
    "href": "pages/AI/Week02.html#combinatorial-explosion-and-dfid",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Combinatorial Explosion and DFID",
    "text": "Combinatorial Explosion and DFID\n\nCoping with Exponential Growth\nThe lecture acknowledges the pervasive issue of combinatorial explosion, where search trees exhibit exponential growth. DFID addresses this challenge by iteratively searching with incrementally expanding depth limits. An in-depth analysis delves into the trade-offs between time and space, revealing the algorithm’s resilience in the face of increasing complexities."
  },
  {
    "objectID": "pages/AI/Week02.html#blind-uninformed-search",
    "href": "pages/AI/Week02.html#blind-uninformed-search",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Blind (Uninformed) Search",
    "text": "Blind (Uninformed) Search\n\nFixed Behaviors\nBlind searches, including DFS, BFS, and DFID, are characterized as uninformed strategies. These approaches lack awareness of the goal’s location during exploration, adhering to predetermined behaviors irrespective of the goal’s position."
  },
  {
    "objectID": "pages/AI/Week02.html#dfid-n-dfid-with-node-reopening",
    "href": "pages/AI/Week02.html#dfid-n-dfid-with-node-reopening",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "DFID-N: DFID with Node Reopening",
    "text": "DFID-N: DFID with Node Reopening\nDFID-N opens only new nodes (nodes not already present in OPEN/CLOSED) and does not reopen any nodes. It aims to find the solution with linear space complexity.\n\nDFID-N(\\(s\\))\ncount ← -1\npath ← empty list\ndepthBound ← 0\n\nrepeat \n    previousCount ← count \n    (count, path) ← DB-DFS-N(s, depthBound)\n    depthBound ← depthBound + 1 \nuntil (path is not empty) or (previousCount = count)\n\nreturn path\n\n\nDB-DFS-N(\\(s\\), depthBound)\n\nOpens only new nodes, i.e., nodes neither in OPEN nor in CLOSED.\nDoes not reopen any nodes.\n\ncount ← 0 \nOPEN ← (s, null, 0): []\nCLOSED ← empty list \n\nwhile OPEN is not empty \n    nodePair ← head OPEN \n    (N, _, depth)← nodePair \n    \n    if GoalTest(N) == TRUE \n        return (count, ReconstructPath(nodePair, CLOSED))\n    \n    else CLOSED← nodePair : CLOSED \n    \n    if depth &lt; depthBound \n        neighbours ← MoveGen(N)\n        newNodes ← SEE(neighbours, OPEN, CLOSED)\n        newPairs ← MAKEPAIRS(newNodes, N, depth + 1 )\n        OPEN ← newPairs ++ tail OPEN \n        \n        count ← count + length newPairs\n    \n    else OPEN = tail OPEN \n\nreturn (count, empty list)"
  },
  {
    "objectID": "pages/AI/Week02.html#dfid-c-dfid-with-closed-node-reopening",
    "href": "pages/AI/Week02.html#dfid-c-dfid-with-closed-node-reopening",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "DFID-C: DFID with Closed Node Reopening",
    "text": "DFID-C: DFID with Closed Node Reopening\nDFID-C opens new nodes (nodes not already present in OPEN/CLOSED) and also reopens nodes present in CLOSED but not present in OPEN.\n\nDFID-C(\\(s\\))\ncount ← -1\npath ← empty list\ndepthBound ← 0\n\nrepeat \n    previousCount ← count \n    (count, path) ← DB-DFS-C(s, depthBound)\n    depthBound ← depthBound + 1 \nuntil (path is not empty) or (previousCount = count)\n\nreturn path\n\n\nDB-DFS-C(\\(s\\), depthBound)\n\nOpens new nodes, i.e., nodes neither in OPEN nor in CLOSED.\nReopens nodes present in CLOSED and not present in OPEN.\n\ncount ← 0 \nOPEN ← (s, null, 0): []\nCLOSED ← empty list \n\nwhile OPEN is not empty \n    nodePair ← head OPEN \n    (N, _, depth)← nodePair \n    \n    if GoalTest(N) == TRUE \n        return (count, ReconstructPath(nodePair, CLOSED))\n    \n    else CLOSED ← nodePair : CLOSED \n    \n    if depth &lt; depthBound \n        neighbours ← MoveGen(N)\n        newNodes ← SEE(neighbours, OPEN, CLOSED)\n        newPairs ← MAKEPAIRS(newNodes, N, depth + 1 )\n        OPEN ← newPairs ++ tail OPEN \n        \n        count ← count + length newPairs\n    \n    else OPEN = tail OPEN \n\nreturn (count, empty list)"
  },
  {
    "objectID": "pages/AI/Week02.html#ancillary-functions-for-dfid-c",
    "href": "pages/AI/Week02.html#ancillary-functions-for-dfid-c",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Ancillary Functions for DFID-C",
    "text": "Ancillary Functions for DFID-C\n\nMAKEPAIRS(nodeList, parent, depth)\n\nCreates node pairs from the given node list, parent, and depth.\nReturns a list of node pairs.\n\nif nodeList is empty\n    return empty list\nelse nodePair ← (head nodeList, parent, depth)\n    return nodePair : MAKEPAIRS(tail nodeList, parent, depth)\n\n\nRECONSTRUCTPATH(nodePair, CLOSED)\n\nReconstructs the path using the given node pair and CLOSED list.\nReturns the reconstructed path.\n\nSKIPTo(parent, nodePairs, depth)\n    if (parent, ..., depth) = head nodePairs\n        return nodePairs\n    else return SKIPTo(parent, tail nodePairs, depth)\n\n(node, parent, depth) ← nodePair\npath ← node : []\n\nwhile parent is not null \n    path ← parent : path \n    CLOSED ← SKIPTo(parent, CLOSED, depth − 1 )\n    (_, _, parent, depth) ← head CLOSED \n\nreturn path"
  },
  {
    "objectID": "pages/AI/Week02.html#points-to-remember",
    "href": "pages/AI/Week02.html#points-to-remember",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nState Space Search Overview:\n\nState space search involves representing problems as graphs, where nodes represent unique states and edges denote possible moves.\nComponents include state representation, move generation, state space exploration, and goal test.\n\nSearch Algorithms:\n\nVarious search algorithms, such as DFS and BFS, offer different exploration strategies and have implications for time and space complexity.\nDFID combines the strengths of DFS and BFS, iteratively increasing depth limits.\n\nAlgorithmic Variations:\n\nDFID-N opens only new nodes, aiming for linear space complexity.\nDFID-C reopens nodes in CLOSED, providing a balance between space complexity and optimality.\n\nAncillary Functions:\n\nAncillary functions like RECONSTRUCTPATH play a crucial role in path reconstruction for algorithms like DFID.\n\nReal-World Applications:\n\nAlgorithms like DFID find practical applications in chess programming, demonstrating adaptability in time-constrained scenarios.\n\nCombinatorial Explosion and Optimization:\n\nCombinatorial explosion is addressed by iterative deepening approaches like DFID, balancing time and space considerations.\n\nBlind (Uninformed) Search:\n\nBlind searches, including DFS, BFS, and DFID, lack knowledge of the goal’s location during exploration.\n\nPath Reconstruction Challenges:\n\nPath reconstruction challenges arise, especially when multiple paths to the goal exist, emphasizing the importance of closed lists.\n\nDepth-Bounded DFS with Node Counting:\n\nDepth-Bounded DFS with node counting provides insights into the number of visited nodes during exploration.\n\nConfigurations and Planning Problems:\n\nState space search involves configuration problems (satisfying criteria) and planning problems (finding optimal paths to a known goal).\n\n\nThese key points collectively contribute to a comprehensive understanding of state space search algorithms, their variations, and their applications in artificial intelligence problem-solving."
  },
  {
    "objectID": "pages/ST/Week01.html",
    "href": "pages/ST/Week01.html",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes.\n\n\n\n\n\nThe initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary.\n\n\n\n\n\n\nThe V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints.\n\n\n\n\nBeyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances.\n\n\n\n\n\nIntegral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "href": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes."
  },
  {
    "objectID": "pages/ST/Week01.html#phases-of-sdlc",
    "href": "pages/ST/Week01.html#phases-of-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary."
  },
  {
    "objectID": "pages/ST/Week01.html#sdlc-models",
    "href": "pages/ST/Week01.html#sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints."
  },
  {
    "objectID": "pages/ST/Week01.html#other-sdlc-models",
    "href": "pages/ST/Week01.html#other-sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Beyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances."
  },
  {
    "objectID": "pages/ST/Week01.html#umbrella-activities",
    "href": "pages/ST/Week01.html#umbrella-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Integral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction",
    "href": "pages/ST/Week01.html#introduction",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Testing\nSoftware testing is a comprehensive process involving the scrutiny of various artifacts, including code, design, architecture documents, and requirements documents. The core objective is to validate and verify these artifacts, ensuring the software’s reliability and functionality.\n\n\nGoals of Software Testing\nThe overarching goals encompass providing an unbiased, independent assessment of the software, verifying its compliance with business capabilities, and evaluating associated risks that may impact its performance."
  },
  {
    "objectID": "pages/ST/Week01.html#standard-glossary",
    "href": "pages/ST/Week01.html#standard-glossary",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Standard Glossary",
    "text": "Standard Glossary\n\nVerification: This process determines whether the products meet specified requirements at various stages of the software development life cycle.\nValidation: Evaluation of the software at the end of the development phase, ensuring it aligns with standards and intended usage.\nFault: A static defect within the software, often originating from a mistake made during development.\nFailure: The visible, external manifestation of incorrect behavior resulting from a fault.\nError: The incorrect state of the program when a failure occurs, indicating a deviation from the intended behavior."
  },
  {
    "objectID": "pages/ST/Week01.html#historical-perspective",
    "href": "pages/ST/Week01.html#historical-perspective",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Historical Perspective",
    "text": "Historical Perspective\nDrawing from the historical lens, luminaries like Edison and Lovelace utilized terms such as “bug” and “error” to emphasize the iterative process of identifying and rectifying faults and difficulties in inventions."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-terminology",
    "href": "pages/ST/Week01.html#testing-terminology",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Terminology",
    "text": "Testing Terminology\n\nTest Case: A comprehensive entity comprising test inputs and expected outputs, evaluated by executing the test case on the code.\nTest Case ID: An identifier crucial for retrieval and management of test cases.\nTraceability: The establishment of links connecting test cases to specific requirements, ensuring thorough validation."
  },
  {
    "objectID": "pages/ST/Week01.html#types-of-testing",
    "href": "pages/ST/Week01.html#types-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Types of Testing",
    "text": "Types of Testing\n\nUnit Testing: A meticulous examination carried out by developers during the coding phase to test individual methods.\nIntegration Testing: An evaluation of the interaction between diverse software components.\nSystem Testing: A holistic examination of the entire system to ensure alignment with design requirements.\nAcceptance Testing: Conducted by end customers to validate that the delivered software meets all committed requirements."
  },
  {
    "objectID": "pages/ST/Week01.html#quality-parameters-testing",
    "href": "pages/ST/Week01.html#quality-parameters-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Quality Parameters Testing",
    "text": "Quality Parameters Testing\n\nFunctional Testing: Ensures the software functions precisely as intended.\nStress Testing: Evaluates software performance under extreme conditions to assess its robustness.\nPerformance Testing: Verifies if the software responds within specified time limits under varying conditions.\nUsability Testing: Ensures the software offers a user-friendly interface, enhancing the overall user experience.\nRegression Testing: Validates that existing functionalities continue to work seamlessly after software changes."
  },
  {
    "objectID": "pages/ST/Week01.html#methods-of-testing",
    "href": "pages/ST/Week01.html#methods-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Methods of Testing",
    "text": "Methods of Testing\n\nBlack Box Testing: A method that evaluates the software without delving into its internal structure, relying solely on inputs and requirements.\nWhite Box Testing: Testing carried out with a comprehensive understanding of the software’s internal structure, design, and code.\nGray Box Testing: An intermediate approach that combines elements of both black box and white box testing."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-activities",
    "href": "pages/ST/Week01.html#testing-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Activities",
    "text": "Testing Activities\n\nTest Case Design:\n\nCritical for efficiently identifying defects.\nRequires a blend of computer science expertise, domain knowledge, and mathematical proficiency.\nEmphasis on the development of effective test case design algorithms. \n\nTest Automation:\n\nInvolves the conversion of test cases into executable scripts.\nAddresses preparatory steps and incorporates concepts of observability and controllability.\nUtilizes both open-source and proprietary test automation tools.\n\nExecution:\n\nAutomated process involving the execution of test cases.\nUtilizes a selection of open-source or proprietary tools chosen by the organization.\n\nEvaluation:\n\nThe critical analysis of test results to determine correctness.\nManual intervention may be required for fault isolation.\nCrucial for drawing inferences about the software’s quality."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-1",
    "href": "pages/ST/Week01.html#introduction-1",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software testing, the pursuit of testing goals is intricately tied to the specificities of the software product in question and the maturity of an organization’s quality processes. This diversity in objectives and approaches underscores the importance of comprehending the nuanced landscape of testing process levels, which range from the rudimentary Level 0 to the pinnacle of maturity at Level 4."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-process-maturity-levels",
    "href": "pages/ST/Week01.html#testing-process-maturity-levels",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Process Maturity Levels",
    "text": "Testing Process Maturity Levels\n\nLevel 0: Low Maturity\nAt this embryonic stage, there is an absence of a clear demarcation between testing and debugging activities. The predominant focus revolves around expedient product releases, potentially at the expense of a rigorous testing regimen.\nLevel 1: Testing for Correctness\nThe next tier witnesses a paradigm shift as testing endeavors to validate software correctness. However, a common misunderstanding prevails — an attempt to prove complete correctness through testing, an inherently unattainable feat.\nLevel 2: Finding Errors\nAs organizations ascend to Level 2, there is a conscious recognition of testing as a mechanism to unearth errors by actively showcasing failures. However, a resistance lingers when it comes to acknowledging and addressing errors identified in the code.\nLevel 3: Sophisticated Testing\nLevel 3 marks a watershed moment where testing is not merely a reactive measure but is embraced as a robust technique for both identifying and eliminating errors. A collaborative ethos emerges, with a collective effort to mitigate risks in software development.\nLevel 4: Mature Process-Oriented Testing\nAt the pinnacle of maturity, testing transcends mere procedural activities; it metamorphoses into a mental discipline. Integrated seamlessly into mainstream development, the focus is on continuous quality improvement. Here, test engineers and developers synergize their efforts to deliver software of the highest quality."
  },
  {
    "objectID": "pages/ST/Week01.html#significance-for-the-course",
    "href": "pages/ST/Week01.html#significance-for-the-course",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Significance for the Course",
    "text": "Significance for the Course\nUnderstanding the nuances of testing process levels assumes paramount importance as it serves as the bedrock for tailoring testing approaches. The focus of this course is strategically directed towards the technical intricacies relevant to Levels 3 and 4, where testing is not just a process but an integral aspect of the software development mindset."
  },
  {
    "objectID": "pages/ST/Week01.html#controllability-and-observability",
    "href": "pages/ST/Week01.html#controllability-and-observability",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Controllability and Observability",
    "text": "Controllability and Observability\n\nControllability: This pertains to the ability to provide inputs and execute the software module. It underscores the necessity of having a structured approach to govern the input parameters and execution environment.\nObservability: The study and recording of outputs form the crux of observability. This involves a meticulous examination of the software’s responses, contributing significantly to the overall understanding of its behavior."
  },
  {
    "objectID": "pages/ST/Week01.html#illustration",
    "href": "pages/ST/Week01.html#illustration",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Illustration",
    "text": "Illustration\nThe challenges of controllability and observability find illustration in real-world scenarios. Designing effective test cases becomes paramount to ensure both the reachability of various modules and the meticulous observation of their outputs. This practical application reinforces the theoretical concepts discussed in the course."
  },
  {
    "objectID": "pages/ST/Week01.html#test-automation-tool-junit",
    "href": "pages/ST/Week01.html#test-automation-tool-junit",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Test Automation Tool: JUnit",
    "text": "Test Automation Tool: JUnit\nThe course introduces JUnit as the designated test automation tool. JUnit’s utility is elucidated through a discussion of its prefix and postfix annotations, providing a structured approach to manage controllability and observability. Subsequent classes delve into both the theoretical underpinnings and the hands-on application of JUnit, ensuring a comprehensive understanding of its role in the testing process."
  },
  {
    "objectID": "pages/RL/Week01_2.html",
    "href": "pages/RL/Week01_2.html",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Reinforcement Learning (RL) is a paradigm within machine learning that focuses on trial-and-error learning. It involves learning from the evaluation of actions taken, rather than receiving explicit instructional feedback. In RL, agents explore various actions to determine their effectiveness through evaluative feedback. This differentiates RL from other learning approaches.\n\n\nIn the realm of RL applications, various domains utilize this learning paradigm. From controlling robots to playing games like tic-tac-toe, RL finds its application in scenarios where learning from experience is crucial.\n\n\n\n\nTo understand the learning mechanism in RL, let’s consider a straightforward example: playing tic-tac-toe. In a traditional supervised learning setting, an expert labels optimal moves for different board positions. However, RL takes a different approach.\n\n\nIn a supervised learning setup for tic-tac-toe, experts label correct moves for specific board positions. The computer is then trained using this labeled dataset to predict the right move for any given position.\n\n\n\nIn RL, the agent is simply told to play the game without explicit instructions on moves. The agent receives points based on the game outcome: +1 for a win, -1 for a loss, and 0 for a draw. The crucial aspect is that the agent is not informed about the winning conditions; it learns solely from playing the game repeatedly.\n\n\n\nA historical example of RL in the form of a simple tic-tac-toe learning system is Menace (Matchbox Educable Noughts and Crosses Engine). This system, developed in the 1960s, used matchboxes with colored beads to learn optimal moves. Each matchbox represented a board position, and colored beads denoted possible moves.\n\nLearning Process\n\nOpen matchbox for the current position.\nSelect a bead representing a move.\nPlay the move on the board.\nUpdate bead counts based on game outcome.\nRepeat the process for subsequent games.\n\nOutcome Influence\n\nWinning increased the probability of selecting specific moves.\nLosing decreased the likelihood of choosing certain moves.\n\n\n\n\n\nUnderstanding RL involves examining the game tree, representing possible moves and outcomes. Temporal Difference (TD) Learning plays a crucial role in RL.\n\nGame Tree\n\nDescribes possible moves and outcomes.\nEach path represents a sequence of moves leading to a win, draw, or loss.\n\nTemporal Difference Learning\n\nCompares predicted outcomes at successive time steps.\nUpdates move probabilities based on the difference in predicted outcomes.\n\n\n\n\nObservations of dopamine activity in monkeys during a reward-based task mirror TD learning predictions. The brain’s dopamine response shifts from the actual reward to the predictive stimulus, showcasing the alignment between computational models and biological learning. \n\n\n\n\n\n\n\nDeep Reinforcement Learning (DRL) merges RL principles with deep learning for enhanced function approximation. DRL has revolutionized the field, enabling solutions to complex problems.\n\nGrowing Excitement\n\nSignificant increase in publications mentioning reinforcement learning.\nDRL has sparked renewed interest and excitement in the RL community.\n\n\n\n\n\n\nReinforcement Learning remains an active area of research with ongoing exploration of fundamental questions. The ultimate goal is to develop omnivorous learning systems capable of consuming diverse information for improved learning.\n\nReinforcement Learning with Human Feedback\n\nIncorporating human feedback into RL processes.\nAiming for more versatile and powerful learning systems."
  },
  {
    "objectID": "pages/RL/Week01_2.html#introduction-to-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#introduction-to-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Reinforcement Learning (RL) is a paradigm within machine learning that focuses on trial-and-error learning. It involves learning from the evaluation of actions taken, rather than receiving explicit instructional feedback. In RL, agents explore various actions to determine their effectiveness through evaluative feedback. This differentiates RL from other learning approaches.\n\n\nIn the realm of RL applications, various domains utilize this learning paradigm. From controlling robots to playing games like tic-tac-toe, RL finds its application in scenarios where learning from experience is crucial."
  },
  {
    "objectID": "pages/RL/Week01_2.html#learning-mechanisms-a-simple-example-with-tic-tac-toe",
    "href": "pages/RL/Week01_2.html#learning-mechanisms-a-simple-example-with-tic-tac-toe",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "To understand the learning mechanism in RL, let’s consider a straightforward example: playing tic-tac-toe. In a traditional supervised learning setting, an expert labels optimal moves for different board positions. However, RL takes a different approach.\n\n\nIn a supervised learning setup for tic-tac-toe, experts label correct moves for specific board positions. The computer is then trained using this labeled dataset to predict the right move for any given position.\n\n\n\nIn RL, the agent is simply told to play the game without explicit instructions on moves. The agent receives points based on the game outcome: +1 for a win, -1 for a loss, and 0 for a draw. The crucial aspect is that the agent is not informed about the winning conditions; it learns solely from playing the game repeatedly.\n\n\n\nA historical example of RL in the form of a simple tic-tac-toe learning system is Menace (Matchbox Educable Noughts and Crosses Engine). This system, developed in the 1960s, used matchboxes with colored beads to learn optimal moves. Each matchbox represented a board position, and colored beads denoted possible moves.\n\nLearning Process\n\nOpen matchbox for the current position.\nSelect a bead representing a move.\nPlay the move on the board.\nUpdate bead counts based on game outcome.\nRepeat the process for subsequent games.\n\nOutcome Influence\n\nWinning increased the probability of selecting specific moves.\nLosing decreased the likelihood of choosing certain moves.\n\n\n\n\n\nUnderstanding RL involves examining the game tree, representing possible moves and outcomes. Temporal Difference (TD) Learning plays a crucial role in RL.\n\nGame Tree\n\nDescribes possible moves and outcomes.\nEach path represents a sequence of moves leading to a win, draw, or loss.\n\nTemporal Difference Learning\n\nCompares predicted outcomes at successive time steps.\nUpdates move probabilities based on the difference in predicted outcomes.\n\n\n\n\nObservations of dopamine activity in monkeys during a reward-based task mirror TD learning predictions. The brain’s dopamine response shifts from the actual reward to the predictive stimulus, showcasing the alignment between computational models and biological learning."
  },
  {
    "objectID": "pages/RL/Week01_2.html#deep-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#deep-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Deep Reinforcement Learning (DRL) merges RL principles with deep learning for enhanced function approximation. DRL has revolutionized the field, enabling solutions to complex problems.\n\nGrowing Excitement\n\nSignificant increase in publications mentioning reinforcement learning.\nDRL has sparked renewed interest and excitement in the RL community."
  },
  {
    "objectID": "pages/RL/Week01_2.html#future-directions-in-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#future-directions-in-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Reinforcement Learning remains an active area of research with ongoing exploration of fundamental questions. The ultimate goal is to develop omnivorous learning systems capable of consuming diverse information for improved learning.\n\nReinforcement Learning with Human Feedback\n\nIncorporating human feedback into RL processes.\nAiming for more versatile and powerful learning systems."
  },
  {
    "objectID": "pages/RL/Week01_2.html#reinforcement-learning-framework",
    "href": "pages/RL/Week01_2.html#reinforcement-learning-framework",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Reinforcement Learning Framework",
    "text": "Reinforcement Learning Framework\nReinforcement learning is characterized by learning through interactions with an environment. The learner receives feedback based on its actions, necessitating a strategic approach to explore different possibilities (exploration) and exploit known optimal actions for favorable outcomes."
  },
  {
    "objectID": "pages/RL/Week01_2.html#immediate-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#immediate-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Immediate Reinforcement Learning",
    "text": "Immediate Reinforcement Learning\nIn the immediate reinforcement learning problem, actions yield immediate payoffs, eliminating the need for a sequence of moves or temporal considerations. This simplification directs attention to the exploration-exploitation dilemma, a critical aspect of reinforcement learning."
  },
  {
    "objectID": "pages/RL/Week01_2.html#exploration-vs.-exploitation-dilemma",
    "href": "pages/RL/Week01_2.html#exploration-vs.-exploitation-dilemma",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Exploration vs. Exploitation Dilemma",
    "text": "Exploration vs. Exploitation Dilemma\nThe core challenge revolves around determining the optimal trade-off between exploring various actions and exploiting the known best action. Excessive exploration may impede performance, while premature exploitation might lead to suboptimal outcomes."
  },
  {
    "objectID": "pages/RL/Week01_2.html#points-to-remember",
    "href": "pages/RL/Week01_2.html#points-to-remember",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nReinforcement Learning Fundamentals\n\nRL focuses on trial-and-error learning, distinguishing it from other machine learning approaches.\nApplications span diverse domains, from robotics to game playing.\n\nLearning Mechanisms in Tic-Tac-Toe\n\nSupervised learning relies on labeled datasets of optimal moves.\nRL involves trial and error, with agents learning from the game’s outcome.\n\nHistorical Example: Menace\n\nMenace used a matchbox system with colored beads to learn optimal moves.\nLearning process involved updating bead counts based on game outcomes.\n\nGame Tree and Temporal Difference Learning\n\nGame tree represents possible moves and outcomes.\nTemporal Difference (TD) Learning updates move probabilities based on predicted outcomes.\n\nDeep Reinforcement Learning (DRL)\n\nIntegration of RL principles with deep learning for enhanced function approximation.\nDRL has led to a significant increase in publications and excitement in the RL community.\n\nFuture Directions in RL\n\nOngoing research aims at developing versatile learning systems.\nIncorporating human feedback for more powerful learning systems.\n\nImmediate Reinforcement Learning: Multi-Arm Bandit Problem\n\nImmediate RL focuses on actions yielding immediate payoffs.\nThe exploration-exploitation dilemma is crucial, requiring a strategic balance."
  }
]