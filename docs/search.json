[
  {
    "objectID": "pages/ST/Week01.html",
    "href": "pages/ST/Week01.html",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes.\n\n\n\n\n\nThe initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary.\n\n\n\n\n\n\nThe V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints.\n\n\n\n\nBeyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances.\n\n\n\n\n\nIntegral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "href": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes."
  },
  {
    "objectID": "pages/ST/Week01.html#phases-of-sdlc",
    "href": "pages/ST/Week01.html#phases-of-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary."
  },
  {
    "objectID": "pages/ST/Week01.html#sdlc-models",
    "href": "pages/ST/Week01.html#sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints."
  },
  {
    "objectID": "pages/ST/Week01.html#other-sdlc-models",
    "href": "pages/ST/Week01.html#other-sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Beyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances."
  },
  {
    "objectID": "pages/ST/Week01.html#umbrella-activities",
    "href": "pages/ST/Week01.html#umbrella-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Integral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction",
    "href": "pages/ST/Week01.html#introduction",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Testing\nSoftware testing is a comprehensive process involving the scrutiny of various artifacts, including code, design, architecture documents, and requirements documents. The core objective is to validate and verify these artifacts, ensuring the software’s reliability and functionality.\n\n\nGoals of Software Testing\nThe overarching goals encompass providing an unbiased, independent assessment of the software, verifying its compliance with business capabilities, and evaluating associated risks that may impact its performance."
  },
  {
    "objectID": "pages/ST/Week01.html#standard-glossary",
    "href": "pages/ST/Week01.html#standard-glossary",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Standard Glossary",
    "text": "Standard Glossary\n\nVerification: This process determines whether the products meet specified requirements at various stages of the software development life cycle.\nValidation: Evaluation of the software at the end of the development phase, ensuring it aligns with standards and intended usage.\nFault: A static defect within the software, often originating from a mistake made during development.\nFailure: The visible, external manifestation of incorrect behavior resulting from a fault.\nError: The incorrect state of the program when a failure occurs, indicating a deviation from the intended behavior."
  },
  {
    "objectID": "pages/ST/Week01.html#historical-perspective",
    "href": "pages/ST/Week01.html#historical-perspective",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Historical Perspective",
    "text": "Historical Perspective\nDrawing from the historical lens, luminaries like Edison and Lovelace utilized terms such as “bug” and “error” to emphasize the iterative process of identifying and rectifying faults and difficulties in inventions."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-terminology",
    "href": "pages/ST/Week01.html#testing-terminology",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Terminology",
    "text": "Testing Terminology\n\nTest Case: A comprehensive entity comprising test inputs and expected outputs, evaluated by executing the test case on the code.\nTest Case ID: An identifier crucial for retrieval and management of test cases.\nTraceability: The establishment of links connecting test cases to specific requirements, ensuring thorough validation."
  },
  {
    "objectID": "pages/ST/Week01.html#types-of-testing",
    "href": "pages/ST/Week01.html#types-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Types of Testing",
    "text": "Types of Testing\n\nUnit Testing: A meticulous examination carried out by developers during the coding phase to test individual methods.\nIntegration Testing: An evaluation of the interaction between diverse software components.\nSystem Testing: A holistic examination of the entire system to ensure alignment with design requirements.\nAcceptance Testing: Conducted by end customers to validate that the delivered software meets all committed requirements."
  },
  {
    "objectID": "pages/ST/Week01.html#quality-parameters-testing",
    "href": "pages/ST/Week01.html#quality-parameters-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Quality Parameters Testing",
    "text": "Quality Parameters Testing\n\nFunctional Testing: Ensures the software functions precisely as intended.\nStress Testing: Evaluates software performance under extreme conditions to assess its robustness.\nPerformance Testing: Verifies if the software responds within specified time limits under varying conditions.\nUsability Testing: Ensures the software offers a user-friendly interface, enhancing the overall user experience.\nRegression Testing: Validates that existing functionalities continue to work seamlessly after software changes."
  },
  {
    "objectID": "pages/ST/Week01.html#methods-of-testing",
    "href": "pages/ST/Week01.html#methods-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Methods of Testing",
    "text": "Methods of Testing\n\nBlack Box Testing: A method that evaluates the software without delving into its internal structure, relying solely on inputs and requirements.\nWhite Box Testing: Testing carried out with a comprehensive understanding of the software’s internal structure, design, and code.\nGray Box Testing: An intermediate approach that combines elements of both black box and white box testing."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-activities",
    "href": "pages/ST/Week01.html#testing-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Activities",
    "text": "Testing Activities\n\nTest Case Design:\n\nCritical for efficiently identifying defects.\nRequires a blend of computer science expertise, domain knowledge, and mathematical proficiency.\nEmphasis on the development of effective test case design algorithms. \n\nTest Automation:\n\nInvolves the conversion of test cases into executable scripts.\nAddresses preparatory steps and incorporates concepts of observability and controllability.\nUtilizes both open-source and proprietary test automation tools.\n\nExecution:\n\nAutomated process involving the execution of test cases.\nUtilizes a selection of open-source or proprietary tools chosen by the organization.\n\nEvaluation:\n\nThe critical analysis of test results to determine correctness.\nManual intervention may be required for fault isolation.\nCrucial for drawing inferences about the software’s quality."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-1",
    "href": "pages/ST/Week01.html#introduction-1",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software testing, the pursuit of testing goals is intricately tied to the specificities of the software product in question and the maturity of an organization’s quality processes. This diversity in objectives and approaches underscores the importance of comprehending the nuanced landscape of testing process levels, which range from the rudimentary Level 0 to the pinnacle of maturity at Level 4."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-process-maturity-levels",
    "href": "pages/ST/Week01.html#testing-process-maturity-levels",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Process Maturity Levels",
    "text": "Testing Process Maturity Levels\n\nLevel 0: Low Maturity\nAt this embryonic stage, there is an absence of a clear demarcation between testing and debugging activities. The predominant focus revolves around expedient product releases, potentially at the expense of a rigorous testing regimen.\nLevel 1: Testing for Correctness\nThe next tier witnesses a paradigm shift as testing endeavors to validate software correctness. However, a common misunderstanding prevails — an attempt to prove complete correctness through testing, an inherently unattainable feat.\nLevel 2: Finding Errors\nAs organizations ascend to Level 2, there is a conscious recognition of testing as a mechanism to unearth errors by actively showcasing failures. However, a resistance lingers when it comes to acknowledging and addressing errors identified in the code.\nLevel 3: Sophisticated Testing\nLevel 3 marks a watershed moment where testing is not merely a reactive measure but is embraced as a robust technique for both identifying and eliminating errors. A collaborative ethos emerges, with a collective effort to mitigate risks in software development.\nLevel 4: Mature Process-Oriented Testing\nAt the pinnacle of maturity, testing transcends mere procedural activities; it metamorphoses into a mental discipline. Integrated seamlessly into mainstream development, the focus is on continuous quality improvement. Here, test engineers and developers synergize their efforts to deliver software of the highest quality."
  },
  {
    "objectID": "pages/ST/Week01.html#significance-for-the-course",
    "href": "pages/ST/Week01.html#significance-for-the-course",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Significance for the Course",
    "text": "Significance for the Course\nUnderstanding the nuances of testing process levels assumes paramount importance as it serves as the bedrock for tailoring testing approaches. The focus of this course is strategically directed towards the technical intricacies relevant to Levels 3 and 4, where testing is not just a process but an integral aspect of the software development mindset."
  },
  {
    "objectID": "pages/ST/Week01.html#controllability-and-observability",
    "href": "pages/ST/Week01.html#controllability-and-observability",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Controllability and Observability",
    "text": "Controllability and Observability\n\nControllability: This pertains to the ability to provide inputs and execute the software module. It underscores the necessity of having a structured approach to govern the input parameters and execution environment.\nObservability: The study and recording of outputs form the crux of observability. This involves a meticulous examination of the software’s responses, contributing significantly to the overall understanding of its behavior."
  },
  {
    "objectID": "pages/ST/Week01.html#illustration",
    "href": "pages/ST/Week01.html#illustration",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Illustration",
    "text": "Illustration\nThe challenges of controllability and observability find illustration in real-world scenarios. Designing effective test cases becomes paramount to ensure both the reachability of various modules and the meticulous observation of their outputs. This practical application reinforces the theoretical concepts discussed in the course."
  },
  {
    "objectID": "pages/ST/Week01.html#test-automation-tool-junit",
    "href": "pages/ST/Week01.html#test-automation-tool-junit",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Test Automation Tool: JUnit",
    "text": "Test Automation Tool: JUnit\nThe course introduces JUnit as the designated test automation tool. JUnit’s utility is elucidated through a discussion of its prefix and postfix annotations, providing a structured approach to manage controllability and observability. Subsequent classes delve into both the theoretical underpinnings and the hands-on application of JUnit, ensuring a comprehensive understanding of its role in the testing process."
  },
  {
    "objectID": "pages/AI/Week01.html",
    "href": "pages/AI/Week01.html",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning.\n\n\n\n\nData Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward.\n\n\n\n\n\n\nThe foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers.\n\n\n\n\n\n\nThe training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology.\n\n\n\n\n\n\n\nThe dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification.\n\n\n\n\n\n\n\nNeural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains.\n\n\n\n\n\n\nThe triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously."
  },
  {
    "objectID": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "href": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning."
  },
  {
    "objectID": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "href": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Data Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward."
  },
  {
    "objectID": "pages/AI/Week01.html#evolution-of-neural-networks",
    "href": "pages/AI/Week01.html#evolution-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers."
  },
  {
    "objectID": "pages/AI/Week01.html#training-process-of-neural-networks",
    "href": "pages/AI/Week01.html#training-process-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology."
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "href": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification."
  },
  {
    "objectID": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "href": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Neural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains."
  },
  {
    "objectID": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "href": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously."
  },
  {
    "objectID": "pages/AI/Week01.html#human-cognition-and-ai",
    "href": "pages/AI/Week01.html#human-cognition-and-ai",
    "title": "A Decade of Machine Learning",
    "section": "Human Cognition and AI",
    "text": "Human Cognition and AI\n\nCognitive Landscape\nHuman intelligence engages in a myriad of activities such as logic, representation, planning, reasoning, and search. The crux of these cognitive endeavors lies in symbolic reasoning, a substantial facet of the human cognitive load.\n\n\nSymbolic Reasoning\nSymbolic reasoning, integral to human cognition, involves the management of symbolic knowledge representation and intricate problem-solving processes."
  },
  {
    "objectID": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "href": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "Distinguishing AI from Machine Learning",
    "text": "Distinguishing AI from Machine Learning\n\nAI Emphasis\nWithin the domain of Artificial Intelligence (AI), the spotlight is on symbolic knowledge representation and advanced problem-solving methodologies.\n\n\nMachine Learning Focus\nIn contrast, Machine Learning (ML) gravitates towards interpreting data, with applications ranging from recommender systems to predictive analytics and classification."
  },
  {
    "objectID": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "href": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Knowledge Representation in AI",
    "text": "Knowledge Representation in AI\n\nDeclarative Knowledge\nAligning with the cognitive domain of humans, explicit symbolic representation, known as declarative knowledge, assumes a pivotal role. It encompasses the representation of the world and engages in reasoned deductions.\n\n\nInferences in AI\nAI agents showcase a spectrum of inferences, ranging from deductive reasoning based on logic to plausible or probabilistic inferences that incorporate an element of likelihood."
  },
  {
    "objectID": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "href": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Symbolic Representation in AI",
    "text": "Symbolic Representation in AI\n\nDefining Symbols\nSymbols, representing abstract concepts, manifest in diverse forms. For example, the number (7) can be expressed in various ways, illustrating the distinction between the conceptualization of numbers and their symbolic representations.\n\n\nMeaning of Symbols\nThe significance of symbols is socially agreed upon, forming the foundation for semiotic systems. Whether in road signs or linguistic characters, symbols encapsulate shared meanings."
  },
  {
    "objectID": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "href": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "title": "A Decade of Machine Learning",
    "section": "Semiotics and Biosemiotics",
    "text": "Semiotics and Biosemiotics\n\nSemiotics\nSemiotics, the scientific study of symbols in spoken and written languages, lays the groundwork for comprehending human communication and representation.\n\n\nBiosemiotics\nDelving deeper, Biosemiotics explores the emergence of complex behavior when simple systems engage in symbolic communication. This is exemplified by phenomena such as ant trails utilizing pheromones."
  },
  {
    "objectID": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "href": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Reasoning Mechanisms in AI",
    "text": "Reasoning Mechanisms in AI\n\nFormal Reasoning\nIn the context of AI, reasoning involves the systematic manipulation of symbols in a meaningful manner. This encompasses algorithms for fundamental operations like addition and multiplication, extending to more intricate processes like the Fourier transform.\n\n\nConceptualizing Algorithms\nUnderstanding AI algorithms necessitates a conceptual grasp of symbolic manipulations. For instance, multiplication algorithms entail conceptualizing the multiplication of unit digits and the subsequent shifting of results."
  },
  {
    "objectID": "pages/AI/Week01.html#automation-vs.-ai",
    "href": "pages/AI/Week01.html#automation-vs.-ai",
    "title": "A Decade of Machine Learning",
    "section": "Automation vs. AI",
    "text": "Automation vs. AI\n\nNavigating Overlap\nWhile Automation and AI share common ground, not all automated systems integrate AI. For instance, implementations such as train reservation systems or basic online shopping may lack substantial AI components."
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "href": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Machine Learning’s Role in AI",
    "text": "Machine Learning’s Role in AI\n\nML as a Component\nMachine Learning constitutes one facet of the multifaceted field of AI. Examples such as self-driving cars leverage ML for tasks including pattern recognition, speech processing, and object classification."
  },
  {
    "objectID": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "href": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Clarifying Data Science in AI",
    "text": "Clarifying Data Science in AI\n\nData’s Multifaceted Role\nData science, encompassing elements of statistics, AI, and machine learning, plays a crucial role in the broader field of AI. It serves as a foundational component but does not encapsulate the entirety of AI."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "href": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to AI and Definitions:",
    "text": "Introduction to AI and Definitions:\n\n\nDefinitions of AI:\n\nHerbert Simon: Programs are considered intelligent if they display behaviors regarded as intelligent in humans.\nBar and Feigenbaum: AI seeks to comprehend the systematic behavior of information processing systems, analogous to physicists and biologists in their respective domains.\nElaine Rich: AI involves solving exponentially hard problems in polynomial time, leveraging domain-specific knowledge.\nJohn Hoagland: AI’s goal is to create machines with minds of their own, treating thinking and computing as fundamentally interconnected."
  },
  {
    "objectID": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "href": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Fundamental Questions in AI:",
    "text": "Fundamental Questions in AI:\n\nQuestions about Intelligence:\nVarious perspectives exist on what constitutes intelligence, encompassing language use, reasoning, and learning. Ongoing debates revolve around whether machines can genuinely exhibit thinking, with insights from philosophers like Roger Penrose exploring quantum mechanics in the human brain."
  },
  {
    "objectID": "pages/AI/Week01.html#turing-test-and-challenges",
    "href": "pages/AI/Week01.html#turing-test-and-challenges",
    "title": "A Decade of Machine Learning",
    "section": "Turing Test and Challenges:",
    "text": "Turing Test and Challenges:\n\nAlan Turing’s Turing Test:\nThe evaluation of machine intelligence through its ability to convincingly engage in natural language conversations with a human judge forms the essence of the Turing Test. Associated challenges include situations where chatbots may impress but lack genuine intelligence. The Löbner Prize Competition attempts a similar test.\n\n\nHector Levesque’s Alternative: Vinograd Schemas\nAn alternate test proposed by Hector Levesque challenges a machine’s understanding through multiple-choice questions that require subject matter knowledge."
  },
  {
    "objectID": "pages/AI/Week01.html#vinograd-schemas-examples",
    "href": "pages/AI/Week01.html#vinograd-schemas-examples",
    "title": "A Decade of Machine Learning",
    "section": "Vinograd Schemas Examples:",
    "text": "Vinograd Schemas Examples:\n\nExample 1:\n\nOriginal Sentence: “The city council refused the demonstrators a permit because they feared violence.”\nAlternate Sentence: “The city council refused the demonstrators a permit because they advocated violence.”\nQuestion: What does “they” refer to? Options: Council, Demonstrators.\n\nExample 2:\n\nOriginal Sentence: “John took the water bottle out of the backpack so that it would be lighter.”\nAlternate Sentence: “John took the water bottle out of the backpack so that it would be handy.”\nQuestion: What does “it” refer to? Options: Backpack, Water Bottle.\n\nExample 3:\n\nOriginal Sentence: “The trophy would not fit into the brown suitcase because it was too small.”\nAlternate Sentence: “The trophy would not fit into the brown suitcase because it was too big.”\nQuestion: What does “it” refer to? Options: Trophy, Brown Suitcase.\n\nExample 4:\n\nOriginal Sentence: “The lawyer asked the witness a question but he was reluctant to repeat it.”\nAlternate Sentence: “The lawyer asked the witness a question but he was reluctant to answer it.”\nQuestion: Who was reluctant? Options: Lawyer, Witness."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "href": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Intelligence and AI Goals",
    "text": "Introduction to Intelligence and AI Goals\nIn the exploration of artificial intelligence (AI), the concept of intelligence takes center stage. AI endeavors to construct intelligent agents capable of complex problem-solving. A historical glimpse into European thinkers sheds light on the roots of AI ideologies.\n\nGalileo Galilei (1623)\nIn his 1623 publication, Galileo Galilei delves into the subjective nature of sensory experiences. He contends that taste, odors, and colors are subjective perceptions residing in consciousness. Galileo challenges the idea that these qualities exist inherently in external objects. Moreover, he posits that philosophy is expressed through the language of mathematics.\n\n\nThomas Hobbs\nThomas Hobbs, often referred to as the grandfather of AI, introduces the notion that thinking involves the manipulation of symbols. He associates reasoning with computation, not in the contemporary sense of computers, but as a form of mathematical operations. Hobbs views computation as the sum of many things added together or the determination of the remainder when one thing is subtracted from another.\n\n\nRené Descartes\nBuilding on Galileo’s ideas, Descartes extends the concept that animals are intricate machines, reserving acknowledgment of a mind solely for humans. He aligns thought with symbols and introduces the mind-body dualism, raising questions about the interaction between the mental world and the physical body."
  },
  {
    "objectID": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "href": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "title": "A Decade of Machine Learning",
    "section": "Early Concepts of Thinking Machines",
    "text": "Early Concepts of Thinking Machines\nThe early stages of envisioning thinking machines were influenced by the use of punch cards in the textile industry’s Jacquard looms.\n\nJacquard Looms\nPunch cards were employed to control patterns in textile looms. This concept of punched cards was later adapted for programming early computers, emphasizing a transition from controlling patterns to controlling programs.\n\n\nCharles Babbage and Augusta Ada Byron\nCharles Babbage, a mathematician and inventor, conceptualized the Difference Engine and the Analytic Engine. Augusta Ada Byron, daughter of Lord Byron, collaborated with Babbage and is recognized as the world’s first programmer. She envisioned computers going beyond mere number crunching, foreseeing applications in music composition and AI-like capabilities."
  },
  {
    "objectID": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "href": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "title": "A Decade of Machine Learning",
    "section": "Mechanical Calculators and Early Computers",
    "text": "Mechanical Calculators and Early Computers\nThe evolution of mechanical calculators and the emergence of early electronic computers marked significant progress in computational capabilities.\n\nPascal’s Calculator and Leibniz’s Step Drum\nPascal’s mechanical calculator incorporated Latin Lantern gears, performing basic arithmetic operations. Leibniz introduced the step drum, a mechanism for counting and representing numbers. Both contributed to the development of early calculating machines.\n\n\nENIAC (Electronic Numerical Integrator and Computer)\nENIAC, the first electronic computer, boasted over 17,000 vacuum tubes. Despite its immense size and weight, it laid the foundation for electronic computing. Augusta Ada Byron’s visionary insights into the potential of computers started to materialize with the advent of ENIAC."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction",
    "href": "pages/AI/Week01.html#introduction",
    "title": "A Decade of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nThe course provides a comprehensive exploration of the evolution and fundamental principles of Artificial Intelligence (AI). With historical roots reaching back to the 1300s, early attempts by figures like Jazari and Ramon Llull set the stage for the development of AI.\n\nCoined Terminology\nThe term “Artificial Intelligence” was officially coined by John McCarthy during the Dartmouth Conference in 1956. This landmark event, organized alongside Marvin Minsky and Claude Shannon, aimed to investigate the potential for machines to simulate human intelligence through precise descriptions."
  },
  {
    "objectID": "pages/AI/Week01.html#key-figures",
    "href": "pages/AI/Week01.html#key-figures",
    "title": "A Decade of Machine Learning",
    "section": "Key Figures",
    "text": "Key Figures\n\n1. John McCarthy\n\nCredited with Naming AI\nAssistant Professor at Dartmouth\nDesigner of Lisp Programming Language\nContributions to Logic and Common Sense Reasoning\n\n\n\n2. Marvin Minsky\n\nCo-founder of MIT AI Lab\nNotable for Frame Systems (Foundation of OOP)\nAuthor of “The Society of the Mind” and “The Emotional Machine”\n\n\n\n3. Nathaniel Rochester\n\nIBM Engineer\nDesigner of IBM 701\nSupervised Arthur Samuel and the Checkers-playing Program\n\n\n\n4. Claude Shannon\n\nFather of Information Theory\nMathematician at Bell Labs\n\n\n\n5. Herbert Simon and Allen Newell\n\nDevelopers of Logic Theorist (LT) Program\nPioneers in Symbolic AI\nIntroduction of Physical Symbol Systems\nSimon’s Diverse Scholarship (Nobel Prize in Economics)"
  },
  {
    "objectID": "pages/AI/Week01.html#physical-symbol-systems",
    "href": "pages/AI/Week01.html#physical-symbol-systems",
    "title": "A Decade of Machine Learning",
    "section": "Physical Symbol Systems",
    "text": "Physical Symbol Systems\n\nSymbolic Representation\nSymbol systems represent perceptible entities and adhere to formal laws, mirroring the structure of the physical world. Simon and Newell’s hypothesis posits that a Physical Symbol System is both necessary and sufficient for general intelligent action, distinguishing it from sub-symbolic AI, where information is stored in weights without explicit symbols."
  },
  {
    "objectID": "pages/AI/Week01.html#philosophical-considerations",
    "href": "pages/AI/Week01.html#philosophical-considerations",
    "title": "A Decade of Machine Learning",
    "section": "Philosophical Considerations",
    "text": "Philosophical Considerations\n\nCopernican Shift\n\nGalileo’s Distinction Between Thought and Reality\nGalileo Galilei’s intellectual endeavors were marked by a profound separation between the realm of thought and the objective reality. This conceptual wedge laid the foundation for a nuanced understanding of how human cognition interfaces with the external world.\n\n\nCopernicus’ Challenge to the Geocentric Model, Emphasizing Subjectivity\nCopernicus, through his revolutionary heliocentric model, not only challenged the prevailing geocentric view but also underscored the subjectivity inherent in our interpretations of celestial motions. This shift forced a reconsideration of humanity’s position in the cosmos.\n\n\nHuman Creation of Mental Models; Reality Comprising Fundamental Particles\nHumans engage in the active creation of mental models to comprehend the intricacies of reality. The Copernican Shift extends to the microscopic realm, where the abundant nature of fundamental particles renders them unsuitable as standalone elements of representation. Instead, reality is approached through disciplined ontologies, focusing on entities like atoms, molecules, or cells based on the context of study.\n\n\nIllustration through the Powers of Ten Film\nThe Powers of Ten film serves as a captivating medium to illustrate the Copernican Shift, visually portraying the vastness and intricacies of the universe at different scales. This cinematic exploration emphasizes the dynamic interplay between our mental representations and the expansive reality they seek to capture."
  },
  {
    "objectID": "pages/AI/Week01.html#representation-and-reasoning",
    "href": "pages/AI/Week01.html#representation-and-reasoning",
    "title": "A Decade of Machine Learning",
    "section": "Representation and Reasoning",
    "text": "Representation and Reasoning\n\nHuman Reasoning\n\nHuman Reasoning Involves Symbolic Representations\nIn the realm of human cognition, symbolic representations play a pivotal role in the process of reasoning. These symbols serve as cognitive tools that humans manipulate to make sense of the world around them.\n\n\nFundamental Particles Unsuitable as Elements of Representation due to Abundance\nDespite the fundamental nature of particles, their sheer abundance makes them impractical as elemental units of representation. Human cognition necessitates a selective focus, leading to the adoption of more manageable entities like atoms, molecules, or cells, depending on the specific domain of study.\n\n\nRepresentation Depends on the Focus of Study (e.g., Atoms, Molecules, Cells)\nThe choice of representation is intricately tied to the focus of study. Whether delving into the microscopic realm of atoms or exploring the complexity of biological systems at the cellular level, the selection of representational units is driven by the demands of the specific discipline.\n\n\nDiscipline-specific Ontologies Define Level of Detail in Representations\nDiscipline-specific ontologies play a crucial role in determining the level of detail embedded in representations. These structured frameworks provide a systematic approach to capturing and organizing knowledge within distinct domains."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-problem-solving",
    "href": "pages/AI/Week01.html#introduction-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Problem Solving",
    "text": "Introduction to Problem Solving\nIn the expansive domain of Artificial Intelligence (AI), problem-solving emerges as the orchestrated actions of autonomous agents navigating predefined objectives within dynamic environments. This course delves into the intricacies of problem-solving, elucidating the diverse methodologies encapsulated within search methods.\n\nProblem-Solving Framework\n\nAgent and Environment:\n\nAutonomous agents operate within a world defined by specific objectives and a repertoire of actions. Decision-making unfolds in real-time, navigating challenges posed by incomplete knowledge and the concurrent activities of other agents.\n\nSimplifying Assumptions:\n\nInitial simplifications envision a static world with a solitary agent making decisions, providing foundational insights into fundamental problem-solving principles."
  },
  {
    "objectID": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "href": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Two Approaches to Problem Solving",
    "text": "Two Approaches to Problem Solving\n\n1. Model-Based Reasoning (Search Methods)\n\nDefinition:\nModel-Based Reasoning involves grounded reasoning in first principles or search approaches, wherein agents experiment with diverse actions to discern their efficacy.\n\n\nAssumptions:\nThis approach assumes a static world, complete knowledge, and actions that never fail, forming the foundational basis for problem-solving methodologies.\n\n\n\n2. Knowledge-Based Approach\n\nCharacteristics:\nThe Knowledge-Based Approach draws upon a societal structure rich in stored experiences, leveraging accumulated knowledge for effective problem-solving. It encompasses memory-based reasoning, case-based reasoning, and machine learning paradigms."
  },
  {
    "objectID": "pages/AI/Week01.html#rubiks-cube-example",
    "href": "pages/AI/Week01.html#rubiks-cube-example",
    "title": "A Decade of Machine Learning",
    "section": "Rubik’s Cube Example",
    "text": "Rubik’s Cube Example\nThe Rubik’s Cube serves as an illustrative example, elucidating the dichotomy between knowledge-based and search-based problem-solving approaches.\n\nLearning Dynamics\n\nInitial Challenge:\n\nThe Rubik’s Cube presents an initial challenge devoid of a known solution, necessitating exploratory actions.\n\nEvolution of Knowledge:\n\nOver time, individuals develop efficient solving methods through experiential learning, showcasing the adaptive nature of human problem-solving.\n\nDeep Reinforcement Learning:\n\nThe introduction of deep reinforcement learning emphasizes autonomous learning without human guidance, mirroring aspects of artificial intelligence."
  },
  {
    "objectID": "pages/AI/Week01.html#sudoku-example",
    "href": "pages/AI/Week01.html#sudoku-example",
    "title": "A Decade of Machine Learning",
    "section": "Sudoku Example",
    "text": "Sudoku Example\nThe Sudoku puzzle exemplifies the synergy between search and reasoning in problem-solving, offering insights into the nuanced interplay of diverse problem-solving methodologies.\n\nCombined Approach\n\nSearch Methods:\n\nBasic search algorithms, such as depth-first search and breadth-first search, lay the foundation for problem-solving endeavors.\n\nReasoning:\n\nReasoning techniques refine available options, harmonizing search methodologies with informed decision-making for a holistic problem-solving strategy."
  },
  {
    "objectID": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "href": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Role of Logic in Problem Solving",
    "text": "Role of Logic in Problem Solving\nLogic, particularly first-order logic, assumes a pivotal role in representing knowledge and facilitating deductive reasoning within the problem-solving paradigm.\n\nLogical Components\n\nDeductive Reasoning:\n\nLogic functions as a tool for deductive reasoning, employing principles such as deduction, induction, abduction, and plausible reasoning to navigate complex problem spaces.\n\nConstraint Processing:\n\nLogic, search methods, and other reasoning approaches converge under the umbrella of constraint processing, offering a comprehensive framework for addressing intricate problem scenarios."
  },
  {
    "objectID": "pages/AI/Week01.html#map-coloring-problem",
    "href": "pages/AI/Week01.html#map-coloring-problem",
    "title": "A Decade of Machine Learning",
    "section": "Map Coloring Problem",
    "text": "Map Coloring Problem\nThe Map Coloring Problem stands as an exemplary challenge within AI, involving the assignment of colors to regions while adhering to specific constraints.\n\nConstraint Graph Representation\n\nGraph Transformation:\n\nRegions and their color preferences undergo a transformative process, manifesting as a constraint graph that encapsulates the intricacies of the problem.\n\nAlgorithmic Solutions:\n\nConstraint processing algorithms come to the forefront as viable solutions to graph-related problems, showcasing the practical application of logical problem-solving methodologies."
  },
  {
    "objectID": "pages/AI/Week01.html#key-takeaways",
    "href": "pages/AI/Week01.html#key-takeaways",
    "title": "A Decade of Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDecade of Machine Learning:\n\nThe surge in machine learning over the past decade is attributed to increased data availability, enhanced computing power, and advancements in training algorithms, particularly within the domain of deep learning. \n\nNeural Network Evolution:\n\nFrom the foundational perceptron era to the transformative deep neural networks in computer vision, the evolution of neural networks has played a pivotal role in shaping the landscape of AI.\n\nTraining Process:\n\nSupervised training, especially in medical diagnosis and face recognition, showcases the practical applications of deep neural networks in real-world scenarios.\n\nMachine Learning in Internet Interaction:\n\nUsers’ dynamic interaction with the internet transforms them into valuable data points, shaping the customization of ads and optimizing user experiences.\n\nPerformance vs. Competence:\n\nNeural networks exhibit proficiency in specific tasks but lack a holistic understanding of the world, highlighting the need for specialized training.\n\nGame of Go and Reinforcement Learning:\n\nThe triumph of AlphaGo exemplifies the success achievable through reinforcement learning, showcasing the capacity for autonomous learning without human intervention.\n\nHuman Cognitive Architecture:\n\nUnderstanding human cognitive abilities, symbolic reasoning, and the distinction between AI and machine learning provides insights into the complex realm of intelligence.\n\nHistory and Philosophy:\n\nThe historical roots of AI, key figures in AI development, and philosophical considerations underscore the interdisciplinary nature of artificial intelligence.\n\nPhysical Symbol Systems:\n\nSymbolic representation, as proposed by Simon and Newell, forms the basis for general intelligent action, distinguishing it from sub-symbolic AI.\n\nProblem Solving:\n\nTwo approaches, model-based reasoning and knowledge-based approaches, along with the role of logic, contribute to nuanced problem-solving methodologies.\n\nMap Coloring Problem:\n\nThe Map Coloring Problem serves as a concrete example, highlighting the integration of graph theory, constraint processing, and algorithmic solutions in logical problem-solving."
  },
  {
    "objectID": "pages/DL/Week01_2.html",
    "href": "pages/DL/Week01_2.html",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units.\n\n\n\n\n\n\nDendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter.\n\n\n\n\n\nLayered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli.\n\n\n\n\n\n\nNeural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction",
    "href": "pages/DL/Week01_2.html#introduction",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units."
  },
  {
    "objectID": "pages/DL/Week01_2.html#biological-neurons",
    "href": "pages/DL/Week01_2.html#biological-neurons",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Dendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter."
  },
  {
    "objectID": "pages/DL/Week01_2.html#neural-network-architecture",
    "href": "pages/DL/Week01_2.html#neural-network-architecture",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Layered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli."
  },
  {
    "objectID": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "href": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Neural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-1",
    "href": "pages/DL/Week01_2.html#introduction-1",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\n\nObjective: Comprehend the McCulloch-Pitts neuron, a simplified computational model inspired by biological neurons.\nHistorical Context: Proposed in 1943 by McCulloch (neuroscientist) and Pitts (logician).\nPurpose: Emulate the brain’s complex processing for decision-making."
  },
  {
    "objectID": "pages/DL/Week01_2.html#neuron-structure",
    "href": "pages/DL/Week01_2.html#neuron-structure",
    "title": "Motivation from Biological Neuron",
    "section": "Neuron Structure",
    "text": "Neuron Structure\n\nComponents: Divided into two parts - g and f.\ng (Aggregation): Aggregates binary inputs via a simple summation process.\nf (Decision): Makes a binary decision based on the aggregation.\nExcitatory and Inhibitory Inputs: Inputs can be either excitatory (positive) or inhibitory (negative)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#functionality",
    "href": "pages/DL/Week01_2.html#functionality",
    "title": "Motivation from Biological Neuron",
    "section": "Functionality",
    "text": "Functionality\n\nAggregation Function g(x):\n\nRepresents the sum of all inputs using the formula \\(g(x) = \\sum_{i=1}^{n} x_i\\), where \\(x_i\\) is a binary input (0 or 1).\n\nDecision Function f(g(x)):\n\nUtilizes a threshold parameter \\(\\theta\\) to determine firing.\nDecision is \\(f(g(x)) = \\begin{cases} 1 & \\text{if } g(x) \\geq \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#boolean-function-implementation",
    "href": "pages/DL/Week01_2.html#boolean-function-implementation",
    "title": "Motivation from Biological Neuron",
    "section": "Boolean Function Implementation",
    "text": "Boolean Function Implementation\n\nExamples:\n\nImplemented using McCulloch-Pitts neuron for boolean functions like AND, OR, NOR, and NOT.\nExcitatory and inhibitory inputs utilized based on boolean function logic."
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation",
    "href": "pages/DL/Week01_2.html#geometric-interpretation",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\nIn 2D:\n\nDraws a line to separate input space into two halves.\n\nIn 3D:\n\nUses a plane for separation.\n\nFor n Inputs:\n\nUtilizes a hyperplane for linear separation."
  },
  {
    "objectID": "pages/DL/Week01_2.html#linear-separability",
    "href": "pages/DL/Week01_2.html#linear-separability",
    "title": "Motivation from Biological Neuron",
    "section": "Linear Separability",
    "text": "Linear Separability\n\nDefinition: Boolean functions representable by a single McCulloch-Pitts neuron are linearly separable.\nImplication: Implies the existence of a plane (or hyperplane) separating points with output 0 and 1."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-2",
    "href": "pages/DL/Week01_2.html#introduction-2",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nPerceptrons, introduced by Frank Rosenblatt circa 1958, extend the concept of McCulloch-Pitts neurons with non-Boolean inputs, input weights, and a learning algorithm for weight adjustment."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-model",
    "href": "pages/DL/Week01_2.html#perceptron-model",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\n\nMathematical Representation\nThe perceptron is represented as \\(y = 1\\) if \\(\\sum_{i=1}^{n} w_i x_i \\geq \\text{threshold}\\); otherwise, \\(y = 0\\).\n\nNotable Differences\n\nInputs can be real, not just Boolean.\nIntroduction of weights, denoted by \\(w_i\\), indicating input importance.\nLearning algorithm to adapt weights based on data.\n\n\n\n\nNeater Formulation\nThe equation is rearranged for simplicity: \\(\\sum_{i=0}^{n} w_i x_i \\geq 0\\), where \\(x_0 = 1\\) and \\(w_0 = -\\text{threshold}\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "href": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation for Boolean Functions",
    "text": "Motivation for Boolean Functions\nBoolean functions provide a foundation for understanding perceptrons. For instance, predicting movie preferences using Boolean inputs such as actor, director, and genre."
  },
  {
    "objectID": "pages/DL/Week01_2.html#importance-of-weights",
    "href": "pages/DL/Week01_2.html#importance-of-weights",
    "title": "Motivation from Biological Neuron",
    "section": "Importance of Weights",
    "text": "Importance of Weights\nWeights signify the importance of specific inputs in decision-making. Learning from data helps adjust weights, reflecting user preferences. For example, assigning a high weight to the director may heavily influence the decision to watch a movie."
  },
  {
    "objectID": "pages/DL/Week01_2.html#bias-w_0",
    "href": "pages/DL/Week01_2.html#bias-w_0",
    "title": "Motivation from Biological Neuron",
    "section": "Bias (\\(w_0\\))",
    "text": "Bias (\\(w_0\\))\n\\(w_0\\) acts as a bias or prior, influencing decision-making. It represents the initial bias or prejudice in decision-making. Adjusting \\(w_0\\) alters the decision threshold, accommodating user preferences."
  },
  {
    "objectID": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "href": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Implementing Boolean Functions",
    "text": "Implementing Boolean Functions\nPerceptrons can implement Boolean functions with linear decision boundaries. For instance, implementing the OR function with a perceptron involves a geometric interpretation where a line separates positive and negative regions based on inputs."
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-adjustments",
    "href": "pages/DL/Week01_2.html#errors-and-adjustments",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Adjustments",
    "text": "Errors and Adjustments\nErrors arise when the decision boundary misclassifies inputs. The learning algorithm adjusts weights iteratively to minimize errors and enhance accuracy. It’s an iterative process where weights are modified until the desired decision boundary is achieved."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-3",
    "href": "pages/DL/Week01_2.html#introduction-3",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThis section delves into errors within the context of perceptrons and introduces error surfaces as a recurring theme in the course, with a focus on understanding errors related to linear separability."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "href": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron for AND Function",
    "text": "Perceptron for AND Function\nConsideration of the AND function showcases an output of 1 for a specific input (green) and 0 for others (red). The decision is based on \\(w_0 + w_1x_1 + w_2x_2 \\geq 0\\), with \\(w_0\\) fixed at -1. Exploration of the impact of \\(w_1\\) and \\(w_2\\) on the decision boundary is undertaken."
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "href": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Decision Boundaries",
    "text": "Errors and Decision Boundaries\nDemonstration of errors occurs with specific \\(w_1\\) and \\(w_2\\) values, showcasing misclassified points due to incorrect decision boundaries. Variability in errors is noted based on different weight values."
  },
  {
    "objectID": "pages/DL/Week01_2.html#error-function",
    "href": "pages/DL/Week01_2.html#error-function",
    "title": "Motivation from Biological Neuron",
    "section": "Error Function",
    "text": "Error Function\nViewing error as a function of \\(w_1\\) and \\(w_2\\) is introduced. The concept of error surfaces is brought in, where error is plotted against \\(w_1\\) and \\(w_2\\) values, each region on the surface corresponding to a distinct error level."
  },
  {
    "objectID": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "href": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "title": "Motivation from Biological Neuron",
    "section": "Visualizing the Error Surface",
    "text": "Visualizing the Error Surface\nThe error surface is plotted for \\(w_1\\) and \\(w_2\\) values in the range -4 to +4. Each region on the surface corresponds to a distinct error level, highlighting the utility of visualizations in comprehending perceptron behavior."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "href": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Learning Algorithm",
    "text": "Perceptron Learning Algorithm\nExploration of the necessity for an algorithmic approach to finding optimal \\(w_1\\) and \\(w_2\\) values is undertaken. Limitations in visual inspection, especially in higher dimensions, are acknowledged. A teaser for the upcoming module on the perceptron learning algorithm is provided as a solution for finding suitable weight values algorithmically."
  },
  {
    "objectID": "pages/DL/Week01_2.html#overview",
    "href": "pages/DL/Week01_2.html#overview",
    "title": "Motivation from Biological Neuron",
    "section": "Overview",
    "text": "Overview\nThis module focuses on the Perceptron Learning Algorithm, building upon the perceptron’s concept and introducing a method to iteratively adjust weights for accurate binary classification."
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation",
    "href": "pages/DL/Week01_2.html#motivation",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation",
    "text": "Motivation\nThe perceptron, initially designed for boolean functions, finds practical application in real-world scenarios. Consider a movie recommendation system based on past preferences, where features include both boolean and real-valued inputs. The goal is to learn weights that enable accurate predictions for new inputs."
  },
  {
    "objectID": "pages/DL/Week01_2.html#algorithm",
    "href": "pages/DL/Week01_2.html#algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Algorithm",
    "text": "Algorithm\n\nNotations\n\n\\(p\\): Inputs with label 1 (positive points)\n\\(n\\): Inputs with label 0 (negative points)\n\n\n\nConvergence\nConvergence is achieved when all positive points satisfy \\(\\sum w_i x_i &gt; 0\\) and all negative points satisfy \\(\\sum w_i x_i &lt; 0\\).\n\n\nSteps\n\nInitialization: Randomly initialize weights \\(w\\).\nIterative Update:\n\nWhile not converged:\n\nPick a random point \\(x\\) from \\(p \\cup n\\).\nIf \\(x\\) is in \\(p\\) and \\(w^T x &lt; 0\\), update \\(w = w + x\\).\nIf \\(x\\) is in \\(n\\) and \\(w^T x \\geq 0\\), update \\(w = w - x\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "href": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nUnderstanding the geometric relationship involves recognizing that the angle between \\(w\\) and a point on the decision boundary is 90 degrees. Positive points’ angles should be acute (&lt; 90 degrees), and negative points’ angles should be obtuse (&gt; 90 degrees). Iteratively adjusting \\(w\\) aligns it better with correctly classified points."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-4",
    "href": "pages/DL/Week01_2.html#introduction-4",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this lecture is to present a formal proof establishing the convergence of the perceptron learning algorithm. The primary focus is to rigorously determine whether the algorithm exhibits convergence or continues weight updates indefinitely."
  },
  {
    "objectID": "pages/DL/Week01_2.html#definitions",
    "href": "pages/DL/Week01_2.html#definitions",
    "title": "Motivation from Biological Neuron",
    "section": "Definitions",
    "text": "Definitions\n\nAbsolutely Linearly Separable Sets\n\nConsider two sets, \\(P\\) and \\(N\\), in an \\(n\\)-dimensional space. They are deemed absolutely linearly separable if there exist \\(n + 1\\) real numbers \\(w_0\\) to \\(w_n\\) such that the following conditions hold: \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n \\geq 0 \\quad \\text{for every } \\mathbf{x} \\in P\n\\] \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n &lt; 0 \\quad \\text{for every } \\mathbf{x} \\in N\n\\]\n\nPerceptron Learning Algorithm Convergence Theorem\n\nIf sets \\(P\\) and \\(N\\) are finite and linearly separable, the perceptron learning algorithm will update the weight vector a finite number of times. This implies that after a finite number of steps, the algorithm will find a weight vector \\(\\mathbf{w}\\) capable of separating sets \\(P\\) and \\(N\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#proof",
    "href": "pages/DL/Week01_2.html#proof",
    "title": "Motivation from Biological Neuron",
    "section": "Proof",
    "text": "Proof\n\nSetup\nDefine \\(P'\\) as the union of \\(P\\) and the negation of \\(N\\). Normalize all inputs for convenience.\n\n\nAssumptions and Definitions\nAssume the existence of a normalized solution vector \\(\\mathbf{w^*}\\). Define the minimum dot product, \\(\\delta\\), as the minimum value obtained by dot products between \\(\\mathbf{w^*}\\) and points in \\(P'\\).\n\n\nPerceptron Learning Algorithm\nThe perceptron learning algorithm can be expressed as follows:\n\nInitialization:\n\nInitialize weight vector \\(\\mathbf{w}\\) randomly.\n\nIteration:\n\nAt each iteration, randomly select a point \\(\\mathbf{p}\\) from \\(P'\\).\nIf the condition \\(\\mathbf{w}^T\\mathbf{p} \\geq 0\\) is not satisfied, update \\(\\mathbf{w}\\) by \\(\\mathbf{w} = \\mathbf{w} + \\mathbf{p}\\).\n\n\n\n\nNormalization and Definitions\nNormalize all inputs, ensuring the norm of \\(\\mathbf{p}\\) is 1. Define the numerator of \\(\\cos \\beta\\) as the dot product between \\(\\mathbf{w^*}\\) and the updated weight vector at each iteration.\n\n\nNumerator Analysis\nShow that the numerator is greater than or equal to \\(\\delta\\) for each iteration.\nFor a randomly selected \\(\\mathbf{p}\\), if \\(\\mathbf{w}^T\\mathbf{p} &lt; 0\\) and an update is performed, the numerator is:\n\\[\n\\mathbf{w^*} \\cdot (\\mathbf{w} + \\mathbf{p}) \\geq \\delta\n\\]\n\n\nDenominator Analysis\nExpand the denominator, the square of the norm of the updated weight vector:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 = \\|\\mathbf{w}\\|^2 + 2\\mathbf{w}^T\\mathbf{p} + \\|\\mathbf{p}\\|^2\n\\]\nShow that the denominator is less than or equal to a value involving \\(k\\), the number of updates made:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 \\leq \\|\\mathbf{w^*}\\|^2 + k\n\\]\n\n\nCombining Numerator and Denominator\nUse the definition of \\(\\cos \\beta\\) to conclude that \\(\\cos \\beta\\) is greater than or equal to a certain quantity involving the square root of \\(k\\):\n\\[\n\\cos \\beta \\geq \\frac{\\delta}{\\sqrt{k}}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BS Degree Notes",
    "section": "",
    "text": "I put the course notes for my own easy perusal."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/DL/Week01_1.html",
    "href": "pages/DL/Week01_1.html",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain.\n\n\n\nIn 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969.\n\n\n\nThe period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape.\n\n\n\nThe Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation.\n\n\n\nA disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy."
  },
  {
    "objectID": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "href": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain."
  },
  {
    "objectID": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "href": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969."
  },
  {
    "objectID": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "href": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape."
  },
  {
    "objectID": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "href": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation."
  },
  {
    "objectID": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "href": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "A disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy."
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction",
    "href": "pages/DL/Week01_1.html#introduction",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\n\nHistorical Perspective\nDeep learning encountered challenges in training via backpropagation. Jeff Hinton’s group proposed a crucial weight initialization idea in 2016, fostering stable training. The improved availability of computing power and data around 2006 laid the foundation for success."
  },
  {
    "objectID": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "href": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Early Challenges and Solutions",
    "text": "Early Challenges and Solutions\n\nUnsupervised Pre-training\nBetween 2007 and 2009, investigations into the effectiveness of unsupervised pre-training led to insights that shaped optimization and regularization algorithms. The course will delve into topics such as initializations, regularizations, and optimizations."
  },
  {
    "objectID": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "href": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Emergence of Deep Learning",
    "text": "Emergence of Deep Learning\n\nPractical Utility\nDeep learning applications started winning competitions, including handwriting recognition on the MNIST dataset, speech recognition, and visual pattern recognition like traffic sign data.\n\n\nImageNet Challenge (2012-2016)\nThe ImageNet challenge, a pivotal turning point, witnessed the evolution from ZFNet to ResNet (152 layers), achieving a remarkable 3.6% error rate in 2016, surpassing human performance."
  },
  {
    "objectID": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "href": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transition Period (2012-2016)",
    "text": "Transition Period (2012-2016)\n\nGolden Period of Deep Learning\nThe universal acceptance of deep learning marked its golden period, with convolutional neural networks dominating image-related problems. Similar trends were observed in natural language processing (NLP) and speech processing."
  },
  {
    "objectID": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "href": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "title": "A Historical Overview of Deep Learning",
    "section": "From Cats to Convolutional Neural Networks",
    "text": "From Cats to Convolutional Neural Networks\n\nMotivation from Neural Science (1959)\nAn experiment with a cat’s brain in 1959 revealed different parts activated for different stick positions, motivating the concept of receptive fields in convolutional neural networks (CNNs).\n\n\nNeocognitron Model (1980)\nInspired by distributed processing observed in the cat experiment, the Neocognitron model utilized receptive fields for different parts of the network.\n\n\nLeNet Model (1989)\nJan Lecun’s contribution to deep learning, the LeNet model, was employed for recognizing handwritten digits, finding applications in postal services for automated sorting of letters.\n\n\nLeNet-5 Model (1998)\nFurther improvements on the LeNet model, introducing the MNIST dataset for testing CNNs."
  },
  {
    "objectID": "pages/DL/Week01_1.html#history-of-deep-learning",
    "href": "pages/DL/Week01_1.html#history-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "History of Deep Learning",
    "text": "History of Deep Learning\n\n1950s: Enthusiasm in AI.\n1990s: Convolutional Neural Networks (CNNs) used for real-world problems, challenges with large networks and training.\n2006-2012: Advances in deep learning, successful training for ImageNet challenges.\n2016 onwards: Acceleration with better optimization methods (Nesterov’s method), leading to faster convergence.\nOptimization Algorithms: Adagrad, RMSprop, Adam, AdamW, etc., focus on faster and better convergence."
  },
  {
    "objectID": "pages/DL/Week01_1.html#activation-functions",
    "href": "pages/DL/Week01_1.html#activation-functions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Activation Functions",
    "text": "Activation Functions\nThe evolution from the logistic function to various activation functions (ReLU, Leaky ReLU, Parametric ReLU, Tanh, etc.) aimed at stabilizing training, achieving better performance, and faster convergence. The use of improved activation functions contributed to enhanced stability and performance."
  },
  {
    "objectID": "pages/DL/Week01_1.html#sequence-processing",
    "href": "pages/DL/Week01_1.html#sequence-processing",
    "title": "A Historical Overview of Deep Learning",
    "section": "Sequence Processing",
    "text": "Sequence Processing\nIntroduction to problems involving sequences in deep learning, featuring Recurrent Neural Networks (RNNs) proposed in 1982 for sequence processing. Long Short-Term Memory Cells (LSTMs) were introduced in 1997 to address the vanishing gradient problem. By 2014, RNNs and LSTMs dominated natural language processing (NLP) and speech applications. In 2017, Transformer networks started replacing RNNs and LSTMs in sequence learning."
  },
  {
    "objectID": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "href": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Game Playing with Deep Learning",
    "text": "Game Playing with Deep Learning\n\n2015: Deep Reinforcement Learning (DRL) agents beat humans in Atari games.\nBreakthrough in Go game playing using DRL in 2015.\n2016: DRL-based agents beat professional poker players.\nComplex strategy games like Dota 2 mastered by DRL agents.\nIntroduction of OpenAI Gym as a toolkit for developing and comparing reinforcement learning algorithms.\nEmergence of AlphaStar and MuZero for mastering multiple games and tasks."
  },
  {
    "objectID": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "href": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "General Trends in Deep Reinforcement Learning",
    "text": "General Trends in Deep Reinforcement Learning\nDeep RL agents consistently outperforming humans in various complex games, progressing from simple environments to mastering complex strategy games. The trend is towards developing “master of all” models (e.g., MuZero) for general intelligence in multiple tasks."
  },
  {
    "objectID": "pages/DL/Week01_1.html#overview",
    "href": "pages/DL/Week01_1.html#overview",
    "title": "A Historical Overview of Deep Learning",
    "section": "Overview",
    "text": "Overview\n\nRevival and Advances\nA recap of deep learning’s revival and recent advances, reflecting an increasing interest in real-world problem-solving and challenges.\n\n\nAI Publications Growth\nThe Stanford AI Index Report highlights a significant increase in AI publications, indicating exponential growth across machine learning, computer vision, and NLP.\n\n\nFunding and Startups\nThe rise of AI startups, coupled with the interest from major tech companies, has led to exponential growth in AI-related patent filings."
  },
  {
    "objectID": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "href": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Evolution of Neural Network Models",
    "text": "Evolution of Neural Network Models\n\nIntroduction of Transformers\nIn 2017, transformers were introduced, revolutionizing AI and finding success in NLP, subsequently adopted in other domains.\n\n\nMachine Translation and Transformers\nA historical overview of machine translation, emphasizing the shift from IBM models to neural machine translation. The impact of sequence-to-sequence models (2014) and transformers (2017) is discussed.\n\n\nTransformer-based Models\nThe BERT model (2018) with a focus on pre-training, the evolution of models with increasing parameters from GPT-3 (175 billion) to 1.6 trillion parameters, and a comparison with human brain synapses provide perspective."
  },
  {
    "objectID": "pages/DL/Week01_1.html#transformers-in-vision",
    "href": "pages/DL/Week01_1.html#transformers-in-vision",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transformers in Vision",
    "text": "Transformers in Vision\n\nAdoption in Image\nClassification The evolution of image classification models, starting with AlexNet (2012), is traced. Transformers entered image classification and object detection in 2019, marking a paradigm shift towards transformers in state-of-the-art models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#generative-models",
    "href": "pages/DL/Week01_1.html#generative-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Generative Models",
    "text": "Generative Models\n\nOverview\nAn introduction to generative models for image synthesis, covering the evolution from variation autoencoders to GANs (Generative Adversarial Networks). Recent developments in diffusion-based models overcoming GAN drawbacks are discussed.\n\n\nDALL-E and DALL-E 2\nDALL-E’s capability to generate realistic images based on text prompts is explored. The introduction of DALL-E 2, a diffusion-based model, exceeding expectations, is highlighted with examples of generated images showcasing photorealistic results.\n\n\nExciting Times in Generative Models\nThe exploration of generative models for realistic image generation is showcased, with examples of prompts generating photorealistic images illustrating field advancements."
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction-1",
    "href": "pages/DL/Week01_1.html#introduction-1",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\nRapid advancements in deep learning have yielded powerful models trained on large datasets, showcasing impressive results. However, there is a growing need for sanity, interpretability, fairness, and responsibility in deploying these models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "href": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Paradox of Deep Learning",
    "text": "Paradox of Deep Learning\nDespite the high capacity of deep learning models, they exhibit remarkable performance. Challenges include numerical instability, sharp minima, and susceptibility to adversarial examples."
  },
  {
    "objectID": "pages/DL/Week01_1.html#calls-for-sanity",
    "href": "pages/DL/Week01_1.html#calls-for-sanity",
    "title": "A Historical Overview of Deep Learning",
    "section": "Calls for Sanity",
    "text": "Calls for Sanity\nEmphasis is placed on explainability and interpretability to comprehend model decisions. Advances include workshops on human interpretability, tools like the Clever Hans toolkit to identify model reliance on cues, and benchmarking on adversarial examples."
  },
  {
    "objectID": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "href": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "title": "A Historical Overview of Deep Learning",
    "section": "Fairness and Responsibility",
    "text": "Fairness and Responsibility\nIncreasing awareness of biases in AI models, particularly in facial recognition and criminal risk predictions, has led to concerns about fairness. Efforts such as the AI audit challenge at Stanford focus on building non-discriminatory models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#green-ai",
    "href": "pages/DL/Week01_1.html#green-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Green AI",
    "text": "Green AI\nRising environmental concerns due to the high computational power and energy consumption of deep learning models have spurred calls for responsible AI, extending to the environmental impact. There is a push for more energy-efficient models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "href": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Exciting Times in AI",
    "text": "Exciting Times in AI\nThe AI revolution is influencing scientific research, evident in DeepMind’s AlphaFold predicting protein folding. Applications in astronomy, predicting galaxy aging, and generating images for fundamental variables in experimental data are emerging. There is an emphasis on efficient deep learning for mobile devices, edge computing, and addressing constraints of power, storage, and real-time processing."
  },
  {
    "objectID": "pages/SE/Week01.html",
    "href": "pages/SE/Week01.html",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "In the contemporary landscape of online platforms, exemplified by industry leader Amazon, the intricate systems governing processes such as ordering and delivery are constructed incrementally. In contrast to a monolithic approach, these systems evolve feature by feature. This incremental strategy arises from the inherent uncertainty surrounding the complete set of required features at the project’s inception.\n\n\n\nWithin the domain of software engineering, the concept of components assumes a pivotal role. These components serve as manageable units that facilitate collaborative efforts by different teams, each working on distinct facets of the system. These individual aspects are later integrated into a coherent whole. Importantly, effective collaboration is achieved by understanding a component’s interface, which shields the intricacies of its internal workings.\n\n\nPurpose: The Inventory Management System is designed to intelligently track and manage inventory.\nDefinition: This involves measuring quantity, location, pricing, and the composition of products available on platforms like Amazon.\nCustomization: Amazon’s homepage dynamically updates based on factors such as purchasing trends, seasonal variations, customer demand, and logistical and analytical considerations.\n\n\n\nPurpose: The Payment Gateway facilitates electronic payments, ensuring a seamless experience for buyers and sellers.\nDefinition: Serving as a service authorizing electronic payments (e.g., online banking, debit cards), the Payment Gateway acts as an intermediary between the bank and the merchant’s platform.\nProcess: The gateway validates payment details, confirming their legitimacy with the bank before transferring the specified amount from the user’s account to the platform.\n\n\n\n\nLarge-scale systems, exemplified by the infrastructure of industry leaders like Amazon, do not materialize in a single endeavor. Instead, they are deconstructed into components or modules that can be independently developed before harmonious integration. This integration phase involves establishing communication pathways between the modules."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction",
    "href": "pages/SE/Week01.html#introduction",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "In the contemporary landscape of online platforms, exemplified by industry leader Amazon, the intricate systems governing processes such as ordering and delivery are constructed incrementally. In contrast to a monolithic approach, these systems evolve feature by feature. This incremental strategy arises from the inherent uncertainty surrounding the complete set of required features at the project’s inception."
  },
  {
    "objectID": "pages/SE/Week01.html#components-in-software-systems",
    "href": "pages/SE/Week01.html#components-in-software-systems",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "Within the domain of software engineering, the concept of components assumes a pivotal role. These components serve as manageable units that facilitate collaborative efforts by different teams, each working on distinct facets of the system. These individual aspects are later integrated into a coherent whole. Importantly, effective collaboration is achieved by understanding a component’s interface, which shields the intricacies of its internal workings.\n\n\nPurpose: The Inventory Management System is designed to intelligently track and manage inventory.\nDefinition: This involves measuring quantity, location, pricing, and the composition of products available on platforms like Amazon.\nCustomization: Amazon’s homepage dynamically updates based on factors such as purchasing trends, seasonal variations, customer demand, and logistical and analytical considerations.\n\n\n\nPurpose: The Payment Gateway facilitates electronic payments, ensuring a seamless experience for buyers and sellers.\nDefinition: Serving as a service authorizing electronic payments (e.g., online banking, debit cards), the Payment Gateway acts as an intermediary between the bank and the merchant’s platform.\nProcess: The gateway validates payment details, confirming their legitimacy with the bank before transferring the specified amount from the user’s account to the platform."
  },
  {
    "objectID": "pages/SE/Week01.html#incremental-system-development",
    "href": "pages/SE/Week01.html#incremental-system-development",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "Large-scale systems, exemplified by the infrastructure of industry leaders like Amazon, do not materialize in a single endeavor. Instead, they are deconstructed into components or modules that can be independently developed before harmonious integration. This integration phase involves establishing communication pathways between the modules."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-1",
    "href": "pages/SE/Week01.html#introduction-1",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software engineering, a comprehensive understanding of a software system’s components and their interactions is fundamental. This lecture explores the intricacies of the software development process, using the example of Amazon Pay, a mobile wallet, to elucidate key concepts."
  },
  {
    "objectID": "pages/SE/Week01.html#amazon-pay-overview",
    "href": "pages/SE/Week01.html#amazon-pay-overview",
    "title": "Thinking of Software in terms of Components",
    "section": "Amazon Pay Overview",
    "text": "Amazon Pay Overview\n\nFeatures\nAmazon Pay, a mobile wallet, facilitates digital cash transactions by offering a spectrum of features. Users can link credit/debit cards, bank accounts, and engage in various transactions, including recharges, bill payments, travel bookings, insurance, and redemption of rewards and gift vouchers. Two notable functionalities include the ability to add money and an auto-reload feature."
  },
  {
    "objectID": "pages/SE/Week01.html#software-development-process",
    "href": "pages/SE/Week01.html#software-development-process",
    "title": "Thinking of Software in terms of Components",
    "section": "Software Development Process",
    "text": "Software Development Process\n\n1. Identifying the Problem\nBefore delving into programming languages, the foremost step in the software development process involves a profound understanding of the problem at hand. This recognition sets the stage for subsequent development efforts.\n\n\n2. Studying Existing Components\nTo gain insights into the intricacies of system components, a meticulous examination of existing elements, such as inventory management and payment gateways, is crucial. Additionally, studying analogous systems, like Paytm and PhonePe, aids in identifying essential features.\n\n\n3. Defining System Requirements\nThe foundation of the development process lies in explicitly defining system requirements. These requirements, derived from a thorough analysis of existing systems, serve as the guiding principles throughout the software development lifecycle."
  },
  {
    "objectID": "pages/SE/Week01.html#clients-in-software-systems",
    "href": "pages/SE/Week01.html#clients-in-software-systems",
    "title": "Thinking of Software in terms of Components",
    "section": "Clients in Software Systems",
    "text": "Clients in Software Systems\n\nDefinition of Client\nClients, referring to users of the software system, can be categorized as either external or internal entities. External clients are end-users or buyers, while internal clients encompass components within the system itself.\n\n\nTypes of Clients\n\nExternal Clients\nFor instance, in mobile banking software, external clients are bank customers utilizing features like account balance checks and money transfers.\n\n\nInternal Clients\nInternal clients may include teams within a company, such as an internal products team constructing an employee resources portal by collaborating with various departments.\n\n\n\nSoftware-to-Software Clients\nIn certain scenarios, software components, like payment gateways (e.g., Razer Pay), act as clients, facilitating communication between an e-commerce website and customers’ banks."
  },
  {
    "objectID": "pages/SE/Week01.html#importance-of-gathering-requirements",
    "href": "pages/SE/Week01.html#importance-of-gathering-requirements",
    "title": "Thinking of Software in terms of Components",
    "section": "Importance of Gathering Requirements",
    "text": "Importance of Gathering Requirements\n\nSignificance of the First Step\nGathering requirements stands as the initial and crucial step in the software development process. This process ensures a holistic understanding of users or clients, and adherence to requirements at every stage is imperative for meeting end-user needs."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-2",
    "href": "pages/SE/Week01.html#introduction-2",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we delved into the initial steps of the software development cycle, primarily focusing on the gathering of requirements. However, a common misconception arises at this juncture—many individuals are inclined to proceed directly to coding. This session aims to dispel this notion through a practical example."
  },
  {
    "objectID": "pages/SE/Week01.html#example-implementation-of-amazon-pay-feature",
    "href": "pages/SE/Week01.html#example-implementation-of-amazon-pay-feature",
    "title": "Thinking of Software in terms of Components",
    "section": "Example: Implementation of Amazon Pay Feature",
    "text": "Example: Implementation of Amazon Pay Feature\nConsider a scenario where a small team is eager to implement the Amazon Pay feature based on gathered requirements. The tendency to immediately engage in coding poses several challenges that warrant careful consideration.\n\nPitfalls of Skipping Design Phase\n\nDivergent Implementation Ideas:\n\nDevelopers may harbor disparate concepts regarding the feature’s implementation.\nChanges made by one developer could inadvertently impact others.\n\nInterconnected Components Challenge:\n\nComponents developed by different individuals may intertwine, resulting in complications.\nLack of a holistic view impedes the seamless integration of features."
  },
  {
    "objectID": "pages/SE/Week01.html#the-significance-of-the-design-phase",
    "href": "pages/SE/Week01.html#the-significance-of-the-design-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "The Significance of the Design Phase",
    "text": "The Significance of the Design Phase\nThe design phase serves as a crucial precursor to the coding phase, offering distinct advantages in the software development process.\n\nCreating a System Overview\nThe primary goal is to construct a comprehensive overview of the entire system. This macroscopic perspective aids in organizing the subsequent coding phase efficiently.\n\n\nBenefits of a Well-Executed Design Phase\n\nConsistency:\n\nMitigates conflicts stemming from diverse developer perspectives.\nEnsures a uniform comprehension of the codebase.\n\nEfficiency Enhancement:\n\nPrecludes unnecessary alterations and errors during the implementation phase.\nFacilitates punctual product delivery.\n\nFuture-Proofing:\n\nStreamlines the addition of new features in subsequent phases.\nEnables seamless integration into the existing system."
  },
  {
    "objectID": "pages/SE/Week01.html#the-development-phase",
    "href": "pages/SE/Week01.html#the-development-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "The Development Phase",
    "text": "The Development Phase\nFollowing the design phase, the development phase entails collaborative coding efforts involving multiple developers. This phase often unfolds in a distributed manner, with team members situated in diverse locations and time zones. Collaboration tools such as GitHub play a pivotal role in this collective coding endeavor."
  },
  {
    "objectID": "pages/SE/Week01.html#imperative-role-of-documentation",
    "href": "pages/SE/Week01.html#imperative-role-of-documentation",
    "title": "Thinking of Software in terms of Components",
    "section": "Imperative Role of Documentation",
    "text": "Imperative Role of Documentation\nGiven the dispersed nature of development efforts, comprehensive documentation becomes imperative. This documentation, encompassing precise interface definitions, ensures a consistent understanding of code functionality among developers.\n\nInterface Definitions\nInterface definitions are foundational descriptions outlining the actions that functions can perform. Distinctively, the focus is on delineating actions rather than delving into intricate implementation details. Such definitions stipulate the types of requests accepted and the corresponding format of responses. The flexibility for code modifications exists, provided the interface remains consistent."
  },
  {
    "objectID": "pages/SE/Week01.html#collaborative-dynamics-in-development",
    "href": "pages/SE/Week01.html#collaborative-dynamics-in-development",
    "title": "Thinking of Software in terms of Components",
    "section": "Collaborative Dynamics in Development",
    "text": "Collaborative Dynamics in Development\nCollaboration during the development phase entails the coordinated efforts of multiple developers, often located in different time zones. Effective communication, facilitated through clear and concise interface definitions, is paramount to achieving seamless integration of components."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-3",
    "href": "pages/SE/Week01.html#introduction-3",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn the preceding video, we explored the design and development phases crucial to software development. However, two additional pivotal phases demand our attention: Testing and Maintenance."
  },
  {
    "objectID": "pages/SE/Week01.html#importance-of-testing",
    "href": "pages/SE/Week01.html#importance-of-testing",
    "title": "Thinking of Software in terms of Components",
    "section": "Importance of Testing",
    "text": "Importance of Testing\nTesting serves as a critical measure to ensure the alignment of software behavior with specified requirements. The existence of bugs and defects, if left unaddressed, may lead\nto substantial financial losses. For instance, a noteworthy study indicates that in 2002, software bugs resulted in a $60 billion loss in the U.S. economy, a figure that surged to $1.1 trillion in 2016. The failure to rectify such bugs can potentially precipitate severe catastrophes."
  },
  {
    "objectID": "pages/SE/Week01.html#testing-granularities",
    "href": "pages/SE/Week01.html#testing-granularities",
    "title": "Thinking of Software in terms of Components",
    "section": "Testing Granularities",
    "text": "Testing Granularities\n\n1. Unit Testing\nUnit testing directs its focus toward a singular component, often a class or function, examined in complete isolation.\n\n\n2. Integration Testing\nIntegration testing scrutinizes the interaction and collaboration of different parts within the application, ensuring seamless functionality as a unified whole.\n\n\n3. Acceptance Testing\nAcceptance testing verifies the fulfillment of user requirements. This testing stage bifurcates into:\n\nAlpha Testing\nInternal employees conduct alpha testing within a controlled environment, such as a lab or staging area.\n\n\nBeta Testing\nActual users undertake beta testing in real-world scenarios, providing valuable insights into the software’s performance."
  },
  {
    "objectID": "pages/SE/Week01.html#maintenance-phase",
    "href": "pages/SE/Week01.html#maintenance-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "Maintenance Phase",
    "text": "Maintenance Phase\n\nPurpose\n\nUser Monitoring:\n\nContinuous observation of user activities and software usage.\n\nCode Changes:\n\nImplementation of code modifications for upgrades, including patch releases.\n\nFeature Addition:\n\nIntroduction of new features to enhance software functionality."
  },
  {
    "objectID": "pages/SE/Week01.html#example---amazon-pay",
    "href": "pages/SE/Week01.html#example---amazon-pay",
    "title": "Thinking of Software in terms of Components",
    "section": "Example - Amazon Pay",
    "text": "Example - Amazon Pay\n\nPost-Release Issues\nAfter the release of a feature like Amazon Pay, potential difficulties or errors that users may encounter must be anticipated. Examples include missed conditions, failures, and UI issues specific to certain browsers.\n\n\nMaintenance Process\nThe maintenance phase involves a systematic approach where the development team identifies issues and engages in a continuous process of rectification to ensure optimal software performance."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-4",
    "href": "pages/SE/Week01.html#introduction-4",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nSoftware engineering is a discipline that advocates a systematic approach to the development of software through a well-defined and structured set of activities. These activities are commonly denoted as the software lifecycle model, software development lifecycle (SDLC), or the software development process model."
  },
  {
    "objectID": "pages/SE/Week01.html#waterfall-model",
    "href": "pages/SE/Week01.html#waterfall-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Waterfall Model",
    "text": "Waterfall Model\n\nSequential Phases\nThe waterfall model entails a linear progression of phases, with each phase following the completion of the previous one. These phases encompass gathering requirements, design, coding, testing, and maintenance. The approach is also recognized as the plan and document perspective.\n\n\nDrawbacks\nDespite its structured nature, the waterfall model has notable drawbacks:\n\nIncreased Cost and Time: Modifications later in the process lead to elevated costs and time consumption.\nClient Understanding: Clients may not fully comprehend their needs initially.\nDesign Challenges: Developers may face challenges in determining the most feasible design.\nLengthy Iterations: Each phase or iteration can span from 6 to 18 months."
  },
  {
    "objectID": "pages/SE/Week01.html#prototype-model",
    "href": "pages/SE/Week01.html#prototype-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Prototype Model",
    "text": "Prototype Model\n\nConcept and Execution\nTo address the drawbacks of the waterfall model, the prototype model advocates the creation of a working prototype of the system before the actual software development begins. The prototype, possessing limited functionality, is subsequently discarded or replaced with the final product.\n\n\nAdvantages and Disadvantages\nAdvantages: - Enhanced understanding for both clients and developers regarding project requirements.\nDisadvantages: - Augmented development costs. - Inability to anticipate risks and bugs emerging later in the development cycle."
  },
  {
    "objectID": "pages/SE/Week01.html#spiral-model",
    "href": "pages/SE/Week01.html#spiral-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Spiral Model",
    "text": "Spiral Model\n\nIntegration of Approaches\nThe spiral model amalgamates features from both the waterfall and prototype models. It unfolds in four distinct phases: determining objectives, evaluating alternatives, developing and testing, and planning for the subsequent phase. Each iteration involves a refinement of the prototype.\n\n\nIterative Process\nThis model fosters an iterative process, where the refinement of the prototype occurs at each iteration. Unlike the waterfall model, requirement documents are progressively developed across iterations. Client involvement at the end of each iteration mitigates misunderstandings.\n\n\nDrawback\nDespite its advantages, the spiral model still encounters a drawback: each iteration may extend from 6 to 24 months."
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-5",
    "href": "pages/SE/Week01.html#introduction-5",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn our previous lectures, we navigated through the intricacies of the software development lifecycle, concentrating particularly on established models like the waterfall model. While these models, falling under the plan and document process category, brought structure to software development, they faced considerable challenges in meeting deadlines and adhering to specified budgets. Surprisingly, studies conducted from 1995 to 2013 indicated that around 80 to 90 percent of software projects encountered issues such as overdue timelines, exceeding budgetary limits, or even abandonment. This realization triggered a significant shift in software development methodologies."
  },
  {
    "objectID": "pages/SE/Week01.html#emergence-of-the-agile-manifesto",
    "href": "pages/SE/Week01.html#emergence-of-the-agile-manifesto",
    "title": "Thinking of Software in terms of Components",
    "section": "Emergence of the Agile Manifesto",
    "text": "Emergence of the Agile Manifesto\nApproximately two decades ago, in February 2001, a coalition of software developers convened to devise a more flexible software development lifecycle. This effort culminated in the creation of the Agile Manifesto, a document founded on four key principles. The manifesto aimed to address the shortcomings of traditional approaches, laying the groundwork for a more lightweight and adaptive software development process.\n\nAgile Manifesto Principles\n\nIndividuals and Interactions over Processes and Tools:\n\nEmphasizes the importance of interpersonal dynamics within the development team and effective communication with clients.\n\nWorking Software over Comprehensive Documentation:\n\nPrioritizes the delivery of functional software in increments over exhaustive documentation.\n\nCustomer Collaboration over Contract Negotiation:\n\nAdvocates for active collaboration with customers to understand their needs rather than fixating on contractual minutiae.\n\nResponding to Change over Following a Plan:\n\nEncourages adaptability to change during the development process, emphasizing responsiveness."
  },
  {
    "objectID": "pages/SE/Week01.html#agile-development-approach",
    "href": "pages/SE/Week01.html#agile-development-approach",
    "title": "Thinking of Software in terms of Components",
    "section": "Agile Development Approach",
    "text": "Agile Development Approach\nThe Agile development approach is characterized by its iterative and incremental model. Teams adopting Agile construct the software product in small, manageable increments through multiple iterations. This process involves developing prototypes for key features, promptly releasing them for feedback. Noteworthy Agile methodologies include Extreme Programming (XP), Scrum, and Kanban.\n\nExtreme Programming (XP)\nExtreme Programming incorporates key practices such as behavior-driven design, test-driven development, and pair programming. These practices contribute to a development environment centered around quick iterations and continuous feedback.\n\n\nScrum\nScrum, another Agile methodology, divides the product development into iterations known as sprints, typically lasting one to two weeks. This approach facilitates breaking down complex projects into more manageable and actionable components.\n\n\nKanban\nIn Kanban, the software is segmented into small work items, visually represented on a Kanban board. This visual aid enables team members to monitor the status of each work item in real-time."
  },
  {
    "objectID": "pages/SE/Week01.html#choosing-the-development-perspective",
    "href": "pages/SE/Week01.html#choosing-the-development-perspective",
    "title": "Thinking of Software in terms of Components",
    "section": "Choosing the Development Perspective",
    "text": "Choosing the Development Perspective\nSelecting between the plan and document perspective and the Agile perspective depends on various factors. Key considerations include the fixity of requirements, client availability, system characteristics, team distribution, team familiarity with documentation models, and the presence of regulatory constraints.\n\nFactors Influencing Choice\n\nRequirements/Specifications Fixity:\n\nAre requirements/specifications mandated to be fixed upfront?\n\nClient Availability:\n\nIs the client or customer consistently available for collaboration?\n\nSystem Characteristics:\n\nDoes the system possess characteristics like size and complexity that warrant extensive planning and documentation?\n\nTeam Distribution:\n\nIs the software team geographically dispersed?\n\nTeam Familiarity:\n\nIs the team already acquainted with the plan and document model?\n\nRegulatory Constraints:\n\nIs the system subject to numerous regulatory requirements?"
  },
  {
    "objectID": "pages/SE/Week01.html#points-to-remember",
    "href": "pages/SE/Week01.html#points-to-remember",
    "title": "Thinking of Software in terms of Components",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nIncremental System Development: Large-scale systems, like those employed by industry leaders such as Amazon, evolve incrementally, emphasizing the construction of components or modules before their integration.\nRequirement Specification: The software development process begins with a deep understanding of the problem, studying existing components, and defining system requirements derived from thorough analyses.\nSoftware Design and Development: The design phase is crucial, providing a macroscopic perspective of the entire system and offering advantages such as consistency, efficiency enhancement, and future-proofing. Effective documentation and collaborative coding are imperative during the development phase.\nTesting and Maintenance: Testing is critical to ensure software behavior aligns with requirements, and maintenance involves continuous monitoring, code changes, and feature additions to enhance software functionality.\nWaterfall Model: A structured, sequential model with phases like gathering requirements, design, coding, testing, and maintenance. However, it has drawbacks, including increased cost and time.\nPrototype Model: Advocates creating a working prototype before actual development to enhance understanding but may incur augmented development costs.\nSpiral Model: Integrates features from both waterfall and prototype models, fostering an iterative process, but each iteration may extend over a considerable duration.\nAgile Development: A response to challenges faced by traditional approaches, characterized by an iterative and incremental model. Agile methodologies include Extreme Programming (XP), Scrum, and Kanban.\nAgile Manifesto Principles:\n\nIndividuals and interactions over processes and tools.\nWorking software over comprehensive documentation.\nCustomer collaboration over contract negotiation.\nResponding to change over following a plan.\n\nChoosing the Development Perspective: Factors influencing the choice between plan and document perspective and Agile perspective include requirements fixity, client availability, system characteristics, team distribution, team familiarity, and regulatory constraints."
  }
]