[
  {
    "objectID": "pages/RL/Week02.html",
    "href": "pages/RL/Week02.html",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The MAB problem conceptualizes actions as arms, each associated with a reward drawn from a probability distribution. The quest is to identify the arm with the highest mean reward, denoted as \\(\\mu^*\\), and consistently exploit it for maximum cumulative reward.\n\n\n\n\n\\(r_{i,k}\\): Reward obtained when selecting the \\(i\\)-th action for the \\(k\\)-th time.\n\\(Q(a_i)\\): Expected reward for selecting action \\(a_i\\) based on historical experiences.\n\\(Q(a^*)\\): The action maximizing the expected reward.\n\\(\\mu_i\\): True average reward for selecting action \\(a_i\\).\n\n\n\n\nThe estimation of \\(Q(a_i)\\) involves aggregating observed rewards for action \\(a_i\\) and dividing by the number of times the action is taken. Mathematically:\n\\[Q(a_i) = \\frac{\\sum_{k=1}^{n_i} r_{i,k}}{n_i}\\]\nHere, \\(n_i\\) represents the number of times action \\(a_i\\) is chosen.\n\n\n\nThe dynamic nature of the estimation process demands continuous updates. The formula for updating the estimate employs a learning rate (\\(\\alpha\\)) to adjust for new information:\n\\[Q_{k+1}(a_i) = Q_k(a_i) + \\alpha [r_{i,k} - Q_k(a_i)]\\]\nIn this formula, \\(Q_{k+1}(a_i)\\) is the updated estimate, \\(Q_k(a_i)\\) is the current estimate, \\(r_{i,k}\\) is the latest reward, and \\(\\alpha\\) governs the rate of adaptation.\n\n\n\nNavigating the exploration-exploitation dilemma requires a delicate balance. Striking the right equilibrium ensures optimal learning and maximizes cumulative rewards over time.\n\n\n\nAn analogy is drawn to a slot machine (one-arm bandit) with multiple levers (arms), each having distinct probabilities of payoff. The challenge mirrors that of identifying the lever (action) with the highest probability of payoff (mean reward).\n\n\n\nThe learning rate (\\(\\alpha\\)) serves as a crucial parameter influencing the rate at which the model adapts to new information. Choices of \\(\\alpha\\) lead to variations in the update rule, determining the emphasis on recent versus older rewards.\n\n\n\nConsider two actions, \\(A_1\\) and \\(A_2\\), each having distinct reward probabilities. For \\(A_1\\), the rewards are \\(+1\\) with a probability of \\(0.8\\) and \\(0\\) with a probability of \\(0.2\\). On the other hand, \\(A_2\\) yields \\(+1\\) with a probability of \\(0.6\\) and \\(0\\) with a probability of \\(0.4\\).\n\n\n\nA challenge arises when choosing actions based on initial rewards. If, for instance, \\(A_2\\) is selected first and a reward of \\(+1\\) is obtained, there is a risk of getting stuck with \\(A_2\\) due to its higher immediate reward probability. The same issue arises if starting with \\(A_1\\).\n\n\n\n\n\nThe Epsilon-Greedy strategy involves a balance between exploitation and exploration. It mainly consists of selecting the action with the highest estimated value most of the time (\\(1 - \\epsilon\\)), while occasionally exploring other actions with a probability of \\(\\epsilon\\). Here, \\(\\epsilon\\) is a small value, typically \\(0.1\\) or \\(0.01\\), determining the exploration rate. The strategy ensures asymptotic convergence, guaranteeing exploration of all actions in the long run.\n\n\n\nThe Softmax strategy employs a mathematical function to convert estimated action values into a probability distribution. The Softmax function is defined as:\n\\[P(A_i) = \\frac{e^{Q(A_i) / \\tau}}{\\sum_{j} e^{Q(A_j) / \\tau}}\\]\nWhere:\n\n\\(Q(A_i)\\) represents the estimated value of action \\(A_i\\),\n\\(\\tau\\) is the temperature parameter.\n\nThe temperature parameter (\\(\\tau\\)) controls the sensitivity to differences in estimated values. When \\(\\tau\\) is high, the probability distribution becomes more uniform, favoring exploration. Conversely, a low \\(\\tau\\) emphasizes exploiting the best-known action. Softmax also provides asymptotic convergence, ensuring exploration of all actions over time.\n\n\n\n\nThe temperature parameter, \\(\\tau\\), is a crucial factor in the Softmax strategy. A higher \\(\\tau\\) results in a more uniform probability distribution, making exploration more likely. Conversely, a lower \\(\\tau\\) amplifies differences in estimated values, making the strategy closer to a greedy approach.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#estimating-expected-reward",
    "href": "pages/RL/Week02.html#estimating-expected-reward",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The estimation of \\(Q(a_i)\\) involves aggregating observed rewards for action \\(a_i\\) and dividing by the number of times the action is taken. Mathematically:\n\\[Q(a_i) = \\frac{\\sum_{k=1}^{n_i} r_{i,k}}{n_i}\\]\nHere, \\(n_i\\) represents the number of times action \\(a_i\\) is chosen.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#updating-estimated-reward",
    "href": "pages/RL/Week02.html#updating-estimated-reward",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The dynamic nature of the estimation process demands continuous updates. The formula for updating the estimate employs a learning rate (\\(\\alpha\\)) to adjust for new information:\n\\[Q_{k+1}(a_i) = Q_k(a_i) + \\alpha [r_{i,k} - Q_k(a_i)]\\]\nIn this formula, \\(Q_{k+1}(a_i)\\) is the updated estimate, \\(Q_k(a_i)\\) is the current estimate, \\(r_{i,k}\\) is the latest reward, and \\(\\alpha\\) governs the rate of adaptation.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#challenges-and-considerations",
    "href": "pages/RL/Week02.html#challenges-and-considerations",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "Navigating the exploration-exploitation dilemma requires a delicate balance. Striking the right equilibrium ensures optimal learning and maximizes cumulative rewards over time.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#multi-arm-bandit-analogy",
    "href": "pages/RL/Week02.html#multi-arm-bandit-analogy",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "An analogy is drawn to a slot machine (one-arm bandit) with multiple levers (arms), each having distinct probabilities of payoff. The challenge mirrors that of identifying the lever (action) with the highest probability of payoff (mean reward).",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#learning-rate-alpha",
    "href": "pages/RL/Week02.html#learning-rate-alpha",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The learning rate (\\(\\alpha\\)) serves as a crucial parameter influencing the rate at which the model adapts to new information. Choices of \\(\\alpha\\) lead to variations in the update rule, determining the emphasis on recent versus older rewards.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#actions-and-reward-probabilities",
    "href": "pages/RL/Week02.html#actions-and-reward-probabilities",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "Consider two actions, \\(A_1\\) and \\(A_2\\), each having distinct reward probabilities. For \\(A_1\\), the rewards are \\(+1\\) with a probability of \\(0.8\\) and \\(0\\) with a probability of \\(0.2\\). On the other hand, \\(A_2\\) yields \\(+1\\) with a probability of \\(0.6\\) and \\(0\\) with a probability of \\(0.4\\).",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#exploitation-challenge",
    "href": "pages/RL/Week02.html#exploitation-challenge",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "A challenge arises when choosing actions based on initial rewards. If, for instance, \\(A_2\\) is selected first and a reward of \\(+1\\) is obtained, there is a risk of getting stuck with \\(A_2\\) due to its higher immediate reward probability. The same issue arises if starting with \\(A_1\\).",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#exploration-strategies",
    "href": "pages/RL/Week02.html#exploration-strategies",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The Epsilon-Greedy strategy involves a balance between exploitation and exploration. It mainly consists of selecting the action with the highest estimated value most of the time (\\(1 - \\epsilon\\)), while occasionally exploring other actions with a probability of \\(\\epsilon\\). Here, \\(\\epsilon\\) is a small value, typically \\(0.1\\) or \\(0.01\\), determining the exploration rate. The strategy ensures asymptotic convergence, guaranteeing exploration of all actions in the long run.\n\n\n\nThe Softmax strategy employs a mathematical function to convert estimated action values into a probability distribution. The Softmax function is defined as:\n\\[P(A_i) = \\frac{e^{Q(A_i) / \\tau}}{\\sum_{j} e^{Q(A_j) / \\tau}}\\]\nWhere:\n\n\\(Q(A_i)\\) represents the estimated value of action \\(A_i\\),\n\\(\\tau\\) is the temperature parameter.\n\nThe temperature parameter (\\(\\tau\\)) controls the sensitivity to differences in estimated values. When \\(\\tau\\) is high, the probability distribution becomes more uniform, favoring exploration. Conversely, a low \\(\\tau\\) emphasizes exploiting the best-known action. Softmax also provides asymptotic convergence, ensuring exploration of all actions over time.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#temperature-parameter-tau",
    "href": "pages/RL/Week02.html#temperature-parameter-tau",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "",
    "text": "The temperature parameter, \\(\\tau\\), is a crucial factor in the Softmax strategy. A higher \\(\\tau\\) results in a more uniform probability distribution, making exploration more likely. Conversely, a lower \\(\\tau\\) amplifies differences in estimated values, making the strategy closer to a greedy approach.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#regret-minimization",
    "href": "pages/RL/Week02.html#regret-minimization",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Regret Minimization",
    "text": "Regret Minimization\n\nDefinition of Regret\nRegret, denoted as \\(R_T\\), quantifies the total loss in rewards incurred due to the agent’s lack of knowledge about the optimal action during the initial \\(T\\) time steps. It is defined as the difference between the cumulative reward obtained by an optimal strategy and the cumulative reward obtained by the learning algorithm.\n\\[R_T = \\sum_{t=1}^T \\mu^* - \\mathbb{E}\\left[\\sum_{t=1}^T r_{a_t}(t)\\right]\\]\nwhere:\n\n\\(\\mu^*\\) is the expected reward of the optimal arm,\n\\(r_{a_t}(t)\\) is the reward obtained at time \\(t\\) from action \\(a_t\\),\n\\(a_t\\) is the action selected by the learning algorithm at time \\(t\\).\n\n\n\nObjective\nThe primary goal is to minimize regret by quickly identifying and exploiting the optimal arm. In dynamic scenarios, like news recommendation, where the optimal action may change frequently, minimizing regret becomes crucial for effective decision-making.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#total-rewards-maximization",
    "href": "pages/RL/Week02.html#total-rewards-maximization",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Total Rewards Maximization",
    "text": "Total Rewards Maximization\n\nLearning Curve\nIn the context of the multi-arm bandit problem, the learning curve represents the evolution of cumulative rewards over time. The objective is to minimize the area under this curve, signifying the loss incurred before reaching optimal performance.\n\\[R(t) = \\sum_{\\tau=1}^t \\mu^* - \\mathbb{E}\\left[\\sum_{\\tau=1}^t r_{a_\\tau}(\\tau)\\right]\\]\nHere, \\(R(t)\\) represents the cumulative regret up to time \\(t\\).\n\n\nQuick Learning\nIn scenarios like news recommendation, algorithms must adapt swiftly to changing optimal arms. The emphasis is on achieving quick learning to minimize the region under the learning curve and accelerate the convergence to optimal performance.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#pac-framework",
    "href": "pages/RL/Week02.html#pac-framework",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "PAC Framework",
    "text": "PAC Framework\n\nDefinition\nThe Probably Approximately Correct (PAC) framework aims to minimize the number of samples required to find an approximately correct solution. It introduces the concept of an \\(\\epsilon\\)-optimal arm, where an arm is considered approximately correct if its expected reward is within \\(\\epsilon\\) of the true optimal reward.\n\\[|\\hat{\\mu}_a - \\mu^*| \\leq \\epsilon\\]\nThe PAC framework also incorporates a confidence parameter \\(\\delta\\), representing the probability that the algorithm fails to provide an \\(\\epsilon\\)-optimal arm.\n\n\nTrade-off\nChoosing suitable values for \\(\\epsilon\\) and \\(\\delta\\) involves a trade-off between the acceptable performance loss (\\(\\epsilon\\)) and the confidence in achieving this performance (\\(\\delta\\)). This trade-off ensures robustness in the face of uncertainty.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#median-elimination-algorithm",
    "href": "pages/RL/Week02.html#median-elimination-algorithm",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Median Elimination Algorithm",
    "text": "Median Elimination Algorithm\n\nRound-Based Approach\nThe Median Elimination Algorithm divides the learning process into rounds. In each round, the algorithm samples each arm and eliminates those with estimated rewards below the median, reducing the set of candidate arms.\n\n\nSample Complexity\nThe total sample complexity is determined by the sum of samples drawn in each round. The algorithm guarantees that at least one arm remains \\(\\epsilon\\)-optimal with high probability.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#upper-confidence-bound-ucb-algorithm",
    "href": "pages/RL/Week02.html#upper-confidence-bound-ucb-algorithm",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Upper Confidence Bound (UCB) Algorithm",
    "text": "Upper Confidence Bound (UCB) Algorithm\n\nObjective\nThe UCB algorithm aims to achieve regret optimality by efficiently balancing exploration and exploitation. Unlike round-based approaches, UCB1 selects arms based on upper confidence bounds of estimated rewards.\n\n\nImplementation\nUCB1 is known for its simplicity and ease of implementation. It provides practical performance in scenarios like ad or news placement, where quick learning and adaptability are crucial.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#thompson-sampling",
    "href": "pages/RL/Week02.html#thompson-sampling",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Thompson Sampling",
    "text": "Thompson Sampling\n\nBayesian Approach\nThompson Sampling adopts a Bayesian approach, modeling uncertainty in the bandit problem through probability distributions. It leverages Bayesian inference to update beliefs about the reward distributions associated with each arm.\n\n\nRegret Optimality\nAgarwal and Goyal (2012) demonstrated that Thompson Sampling achieves regret optimality. This means that, asymptotically, the cumulative regret approaches the lower bound, signifying optimal learning performance.\n\n\nAdvantage over UCB\nThompson Sampling tends to have better constants than UCB-based methods, providing improved practical performance. It is particularly advantageous in scenarios where the underlying distribution of arms is uncertain.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#introduction",
    "href": "pages/RL/Week02.html#introduction",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of reinforcement learning, the Upper Confidence Bound (UCB) algorithm stands out as an effective strategy for addressing the multi-armed bandit problem. This algorithm offers a nuanced approach to the exploration-exploitation trade-off, mitigating the drawbacks associated with simpler strategies such as Epsilon-Greedy.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#challenges-with-epsilon-greedy",
    "href": "pages/RL/Week02.html#challenges-with-epsilon-greedy",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Challenges with Epsilon-Greedy",
    "text": "Challenges with Epsilon-Greedy\n\nExpected Value Maintenance\nIn the Epsilon-Greedy approach, the algorithm maintains the expected values (Q values) for each arm. However, a crucial limitation arises during exploration. The algorithm, guided by a fixed exploration probability (Epsilon), often expends valuable samples on suboptimal arms.\n\n\nWasted Samples and Regret Impact\nThe consequences of this exploration strategy are two-fold. Firstly, it results in wasted opportunities, as the algorithm neglects gathering valuable information about potentially optimal arms in favor of the suboptimal ones. Secondly, the impact on regret is substantial, particularly when selecting arms with low rewards.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#ucb-a-solution-to-exploration-challenges",
    "href": "pages/RL/Week02.html#ucb-a-solution-to-exploration-challenges",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "UCB: A Solution to Exploration Challenges",
    "text": "UCB: A Solution to Exploration Challenges\n\nIntroduction of Confidence Intervals\nUCB introduces a novel approach by not only maintaining mean estimates (Q values) for each arm but also incorporating confidence intervals. These intervals signify the algorithm’s confidence that the true value of Q for a particular arm lies within a specified range. \n\n\nAction Selection Mechanism\nThe key to UCB’s success lies in its action selection mechanism. Instead of relying solely on mean estimates, it considers an upper confidence bound for each arm. Mathematically, this can be expressed as:\n\\[\\text{UCB}_{j} = \\bar{X}_{j} + \\sqrt{\\frac{2 \\ln{N}}{n_{j}}}\\]\nHere,\n\n\\(\\bar{X}_{j}\\) is the mean estimate for arm j.\n\\(N\\) is the total number of actions taken.\n\\(n_{j}\\) represents the number of times arm j has been played.\n\nThis formulation balances exploration and exploitation, with the exploration term gradually diminishing as the number of plays (\\(n_{j}\\)) increases.\n\n\nRegret Minimization\nUCB is designed to minimize regret, a measure of the algorithm’s deviation from the optimal strategy. The regret for playing a suboptimal arm (arm J) is limited by:\n\\[\\text{Regret}_{J} \\leq 8 \\Delta_{J} \\ln{N}\\]\nHere,\n\n\\(\\Delta_{J}\\) represents the difference between the optimal arm’s expected reward and that of arm J.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#advantages-of-ucb",
    "href": "pages/RL/Week02.html#advantages-of-ucb",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Advantages of UCB",
    "text": "Advantages of UCB\n\nEfficient Exploration\nUCB efficiently focuses exploration efforts on arms with the potential for high rewards, reducing the occurrence of wasted samples on suboptimal choices.\n\n\nRegret Optimality\nBy limiting the number of plays for suboptimal arms, UCB minimizes regret and ensures that the algorithm converges towards optimal choices over time.\n\n\nSimplicity and Practical Performance\nUCB’s elegance lies in its simplicity of implementation, requiring no random number generation for exploration. This simplicity, coupled with its strong performance in practical scenarios, establishes UCB as a formidable algorithm for real-world applications.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#introduction-1",
    "href": "pages/RL/Week02.html#introduction-1",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Introduction",
    "text": "Introduction\nThe focus of this discussion is on addressing the challenge of customization in online platforms, specifically in the realms of ad selection and news story recommendations. The proposed solution is the utilization of contextual bandits, an extension of traditional bandit algorithms designed to incorporate user-specific attributes for a more personalized experience.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#contextual-bandits-a-conceptual-framework",
    "href": "pages/RL/Week02.html#contextual-bandits-a-conceptual-framework",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Contextual Bandits: A Conceptual Framework",
    "text": "Contextual Bandits: A Conceptual Framework\n\nTraditional Bandits vs. Contextual Bandits\nTraditional bandit algorithms involve the selection of actions without considering any contextual information. Contextual bandits, on the other hand, extend this paradigm by introducing the consideration of features related to both users and the available actions.\n\n\nMotivation for Contextual Bandits\nThe motivation behind introducing contextual bandits arises from the inherent challenge of tailoring recommendations for each user. In the context of ad selection and news story recommendations, a one-size-fits-all approach proves inadequate. Contextual bandits address this by accommodating user-specific features in the decision-making process.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#challenges-and-solutions",
    "href": "pages/RL/Week02.html#challenges-and-solutions",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Challenges and Solutions",
    "text": "Challenges and Solutions\n\nIndividual Bandits per User: Training Difficulties\nA significant challenge in implementing bandit algorithms for each user lies in the impracticality of training due to the extensive user base. Users’ infrequent visits to pages make it challenging to accumulate sufficient training data.\n\n\nGrouping Users Based on Features\nTo overcome the challenges of individual bandits per user, a strategy is proposed wherein users are grouped based on a set of parameters such as age, gender, and browsing behavior. This grouping allows for a more efficient handling of user features.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#mathematical-foundations",
    "href": "pages/RL/Week02.html#mathematical-foundations",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Mathematical Foundations",
    "text": "Mathematical Foundations\n\nLinear Parameterization of Features\nIn contextual bandits, the mean (\\(\\mu\\)) and variance (\\(\\sigma\\)) of the reward distribution associated with each action are influenced by user features. This relationship is commonly expressed through linear parameterization. Mathematically, this can be represented as:\n\\[\\mu_{a,s} = \\mathbf{w}_a \\cdot \\mathbf{X}_s\\]\nWhere:\n\n\\(\\mu_{a,s}\\) is the mean for action \\(a\\) and user features \\(s\\).\n\\(\\mathbf{w}_a\\) is the weight vector associated with action \\(a\\).\n\\(\\mathbf{X}_s\\) represents the feature vector for user \\(s\\).\n\n\n\nContextual Bandits for Actions and Context\nExtending the mathematical framework, features are considered not only for users but also for actions. This enhancement allows for a more nuanced approach, facilitating the reuse of information when actions change. The revised equation becomes:\n\\[Q_{s,a} = \\mathbf{w}_a \\cdot \\mathbf{X}_s\\]\nHere, \\(Q_{s,a}\\) represents the expected reward for action \\(a\\) given user features \\(s\\).\n\n\nLinUCB Algorithm\nThe LinUCB algorithm is introduced as a practical implementation of contextual bandits. It leverages ridge regression to predict expected rewards, creating a linear function of features. The ridge regression is expressed as:\n\\[\\hat{\\mathbf{w}}_a = \\arg \\min_{\\mathbf{w}_a} \\sum_{t=1}^{T} (r_{t,a} - \\mathbf{w}_a \\cdot \\mathbf{X}_{t,s})^2 + \\lambda \\|\\mathbf{w}_a\\|_2^2\\]\nWhere:\n\n\\(\\hat{\\mathbf{w}}_a\\) is the estimated weight vector for action \\(a\\).\n\\(r_{t,a}\\) is the observed reward for action \\(a\\) at time \\(t\\).\n\\(\\mathbf{X}_{t,s}\\) is the feature vector for user \\(s\\) at time \\(t\\).\n\\(\\lambda\\) is the regularization parameter.\n\n\n\nAdvantages of Contextual Bandits\nContextual bandits offer several advantages: - Personalized recommendations based on user features. - Efficient learning and adaptation even when the set of actions changes.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#contextual-bandits-in-the-reinforcement-learning-spectrum",
    "href": "pages/RL/Week02.html#contextual-bandits-in-the-reinforcement-learning-spectrum",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Contextual Bandits in the Reinforcement Learning Spectrum",
    "text": "Contextual Bandits in the Reinforcement Learning Spectrum\nContextual bandits serve as a crucial link between traditional bandits and full reinforcement learning. While considering both actions and context, they do not explicitly address the sequence, providing a bridge in the learning spectrum.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#full-reinforcement-learning-problem",
    "href": "pages/RL/Week02.html#full-reinforcement-learning-problem",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Full Reinforcement Learning Problem",
    "text": "Full Reinforcement Learning Problem\n\nSequence of Decisions\nIn contrast, the full RL problem involves a sequence of actions. Each decision influences subsequent situations, introducing complexity compared to the immediate and contextual Bandit problems.\n\n\nDelayed Rewards\nUnlike Bandits, the RL problem deals with delayed rewards. The consequences of an action may not manifest immediately but rather at the conclusion of a sequence of decisions. This delayed reward challenges the agent to associate distant outcomes with earlier choices.\n\n\nContext-Dependent Sequences\nMoreover, the sequence of problems in RL is context-dependent. The nature of the second problem depends on the action taken in the first, introducing an interdependence that was absent in contextual Bandit scenarios.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#temporal-distance-and-stochasticity",
    "href": "pages/RL/Week02.html#temporal-distance-and-stochasticity",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Temporal Distance and Stochasticity",
    "text": "Temporal Distance and Stochasticity\nThe concept of temporal distance in RL, where rewards are tied to actions in the past, is essential. Additionally, RL often involves stochastic environments, where variations or noise influence the outcomes.\n\nStochastic Environment\nStochasticity in the environment implies uncertainty in the response to an action. For instance, in a maze-running scenario, the mouse’s decision might lead to different outcomes due to environmental variability.\n\n\nNeed for Stochastic Models\nStochastic environments are employed in RL due to the impracticality of measuring or modeling every aspect precisely. For example, even in a simple coin toss, various unobservable factors contribute to the randomness observed.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#reinforcement-learning-framework",
    "href": "pages/RL/Week02.html#reinforcement-learning-framework",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Reinforcement Learning Framework",
    "text": "Reinforcement Learning Framework\n\nAgent-Environment Interaction\nThe RL framework comprises an agent and an environment in close interaction. The agent senses the environment’s state, takes actions, and receives rewards, leading to a continuous loop of interaction.\n\n\nStochasticity in State Transitions\nBoth state transitions and action selections can be stochastic, adding an element of unpredictability to the RL setting. The agent’s decisions are based on incomplete information and uncertain outcomes.\n\n\nEvaluation and Rewards\nCentral to RL is the concept of evaluation through rewards. The agent’s goal is to learn a mapping from states to actions, aiming to maximize cumulative rewards over the long term.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#temporal-difference-in-rewards",
    "href": "pages/RL/Week02.html#temporal-difference-in-rewards",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Temporal Difference in Rewards",
    "text": "Temporal Difference in Rewards\nThe delayed and noisy nature of rewards in RL introduces the need for temporal difference considerations. Agents must predict future rewards based on their current actions, leading to a more intricate decision-making process.\n\nExample: Tic-Tac-Toe\nIllustrating this, in a game of tic-tac-toe, a move made early in the game may strongly influence the eventual outcome, even though the final reward is received only at the game’s end.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#full-rl-problem-solving-approach",
    "href": "pages/RL/Week02.html#full-rl-problem-solving-approach",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Full RL Problem Solving Approach",
    "text": "Full RL Problem Solving Approach\n\nSequence of Bandit Problems\nTo solve the full RL problem, a sequence of Bandit problems is employed. Each state-action pair corresponds to a Bandit problem that the agent must solve, and the solutions cascade to form a comprehensive strategy.\n\n\nDynamic Programming\nThe solution approach aligns with dynamic programming, where the value derived from solving one Bandit problem serves as the reward for the preceding state-action pair. This recursive approach forms the basis for tackling the complexity of RL scenarios.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week02.html#points-to-remember",
    "href": "pages/RL/Week02.html#points-to-remember",
    "title": "Decoding Decision-Making: From Multi-Arm Bandits to Full Reinforcement Learning",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nMulti-Arm Bandit Problem: Conceptualizes actions as arms, each with a reward drawn from a probability distribution. Balancing exploration and exploitation is crucial for optimal learning and cumulative rewards.\nExploration Strategies:\n\nEpsilon-Greedy: Balances exploitation and exploration, with a small exploration rate (\\(\\epsilon\\)).\nSoftmax: Converts estimated action values into a probability distribution, controlled by a temperature parameter (\\(\\tau\\)).\n\nRegret Minimization Framework: Aims to minimize regret (\\(R_T\\)), the total loss in rewards compared to an optimal strategy. Efficient learning and quick adaptation are essential in dynamic scenarios.\nPAC Framework: Probably Approximately Correct framework introduces the concept of an \\(\\epsilon\\)-optimal arm, balancing performance loss (\\(\\epsilon\\)) and confidence (\\(\\delta\\)).\nUCB Algorithm: Upper Confidence Bound algorithm efficiently balances exploration and exploitation by considering confidence intervals. It minimizes regret and converges towards optimal choices over time.\nContextual Bandits: Extend traditional bandits by incorporating user-specific features for personalized recommendations. The LinUCB algorithm is a practical implementation.\nFull Reinforcement Learning (RL): Involves sequences of decisions, delayed rewards, and stochastic environments. Temporal difference considerations become crucial in predicting future rewards.\nDynamic Programming in RL: Solving the Full RL Problem involves treating it as a sequence of Bandit problems, employing dynamic programming for recursive learning and optimization.\n\nThe journey through Multi-Arm Bandit Problems, regret minimization, contextual bandits, and Full RL has equipped us with a comprehensive understanding of decision-making in uncertain and dynamic environments. These concepts provide a solid foundation for addressing challenges in various real-world scenarios.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html",
    "href": "pages/RL/Week01_1.html",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "In the realm of machine learning, a predominant paradigm involves the acquisition of knowledge through the learning of functions that map input features to specific outputs. This conventional approach, known as supervised learning, relies on the provision of explicit instructions and training data to inform the learning process. Typically, the model generalizes from examples presented during training to make predictions or classifications on new, unseen data.\n\n\n\nReinforcement Learning (RL), in contrast, embodies a distinctive methodology centered around trial-and-error learning within systems characterized by intricate and challenging control dynamics. In the RL framework, explicit instructions are eschewed in favor of evaluating actions based on received rewards and punishments. This departure from prescriptive learning mirrors the way humans acquire skills such as cycling or walking, where trial and error, coupled with feedback, plays a pivotal role.\n\n\n\nTrial and Error Approach: RL stands out for its reliance on the iterative process of trying various actions and subsequently gauging their efficacy through outcomes, be they positive rewards or negative consequences.\nAbsence of Upfront Instructions: Unlike supervised learning, RL lacks a predetermined set of instructions provided beforehand. Instead, the system learns by interacting with its environment and adapting based on the consequences of its actions.\n\n\n\n\n\nThe dichotomy between supervised learning and reinforcement learning can be elucidated by highlighting their fundamental disparities.\n\n\nIn supervised learning, explicit instructions are imparted to the learning algorithm upfront. The model is trained to generate outputs conforming to the provided instructions, drawing insights from labeled examples.\n\n\n\nConversely, reinforcement learning refrains from pre-established instructions. Actions are executed, and their merit is subsequently appraised through a feedback mechanism of rewards and punishments. The system learns to optimize its behavior based on experiential outcomes.\n\n\n\n\nDrawing parallels with how humans assimilate complex skills, reinforcement learning aligns with a trial-and-error learning paradigm. Consider the analogy of a child learning to cycle; the process involves attempts, feedback (both positive and negative), and an eventual refinement of the skill through repeated iterations.\n\n\nRooted in behavioral psychology, reinforcement learning embodies a system’s interaction with its environment, learning through the consequences of its actions. The classical example of Pavlov’s dog underscores the association of stimuli (bell ringing) with rewards (food), illustrating the behavioral conditioning inherent in RL principles.\n\n\n\n\nReinforcement learning finds diverse applications across various domains, demonstrating its efficacy in addressing complex challenges.\n\n\nIn domains like autonomous driving or the control of a helicopter, reinforcement learning proves invaluable. The ability to navigate complex environments and execute intricate maneuvers showcases the adaptability of RL in real-world scenarios.\n\n\n\nHumanoid robots, engaged in tasks like playing soccer, leverage reinforcement learning to master complex movements, such as kicking a ball. This exemplifies the adaptability of RL in training systems to perform dynamic and agile actions.\n\n\n\nReinforcement learning excels in navigating cluttered and intricate spaces, offering a more pragmatic approach compared to conventional control methods. Examples range from traffic scenarios to multi-roomed buildings.\n\n\n\nWhen dealing with stochastic systems and probabilistic outcomes, reinforcement learning provides an effective solution. Its application in scenarios where precise control or prediction is challenging due to inherent uncertainty demonstrates its versatility.\n\n\n\nIn tasks requiring human-like cognitive processes, such as determining where to focus attention in a complex environment, reinforcement learning, under the banner of cognitively motivated learning, strives to emulate human decision-making patterns.\n\n\n\nReinforcement learning extends its utility to customization and personalization tasks in various industries. Tailoring products or services based on individual preferences underscores its role in enhancing user experiences.\n\n\n\n\nThe success of reinforcement learning in solving real-world challenges is underscored by notable achievements such as ChatGPT. Ongoing advancements contribute to its widespread adoption, positioning RL as a potent tool for addressing intricate problems characterized by complexity, uncertainty, and human-like decision-making processes.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#introduction-to-machine-learning",
    "href": "pages/RL/Week01_1.html#introduction-to-machine-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "In the realm of machine learning, a predominant paradigm involves the acquisition of knowledge through the learning of functions that map input features to specific outputs. This conventional approach, known as supervised learning, relies on the provision of explicit instructions and training data to inform the learning process. Typically, the model generalizes from examples presented during training to make predictions or classifications on new, unseen data.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#fundamentals-of-reinforcement-learning",
    "href": "pages/RL/Week01_1.html#fundamentals-of-reinforcement-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "Reinforcement Learning (RL), in contrast, embodies a distinctive methodology centered around trial-and-error learning within systems characterized by intricate and challenging control dynamics. In the RL framework, explicit instructions are eschewed in favor of evaluating actions based on received rewards and punishments. This departure from prescriptive learning mirrors the way humans acquire skills such as cycling or walking, where trial and error, coupled with feedback, plays a pivotal role.\n\n\n\nTrial and Error Approach: RL stands out for its reliance on the iterative process of trying various actions and subsequently gauging their efficacy through outcomes, be they positive rewards or negative consequences.\nAbsence of Upfront Instructions: Unlike supervised learning, RL lacks a predetermined set of instructions provided beforehand. Instead, the system learns by interacting with its environment and adapting based on the consequences of its actions.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#contrast-with-supervised-learning",
    "href": "pages/RL/Week01_1.html#contrast-with-supervised-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "The dichotomy between supervised learning and reinforcement learning can be elucidated by highlighting their fundamental disparities.\n\n\nIn supervised learning, explicit instructions are imparted to the learning algorithm upfront. The model is trained to generate outputs conforming to the provided instructions, drawing insights from labeled examples.\n\n\n\nConversely, reinforcement learning refrains from pre-established instructions. Actions are executed, and their merit is subsequently appraised through a feedback mechanism of rewards and punishments. The system learns to optimize its behavior based on experiential outcomes.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#learning-paradigm-in-reinforcement-learning",
    "href": "pages/RL/Week01_1.html#learning-paradigm-in-reinforcement-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "Drawing parallels with how humans assimilate complex skills, reinforcement learning aligns with a trial-and-error learning paradigm. Consider the analogy of a child learning to cycle; the process involves attempts, feedback (both positive and negative), and an eventual refinement of the skill through repeated iterations.\n\n\nRooted in behavioral psychology, reinforcement learning embodies a system’s interaction with its environment, learning through the consequences of its actions. The classical example of Pavlov’s dog underscores the association of stimuli (bell ringing) with rewards (food), illustrating the behavioral conditioning inherent in RL principles.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#applications-of-reinforcement-learning",
    "href": "pages/RL/Week01_1.html#applications-of-reinforcement-learning",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "Reinforcement learning finds diverse applications across various domains, demonstrating its efficacy in addressing complex challenges.\n\n\nIn domains like autonomous driving or the control of a helicopter, reinforcement learning proves invaluable. The ability to navigate complex environments and execute intricate maneuvers showcases the adaptability of RL in real-world scenarios.\n\n\n\nHumanoid robots, engaged in tasks like playing soccer, leverage reinforcement learning to master complex movements, such as kicking a ball. This exemplifies the adaptability of RL in training systems to perform dynamic and agile actions.\n\n\n\nReinforcement learning excels in navigating cluttered and intricate spaces, offering a more pragmatic approach compared to conventional control methods. Examples range from traffic scenarios to multi-roomed buildings.\n\n\n\nWhen dealing with stochastic systems and probabilistic outcomes, reinforcement learning provides an effective solution. Its application in scenarios where precise control or prediction is challenging due to inherent uncertainty demonstrates its versatility.\n\n\n\nIn tasks requiring human-like cognitive processes, such as determining where to focus attention in a complex environment, reinforcement learning, under the banner of cognitively motivated learning, strives to emulate human decision-making patterns.\n\n\n\nReinforcement learning extends its utility to customization and personalization tasks in various industries. Tailoring products or services based on individual preferences underscores its role in enhancing user experiences.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#success-stories-and-advancements",
    "href": "pages/RL/Week01_1.html#success-stories-and-advancements",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "",
    "text": "The success of reinforcement learning in solving real-world challenges is underscored by notable achievements such as ChatGPT. Ongoing advancements contribute to its widespread adoption, positioning RL as a potent tool for addressing intricate problems characterized by complexity, uncertainty, and human-like decision-making processes.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#customization-on-yahoo-news",
    "href": "pages/RL/Week01_1.html#customization-on-yahoo-news",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Customization on Yahoo News",
    "text": "Customization on Yahoo News\n\nOverview\nCustomization involves tailoring content based on user preferences and behavior. Yahoo News, for example, uses a personalized approach in presenting news stories to users.\n\n\nManual Labeling Challenges\nDue to the dynamic nature of news content and the vast user diversity, manual labeling of stories for individual users is impractical. It is neither feasible nor efficient to have editors constantly labeling stories for the millions of users who access the platform.\n\n\nReinforcement Learning Solution\nTo address this challenge, a reinforcement learning (RL) approach is employed. Instead of explicit human instructions, RL utilizes user interactions as feedback for personalization. Editors initially select a set of stories, and based on user actions (clicks or dislikes), the system learns to predict the likelihood of future user interactions. This way, the content presented to users becomes customized based on their preferences.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#ad-selection-in-computational-advertising",
    "href": "pages/RL/Week01_1.html#ad-selection-in-computational-advertising",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Ad Selection in Computational Advertising",
    "text": "Ad Selection in Computational Advertising\n\nComputational Advertising\nComputational advertising is a field that involves the automated selection of relevant advertisements for users, a process crucial for revenue generation, especially for platforms like Google.\n\n\nReinforcement Learning for Ad Selection\nIn ad selection, RL plays a key role in determining the probability of a user clicking on a specific ad. User interactions, such as clicks or dislikes, serve as positive or negative feedback. This information refines the ad selection process, making it more effective in presenting ads that are likely to engage users.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#reinforcement-learning-in-recommendation-engines",
    "href": "pages/RL/Week01_1.html#reinforcement-learning-in-recommendation-engines",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Reinforcement Learning in Recommendation Engines",
    "text": "Reinforcement Learning in Recommendation Engines\n\nTraditional Recommendation Systems\nRecommendation engines traditionally employ collaborative filtering, using methods like “customers who bought this item also bought.” However, this approach has limitations in handling a vast pool of potential recommendations.\n\n\nTrial-and-Error with Reinforcement Learning\nReinforcement learning complements traditional methods by introducing a trial-and-error layer. Users’ feedback becomes a crucial component in refining recommendations over time. This allows the system to adapt to changing user preferences dynamically.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#content-and-comment-recommendations",
    "href": "pages/RL/Week01_1.html#content-and-comment-recommendations",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Content and Comment Recommendations",
    "text": "Content and Comment Recommendations\n\nContent Recommendation Systems\nRL is applied in content recommendation systems, leveraging user history and feedback to personalize suggestions. This goes beyond conventional methods and incorporates a trial-and-error approach for more accurate predictions.\n\n\nComment Recommendations\nIn websites where comments are displayed, RL is utilized to reorder and present comments based on user feedback. Thumbs up or thumbs down serve as positive and negative rewards, influencing the order in which comments are displayed.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#beyond-human-knowledge",
    "href": "pages/RL/Week01_1.html#beyond-human-knowledge",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Beyond Human Knowledge",
    "text": "Beyond Human Knowledge\n\nReinforcement Learning Autonomy\nReinforcement learning demonstrates the capability to operate autonomously without explicit human guidance. Early successes, such as Jerry Tesauro’s TD Gammon in backgammon, exemplify RL’s ability to learn from self-play.\n\n\nBreakthrough in 2014\nA pivotal moment occurred in 2014 when DeepMind trained RL agents to play Atari games. This breakthrough showcased RL’s capacity to learn complex tasks with minimal input, opening the door to widespread replication of success stories.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#success-in-strategic-games",
    "href": "pages/RL/Week01_1.html#success-in-strategic-games",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Success in Strategic Games",
    "text": "Success in Strategic Games\n\nAlphaGo’s Triumph\nDeepMind’s AlphaGo achieved unprecedented success by defeating the world champion in the ancient game of Go. RL’s application extended to mastering various strategy games, surpassing human-level performance in competitive scenarios.\n\n\nAlphaZero’s Versatility\nAlphaZero demonstrated versatility by playing and excelling in multiple games, including chess and shogi. Its success showcased RL’s ability to adapt and learn across diverse gaming environments without relying on human data.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#applications-beyond-gaming",
    "href": "pages/RL/Week01_1.html#applications-beyond-gaming",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Applications Beyond Gaming",
    "text": "Applications Beyond Gaming\n\nRL in Real-world Challenges\nReinforcement learning’s success in gaming applications paved the way for its adoption in solving real-world challenges. RL is utilized in optimizing data center cooling, controlling chemical plants, and even improving airport conveyor belt efficiency.\n\n\nImpact on Combinatorial Optimization\nRL has played a crucial role in solving combinatorial optimization problems, including scheduling, routing, and call admission control. Its application extends to diverse domains, showcasing its versatility in addressing complex decision-making challenges.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#control-systems-and-optimization",
    "href": "pages/RL/Week01_1.html#control-systems-and-optimization",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Control Systems and Optimization",
    "text": "Control Systems and Optimization\n\nRL in Control Systems\nReinforcement learning finds applications in controlling various systems, from chemical plants to robot navigation. Its adaptability and ability to optimize processes make it a valuable tool in real-world applications.\n\n\nAirport Conveyor Belt Control\nAn interesting application involves using RL to control conveyor belts in airports. RL-based controllers aim to ensure timely package delivery, showcasing the technology’s potential in optimizing large-scale logistical systems.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#connections-with-neuroscience-and-psychology",
    "href": "pages/RL/Week01_1.html#connections-with-neuroscience-and-psychology",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Connections with Neuroscience and Psychology",
    "text": "Connections with Neuroscience and Psychology\n\nRoots in Behavioral Psychology\nReinforcement learning has roots in behavioral psychology, emphasizing learning through trial and error. This connection provides insights into human decision-making processes.\n\n\nInteraction with Neuroscience\nRL’s impact extends to neuroscience, with some suggesting that RL could be a primary mechanism of learning in certain brain regions. This reciprocal interaction enriches both the computational neuroscience and RL fields.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#real-world-applications-of-rl",
    "href": "pages/RL/Week01_1.html#real-world-applications-of-rl",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Real-world Applications of RL",
    "text": "Real-world Applications of RL\n\nIntelligent Tutoring Systems\nReinforcement learning contributes to the development of intelligent tutoring systems, providing personalized and adaptive learning experiences for students based on their interactions.\n\n\nRL in Dialogue Systems and Chatbots\nDialogue systems and chatbots benefit from RL, enabling more natural and context-aware interactions. RL’s trial-and-error learning enhances these systems’ ability to understand and respond to user inputs effectively.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/RL/Week01_1.html#points-to-remember",
    "href": "pages/RL/Week01_1.html#points-to-remember",
    "title": "Reinforcement Learning Unveiled: From Theory to Triumph in AI Applications",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nFundamentals of RL\n\nRL is distinguished by a trial-and-error approach, relying on received rewards and punishments for learning.\nUnlike supervised learning, RL lacks upfront instructions, allowing the system to adapt through interaction with its environment. \n\nContrast with Supervised Learning\n\nSupervised learning relies on explicit instructions provided beforehand, while RL refrains from pre-established instructions.\n\nLearning Paradigm in RL\n\nRL aligns with a trial-and-error learning paradigm, resembling how humans acquire complex skills through attempts, feedback, and refinement.\n\nApplications of RL\n\nRL excels in domains such as autonomous systems, humanoid control, complex environments, uncertain environments, cognitively motivated learning, and customization/personalization tasks.\nSuccess stories like ChatGPT showcase RL’s efficacy in addressing intricate problems.\n\nRL in Personalization and Customization\n\nRL is applied in platforms like Yahoo News for content customization, utilizing user interactions as feedback.\nComputational advertising and recommendation engines leverage RL for ad selection and dynamic adaptation to changing user preferences.\n\nAdvancements in RL\n\nRL demonstrates autonomy in learning, as seen in early successes like TD Gammon and breakthroughs in gaming applications.\nSuccess in strategic games, such as AlphaGo and AlphaZero, highlights RL’s adaptability and versatility.\n\nRL’s Impact on Problem Solving\n\nRL contributes to solving real-world challenges in areas like data center cooling, chemical plant control, and combinatorial optimization.\nApplications in control systems, airport conveyor belt control, and connections with neuroscience showcase RL’s broad impact.\n\nReal-world Applications of RL\n\nRL is instrumental in intelligent tutoring systems, providing personalized learning experiences.\nDialogue systems and chatbots benefit from RL, enhancing natural and context-aware interactions.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html",
    "href": "pages/ST/Week01.html",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes.\n\n\n\n\n\nThe initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary.\n\n\n\n\n\n\nThe V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints.\n\n\n\n\nBeyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances.\n\n\n\n\n\nIntegral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "href": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#phases-of-sdlc",
    "href": "pages/ST/Week01.html#phases-of-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#sdlc-models",
    "href": "pages/ST/Week01.html#sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#other-sdlc-models",
    "href": "pages/ST/Week01.html#other-sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Beyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#umbrella-activities",
    "href": "pages/ST/Week01.html#umbrella-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Integral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#introduction",
    "href": "pages/ST/Week01.html#introduction",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Testing\nSoftware testing is a comprehensive process involving the scrutiny of various artifacts, including code, design, architecture documents, and requirements documents. The core objective is to validate and verify these artifacts, ensuring the software’s reliability and functionality.\n\n\nGoals of Software Testing\nThe overarching goals encompass providing an unbiased, independent assessment of the software, verifying its compliance with business capabilities, and evaluating associated risks that may impact its performance.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#standard-glossary",
    "href": "pages/ST/Week01.html#standard-glossary",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Standard Glossary",
    "text": "Standard Glossary\n\nVerification: This process determines whether the products meet specified requirements at various stages of the software development life cycle.\nValidation: Evaluation of the software at the end of the development phase, ensuring it aligns with standards and intended usage.\nFault: A static defect within the software, often originating from a mistake made during development.\nFailure: The visible, external manifestation of incorrect behavior resulting from a fault.\nError: The incorrect state of the program when a failure occurs, indicating a deviation from the intended behavior.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#historical-perspective",
    "href": "pages/ST/Week01.html#historical-perspective",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Historical Perspective",
    "text": "Historical Perspective\nDrawing from the historical lens, luminaries like Edison and Lovelace utilized terms such as “bug” and “error” to emphasize the iterative process of identifying and rectifying faults and difficulties in inventions.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#testing-terminology",
    "href": "pages/ST/Week01.html#testing-terminology",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Terminology",
    "text": "Testing Terminology\n\nTest Case: A comprehensive entity comprising test inputs and expected outputs, evaluated by executing the test case on the code.\nTest Case ID: An identifier crucial for retrieval and management of test cases.\nTraceability: The establishment of links connecting test cases to specific requirements, ensuring thorough validation.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#types-of-testing",
    "href": "pages/ST/Week01.html#types-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Types of Testing",
    "text": "Types of Testing\n\nUnit Testing: A meticulous examination carried out by developers during the coding phase to test individual methods.\nIntegration Testing: An evaluation of the interaction between diverse software components.\nSystem Testing: A holistic examination of the entire system to ensure alignment with design requirements.\nAcceptance Testing: Conducted by end customers to validate that the delivered software meets all committed requirements.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#quality-parameters-testing",
    "href": "pages/ST/Week01.html#quality-parameters-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Quality Parameters Testing",
    "text": "Quality Parameters Testing\n\nFunctional Testing: Ensures the software functions precisely as intended.\nStress Testing: Evaluates software performance under extreme conditions to assess its robustness.\nPerformance Testing: Verifies if the software responds within specified time limits under varying conditions.\nUsability Testing: Ensures the software offers a user-friendly interface, enhancing the overall user experience.\nRegression Testing: Validates that existing functionalities continue to work seamlessly after software changes.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#methods-of-testing",
    "href": "pages/ST/Week01.html#methods-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Methods of Testing",
    "text": "Methods of Testing\n\nBlack Box Testing: A method that evaluates the software without delving into its internal structure, relying solely on inputs and requirements.\nWhite Box Testing: Testing carried out with a comprehensive understanding of the software’s internal structure, design, and code.\nGray Box Testing: An intermediate approach that combines elements of both black box and white box testing.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#testing-activities",
    "href": "pages/ST/Week01.html#testing-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Activities",
    "text": "Testing Activities\n\nTest Case Design:\n\nCritical for efficiently identifying defects.\nRequires a blend of computer science expertise, domain knowledge, and mathematical proficiency.\nEmphasis on the development of effective test case design algorithms. \n\nTest Automation:\n\nInvolves the conversion of test cases into executable scripts.\nAddresses preparatory steps and incorporates concepts of observability and controllability.\nUtilizes both open-source and proprietary test automation tools.\n\nExecution:\n\nAutomated process involving the execution of test cases.\nUtilizes a selection of open-source or proprietary tools chosen by the organization.\n\nEvaluation:\n\nThe critical analysis of test results to determine correctness.\nManual intervention may be required for fault isolation.\nCrucial for drawing inferences about the software’s quality.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-1",
    "href": "pages/ST/Week01.html#introduction-1",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software testing, the pursuit of testing goals is intricately tied to the specificities of the software product in question and the maturity of an organization’s quality processes. This diversity in objectives and approaches underscores the importance of comprehending the nuanced landscape of testing process levels, which range from the rudimentary Level 0 to the pinnacle of maturity at Level 4.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#testing-process-maturity-levels",
    "href": "pages/ST/Week01.html#testing-process-maturity-levels",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Process Maturity Levels",
    "text": "Testing Process Maturity Levels\n\nLevel 0: Low Maturity\nAt this embryonic stage, there is an absence of a clear demarcation between testing and debugging activities. The predominant focus revolves around expedient product releases, potentially at the expense of a rigorous testing regimen.\nLevel 1: Testing for Correctness\nThe next tier witnesses a paradigm shift as testing endeavors to validate software correctness. However, a common misunderstanding prevails — an attempt to prove complete correctness through testing, an inherently unattainable feat.\nLevel 2: Finding Errors\nAs organizations ascend to Level 2, there is a conscious recognition of testing as a mechanism to unearth errors by actively showcasing failures. However, a resistance lingers when it comes to acknowledging and addressing errors identified in the code.\nLevel 3: Sophisticated Testing\nLevel 3 marks a watershed moment where testing is not merely a reactive measure but is embraced as a robust technique for both identifying and eliminating errors. A collaborative ethos emerges, with a collective effort to mitigate risks in software development.\nLevel 4: Mature Process-Oriented Testing\nAt the pinnacle of maturity, testing transcends mere procedural activities; it metamorphoses into a mental discipline. Integrated seamlessly into mainstream development, the focus is on continuous quality improvement. Here, test engineers and developers synergize their efforts to deliver software of the highest quality.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#significance-for-the-course",
    "href": "pages/ST/Week01.html#significance-for-the-course",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Significance for the Course",
    "text": "Significance for the Course\nUnderstanding the nuances of testing process levels assumes paramount importance as it serves as the bedrock for tailoring testing approaches. The focus of this course is strategically directed towards the technical intricacies relevant to Levels 3 and 4, where testing is not just a process but an integral aspect of the software development mindset.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#controllability-and-observability",
    "href": "pages/ST/Week01.html#controllability-and-observability",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Controllability and Observability",
    "text": "Controllability and Observability\n\nControllability: This pertains to the ability to provide inputs and execute the software module. It underscores the necessity of having a structured approach to govern the input parameters and execution environment.\nObservability: The study and recording of outputs form the crux of observability. This involves a meticulous examination of the software’s responses, contributing significantly to the overall understanding of its behavior.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#illustration",
    "href": "pages/ST/Week01.html#illustration",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Illustration",
    "text": "Illustration\nThe challenges of controllability and observability find illustration in real-world scenarios. Designing effective test cases becomes paramount to ensure both the reachability of various modules and the meticulous observation of their outputs. This practical application reinforces the theoretical concepts discussed in the course.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/ST/Week01.html#test-automation-tool-junit",
    "href": "pages/ST/Week01.html#test-automation-tool-junit",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Test Automation Tool: JUnit",
    "text": "Test Automation Tool: JUnit\nThe course introduces JUnit as the designated test automation tool. JUnit’s utility is elucidated through a discussion of its prefix and postfix annotations, providing a structured approach to manage controllability and observability. Subsequent classes delve into both the theoretical underpinnings and the hands-on application of JUnit, ensuring a comprehensive understanding of its role in the testing process.",
    "crumbs": [
      "Software Testing",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html",
    "href": "pages/SE/Week01.html",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "In the contemporary landscape of online platforms, exemplified by industry leader Amazon, the intricate systems governing processes such as ordering and delivery are constructed incrementally. In contrast to a monolithic approach, these systems evolve feature by feature. This incremental strategy arises from the inherent uncertainty surrounding the complete set of required features at the project’s inception.\n\n\n\nWithin the domain of software engineering, the concept of components assumes a pivotal role. These components serve as manageable units that facilitate collaborative efforts by different teams, each working on distinct facets of the system. These individual aspects are later integrated into a coherent whole. Importantly, effective collaboration is achieved by understanding a component’s interface, which shields the intricacies of its internal workings.\n\n\nPurpose: The Inventory Management System is designed to intelligently track and manage inventory.\nDefinition: This involves measuring quantity, location, pricing, and the composition of products available on platforms like Amazon.\nCustomization: Amazon’s homepage dynamically updates based on factors such as purchasing trends, seasonal variations, customer demand, and logistical and analytical considerations.\n\n\n\nPurpose: The Payment Gateway facilitates electronic payments, ensuring a seamless experience for buyers and sellers.\nDefinition: Serving as a service authorizing electronic payments (e.g., online banking, debit cards), the Payment Gateway acts as an intermediary between the bank and the merchant’s platform.\nProcess: The gateway validates payment details, confirming their legitimacy with the bank before transferring the specified amount from the user’s account to the platform.\n\n\n\n\nLarge-scale systems, exemplified by the infrastructure of industry leaders like Amazon, do not materialize in a single endeavor. Instead, they are deconstructed into components or modules that can be independently developed before harmonious integration. This integration phase involves establishing communication pathways between the modules.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#introduction",
    "href": "pages/SE/Week01.html#introduction",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "In the contemporary landscape of online platforms, exemplified by industry leader Amazon, the intricate systems governing processes such as ordering and delivery are constructed incrementally. In contrast to a monolithic approach, these systems evolve feature by feature. This incremental strategy arises from the inherent uncertainty surrounding the complete set of required features at the project’s inception.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#components-in-software-systems",
    "href": "pages/SE/Week01.html#components-in-software-systems",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "Within the domain of software engineering, the concept of components assumes a pivotal role. These components serve as manageable units that facilitate collaborative efforts by different teams, each working on distinct facets of the system. These individual aspects are later integrated into a coherent whole. Importantly, effective collaboration is achieved by understanding a component’s interface, which shields the intricacies of its internal workings.\n\n\nPurpose: The Inventory Management System is designed to intelligently track and manage inventory.\nDefinition: This involves measuring quantity, location, pricing, and the composition of products available on platforms like Amazon.\nCustomization: Amazon’s homepage dynamically updates based on factors such as purchasing trends, seasonal variations, customer demand, and logistical and analytical considerations.\n\n\n\nPurpose: The Payment Gateway facilitates electronic payments, ensuring a seamless experience for buyers and sellers.\nDefinition: Serving as a service authorizing electronic payments (e.g., online banking, debit cards), the Payment Gateway acts as an intermediary between the bank and the merchant’s platform.\nProcess: The gateway validates payment details, confirming their legitimacy with the bank before transferring the specified amount from the user’s account to the platform.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#incremental-system-development",
    "href": "pages/SE/Week01.html#incremental-system-development",
    "title": "Thinking of Software in terms of Components",
    "section": "",
    "text": "Large-scale systems, exemplified by the infrastructure of industry leaders like Amazon, do not materialize in a single endeavor. Instead, they are deconstructed into components or modules that can be independently developed before harmonious integration. This integration phase involves establishing communication pathways between the modules.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-1",
    "href": "pages/SE/Week01.html#introduction-1",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software engineering, a comprehensive understanding of a software system’s components and their interactions is fundamental. This lecture explores the intricacies of the software development process, using the example of Amazon Pay, a mobile wallet, to elucidate key concepts.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#amazon-pay-overview",
    "href": "pages/SE/Week01.html#amazon-pay-overview",
    "title": "Thinking of Software in terms of Components",
    "section": "Amazon Pay Overview",
    "text": "Amazon Pay Overview\n\nFeatures\nAmazon Pay, a mobile wallet, facilitates digital cash transactions by offering a spectrum of features. Users can link credit/debit cards, bank accounts, and engage in various transactions, including recharges, bill payments, travel bookings, insurance, and redemption of rewards and gift vouchers. Two notable functionalities include the ability to add money and an auto-reload feature.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#software-development-process",
    "href": "pages/SE/Week01.html#software-development-process",
    "title": "Thinking of Software in terms of Components",
    "section": "Software Development Process",
    "text": "Software Development Process\n\n1. Identifying the Problem\nBefore delving into programming languages, the foremost step in the software development process involves a profound understanding of the problem at hand. This recognition sets the stage for subsequent development efforts.\n\n\n2. Studying Existing Components\nTo gain insights into the intricacies of system components, a meticulous examination of existing elements, such as inventory management and payment gateways, is crucial. Additionally, studying analogous systems, like Paytm and PhonePe, aids in identifying essential features.\n\n\n3. Defining System Requirements\nThe foundation of the development process lies in explicitly defining system requirements. These requirements, derived from a thorough analysis of existing systems, serve as the guiding principles throughout the software development lifecycle.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#clients-in-software-systems",
    "href": "pages/SE/Week01.html#clients-in-software-systems",
    "title": "Thinking of Software in terms of Components",
    "section": "Clients in Software Systems",
    "text": "Clients in Software Systems\n\nDefinition of Client\nClients, referring to users of the software system, can be categorized as either external or internal entities. External clients are end-users or buyers, while internal clients encompass components within the system itself.\n\n\nTypes of Clients\n\nExternal Clients\nFor instance, in mobile banking software, external clients are bank customers utilizing features like account balance checks and money transfers.\n\n\nInternal Clients\nInternal clients may include teams within a company, such as an internal products team constructing an employee resources portal by collaborating with various departments.\n\n\n\nSoftware-to-Software Clients\nIn certain scenarios, software components, like payment gateways (e.g., Razer Pay), act as clients, facilitating communication between an e-commerce website and customers’ banks.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#importance-of-gathering-requirements",
    "href": "pages/SE/Week01.html#importance-of-gathering-requirements",
    "title": "Thinking of Software in terms of Components",
    "section": "Importance of Gathering Requirements",
    "text": "Importance of Gathering Requirements\n\nSignificance of the First Step\nGathering requirements stands as the initial and crucial step in the software development process. This process ensures a holistic understanding of users or clients, and adherence to requirements at every stage is imperative for meeting end-user needs.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-2",
    "href": "pages/SE/Week01.html#introduction-2",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nPreviously, we delved into the initial steps of the software development cycle, primarily focusing on the gathering of requirements. However, a common misconception arises at this juncture—many individuals are inclined to proceed directly to coding. This session aims to dispel this notion through a practical example.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#example-implementation-of-amazon-pay-feature",
    "href": "pages/SE/Week01.html#example-implementation-of-amazon-pay-feature",
    "title": "Thinking of Software in terms of Components",
    "section": "Example: Implementation of Amazon Pay Feature",
    "text": "Example: Implementation of Amazon Pay Feature\nConsider a scenario where a small team is eager to implement the Amazon Pay feature based on gathered requirements. The tendency to immediately engage in coding poses several challenges that warrant careful consideration.\n\nPitfalls of Skipping Design Phase\n\nDivergent Implementation Ideas:\n\nDevelopers may harbor disparate concepts regarding the feature’s implementation.\nChanges made by one developer could inadvertently impact others.\n\nInterconnected Components Challenge:\n\nComponents developed by different individuals may intertwine, resulting in complications.\nLack of a holistic view impedes the seamless integration of features.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#the-significance-of-the-design-phase",
    "href": "pages/SE/Week01.html#the-significance-of-the-design-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "The Significance of the Design Phase",
    "text": "The Significance of the Design Phase\nThe design phase serves as a crucial precursor to the coding phase, offering distinct advantages in the software development process.\n\nCreating a System Overview\nThe primary goal is to construct a comprehensive overview of the entire system. This macroscopic perspective aids in organizing the subsequent coding phase efficiently.\n\n\nBenefits of a Well-Executed Design Phase\n\nConsistency:\n\nMitigates conflicts stemming from diverse developer perspectives.\nEnsures a uniform comprehension of the codebase.\n\nEfficiency Enhancement:\n\nPrecludes unnecessary alterations and errors during the implementation phase.\nFacilitates punctual product delivery.\n\nFuture-Proofing:\n\nStreamlines the addition of new features in subsequent phases.\nEnables seamless integration into the existing system.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#the-development-phase",
    "href": "pages/SE/Week01.html#the-development-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "The Development Phase",
    "text": "The Development Phase\nFollowing the design phase, the development phase entails collaborative coding efforts involving multiple developers. This phase often unfolds in a distributed manner, with team members situated in diverse locations and time zones. Collaboration tools such as GitHub play a pivotal role in this collective coding endeavor.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#imperative-role-of-documentation",
    "href": "pages/SE/Week01.html#imperative-role-of-documentation",
    "title": "Thinking of Software in terms of Components",
    "section": "Imperative Role of Documentation",
    "text": "Imperative Role of Documentation\nGiven the dispersed nature of development efforts, comprehensive documentation becomes imperative. This documentation, encompassing precise interface definitions, ensures a consistent understanding of code functionality among developers.\n\nInterface Definitions\nInterface definitions are foundational descriptions outlining the actions that functions can perform. Distinctively, the focus is on delineating actions rather than delving into intricate implementation details. Such definitions stipulate the types of requests accepted and the corresponding format of responses. The flexibility for code modifications exists, provided the interface remains consistent.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#collaborative-dynamics-in-development",
    "href": "pages/SE/Week01.html#collaborative-dynamics-in-development",
    "title": "Thinking of Software in terms of Components",
    "section": "Collaborative Dynamics in Development",
    "text": "Collaborative Dynamics in Development\nCollaboration during the development phase entails the coordinated efforts of multiple developers, often located in different time zones. Effective communication, facilitated through clear and concise interface definitions, is paramount to achieving seamless integration of components.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-3",
    "href": "pages/SE/Week01.html#introduction-3",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn the preceding video, we explored the design and development phases crucial to software development. However, two additional pivotal phases demand our attention: Testing and Maintenance.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#importance-of-testing",
    "href": "pages/SE/Week01.html#importance-of-testing",
    "title": "Thinking of Software in terms of Components",
    "section": "Importance of Testing",
    "text": "Importance of Testing\nTesting serves as a critical measure to ensure the alignment of software behavior with specified requirements. The existence of bugs and defects, if left unaddressed, may lead\nto substantial financial losses. For instance, a noteworthy study indicates that in 2002, software bugs resulted in a $60 billion loss in the U.S. economy, a figure that surged to $1.1 trillion in 2016. The failure to rectify such bugs can potentially precipitate severe catastrophes.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#testing-granularities",
    "href": "pages/SE/Week01.html#testing-granularities",
    "title": "Thinking of Software in terms of Components",
    "section": "Testing Granularities",
    "text": "Testing Granularities\n\n1. Unit Testing\nUnit testing directs its focus toward a singular component, often a class or function, examined in complete isolation.\n\n\n2. Integration Testing\nIntegration testing scrutinizes the interaction and collaboration of different parts within the application, ensuring seamless functionality as a unified whole.\n\n\n3. Acceptance Testing\nAcceptance testing verifies the fulfillment of user requirements. This testing stage bifurcates into:\n\nAlpha Testing\nInternal employees conduct alpha testing within a controlled environment, such as a lab or staging area.\n\n\nBeta Testing\nActual users undertake beta testing in real-world scenarios, providing valuable insights into the software’s performance.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#maintenance-phase",
    "href": "pages/SE/Week01.html#maintenance-phase",
    "title": "Thinking of Software in terms of Components",
    "section": "Maintenance Phase",
    "text": "Maintenance Phase\n\nPurpose\n\nUser Monitoring:\n\nContinuous observation of user activities and software usage.\n\nCode Changes:\n\nImplementation of code modifications for upgrades, including patch releases.\n\nFeature Addition:\n\nIntroduction of new features to enhance software functionality.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#example---amazon-pay",
    "href": "pages/SE/Week01.html#example---amazon-pay",
    "title": "Thinking of Software in terms of Components",
    "section": "Example - Amazon Pay",
    "text": "Example - Amazon Pay\n\nPost-Release Issues\nAfter the release of a feature like Amazon Pay, potential difficulties or errors that users may encounter must be anticipated. Examples include missed conditions, failures, and UI issues specific to certain browsers.\n\n\nMaintenance Process\nThe maintenance phase involves a systematic approach where the development team identifies issues and engages in a continuous process of rectification to ensure optimal software performance.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-4",
    "href": "pages/SE/Week01.html#introduction-4",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nSoftware engineering is a discipline that advocates a systematic approach to the development of software through a well-defined and structured set of activities. These activities are commonly denoted as the software lifecycle model, software development lifecycle (SDLC), or the software development process model.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#waterfall-model",
    "href": "pages/SE/Week01.html#waterfall-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Waterfall Model",
    "text": "Waterfall Model\n\nSequential Phases\nThe waterfall model entails a linear progression of phases, with each phase following the completion of the previous one. These phases encompass gathering requirements, design, coding, testing, and maintenance. The approach is also recognized as the plan and document perspective.\n\n\nDrawbacks\nDespite its structured nature, the waterfall model has notable drawbacks:\n\nIncreased Cost and Time: Modifications later in the process lead to elevated costs and time consumption.\nClient Understanding: Clients may not fully comprehend their needs initially.\nDesign Challenges: Developers may face challenges in determining the most feasible design.\nLengthy Iterations: Each phase or iteration can span from 6 to 18 months.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#prototype-model",
    "href": "pages/SE/Week01.html#prototype-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Prototype Model",
    "text": "Prototype Model\n\nConcept and Execution\nTo address the drawbacks of the waterfall model, the prototype model advocates the creation of a working prototype of the system before the actual software development begins. The prototype, possessing limited functionality, is subsequently discarded or replaced with the final product.\n\n\nAdvantages and Disadvantages\nAdvantages: - Enhanced understanding for both clients and developers regarding project requirements.\nDisadvantages: - Augmented development costs. - Inability to anticipate risks and bugs emerging later in the development cycle.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#spiral-model",
    "href": "pages/SE/Week01.html#spiral-model",
    "title": "Thinking of Software in terms of Components",
    "section": "Spiral Model",
    "text": "Spiral Model\n\nIntegration of Approaches\nThe spiral model amalgamates features from both the waterfall and prototype models. It unfolds in four distinct phases: determining objectives, evaluating alternatives, developing and testing, and planning for the subsequent phase. Each iteration involves a refinement of the prototype.\n\n\nIterative Process\nThis model fosters an iterative process, where the refinement of the prototype occurs at each iteration. Unlike the waterfall model, requirement documents are progressively developed across iterations. Client involvement at the end of each iteration mitigates misunderstandings.\n\n\nDrawback\nDespite its advantages, the spiral model still encounters a drawback: each iteration may extend from 6 to 24 months.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#introduction-5",
    "href": "pages/SE/Week01.html#introduction-5",
    "title": "Thinking of Software in terms of Components",
    "section": "Introduction",
    "text": "Introduction\nIn our previous lectures, we navigated through the intricacies of the software development lifecycle, concentrating particularly on established models like the waterfall model. While these models, falling under the plan and document process category, brought structure to software development, they faced considerable challenges in meeting deadlines and adhering to specified budgets. Surprisingly, studies conducted from 1995 to 2013 indicated that around 80 to 90 percent of software projects encountered issues such as overdue timelines, exceeding budgetary limits, or even abandonment. This realization triggered a significant shift in software development methodologies.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#emergence-of-the-agile-manifesto",
    "href": "pages/SE/Week01.html#emergence-of-the-agile-manifesto",
    "title": "Thinking of Software in terms of Components",
    "section": "Emergence of the Agile Manifesto",
    "text": "Emergence of the Agile Manifesto\nApproximately two decades ago, in February 2001, a coalition of software developers convened to devise a more flexible software development lifecycle. This effort culminated in the creation of the Agile Manifesto, a document founded on four key principles. The manifesto aimed to address the shortcomings of traditional approaches, laying the groundwork for a more lightweight and adaptive software development process.\n\nAgile Manifesto Principles\n\nIndividuals and Interactions over Processes and Tools:\n\nEmphasizes the importance of interpersonal dynamics within the development team and effective communication with clients.\n\nWorking Software over Comprehensive Documentation:\n\nPrioritizes the delivery of functional software in increments over exhaustive documentation.\n\nCustomer Collaboration over Contract Negotiation:\n\nAdvocates for active collaboration with customers to understand their needs rather than fixating on contractual minutiae.\n\nResponding to Change over Following a Plan:\n\nEncourages adaptability to change during the development process, emphasizing responsiveness.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#agile-development-approach",
    "href": "pages/SE/Week01.html#agile-development-approach",
    "title": "Thinking of Software in terms of Components",
    "section": "Agile Development Approach",
    "text": "Agile Development Approach\nThe Agile development approach is characterized by its iterative and incremental model. Teams adopting Agile construct the software product in small, manageable increments through multiple iterations. This process involves developing prototypes for key features, promptly releasing them for feedback. Noteworthy Agile methodologies include Extreme Programming (XP), Scrum, and Kanban.\n\nExtreme Programming (XP)\nExtreme Programming incorporates key practices such as behavior-driven design, test-driven development, and pair programming. These practices contribute to a development environment centered around quick iterations and continuous feedback.\n\n\nScrum\nScrum, another Agile methodology, divides the product development into iterations known as sprints, typically lasting one to two weeks. This approach facilitates breaking down complex projects into more manageable and actionable components.\n\n\nKanban\nIn Kanban, the software is segmented into small work items, visually represented on a Kanban board. This visual aid enables team members to monitor the status of each work item in real-time.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#choosing-the-development-perspective",
    "href": "pages/SE/Week01.html#choosing-the-development-perspective",
    "title": "Thinking of Software in terms of Components",
    "section": "Choosing the Development Perspective",
    "text": "Choosing the Development Perspective\nSelecting between the plan and document perspective and the Agile perspective depends on various factors. Key considerations include the fixity of requirements, client availability, system characteristics, team distribution, team familiarity with documentation models, and the presence of regulatory constraints.\n\nFactors Influencing Choice\n\nRequirements/Specifications Fixity:\n\nAre requirements/specifications mandated to be fixed upfront?\n\nClient Availability:\n\nIs the client or customer consistently available for collaboration?\n\nSystem Characteristics:\n\nDoes the system possess characteristics like size and complexity that warrant extensive planning and documentation?\n\nTeam Distribution:\n\nIs the software team geographically dispersed?\n\nTeam Familiarity:\n\nIs the team already acquainted with the plan and document model?\n\nRegulatory Constraints:\n\nIs the system subject to numerous regulatory requirements?",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/SE/Week01.html#points-to-remember",
    "href": "pages/SE/Week01.html#points-to-remember",
    "title": "Thinking of Software in terms of Components",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nIncremental System Development: Large-scale systems, like those employed by industry leaders such as Amazon, evolve incrementally, emphasizing the construction of components or modules before their integration.\nRequirement Specification: The software development process begins with a deep understanding of the problem, studying existing components, and defining system requirements derived from thorough analyses.\nSoftware Design and Development: The design phase is crucial, providing a macroscopic perspective of the entire system and offering advantages such as consistency, efficiency enhancement, and future-proofing. Effective documentation and collaborative coding are imperative during the development phase.\nTesting and Maintenance: Testing is critical to ensure software behavior aligns with requirements, and maintenance involves continuous monitoring, code changes, and feature additions to enhance software functionality.\nWaterfall Model: A structured, sequential model with phases like gathering requirements, design, coding, testing, and maintenance. However, it has drawbacks, including increased cost and time.\nPrototype Model: Advocates creating a working prototype before actual development to enhance understanding but may incur augmented development costs.\nSpiral Model: Integrates features from both waterfall and prototype models, fostering an iterative process, but each iteration may extend over a considerable duration.\nAgile Development: A response to challenges faced by traditional approaches, characterized by an iterative and incremental model. Agile methodologies include Extreme Programming (XP), Scrum, and Kanban.\nAgile Manifesto Principles:\n\nIndividuals and interactions over processes and tools.\nWorking software over comprehensive documentation.\nCustomer collaboration over contract negotiation.\nResponding to change over following a plan.\n\nChoosing the Development Perspective: Factors influencing the choice between plan and document perspective and Agile perspective include requirements fixity, client availability, system characteristics, team distribution, team familiarity, and regulatory constraints.",
    "crumbs": [
      "Software Engineering",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html",
    "href": "pages/AI/Week02.html",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "In the realm of Artificial Intelligence, the study of search algorithms plays a pivotal role in problem-solving strategies. These algorithms, designed to explore the state space of a given problem, can be categorized into brute force search, informed search, and a general algorithmic approach. To illustrate these concepts, we delve into the map coloring problem, showcasing various problem-solving strategies.\n\n\n\n\n\nState space search involves representing a problem as a graph, where each node represents a unique state, and edges denote possible moves between states. The primary goal of this course is to explore general search methods, incorporating heuristic techniques for improved efficiency. The methods under consideration include state space search and constraint processing.\n\n\n\n\n\nStates are representations of specific situations in a given problem. These states are treated as nodes within the search space graph, with each node denoted by a symbol, such as \\(s\\). The state space, essentially an implicit graph, is defined by a move generation function.\n\n\n\nA move generation function is critical in navigating the state space. It determines the possible moves from a given state, producing a set of neighboring states. In functional terms, this function, denoted as \\(MoveGen(s)\\), takes a state \\(s\\) as input and returns a set of states, or neighbors, achievable from the current state.\n\n\n\nThe exploration of the state space is facilitated by a search algorithm, which employs the move generation function to navigate through the graph. The algorithm terminates based on the results of a goal test function.\n\n\n\nThe goal test function, denoted as \\(GoalTest(s)\\), checks whether a given state \\(s\\) is the desired goal state. It serves as the criterion for terminating the search algorithm.\n\n\n\n\n\nGeneral search methods are designed to create adaptable algorithms capable of addressing a variety of problems. The two primary approaches discussed in this course are state space search and constraint processing, with a primary focus on the former.\n\n\n\n\n\n\n\nThe water jug problem involves three jugs with different capacities, requiring the measurement of a specific amount of water.\n\n\n\nStates are described as a list of three numbers, representing the water levels in each jug.\n\n\n\nMoves involve pouring water between jugs, and the goal test function is contingent on achieving the desired water measurement.\n\n\n\n\n\n\nThe eight puzzle, a two-dimensional puzzle, requires rearranging tiles to achieve a specific configuration.\n\n\n\nStates are represented by an 8-puzzle configuration, and moves involve sliding tiles into the blank space.\n\n\n\nThe goal test function checks whether the configuration matches the desired goal configuration.\n\n\n\n\n\n\nThis classic problem involves transporting individuals across a river without violating specific constraints.\n\n\n\nVarious representations are discussed, including objects on the left bank or based on the boat’s location.\n\n\n\nThe goal test function checks for a specific configuration that adheres to the constraints.\n\n\n\n\n\n\nThe N-Queens problem requires placing N queens on an N×N chessboard with no mutual attacks.\n\n\n\nSolving involves finding a valid arrangement of queens on the chessboard.\n\n\n\n\n\n\nThe traveling salesman problem involves finding the optimal tour, with the lowest cost, visiting each city exactly once and returning to the starting city.\n\n\n\nThe objective is to discover the tour with the lowest cost, a challenging problem with factorial time complexity.\n\n\n\n\n\n\nMaze solving requires finding a path through a maze from the entrance to the exit.\n\n\n\nThe maze can be represented as a graph, with each node representing a choice point.\n\n\n\nThe goal is to find a path through the maze to reach the exit.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#introduction-to-search-algorithms-in-ai",
    "href": "pages/AI/Week02.html#introduction-to-search-algorithms-in-ai",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "In the realm of Artificial Intelligence, the study of search algorithms plays a pivotal role in problem-solving strategies. These algorithms, designed to explore the state space of a given problem, can be categorized into brute force search, informed search, and a general algorithmic approach. To illustrate these concepts, we delve into the map coloring problem, showcasing various problem-solving strategies.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#state-space-search-1",
    "href": "pages/AI/Week02.html#state-space-search-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "State space search involves representing a problem as a graph, where each node represents a unique state, and edges denote possible moves between states. The primary goal of this course is to explore general search methods, incorporating heuristic techniques for improved efficiency. The methods under consideration include state space search and constraint processing.\n\n\n\n\n\nStates are representations of specific situations in a given problem. These states are treated as nodes within the search space graph, with each node denoted by a symbol, such as \\(s\\). The state space, essentially an implicit graph, is defined by a move generation function.\n\n\n\nA move generation function is critical in navigating the state space. It determines the possible moves from a given state, producing a set of neighboring states. In functional terms, this function, denoted as \\(MoveGen(s)\\), takes a state \\(s\\) as input and returns a set of states, or neighbors, achievable from the current state.\n\n\n\nThe exploration of the state space is facilitated by a search algorithm, which employs the move generation function to navigate through the graph. The algorithm terminates based on the results of a goal test function.\n\n\n\nThe goal test function, denoted as \\(GoalTest(s)\\), checks whether a given state \\(s\\) is the desired goal state. It serves as the criterion for terminating the search algorithm.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#search-algorithms-overview",
    "href": "pages/AI/Week02.html#search-algorithms-overview",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "General search methods are designed to create adaptable algorithms capable of addressing a variety of problems. The two primary approaches discussed in this course are state space search and constraint processing, with a primary focus on the former.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#sample-problems-in-state-space-search",
    "href": "pages/AI/Week02.html#sample-problems-in-state-space-search",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "",
    "text": "The water jug problem involves three jugs with different capacities, requiring the measurement of a specific amount of water.\n\n\n\nStates are described as a list of three numbers, representing the water levels in each jug.\n\n\n\nMoves involve pouring water between jugs, and the goal test function is contingent on achieving the desired water measurement.\n\n\n\n\n\n\nThe eight puzzle, a two-dimensional puzzle, requires rearranging tiles to achieve a specific configuration.\n\n\n\nStates are represented by an 8-puzzle configuration, and moves involve sliding tiles into the blank space.\n\n\n\nThe goal test function checks whether the configuration matches the desired goal configuration.\n\n\n\n\n\n\nThis classic problem involves transporting individuals across a river without violating specific constraints.\n\n\n\nVarious representations are discussed, including objects on the left bank or based on the boat’s location.\n\n\n\nThe goal test function checks for a specific configuration that adheres to the constraints.\n\n\n\n\n\n\nThe N-Queens problem requires placing N queens on an N×N chessboard with no mutual attacks.\n\n\n\nSolving involves finding a valid arrangement of queens on the chessboard.\n\n\n\n\n\n\nThe traveling salesman problem involves finding the optimal tour, with the lowest cost, visiting each city exactly once and returning to the starting city.\n\n\n\nThe objective is to discover the tour with the lowest cost, a challenging problem with factorial time complexity.\n\n\n\n\n\n\nMaze solving requires finding a path through a maze from the entrance to the exit.\n\n\n\nThe maze can be represented as a graph, with each node representing a choice point.\n\n\n\nThe goal is to find a path through the maze to reach the exit.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#introduction",
    "href": "pages/AI/Week02.html#introduction",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Introduction",
    "text": "Introduction\nIn the pursuit of developing domain-independent problem-solving algorithms within the realm of artificial intelligence (AI), the focus is on general search algorithms. These algorithms aim to provide solutions to diverse problems in a domain-independent form. This discussion revolves around two key algorithms: “Simple Search 1” and its modification, “Simple Search 2.”",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#components-of-state-space-search-1",
    "href": "pages/AI/Week02.html#components-of-state-space-search-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Components of State Space Search",
    "text": "Components of State Space Search\n\n1. Start and Goal States\nThe state space comprises a set of states, a defined start state, and a specified goal state. These elements form the foundational framework for problem-solving in AI.\n\n\n2. Move Gen Function\nThe move generation function, denoted as $ (n) $, serves as a domain-specific function responsible for generating the neighbors of a given node $ n $. Importantly, it dynamically constructs the graph as the algorithm progresses.\n\n\n3. Goal Test Function\nThe goal test function, $ (n) $, determines whether a given state $ n $ aligns with the defined goal state. This function plays a crucial role in assessing the success of the search algorithm.\n\n\n4. Scene Nodes and Candidate Nodes\nIn the process of state space search, two categories of nodes emerge: scene nodes and candidate nodes. - Scene Nodes: These nodes represent states that have been visited and tested for the goal. They are stored in a set or list termed “closed.” - Candidate Nodes: Generated by the move gen function, these nodes are candidates for exploration but have not yet been visited. They are stored in a set or list referred to as “open.”",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#simple-search-1-algorithm",
    "href": "pages/AI/Week02.html#simple-search-1-algorithm",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Simple Search 1 Algorithm",
    "text": "Simple Search 1 Algorithm\n\nAlgorithm Overview\nThe “Simple Search 1” algorithm adheres to the generate-and-test approach, a fundamental strategy in AI problem-solving. The algorithm iteratively generates nodes, tests them for the goal, and continues until either the goal is found or the open set becomes empty.\n\n\nNode Selection\nA node is selected from the open set. If this node corresponds to the goal state, the algorithm terminates successfully. Otherwise, the process continues.\n\n\nGraph Exploration\nThe algorithm leverages the move gen function to generate neighbors of the selected node. These generated nodes are then added to the open set for further exploration.\n\n\nPseudocode for Simple Search 1\nOPEN ← {S}\nwhile OPEN is not empty\n   Pick some node N from OPEN\n   OPEN ← OPEN - {N}\n   if GoalTest(N) = TRUE\n     return N\n   else \n     OPEN ← OPEN ∪ MoveGen(N)\nreturn null\n\n\nChallenge - Cyclic Exploration\nA notable challenge with “Simple Search 1” is its susceptibility to entering cycles, leading to the revisiting of nodes without making progress. This cyclic exploration issue poses a potential impediment to the algorithm’s effectiveness.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#simple-search-2-algorithm",
    "href": "pages/AI/Week02.html#simple-search-2-algorithm",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Simple Search 2 Algorithm",
    "text": "Simple Search 2 Algorithm\n\nIntroduction of Closed Set\nTo address the cyclic exploration problem, “Simple Search 2” introduces a new set named “closed.” This set serves as a repository for scene nodes, preventing their reevaluation during the search process.\n\n\nPurpose of Closed Set\nThe closed set’s primary function is to avoid revisiting nodes already assessed for the goal. By maintaining a record of scene nodes, the algorithm reduces the search space and mitigates the cyclic exploration challenge.\n\n\nAlgorithm Adjustment\nThe node selected from the open set is now moved to the closed set before testing for the goal. Additionally, during the generation of neighbors, nodes already present in the closed set are excluded from being added to the open set.\n\n\nPseudocode for Simple Search 2\nOPEN ← {S}\nCLOSED ← empty set\nwhile OPEN is not empty\n   Pick some node N from OPEN\n   OPEN ← OPEN – {N}\n   CLOSED ← CLOSED ∪ {N}\n   if GoalTest(N) = TRUE \n     return N \n   else \n     OPEN ← OPEN ∪ (MoveGen(N) – CLOSED)\nreturn null  \n\n\nImproved Exploration\n“Simple Search 2” demonstrates enhanced efficiency by minimizing the search space. The exclusion of nodes already visited contributes to a more focused exploration, addressing the cyclic exploration issue encountered in “Simple Search 1.”",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#consideration---solution-path",
    "href": "pages/AI/Week02.html#consideration---solution-path",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Consideration - Solution Path",
    "text": "Consideration - Solution Path\nWhile both algorithms aim to find the goal node, it’s essential to note that they do not provide the solution path. The goal test confirms the existence of a solution without specifying the sequence of states leading to it. Further considerations may be necessary to obtain the complete solution path.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#impact-of-algorithm-choice",
    "href": "pages/AI/Week02.html#impact-of-algorithm-choice",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Impact of Algorithm Choice",
    "text": "Impact of Algorithm Choice\nThe choice of algorithm significantly influences the exploration of the search space. Different algorithms may yield distinct search spaces for the same state space. The efficiency and effectiveness of the search process hinge on the algorithm’s ability to circumvent cyclic exploration and avoid unnecessary node revisits.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#problem-classification",
    "href": "pages/AI/Week02.html#problem-classification",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Problem Classification",
    "text": "Problem Classification\nIn the realm of state space search, two distinctive problem types emerge: Configuration Problems and Planning Problems.\n\nConfiguration Problems\nConfiguration problems involve seeking a state that satisfies a given description. For instance, classic problems like the N-Queens puzzle, Sudoku, Map Coloring, and others fall into this category. The primary objective is to identify a state that adheres to the specified criteria.\n\n\nPlanning Problems\nContrarily, planning problems revolve around scenarios where the goal is either explicitly known or described, and the pursuit is directed towards determining the optimal path to that goal. This type includes real-world situations such as finding a suitable restaurant, where the algorithm must discern both the destination and the most efficient route.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#graph-representation",
    "href": "pages/AI/Week02.html#graph-representation",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Graph Representation",
    "text": "Graph Representation\nIn the context of state space search, the graph serves as the fundamental model. Each node within this graph represents a unique state. However, in planning problems, the goal extends beyond merely reaching the final state; it includes the necessity to ascertain the path leading to that state.\n\nNode Pairs\nTo address this, the concept of node pairs is introduced. In this representation, every node is accompanied by information about its parent node. This augmentation proves pivotal when reconstructing the path to the goal.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#path-reconstruction",
    "href": "pages/AI/Week02.html#path-reconstruction",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Path Reconstruction",
    "text": "Path Reconstruction\nEfficient path reconstruction relies on the inclusion of node pairs within the search space. As the algorithm traverses the search space and identifies the goal state, the closed list—housing node pairs—facilitates the backward tracing of the path. Each node pair encapsulates information about the current node and its parent, enabling a step-by-step reconstruction.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#search-algorithm-overview",
    "href": "pages/AI/Week02.html#search-algorithm-overview",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Search Algorithm Overview",
    "text": "Search Algorithm Overview\nThe overarching search algorithm is designed to systematically explore the search space, attempting different paths until a viable route to the goal state is discovered.\n\nDeterministic Approach\nIn contrast to the initial non-deterministic approach of picking any node from the open set, the algorithm undergoes a modification. It transitions to a deterministic strategy, consistently selecting the node positioned at the head of the open list.\n\n\nList Structure\nThe traditional use of sets for open and closed is superseded by the adoption of lists. This shift is accompanied by a preference for adding new nodes to a specified location in the list, influencing their order and impact on the search algorithm.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#notational-conventions",
    "href": "pages/AI/Week02.html#notational-conventions",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Notational Conventions",
    "text": "Notational Conventions\n\nList Notation\n\nThe empty list is represented as square brackets: \\([]\\).\nOperations include the colon operator for adding an element to the head of a list and the plus plus operator for appending two lists.\nEssential functions, such as head and tail, serve in extracting elements and conducting tests.\n\n\n\nTuple Notation\nTuples, denoted by parentheses, accommodate ordered elements. Accessing tuple elements involves positional identification or leveraging built-in functions like first and second.\nFor further reference on operations and functions, refer to this pdf.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#algorithm-refinement",
    "href": "pages/AI/Week02.html#algorithm-refinement",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Algorithm Refinement",
    "text": "Algorithm Refinement\nThe transition from non-deterministic node selection to a deterministic strategy represents a pivotal refinement. This evolution ensures the consistent selection of the node residing at the forefront of the open list. Additionally, the determination of where new nodes are inserted in the list assumes significance, shaping their influence on the algorithm’s behavior.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#initialization",
    "href": "pages/AI/Week02.html#initialization",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Initialization",
    "text": "Initialization\n- OPEN ← (S, null) : []\n- CLOSED ← empty list\nThe algorithm starts with an open list containing the start node (S, null) where S is the start node, and null represents the absence of a parent. The CLOSED list is initially empty.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#main-algorithm",
    "href": "pages/AI/Week02.html#main-algorithm",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Main Algorithm",
    "text": "Main Algorithm\n- while OPEN is not empty\n  - nodePair ← head OPEN\n  - (N, _) ← nodePair\n  - if GoalTest(N) = TRUE\n    - return RECONSTRUCTPATH(nodePair, CLOSED)\n  - else CLOSED ← nodePair : CLOSED\n    - neighbours ← MoveGen(N)\n    - newNodes ← REMOVESEEN(neighbours, OPEN, CLOSED)\n    - newPairs ← MAKEPAIRS(newNodes, N)\n    - OPEN ← newPairs ++ (tail OPEN)\n- return empty list\nThe algorithm iteratively selects the first element from the open list and explores the node (N, _). If the goal test is satisfied, it calls the RECONSTRUCTPATH function. Otherwise, it adds the node pair to the closed list, generates and filters the children using REMOVESEEN, creates pairs with parents using MAKEPAIRS, and appends them to the front of the open list.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#ancillary-functions",
    "href": "pages/AI/Week02.html#ancillary-functions",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Ancillary Functions",
    "text": "Ancillary Functions\n\nRECONSTRUCTPATH Function\n- RECONSTRUCTPATH(nodePair, CLOSED)\n  - SKIPTO(parent, nodePairs)\n    - if parent = first head nodePairs\n      - return nodePairs\n    - else return SKIPTO(parent, tail nodePairs)\n  - (node, parent) ← nodePair\n  - path ← node : []\n  - while parent is not null\n    - path ← parent : path\n    - CLOSED ← SKIPTO(parent, CLOSED)\n    - (_, parent) ← head CLOSED\n  - return path\nThe RECONSTRUCTPATH function traces back from the goal node to the start node using parent pointers stored in the CLOSED list.\n\n\nMAKEPAIRS Function\n- MAKEPAIRS(nodeList, parent)\n  - if nodeList is empty\n    - return empty list\n  - else return (head nodeList, parent) : MAKEPAIRS(tail nodeList, parent)\nThe MAKEPAIRS function takes a list of nodes and a parent, creating pairs with each node and the given parent.\n\n\nREMOVESEEN Function\n- REMOVESEEN(nodeList, OPEN, CLOSED)\n  - if nodeList is empty\n    - return empty list\n  - else node ← head nodeList\n    - if OCCURSIN(node, OPEN) or OCCURSIN(node, CLOSED)\n      - return REMOVESEEN(tail nodeList, OPEN, CLOSED)\n    - else return node : REMOVESEEN(tail nodeList, OPEN, CLOSED)\nThe REMOVESEEN function filters out nodes already present in the OPEN or CLOSED lists.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#initialization-1",
    "href": "pages/AI/Week02.html#initialization-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Initialization",
    "text": "Initialization\n- OPEN ← (S, null) : []\n- CLOSED ← empty list\nSimilar to DFS, BFS starts with an open list containing the start node (S, null) and an empty CLOSED list.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#main-algorithm-1",
    "href": "pages/AI/Week02.html#main-algorithm-1",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Main Algorithm",
    "text": "Main Algorithm\n- while OPEN is not empty\n  - nodePair ← head OPEN\n  - (N, _) ← nodePair\n  - if GoalTest(N) = TRUE\n    - return RECONSTRUCTPATH(nodePair, CLOSED)\n  - else CLOSED ← nodePair : CLOSED\n    - neighbours ← MoveGen(N)\n    - newNodes ← REMOVESEEN(neighbours, OPEN, CLOSED)\n    - newPairs ← MAKEPAIRS(newNodes, N)\n    - OPEN ← (tail OPEN) ++ newPairs\n- return empty list\nThe main algorithm for BFS is identical to DFS, except for the addition of new nodes to the end of the OPEN list.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#analysis-of-dfs",
    "href": "pages/AI/Week02.html#analysis-of-dfs",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Analysis of DFS",
    "text": "Analysis of DFS\n\nOverview\nDepth First Search (DFS) is a search algorithm employed in problem-solving within the field of Artificial Intelligence. It is characterized by its treatment of the open set as a stack, following the Last In, First Out (LIFO) principle.\n\n\nExploration Strategy\nDFS explores the search tree in a deep-first manner, descending into the tree until it reaches a dead end. Upon encountering a dead end, the algorithm backtracks to explore alternative paths.\n\n\nBehavior\nDFS tends to find paths that are farther from the source node, emphasizing deep exploration rather than a systematic examination of all possibilities. It exhibits a distinct behavior of diving deep into the search tree.\n\n\nTime Complexity\nThe time complexity of DFS is exponential and can be expressed as \\(O(b^d)\\), where \\(b\\) represents the branching factor of the search tree, and \\(d\\) is the depth. This exponential growth can lead to infinite loops in scenarios with infinite search spaces.\n\n\nSpace Complexity\nDFS demonstrates linear space complexity. The space required is proportional to the depth of the search tree, making it more space-efficient compared to other algorithms with exponential space growth.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#analysis-of-bfs",
    "href": "pages/AI/Week02.html#analysis-of-bfs",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Analysis of BFS",
    "text": "Analysis of BFS\n\nOverview\nBreadth First Search (BFS) is another search algorithm used in problem-solving for Artificial Intelligence. Unlike DFS, BFS treats the open set as a queue, adhering to the First In, First Out (FIFO) principle.\n\n\nExploration Strategy\nBFS explores the search tree level by level, starting from the source node and moving outward systematically. It ensures a conservative approach by prioritizing paths closer to the source.\n\n\nBehavior\nBFS is designed to find paths that are closer to the source node, ensuring a more methodical exploration of the search tree. It guarantees the discovery of the shortest path due to its systematic approach.\n\n\nTime Complexity\nSimilar to DFS, BFS exhibits exponential time complexity, expressed as \\(O(b^d)\\), where \\(b\\) is the branching factor, and \\(d\\) is the depth. However, BFS explores paths of increasing length systematically, ensuring the identification of the shortest path.\n\n\nSpace Complexity\nBFS has exponential space complexity, with the size of the open set growing exponentially. This makes BFS less space-efficient compared to DFS, but it guarantees finding the shortest path.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#comparison",
    "href": "pages/AI/Week02.html#comparison",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Comparison",
    "text": "Comparison\n\nTime Complexity\nBoth DFS and BFS share exponential time complexity, posing challenges in scenarios with large search trees.\n\n\nSpace Complexity\nDFS outperforms BFS in terms of space efficiency, having linear space complexity compared to BFS’s exponential growth.\n\n\nQuality of Solution\nDFS does not guarantee the shortest path, while BFS ensures the identification of the shortest path due to its systematic exploration.\n\n\nCompleteness\nDFS may not be complete, especially in infinite search spaces, where it can get lost in infinite paths. On the other hand, BFS is complete, provided there exists a path of finite length from the source to the goal.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#search-space-characteristics-and-solution-strategies",
    "href": "pages/AI/Week02.html#search-space-characteristics-and-solution-strategies",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Search Space Characteristics and Solution Strategies",
    "text": "Search Space Characteristics and Solution Strategies\n\nInfinite Search Space Dilemma\nWhen confronted with an infinite search space, the choice between Depth-First Search (DFS) and Breadth-First Search (BFS) becomes contingent upon the problem’s specifics. BFS is the preferred option if the search space is infinite but a solution is known to exist. Conversely, DFS might be more suitable if the search space is finite, albeit without guaranteeing the shortest path.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#depth-bounded-depth-first-search",
    "href": "pages/AI/Week02.html#depth-bounded-depth-first-search",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Depth-Bounded Depth-First Search",
    "text": "Depth-Bounded Depth-First Search\n\nStrategy Overview\nDepth-Bounded Depth-First Search strikes a balance between the characteristics of DFS and BFS. It limits the exploration depth, ensuring linear space complexity while compromising on completeness and the guarantee of finding the shortest path. The algorithm delves into the search space up to a specified depth, potentially missing the goal if it exceeds this depth.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#depth-bounded-dfs-with-node-counting",
    "href": "pages/AI/Week02.html#depth-bounded-dfs-with-node-counting",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Depth-Bounded DFS with Node Counting",
    "text": "Depth-Bounded DFS with Node Counting\n\nEnhanced Exploration\nAn augmentation to Depth-Bounded DFS involves incorporating node counting during the search process. This count of visited nodes provides additional insights, proving advantageous in certain problem scenarios and facilitating subsequent analysis.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#depth-first-iterative-deepening-dfid",
    "href": "pages/AI/Week02.html#depth-first-iterative-deepening-dfid",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Depth-First Iterative Deepening (DFID)",
    "text": "Depth-First Iterative Deepening (DFID)\n\nIterative Depth Expansion\nDFID emerges as a solution that combines the strengths of DFS and BFS. It iteratively increases the depth limit for DFS until a solution is encountered. The algorithm mitigates the risk of failing to find a path due to depth constraints but introduces the challenge of revisiting nodes multiple times. The careful tracking of node counts prevents infinite loops and enhances overall efficiency.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#path-reconstruction-challenges",
    "href": "pages/AI/Week02.html#path-reconstruction-challenges",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Path Reconstruction Challenges",
    "text": "Path Reconstruction Challenges\n\nDilemma Overview\nPath reconstruction poses challenges, particularly when multiple paths to the goal exist. The lecture delves into the complexities of maintaining closed lists and the importance of judiciously selecting parents during the path reconstruction process.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#dfid-in-chess-programming",
    "href": "pages/AI/Week02.html#dfid-in-chess-programming",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "DFID in Chess Programming",
    "text": "DFID in Chess Programming\n\nTactical Application\nDFID finds practical application in chess programming, particularly in scenarios where players face time constraints. The algorithm’s iterative deepening approach accommodates the limited time available for move selection.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#combinatorial-explosion-and-dfid",
    "href": "pages/AI/Week02.html#combinatorial-explosion-and-dfid",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Combinatorial Explosion and DFID",
    "text": "Combinatorial Explosion and DFID\n\nCoping with Exponential Growth\nThe lecture acknowledges the pervasive issue of combinatorial explosion, where search trees exhibit exponential growth. DFID addresses this challenge by iteratively searching with incrementally expanding depth limits. An in-depth analysis delves into the trade-offs between time and space, revealing the algorithm’s resilience in the face of increasing complexities.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#blind-uninformed-search",
    "href": "pages/AI/Week02.html#blind-uninformed-search",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Blind (Uninformed) Search",
    "text": "Blind (Uninformed) Search\n\nFixed Behaviors\nBlind searches, including DFS, BFS, and DFID, are characterized as uninformed strategies. These approaches lack awareness of the goal’s location during exploration, adhering to predetermined behaviors irrespective of the goal’s position.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#dfid-n-dfid-with-node-reopening",
    "href": "pages/AI/Week02.html#dfid-n-dfid-with-node-reopening",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "DFID-N: DFID with Node Reopening",
    "text": "DFID-N: DFID with Node Reopening\nDFID-N opens only new nodes (nodes not already present in OPEN/CLOSED) and does not reopen any nodes. It aims to find the solution with linear space complexity.\n\nDFID-N(\\(s\\))\ncount ← -1\npath ← empty list\ndepthBound ← 0\n\nrepeat \n    previousCount ← count \n    (count, path) ← DB-DFS-N(s, depthBound)\n    depthBound ← depthBound + 1 \nuntil (path is not empty) or (previousCount = count)\n\nreturn path\n\n\nDB-DFS-N(\\(s\\), depthBound)\n\nOpens only new nodes, i.e., nodes neither in OPEN nor in CLOSED.\nDoes not reopen any nodes.\n\ncount ← 0 \nOPEN ← (s, null, 0): []\nCLOSED ← empty list \n\nwhile OPEN is not empty \n    nodePair ← head OPEN \n    (N, _, depth)← nodePair \n    \n    if GoalTest(N) == TRUE \n        return (count, ReconstructPath(nodePair, CLOSED))\n    \n    else CLOSED← nodePair : CLOSED \n    \n    if depth &lt; depthBound \n        neighbours ← MoveGen(N)\n        newNodes ← SEE(neighbours, OPEN, CLOSED)\n        newPairs ← MAKEPAIRS(newNodes, N, depth + 1 )\n        OPEN ← newPairs ++ tail OPEN \n        \n        count ← count + length newPairs\n    \n    else OPEN = tail OPEN \n\nreturn (count, empty list)",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#dfid-c-dfid-with-closed-node-reopening",
    "href": "pages/AI/Week02.html#dfid-c-dfid-with-closed-node-reopening",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "DFID-C: DFID with Closed Node Reopening",
    "text": "DFID-C: DFID with Closed Node Reopening\nDFID-C opens new nodes (nodes not already present in OPEN/CLOSED) and also reopens nodes present in CLOSED but not present in OPEN.\n\nDFID-C(\\(s\\))\ncount ← -1\npath ← empty list\ndepthBound ← 0\n\nrepeat \n    previousCount ← count \n    (count, path) ← DB-DFS-C(s, depthBound)\n    depthBound ← depthBound + 1 \nuntil (path is not empty) or (previousCount = count)\n\nreturn path\n\n\nDB-DFS-C(\\(s\\), depthBound)\n\nOpens new nodes, i.e., nodes neither in OPEN nor in CLOSED.\nReopens nodes present in CLOSED and not present in OPEN.\n\ncount ← 0 \nOPEN ← (s, null, 0): []\nCLOSED ← empty list \n\nwhile OPEN is not empty \n    nodePair ← head OPEN \n    (N, _, depth)← nodePair \n    \n    if GoalTest(N) == TRUE \n        return (count, ReconstructPath(nodePair, CLOSED))\n    \n    else CLOSED ← nodePair : CLOSED \n    \n    if depth &lt; depthBound \n        neighbours ← MoveGen(N)\n        newNodes ← SEE(neighbours, OPEN, CLOSED)\n        newPairs ← MAKEPAIRS(newNodes, N, depth + 1 )\n        OPEN ← newPairs ++ tail OPEN \n        \n        count ← count + length newPairs\n    \n    else OPEN = tail OPEN \n\nreturn (count, empty list)",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#ancillary-functions-for-dfid-c",
    "href": "pages/AI/Week02.html#ancillary-functions-for-dfid-c",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Ancillary Functions for DFID-C",
    "text": "Ancillary Functions for DFID-C\n\nMAKEPAIRS(nodeList, parent, depth)\n\nCreates node pairs from the given node list, parent, and depth.\nReturns a list of node pairs.\n\nif nodeList is empty\n    return empty list\nelse nodePair ← (head nodeList, parent, depth)\n    return nodePair : MAKEPAIRS(tail nodeList, parent, depth)\n\n\nRECONSTRUCTPATH(nodePair, CLOSED)\n\nReconstructs the path using the given node pair and CLOSED list.\nReturns the reconstructed path.\n\nSKIPTo(parent, nodePairs, depth)\n    if (parent, ..., depth) = head nodePairs\n        return nodePairs\n    else return SKIPTo(parent, tail nodePairs, depth)\n\n(node, parent, depth) ← nodePair\npath ← node : []\n\nwhile parent is not null \n    path ← parent : path \n    CLOSED ← SKIPTo(parent, CLOSED, depth − 1 )\n    (_, _, parent, depth) ← head CLOSED \n\nreturn path",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week02.html#points-to-remember",
    "href": "pages/AI/Week02.html#points-to-remember",
    "title": "State Space Search and Search Algorithms Overview",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nState Space Search Overview:\n\nState space search involves representing problems as graphs, where nodes represent unique states and edges denote possible moves.\nComponents include state representation, move generation, state space exploration, and goal test.\n\nSearch Algorithms:\n\nVarious search algorithms, such as DFS and BFS, offer different exploration strategies and have implications for time and space complexity.\nDFID combines the strengths of DFS and BFS, iteratively increasing depth limits.\n\nAlgorithmic Variations:\n\nDFID-N opens only new nodes, aiming for linear space complexity.\nDFID-C reopens nodes in CLOSED, providing a balance between space complexity and optimality.\n\nAncillary Functions:\n\nAncillary functions like RECONSTRUCTPATH play a crucial role in path reconstruction for algorithms like DFID.\n\nReal-World Applications:\n\nAlgorithms like DFID find practical applications in chess programming, demonstrating adaptability in time-constrained scenarios.\n\nCombinatorial Explosion and Optimization:\n\nCombinatorial explosion is addressed by iterative deepening approaches like DFID, balancing time and space considerations.\n\nBlind (Uninformed) Search:\n\nBlind searches, including DFS, BFS, and DFID, lack knowledge of the goal’s location during exploration.\n\nPath Reconstruction Challenges:\n\nPath reconstruction challenges arise, especially when multiple paths to the goal exist, emphasizing the importance of closed lists.\n\nDepth-Bounded DFS with Node Counting:\n\nDepth-Bounded DFS with node counting provides insights into the number of visited nodes during exploration.\n\nConfigurations and Planning Problems:\n\nState space search involves configuration problems (satisfying criteria) and planning problems (finding optimal paths to a known goal).\n\n\nThese key points collectively contribute to a comprehensive understanding of state space search algorithms, their variations, and their applications in artificial intelligence problem-solving.",
    "crumbs": [
      "AI: Search Methods",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html",
    "href": "pages/DL/Week03.html",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "",
    "text": "Feed forward neural networks are a fundamental architecture in the realm of artificial neural networks (ANNs), designed to process data in a forward direction, from input to output. This architecture is crucial for various machine learning tasks, including classification, regression, and pattern recognition. In this section, we delve into the intricacies of feed forward neural networks, including their components, computation processes, parameter learning techniques, and the crucial back propagation algorithm.\n\n\n\n\nThe input layer serves as the entry point for data into the neural network. It consists of an \\(n\\)-dimensional vector, where each element represents a feature or attribute of the input data. Mathematically, the input layer can be represented as:\n\\[\n\\mathbf{x} \\in \\mathbb{R}^n\n\\]\nHere, \\(\\mathbf{x}\\) denotes the input vector, and \\(n\\) represents the number of input features.\n\n\n\nHidden layers form the core computational units of a feed forward neural network. These layers, typically denoted as \\(L - 1\\), are responsible for processing and transforming the input data through a series of non-linear transformations. Each hidden layer comprises a set of neurons, with each neuron connected to every neuron in the previous layer. Mathematically, the \\(i\\)-th hidden layer can be represented as:\n\\[\n\\text{Layer } i : \\mathbf{a}^{(i)} = \\mathbf{g}^{(i)}(\\mathbf{z}^{(i)})\n\\]\nHere, \\(\\mathbf{a}^{(i)}\\) represents the activation vector of the \\(i\\)-th layer, \\(\\mathbf{z}^{(i)}\\) denotes the pre-activation vector, and \\(\\mathbf{g}^{(i)}\\) represents the activation function applied element-wise to \\(\\mathbf{z}^{(i)}\\).\n\n\n\nThe output layer is the final layer of the neural network, responsible for generating the network’s predictions or outputs. The number of neurons in the output layer depends on the nature of the task (e.g., binary classification, multi-class classification, regression). Mathematically, the output layer can be represented as:\n\\[\n\\text{Output Layer: } \\mathbf{y} = \\mathbf{f}(\\mathbf{a}^{(L)})\n\\]\nHere, \\(\\mathbf{y}\\) represents the output vector, and \\(\\mathbf{f}\\) denotes the output activation function.\n\n\n\n\nThe computation process in a feed forward neural network involves computing the pre-activation and activation values for each neuron in the network.\n\n\nPre-activation refers to the linear transformation applied to the input data, followed by the addition of a bias term. Mathematically, the pre-activation for the \\(i\\)-th layer can be expressed as:\n\\[\n\\mathbf{z}^{(i)} = \\mathbf{W}^{(i)} \\mathbf{a}^{(i-1)} + \\mathbf{b}^{(i)}\n\\]\nHere, \\(\\mathbf{W}^{(i)}\\) represents the weight matrix connecting the \\((i-1)\\)-th and \\(i\\)-th layers, \\(\\mathbf{a}^{(i-1)}\\) denotes the activation vector of the previous layer, and \\(\\mathbf{b}^{(i)}\\) represents the bias vector for the \\(i\\)-th layer.\n\n\n\nActivation involves applying a non-linear function to the pre-activation values, introducing non-linearity into the network’s computations. Common activation functions include sigmoid, tanh, ReLU, and softmax. Mathematically, the activation for the \\(i\\)-th layer can be expressed as:\n\\[\n\\mathbf{a}^{(i)} = \\mathbf{g}^{(i)}(\\mathbf{z}^{(i)})\n\\]\nWhere \\(\\mathbf{g}^{(i)}\\) represents the activation function applied element-wise to \\(\\mathbf{z}^{(i)}\\).\n\n\n\n\nThe output activation function plays a crucial role in determining the nature of the network’s predictions. Depending on the task at hand, different activation functions may be employed to ensure appropriate output scaling and behavior.\n\n\nThe output activation function governs the transformation of the final layer’s pre-activation values into the network’s outputs. Common choices include softmax for multi-class classification tasks and linear functions for regression tasks. Mathematically, the output activation function can be expressed as:\n\\[\n\\mathbf{y} = \\mathbf{f}(\\mathbf{a}^{(L)})\n\\]\nWhere \\(\\mathbf{f}\\) denotes the output activation function.\n\n\n\nThe feed forward neural network serves as a powerful function approximator, capable of capturing complex relationships between inputs and outputs. By iteratively adjusting the network’s parameters through training, the network learns to approximate the underlying function mapping inputs to outputs. Mathematically, the network’s output (\\(\\hat{\\mathbf{y}}\\)) can be expressed as:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{f}(\\mathbf{x}; \\mathbf{W}, \\mathbf{b})\n\\]\nWhere \\(\\mathbf{W}\\) and \\(\\mathbf{b}\\) represent the network’s parameters, and \\(\\mathbf{x}\\) denotes the input vector.\n\n\n\n\nThe process of learning in a feed forward neural network involves optimizing the network’s parameters to minimize a predefined loss function. This optimization process typically utilizes gradient-based techniques, such as gradient descent, coupled with the back propagation algorithm.\n\n\nThe parameters of a feed forward neural network, including weights (\\(\\mathbf{W}\\)) and biases (\\(\\mathbf{b}\\)), are learned through iterative optimization algorithms. The objective is to minimize the discrepancy between the network’s predictions and the true target values.\n\n\n\nThe loss function quantifies the disparity between the predicted outputs of the network and the actual target values. Common choices for the loss function include the squared error loss for regression tasks and the categorical cross-entropy loss for classification tasks.\n\n\n\n\nThe back propagation algorithm serves as the cornerstone of parameter learning in feed forward neural networks. It facilitates the efficient computation of gradients with respect to network parameters, enabling gradient-based optimization techniques to adjust the parameters iteratively.\n\n\nDuring the forward pass, input data is propagated through the network, and pre-activation and activation values are computed for each layer.\n\n\n\nDuring the backward pass,\ngradients of the loss function with respect to network parameters are computed recursively using the chain rule of calculus. These gradients are then used to update the parameters in the direction that minimizes the loss function.\n\n\n\nThe update rule dictates how the network parameters are adjusted based on the computed gradients. Common choices include gradient descent, stochastic gradient descent, and variants such as Adam and RMSprop.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#components-of-a-feed-forward-neural-network",
    "href": "pages/DL/Week03.html#components-of-a-feed-forward-neural-network",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "",
    "text": "The input layer serves as the entry point for data into the neural network. It consists of an \\(n\\)-dimensional vector, where each element represents a feature or attribute of the input data. Mathematically, the input layer can be represented as:\n\\[\n\\mathbf{x} \\in \\mathbb{R}^n\n\\]\nHere, \\(\\mathbf{x}\\) denotes the input vector, and \\(n\\) represents the number of input features.\n\n\n\nHidden layers form the core computational units of a feed forward neural network. These layers, typically denoted as \\(L - 1\\), are responsible for processing and transforming the input data through a series of non-linear transformations. Each hidden layer comprises a set of neurons, with each neuron connected to every neuron in the previous layer. Mathematically, the \\(i\\)-th hidden layer can be represented as:\n\\[\n\\text{Layer } i : \\mathbf{a}^{(i)} = \\mathbf{g}^{(i)}(\\mathbf{z}^{(i)})\n\\]\nHere, \\(\\mathbf{a}^{(i)}\\) represents the activation vector of the \\(i\\)-th layer, \\(\\mathbf{z}^{(i)}\\) denotes the pre-activation vector, and \\(\\mathbf{g}^{(i)}\\) represents the activation function applied element-wise to \\(\\mathbf{z}^{(i)}\\).\n\n\n\nThe output layer is the final layer of the neural network, responsible for generating the network’s predictions or outputs. The number of neurons in the output layer depends on the nature of the task (e.g., binary classification, multi-class classification, regression). Mathematically, the output layer can be represented as:\n\\[\n\\text{Output Layer: } \\mathbf{y} = \\mathbf{f}(\\mathbf{a}^{(L)})\n\\]\nHere, \\(\\mathbf{y}\\) represents the output vector, and \\(\\mathbf{f}\\) denotes the output activation function.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#computing-pre-activation-and-activation",
    "href": "pages/DL/Week03.html#computing-pre-activation-and-activation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "",
    "text": "The computation process in a feed forward neural network involves computing the pre-activation and activation values for each neuron in the network.\n\n\nPre-activation refers to the linear transformation applied to the input data, followed by the addition of a bias term. Mathematically, the pre-activation for the \\(i\\)-th layer can be expressed as:\n\\[\n\\mathbf{z}^{(i)} = \\mathbf{W}^{(i)} \\mathbf{a}^{(i-1)} + \\mathbf{b}^{(i)}\n\\]\nHere, \\(\\mathbf{W}^{(i)}\\) represents the weight matrix connecting the \\((i-1)\\)-th and \\(i\\)-th layers, \\(\\mathbf{a}^{(i-1)}\\) denotes the activation vector of the previous layer, and \\(\\mathbf{b}^{(i)}\\) represents the bias vector for the \\(i\\)-th layer.\n\n\n\nActivation involves applying a non-linear function to the pre-activation values, introducing non-linearity into the network’s computations. Common activation functions include sigmoid, tanh, ReLU, and softmax. Mathematically, the activation for the \\(i\\)-th layer can be expressed as:\n\\[\n\\mathbf{a}^{(i)} = \\mathbf{g}^{(i)}(\\mathbf{z}^{(i)})\n\\]\nWhere \\(\\mathbf{g}^{(i)}\\) represents the activation function applied element-wise to \\(\\mathbf{z}^{(i)}\\).",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#output-activation-and-function-approximation",
    "href": "pages/DL/Week03.html#output-activation-and-function-approximation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "",
    "text": "The output activation function plays a crucial role in determining the nature of the network’s predictions. Depending on the task at hand, different activation functions may be employed to ensure appropriate output scaling and behavior.\n\n\nThe output activation function governs the transformation of the final layer’s pre-activation values into the network’s outputs. Common choices include softmax for multi-class classification tasks and linear functions for regression tasks. Mathematically, the output activation function can be expressed as:\n\\[\n\\mathbf{y} = \\mathbf{f}(\\mathbf{a}^{(L)})\n\\]\nWhere \\(\\mathbf{f}\\) denotes the output activation function.\n\n\n\nThe feed forward neural network serves as a powerful function approximator, capable of capturing complex relationships between inputs and outputs. By iteratively adjusting the network’s parameters through training, the network learns to approximate the underlying function mapping inputs to outputs. Mathematically, the network’s output (\\(\\hat{\\mathbf{y}}\\)) can be expressed as:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{f}(\\mathbf{x}; \\mathbf{W}, \\mathbf{b})\n\\]\nWhere \\(\\mathbf{W}\\) and \\(\\mathbf{b}\\) represent the network’s parameters, and \\(\\mathbf{x}\\) denotes the input vector.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#parameter-learning-and-loss-function",
    "href": "pages/DL/Week03.html#parameter-learning-and-loss-function",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "",
    "text": "The process of learning in a feed forward neural network involves optimizing the network’s parameters to minimize a predefined loss function. This optimization process typically utilizes gradient-based techniques, such as gradient descent, coupled with the back propagation algorithm.\n\n\nThe parameters of a feed forward neural network, including weights (\\(\\mathbf{W}\\)) and biases (\\(\\mathbf{b}\\)), are learned through iterative optimization algorithms. The objective is to minimize the discrepancy between the network’s predictions and the true target values.\n\n\n\nThe loss function quantifies the disparity between the predicted outputs of the network and the actual target values. Common choices for the loss function include the squared error loss for regression tasks and the categorical cross-entropy loss for classification tasks.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#back-propagation-algorithm",
    "href": "pages/DL/Week03.html#back-propagation-algorithm",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "",
    "text": "The back propagation algorithm serves as the cornerstone of parameter learning in feed forward neural networks. It facilitates the efficient computation of gradients with respect to network parameters, enabling gradient-based optimization techniques to adjust the parameters iteratively.\n\n\nDuring the forward pass, input data is propagated through the network, and pre-activation and activation values are computed for each layer.\n\n\n\nDuring the backward pass,\ngradients of the loss function with respect to network parameters are computed recursively using the chain rule of calculus. These gradients are then used to update the parameters in the direction that minimizes the loss function.\n\n\n\nThe update rule dictates how the network parameters are adjusted based on the computed gradients. Common choices include gradient descent, stochastic gradient descent, and variants such as Adam and RMSprop.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#gradient-descent-revisited",
    "href": "pages/DL/Week03.html#gradient-descent-revisited",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Gradient Descent Revisited",
    "text": "Gradient Descent Revisited\nGradient descent serves as a cornerstone algorithm in the realm of neural network training, facilitating the iterative adjustment of parameters to minimize the loss function. Mathematically, the process can be succinctly represented as follows:\n\\[\n\\theta_{t+1} = \\theta_{t} - \\alpha \\cdot \\nabla_{\\theta} \\mathcal{L}(\\theta_t)\n\\]\nHere, \\(\\theta\\) symbolizes the parameters of the neural network, \\(\\alpha\\) denotes the learning rate, and \\(\\nabla_{\\theta} \\mathcal{L}(\\theta_t)\\) signifies the gradient of the loss function with respect to the parameters at iteration \\(t\\).",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#transition-to-feedforward-neural-networks",
    "href": "pages/DL/Week03.html#transition-to-feedforward-neural-networks",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Transition to Feedforward Neural Networks",
    "text": "Transition to Feedforward Neural Networks\nMoving beyond the realm of single neurons, we extend our focus to encompass feedforward neural networks, characterized by their layered architecture and interconnected nodes. In this context, the parameters of interest include weight matrices \\(\\mathbf{W}^{(i)}\\) and bias vectors \\(\\mathbf{b}^{(i)}\\) for each layer \\(i\\).",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#parameter-representation",
    "href": "pages/DL/Week03.html#parameter-representation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Parameter Representation",
    "text": "Parameter Representation\nIn contrast to the simplistic parameter representation in single neurons, where \\(\\theta\\) encapsulated only a handful of parameters, the scope expands significantly in feedforward neural networks. Now, \\(\\theta\\) encompasses a multitude of elements, incorporating the weights and biases across all layers of the network. Mathematically, we express this as:\n\\[\n\\theta = (\\mathbf{W}^{(1)}, \\mathbf{b}^{(1)}, \\ldots, \\mathbf{W}^{(L)}, \\mathbf{b}^{(L)})\n\\]\nHere, \\(L\\) denotes the total number of layers in the network.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#complexity-of-parameters",
    "href": "pages/DL/Week03.html#complexity-of-parameters",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Complexity of Parameters",
    "text": "Complexity of Parameters\nWith the proliferation of layers and neurons in feedforward neural networks, the parameter space expands exponentially, posing computational challenges. Despite this complexity, the fundamental principles of gradient descent remain applicable, albeit with adaptations to accommodate the increased dimensionality of the parameter space.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#algorithm-adaptation",
    "href": "pages/DL/Week03.html#algorithm-adaptation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Algorithm Adaptation",
    "text": "Algorithm Adaptation\nThe essence of gradient descent persists in the context of feedforward neural networks, albeit with modifications to accommodate the augmented parameter space. The core objective remains unchanged: iteratively updating parameters to minimize the loss function. Through meticulous computation of gradients, facilitated by techniques such as backpropagation, the network adjusts its parameters to optimize performance.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#challenges-in-parameter-learning",
    "href": "pages/DL/Week03.html#challenges-in-parameter-learning",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Challenges in Parameter Learning",
    "text": "Challenges in Parameter Learning\nThe transition to feedforward neural networks introduces several challenges in the realm of parameter learning. Chief among these challenges is the computation of gradients, which necessitates the derivation of partial derivatives with respect to each parameter. In the context of complex architectures, this process can be computationally intensive and prone to errors.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#choice-of-loss-function",
    "href": "pages/DL/Week03.html#choice-of-loss-function",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Choice of Loss Function",
    "text": "Choice of Loss Function\nCentral to the parameter learning process is the selection of an appropriate loss function, which quantifies the disparity between predicted and actual outputs. The choice of loss function is contingent upon the nature of the task at hand, with options ranging from mean squared error for regression tasks to cross-entropy loss for classification problems.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#efficient-computation-of-gradients",
    "href": "pages/DL/Week03.html#efficient-computation-of-gradients",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Efficient Computation of Gradients",
    "text": "Efficient Computation of Gradients\nEfficient computation of gradients is paramount in the realm of parameter learning, particularly in the context of feedforward neural networks with intricate architectures. Techniques such as vectorization and parallelization play a pivotal role in enhancing computational efficiency, enabling rapid convergence during training.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#regression-problems",
    "href": "pages/DL/Week03.html#regression-problems",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Regression Problems",
    "text": "Regression Problems\nRegression problems involve predicting continuous values based on input data. For instance, in predicting movie ratings, the goal is to estimate a numerical value (rating) for each input (movie).\n\nLoss Function: Mean Squared Error (MSE)\nThe mean squared error (MSE) is a common choice for regression tasks. It quantifies the average squared difference between the predicted and true values. Mathematically, MSE is expressed as:\n\\[\n\\mathcal{L}_{\\text{MSE}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2\n\\]\nwhere:\n\n\\(N\\) is the number of training examples,\n\\(\\hat{y}_i\\) is the predicted value for the \\(i\\)-th example,\n\\(y_i\\) is the true value for the \\(i\\)-th example.\n\n\n\nOutput Function: Linear Activation\nIn regression tasks, a linear activation function is often employed at the output layer. This choice allows the model to produce unbounded output values, accommodating the natural range of the target variable. The output \\(\\hat{\\mathbf{y}}\\) is computed as a linear transformation of the last hidden layer activations:\n\\[\n\\hat{\\mathbf{y}} = \\mathbf{W}^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\n\\]\nwhere \\(\\mathbf{W}^{(L)}\\) and \\(\\mathbf{b}^{(L)}\\) are the weight matrix and bias vector of the output layer, respectively.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#classification-problems",
    "href": "pages/DL/Week03.html#classification-problems",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Classification Problems",
    "text": "Classification Problems\nClassification tasks involve assigning input data to discrete categories or classes. For example, in image classification, the aim is to categorize images into predefined classes.\n\nOutput Function: Softmax Activation\nTo obtain probabilities for each class in a classification problem, the softmax activation function is commonly used at the output layer. Softmax transforms the raw scores (logits) into a probability distribution over the classes. The softmax function is defined as:\n\\[\n\\text{Softmax}(\\mathbf{z}^{(L)})_i = \\frac{e^{z_i^{(L)}}}{\\sum_{j=1}^{K} e^{z_j^{(L)}}}, \\quad i = 1, 2, \\ldots, K\n\\]\nwhere:\n\n\\(K\\) is the number of classes,\n\\(\\mathbf{z}^{(L)}\\) is the pre-activation vector at the output layer.\n\n\n\nLoss Function: Cross Entropy\nCross entropy is a commonly used loss function for classification tasks. It measures the dissimilarity between the predicted probability distribution and the true distribution of class labels. The cross entropy loss is given by:\n\\[\n\\mathcal{L}_{\\text{CE}}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i,k} \\log(\\hat{y}_{i,k})\n\\]\nwhere:\n\n\\(N\\) is the number of training examples,\n\\(K\\) is the number of classes,\n\\(y_{i,k}\\) is the indicator function for the \\(k\\)-th class of the \\(i\\)-th example,\n\\(\\hat{y}_{i,k}\\) is the predicted probability of the \\(k\\)-th class for the \\(i\\)-th example.\n\nThe cross entropy loss penalizes deviations between the predicted and true class probabilities, encouraging the model to assign high probabilities to the correct classes.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#softmax-function",
    "href": "pages/DL/Week03.html#softmax-function",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Softmax Function",
    "text": "Softmax Function\nThe softmax function is employed to convert raw scores into probabilities for multiclass classification tasks. It ensures that the output represents a valid probability distribution over the classes, with values between 0 and 1 that sum up to 1. The softmax function is mathematically defined as:\n\\[\n\\text{Softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}, \\quad i = 1, 2, \\ldots, K\n\\]\nwhere \\(\\mathbf{z}\\) is the input vector, and \\(K\\) is the number of classes.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#cross-entropy-loss",
    "href": "pages/DL/Week03.html#cross-entropy-loss",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Cross Entropy Loss",
    "text": "Cross Entropy Loss\nCross entropy loss quantifies the difference between the predicted and true distributions of class labels in classification tasks. It is a fundamental component in training neural networks for classification. The cross entropy loss is given by the formula:\n\\[\n\\mathcal{L}_{\\text{CE}}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i,k} \\log(\\hat{y}_{i,k})\n\\]\nwhere:\n\n\\(N\\) is the number of training examples,\n\\(K\\) is the number of classes,\n\\(y_{i,k}\\) is the indicator function for the \\(k\\)-th class of the \\(i\\)-th example,\n\\(\\hat{y}_{i,k}\\) is the predicted probability of the \\(k\\)-th class for the \\(i\\)-th example.\n\nThe cross entropy loss penalizes deviations between the predicted and true class probabilities. Minimizing this loss encourages the model to produce accurate probability distributions over the classes.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#derivatives-and-chain-rule",
    "href": "pages/DL/Week03.html#derivatives-and-chain-rule",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Derivatives and Chain Rule",
    "text": "Derivatives and Chain Rule\n\nDerivative Calculation\nIn the context of neural networks, the derivative of the loss function with respect to the parameters (weights and biases) is essential for updating these parameters during the training process. This derivative quantifies how changes in the parameters affect the overall loss.\n\n\nChallenges in Deep Neural Networks\nUnlike simpler networks, deep neural networks entail a more complex structure with multiple layers and numerous parameters. Computing derivatives in such networks requires careful consideration and efficient algorithms.\n\n\nLeveraging the Chain Rule\nThe chain rule of calculus provides a systematic approach to compute derivatives in composite functions. In the context of neural networks, it enables the computation of derivatives layer by layer, propagating the error from the output layer to the input layer.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#chain-rule-intuition",
    "href": "pages/DL/Week03.html#chain-rule-intuition",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Chain Rule Intuition",
    "text": "Chain Rule Intuition\n\nStep-by-Step Derivative Calculation\nVisualizing the computation of derivatives as a chain of functions helps in understanding the iterative nature of back propagation. Each layer in the network contributes to the overall derivative calculation, with the chain rule facilitating this process.\n\n\nReusability of Computations\nOnce a segment of the derivative chain is computed, it can be reused for similar computations across different parameters. This reusability reduces redundancy and computational complexity, making the back propagation algorithm more efficient.\n\n\nGeneralization Across Layers\nThe principles of back propagation can be generalized across different layers and parameters in the network. By establishing a unified framework for derivative computation, the algorithm becomes more scalable and adaptable to varying network architectures.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#responsibilities-in-back-propagation",
    "href": "pages/DL/Week03.html#responsibilities-in-back-propagation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Responsibilities in Back Propagation",
    "text": "Responsibilities in Back Propagation\n\nError Propagation\nBack propagation involves tracing the propagation of errors from the output layer back to the input layer through the network’s connections. Each layer in the network bears responsibility for contributing to this error propagation process.\n\n\nInfluence of Weights and Biases\nThe weights and biases in the network play a crucial role in determining the magnitude of error propagation. Adjusting these parameters based on their influence on the loss function is key to optimizing the network’s performance.\n\n\nDerivatives as Indicators of Influence\nThe derivatives of the loss function with respect to the parameters serve as indicators of their influence on the overall loss. Larger derivatives imply stronger influence, guiding the optimization process towards more effective parameter adjustments.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#mathematical-realization",
    "href": "pages/DL/Week03.html#mathematical-realization",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Mathematical Realization",
    "text": "Mathematical Realization\n\nDerivatives and Responsibilities\nMathematically, derivatives quantify the sensitivity of the loss function to changes in the parameters. By computing these derivatives, the algorithm assigns responsibilities to each parameter based on its impact on the overall loss.\n\n\nPartial Derivatives\nPartial derivatives measure how the loss function changes with infinitesimal adjustments to individual parameters. This information guides the gradient-based optimization process, enabling efficient parameter updates.\n\n\nObjective of Back Propagation\nThe primary objective of back propagation is to compute gradients with respect to various components of the network, including output, hidden units, weights, and biases. These gradients drive the optimization process towards minimizing the loss function.\n\n\nEmphasis on Cross Entropy\nIn classification problems, where the network’s output is represented using softmax activation, cross-entropy loss is commonly used. Back propagation algorithms are tailored to handle such loss functions efficiently, facilitating effective training of classification models.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#talking-to-the-output-layer",
    "href": "pages/DL/Week03.html#talking-to-the-output-layer",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Talking to the Output Layer",
    "text": "Talking to the Output Layer\n\nGoal\nThe primary objective in back propagation is to compute the derivative of the loss function with respect to the output layer activations. Let’s denote the output vector as \\(\\mathbf{y}\\), representing the network’s predictions or outputs.\n\n\nLoss Function\nThe loss function, denoted as \\(\\mathcal{L}(\\theta_t)\\), measures the discrepancy between the predicted output \\(\\hat{\\mathbf{y}}\\) and the true labels \\(\\mathbf{y}\\). It is often defined as the negative logarithm of the predicted probability of the true class.\n\\[\n\\mathcal{L}(\\theta_t) = -\\log(\\hat{y}_l)\n\\]\nwhere \\(l\\) is the true class label.\n\n\nDerivative Calculation\nWe aim to compute the derivative of the loss function with respect to each output neuron activation. This involves determining how a change in each output activation affects the overall loss.\nThe derivative can be expressed as follows:\n\\[\n\\frac{\\partial \\mathcal{L}(\\theta_t)}{\\partial a^{(L)}_i} =\n\\begin{cases}\n-\\frac{1}{\\hat{y}_l} & \\text{if } i = l \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhere \\(a^{(L)}_i\\) represents the \\(i\\)-th output neuron activation, and \\(\\hat{y}_l\\) is the predicted probability corresponding to the true class label.\n\n\nGradient Vector\nThe gradient of the loss function with respect to the output layer, denoted as \\(\\nabla_{\\mathbf{y}} \\mathcal{L}(\\theta_t)\\), is a vector containing the partial derivatives of the loss function with respect to each output neuron activation. It can be represented as:\n\\[\n\\nabla_{\\mathbf{y}} \\mathcal{L}(\\theta_t) = \\begin{bmatrix}\n-\\frac{1}{\\hat{y}_1} & 0 & \\cdots & 0 \\\\\n0 & -\\frac{1}{\\hat{y}_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & -\\frac{1}{\\hat{y}_k}\n\\end{bmatrix}\n\\]\nThis gradient vector provides insights into how changes in the output layer activations affect the loss function.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#talking-to-the-hidden-layers",
    "href": "pages/DL/Week03.html#talking-to-the-hidden-layers",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Talking to the Hidden Layers",
    "text": "Talking to the Hidden Layers\n\nObjective\nAfter understanding the derivatives at the output layer, the next step is to compute the derivatives with respect to the pre-activation values of the hidden layers. This involves understanding how changes in the pre-activations affect the output activations and, consequently, the loss function.\n\n\nChain Rule Application\nTo compute the derivative of the loss function with respect to the pre-activation values of the hidden layers, we apply the chain rule. This breaks down the computation into two steps:\n\nDerivative of the loss function with respect to the output activations.\nDerivative of the output activations with respect to the pre-activation values.\n\n\n\nPre-Activation to Activation\nThe pre-activation values of the hidden layers are passed through an activation function to obtain the output activations. Mathematically, this can be expressed as:\n\\[\n\\mathbf{a}^{(i)} = \\mathbf{g}^{(i)}(\\mathbf{z}^{(i)})\n\\]\nwhere \\(\\mathbf{z}^{(i)}\\) represents the pre-activation vector for the \\(i\\)-th layer, and \\(\\mathbf{g}^{(i)}(\\cdot)\\) is the activation function applied element-wise.\n\n\nDerivative Calculation\nThe derivative of the output activations with respect to the pre-activation values depends on the choice of activation function. For commonly used activation functions like sigmoid, tanh, and ReLU, the derivatives can be computed analytically.\n\n\nGradient Flow\nUnderstanding the gradient flow from the output layer to the hidden layers is crucial for parameter updates during training. The gradients propagate backward through the network, allowing for efficient computation of parameter updates.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#talking-to-the-weights",
    "href": "pages/DL/Week03.html#talking-to-the-weights",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Talking to the Weights",
    "text": "Talking to the Weights\n\nObjective\nOnce the derivatives with respect to the pre-activation values are computed, the next step is to calculate the derivatives with respect to the weights connecting the neurons. This step enables us to understand how changes in the weights influence the loss function.\n\n\nChain Rule Application\nSimilar to the computation at the hidden layers, we apply the chain rule to compute the derivatives of the loss function with respect to the weights. This involves breaking down the computation into two parts:\n\nDerivative of the loss function with respect to the output activations.\nDerivative of the output activations with respect to the pre-activation values.\n\n\n\nDerivative Calculation\nThe derivative of the pre-activation values with respect to the weights connecting the neurons can be straightforwardly calculated using the input vector, output activations, and the derivative of the activation function.\n\n\nWeight Update\nOnce the derivatives with respect to the weights are computed, they are used to update the weights through optimization algorithms like gradient descent. By iteratively updating the weights based on the computed gradients, the network learns to minimize the loss function and improve its performance on the given task.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#introduction-to-hidden-units",
    "href": "pages/DL/Week03.html#introduction-to-hidden-units",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Introduction to Hidden Units",
    "text": "Introduction to Hidden Units\nHidden units, also known as hidden layers, are intermediary layers in neural networks responsible for capturing complex patterns in the input data. These layers play a crucial role in the network’s ability to learn and generalize from the training data.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#chain-rule-for-gradient-computation",
    "href": "pages/DL/Week03.html#chain-rule-for-gradient-computation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Chain Rule for Gradient Computation",
    "text": "Chain Rule for Gradient Computation\nThe chain rule of calculus is a fundamental concept used extensively in computing derivatives of composite functions. In the context of neural networks, where the activation of each layer depends on the activations of the previous layers, the chain rule becomes essential for gradient computation.\nMathematically, let \\(P(Z)\\) be a function dependent on intermediate functions \\(Q_1(Z), Q_2(Z),\\) etc., and \\(P\\) being a function of \\(Z\\). The derivative of \\(P\\) with respect to \\(Z\\) is computed as follows:\n\\[\n\\frac{dP}{dZ} = \\sum_{i=1}^{m} \\frac{dP}{dQ_i} \\cdot \\frac{dQ_i}{dZ}\n\\]\nHere, we sum over all paths from \\(Z\\) to \\(P\\), multiplying the derivatives along each path.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#deriving-the-formula",
    "href": "pages/DL/Week03.html#deriving-the-formula",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Deriving the Formula",
    "text": "Deriving the Formula\nTo compute gradients for hidden units, we apply the chain rule to derive a generic formula. Consider a specific hidden unit \\(H_{ij}\\), where \\(i\\) denotes the layer number and \\(j\\) represents the neuron number within that layer.\nWe aim to compute the derivative of the loss function with respect to \\(H_{ij}\\). This involves summing over all paths from \\(H_{ij}\\) to the loss function, considering each path’s contribution via the chain rule.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#computing-gradients-for-hidden-units",
    "href": "pages/DL/Week03.html#computing-gradients-for-hidden-units",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Computing Gradients for Hidden Units",
    "text": "Computing Gradients for Hidden Units\nThe derivative of the loss function with respect to \\(H_{ij}\\) can be expressed as a dot product between two vectors:\n\\[\n\\frac{d\\mathcal{L}}{dH_{ij}} = \\mathbf{W}^{(i+1)}_j \\cdot \\frac{d\\mathcal{L}}{d\\mathbf{a}^{(i+1)}}\n\\]\nHere, \\(\\mathbf{W}^{(i+1)}_j\\) represents the \\(j\\)-th column of the weight matrix connecting the \\((i+1)\\)-th and \\(i\\)-th layers, and \\(\\frac{d\\mathcal{L}}{d\\mathbf{a}^{(i+1)}}\\) denotes the gradient of the loss function with respect to the activations in the next layer.\nThis computation involves the element-wise multiplication of the weight vector and the gradient vector.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#generalizing-the-formula",
    "href": "pages/DL/Week03.html#generalizing-the-formula",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Generalizing the Formula",
    "text": "Generalizing the Formula\nWe generalize the formula to compute gradients for any hidden layer \\(H_i\\) with multiple units. The derivative of the loss function with respect to \\(H_i\\) is given by:\n\\[\n\\frac{d\\mathcal{L}}{d\\mathbf{H}_i} = \\mathbf{W}^{(i+1)T} \\cdot \\frac{d\\mathcal{L}}{d\\mathbf{a}^{(i+1)}}\n\\]\nHere, \\(\\mathbf{W}^{(i+1)T}\\) denotes the transpose of the weight matrix connecting the \\((i+1)\\)-th and \\(i\\)-th layers, and \\(\\frac{d\\mathcal{L}}{d\\mathbf{a}^{(i+1)}}\\) represents the gradient of the loss function with respect to the activations in the next layer.\nThis formulation enables efficient computation of gradients for hidden units across all layers of the neural network.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#computing-derivatives-of-loss-function",
    "href": "pages/DL/Week03.html#computing-derivatives-of-loss-function",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Computing Derivatives of Loss Function",
    "text": "Computing Derivatives of Loss Function\n\nIterative Approach\nRather than computing the derivatives of the loss function with respect to all parameters simultaneously, we adopt an iterative approach. This involves focusing on one parameter at a time, specifically one element of the weight matrix or one element of the bias vector.\n\n\nDerivative with Respect to Weight Matrix Element\nConsider the derivative of the loss function with respect to one element of the weight matrix, \\(w_{ij}^{(k)}\\), connecting the \\((k-1)\\)-th and \\(k\\)-th layers. This derivative is obtained iteratively.\n\nDerivative with Respect to Activation\nFirst, compute the derivative of the loss function with respect to the corresponding activation, \\(a_{i}^{(k)}\\), using chain rule.\n\n\nDerivative of Activation with Respect to Weight\nNext, compute the derivative of the activation with respect to \\(w_{ij}^{(k)}\\), denoted as \\(\\frac{\\partial a_{i}^{(k)}}{\\partial w_{ij}^{(k)}}\\).\n\nMathematical Formulation\nMathematically, this derivative equals the activation of the preceding layer at index \\(i\\), denoted as \\(h_{ij}^{(k-1)}\\).\n\\[\\frac{\\partial a_{i}^{(k)}}{\\partial w_{ij}^{(k)}} = h_{ij}^{(k-1)}\\]\n\n\n\n\nOuter Product Representation\nThe derivative of the loss function with respect to \\(w_{ij}^{(k)}\\) can be expressed as the outer product of two vectors: the derivative of the loss function with respect to the activations (\\(\\mathbf{a}^{(k)}\\)) and the activations of the preceding layer (\\(\\mathbf{h}^{(k-1)}\\)).\n\nMathematical Representation\n\\[\\frac{\\partial \\mathcal{L}(\\theta_t)}{\\partial w_{ij}^{(k)}} = \\frac{\\partial \\mathcal{L}(\\theta_t)}{\\partial \\mathbf{a}^{(k)}} \\otimes \\mathbf{h}^{(k-1)}\\]\nwhere \\(\\otimes\\) represents the outer product operation.\n\n\n\nEfficient Computation\nBoth the quantities involved in the derivative computation can be efficiently computed during the forward pass of the neural network, requiring no additional computations during the backward pass.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#derivative-with-respect-to-bias",
    "href": "pages/DL/Week03.html#derivative-with-respect-to-bias",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Derivative with Respect to Bias",
    "text": "Derivative with Respect to Bias\nSimilar to the approach for weight matrices, the derivative of the loss function with respect to the bias vector (\\(\\mathbf{b}^{(k)}\\)) is computed iteratively.\n\nSplitting into Two Parts\n\nDerivative with Respect to Activation\nFirst, compute the derivative of the loss function with respect to the activations (\\(\\mathbf{a}^{(k)}\\)) using chain rule.\n\n\nDerivative of Activation with Respect to Bias\nNext, compute the derivative of the activation with respect to the bias vector, denoted as \\(\\frac{\\partial a_{i}^{(k)}}{\\partial b_{i}^{(k)}}\\).\n\nMathematical Formulation\nMathematically, this derivative is simply 1, as the bias term directly contributes to the activation.\n\\[\\frac{\\partial a_{i}^{(k)}}{\\partial b_{i}^{(k)}} = 1\\]\n\n\n\n\nGradient Vector\nThe derivative of the loss function with respect to the bias vector (\\(\\mathbf{b}^{(k)}\\)) is obtained by collecting all the partial derivatives, representing the gradient of the loss function with respect to the activations.\n\nMathematical Representation\n\\[\\frac{\\partial \\mathcal{L}(\\theta_t)}{\\partial \\mathbf{b}^{(k)}} = \\frac{\\partial \\mathcal{L}(\\theta_t)}{\\partial \\mathbf{a}^{(k)}}\\]",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#forward-propagation",
    "href": "pages/DL/Week03.html#forward-propagation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Forward Propagation",
    "text": "Forward Propagation\n\nOverview\nForward propagation refers to the process of computing the network’s output given an input. It involves passing the input data through the network layers, computing pre-activations and activations, and finally obtaining the network’s predictions.\n\n\nComputation of Pre-activations and Activations\nFor each layer \\(i\\) in the network, forward propagation involves the following steps:\n\nPre-activation: Compute the pre-activation vector \\(\\mathbf{z}^{(i)}\\) using the formula: \\[\\mathbf{z}^{(i)} = \\mathbf{W}^{(i)} \\mathbf{a}^{(i-1)} + \\mathbf{b}^{(i)}\\] Here, \\(\\mathbf{W}^{(i)}\\) is the weight matrix connecting the \\((i-1)\\)-th and \\(i\\)-th layers, \\(\\mathbf{a}^{(i-1)}\\) is the activation vector from the previous layer, and \\(\\mathbf{b}^{(i)}\\) is the bias vector for the \\(i\\)-th layer.\nActivation: Apply the activation function \\(\\mathbf{g}^{(i)}(\\cdot)\\) element-wise to the pre-activation vector \\(\\mathbf{z}^{(i)}\\) to obtain the activation vector \\(\\mathbf{a}^{(i)}\\): \\[\\mathbf{a}^{(i)} = \\mathbf{g}^{(i)}(\\mathbf{z}^{(i)})\\]\nOutput Activation: For the output layer, apply a specific output activation function \\(\\mathbf{f}(\\cdot)\\) to obtain the final output \\(\\hat{\\mathbf{y}}\\): \\[\\hat{\\mathbf{y}} = \\mathbf{f}(\\mathbf{z}^{(L)})\\]",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#loss-computation",
    "href": "pages/DL/Week03.html#loss-computation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Loss Computation",
    "text": "Loss Computation\nAfter forward propagation, the next step is to compute the loss function, which measures the difference between the predicted output \\(\\hat{\\mathbf{y}}\\) and the true output \\(\\mathbf{y}\\).\n\nLoss Function\nThe loss function \\(\\mathcal{L}(\\theta_t)\\) is a measure of the error between the predicted and true outputs. It depends on the specific task and can be chosen based on the problem domain. Common loss functions include mean squared error (MSE), cross-entropy loss, and hinge loss.\n\n\nLoss Computation\nGiven the predicted output \\(\\hat{\\mathbf{y}}\\) and the true output \\(\\mathbf{y}\\), the loss function is computed using the following formula: \\[\\mathcal{L}(\\theta_t) = \\text{Loss}(\\hat{\\mathbf{y}}, \\mathbf{y})\\]",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#backward-propagation",
    "href": "pages/DL/Week03.html#backward-propagation",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Backward Propagation",
    "text": "Backward Propagation\n\nOverview\nBackward propagation, also known as backpropagation, is the process of computing gradients of the loss function with respect to the network parameters. These gradients are then used to update the parameters in order to minimize the loss.\n\n\nGradient Computation\nFor each layer \\(i\\) in the network, backward propagation involves the following steps:\n\nGradient of Loss Function with Respect to Output Layer: Compute the gradient of the loss function with respect to the output layer activations \\(\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{a}^{(L)}}\\).\nGradient of Loss Function with Respect to Weights: Use the chain rule to compute the gradient of the loss function with respect to the weights \\(\\mathbf{W}^{(i)}\\) for each layer \\(i\\): \\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(i)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(i)}} \\cdot \\frac{\\partial \\mathbf{z}^{(i)}}{\\partial \\mathbf{W}^{(i)}}\\]\nGradient of Loss Function with Respect to Biases: Similarly, compute the gradient of the loss function with respect to the biases \\(\\mathbf{b}^{(i)}\\) for each layer \\(i\\): \\[\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}^{(i)}} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{z}^{(i)}} \\cdot \\frac{\\partial \\mathbf{z}^{(i)}}{\\partial \\mathbf{b}^{(i)}}\\]\n\n\n\nChain Rule\nThe chain rule is used to compute the gradients of the loss function with respect to the weights and biases. It allows us to decompose the overall gradient into smaller gradients that can be computed efficiently.\n\n\nUpdate Rule\nOnce the gradients have been computed, they are used to update the network parameters using an optimization algorithm such as gradient descent. The update rule for the weights is given by: \\[\\mathbf{W}^{(i)}_{\\text{new}} = \\mathbf{W}^{(i)}_{\\text{old}} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}^{(i)}}\\] Where \\(\\alpha\\) is the learning rate, controlling the size of the updates.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week03.html#points-to-remember",
    "href": "pages/DL/Week03.html#points-to-remember",
    "title": "Feed Forward Neural Networks and Back Propagation",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nForward Propagation:\n\nForward propagation computes the network’s output given an input by passing it through the layers and applying activation functions.\nPre-activations are computed using weight matrices, activation vectors, and biases, followed by activation function application.\nOutput activation function transforms the final pre-activation into the network’s prediction.\n\nLoss Computation:\n\nLoss function measures the error between predicted and true outputs and guides the training process.\nCommon loss functions include mean squared error, cross-entropy loss, and hinge loss.\nLoss computation involves comparing predicted and true outputs using the chosen loss function.\n\nBackward Propagation:\n\nBackward propagation computes gradients of the loss function with respect to network parameters.\nGradient computation involves the chain rule to decompose gradients efficiently.\nGradients are used to update weights and biases, facilitating model improvement over iterations.\n\nChain Rule:\n\nThe chain rule allows the decomposition of complex gradients, simplifying the computation of gradients with respect to weights and biases.\n\nUpdate Rule:\n\nUpdate rule adjusts network parameters using gradients and a learning rate.\nLearning rate controls the size of parameter updates, influencing the convergence and stability of the training process.\n\nOptimization Algorithms:\n\nGradient descent is a common optimization algorithm used in conjunction with backpropagation for training neural networks.\nOther optimization algorithms like Adam, RMSprop, and SGD with momentum offer variations for improved convergence and performance.\n\nTraining Process:\n\nTraining neural networks involves iterative forward and backward passes, adjusting parameters to minimize the loss function.\nEffective training requires careful selection of hyperparameters, regularization techniques, and monitoring of model performance.",
    "crumbs": [
      "Deep Learning",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html",
    "href": "pages/DL/Week01_1.html",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain.\n\n\n\nIn 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969.\n\n\n\nThe period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape.\n\n\n\nThe Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation.\n\n\n\nA disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "href": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "href": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "href": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "href": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "href": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "A disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction",
    "href": "pages/DL/Week01_1.html#introduction",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\n\nHistorical Perspective\nDeep learning encountered challenges in training via backpropagation. Jeff Hinton’s group proposed a crucial weight initialization idea in 2016, fostering stable training. The improved availability of computing power and data around 2006 laid the foundation for success.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "href": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Early Challenges and Solutions",
    "text": "Early Challenges and Solutions\n\nUnsupervised Pre-training\nBetween 2007 and 2009, investigations into the effectiveness of unsupervised pre-training led to insights that shaped optimization and regularization algorithms. The course will delve into topics such as initializations, regularizations, and optimizations.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "href": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Emergence of Deep Learning",
    "text": "Emergence of Deep Learning\n\nPractical Utility\nDeep learning applications started winning competitions, including handwriting recognition on the MNIST dataset, speech recognition, and visual pattern recognition like traffic sign data.\n\n\nImageNet Challenge (2012-2016)\nThe ImageNet challenge, a pivotal turning point, witnessed the evolution from ZFNet to ResNet (152 layers), achieving a remarkable 3.6% error rate in 2016, surpassing human performance.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "href": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transition Period (2012-2016)",
    "text": "Transition Period (2012-2016)\n\nGolden Period of Deep Learning\nThe universal acceptance of deep learning marked its golden period, with convolutional neural networks dominating image-related problems. Similar trends were observed in natural language processing (NLP) and speech processing.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "href": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "title": "A Historical Overview of Deep Learning",
    "section": "From Cats to Convolutional Neural Networks",
    "text": "From Cats to Convolutional Neural Networks\n\nMotivation from Neural Science (1959)\nAn experiment with a cat’s brain in 1959 revealed different parts activated for different stick positions, motivating the concept of receptive fields in convolutional neural networks (CNNs).\n\n\nNeocognitron Model (1980)\nInspired by distributed processing observed in the cat experiment, the Neocognitron model utilized receptive fields for different parts of the network.\n\n\nLeNet Model (1989)\nJan Lecun’s contribution to deep learning, the LeNet model, was employed for recognizing handwritten digits, finding applications in postal services for automated sorting of letters.\n\n\nLeNet-5 Model (1998)\nFurther improvements on the LeNet model, introducing the MNIST dataset for testing CNNs.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#history-of-deep-learning",
    "href": "pages/DL/Week01_1.html#history-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "History of Deep Learning",
    "text": "History of Deep Learning\n\n1950s: Enthusiasm in AI.\n1990s: Convolutional Neural Networks (CNNs) used for real-world problems, challenges with large networks and training.\n2006-2012: Advances in deep learning, successful training for ImageNet challenges.\n2016 onwards: Acceleration with better optimization methods (Nesterov’s method), leading to faster convergence.\nOptimization Algorithms: Adagrad, RMSprop, Adam, AdamW, etc., focus on faster and better convergence.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#activation-functions",
    "href": "pages/DL/Week01_1.html#activation-functions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Activation Functions",
    "text": "Activation Functions\nThe evolution from the logistic function to various activation functions (ReLU, Leaky ReLU, Parametric ReLU, Tanh, etc.) aimed at stabilizing training, achieving better performance, and faster convergence. The use of improved activation functions contributed to enhanced stability and performance.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#sequence-processing",
    "href": "pages/DL/Week01_1.html#sequence-processing",
    "title": "A Historical Overview of Deep Learning",
    "section": "Sequence Processing",
    "text": "Sequence Processing\nIntroduction to problems involving sequences in deep learning, featuring Recurrent Neural Networks (RNNs) proposed in 1982 for sequence processing. Long Short-Term Memory Cells (LSTMs) were introduced in 1997 to address the vanishing gradient problem. By 2014, RNNs and LSTMs dominated natural language processing (NLP) and speech applications. In 2017, Transformer networks started replacing RNNs and LSTMs in sequence learning.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "href": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Game Playing with Deep Learning",
    "text": "Game Playing with Deep Learning\n\n2015: Deep Reinforcement Learning (DRL) agents beat humans in Atari games.\nBreakthrough in Go game playing using DRL in 2015.\n2016: DRL-based agents beat professional poker players.\nComplex strategy games like Dota 2 mastered by DRL agents.\nIntroduction of OpenAI Gym as a toolkit for developing and comparing reinforcement learning algorithms.\nEmergence of AlphaStar and MuZero for mastering multiple games and tasks.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "href": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "General Trends in Deep Reinforcement Learning",
    "text": "General Trends in Deep Reinforcement Learning\nDeep RL agents consistently outperforming humans in various complex games, progressing from simple environments to mastering complex strategy games. The trend is towards developing “master of all” models (e.g., MuZero) for general intelligence in multiple tasks.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#overview",
    "href": "pages/DL/Week01_1.html#overview",
    "title": "A Historical Overview of Deep Learning",
    "section": "Overview",
    "text": "Overview\n\nRevival and Advances\nA recap of deep learning’s revival and recent advances, reflecting an increasing interest in real-world problem-solving and challenges.\n\n\nAI Publications Growth\nThe Stanford AI Index Report highlights a significant increase in AI publications, indicating exponential growth across machine learning, computer vision, and NLP.\n\n\nFunding and Startups\nThe rise of AI startups, coupled with the interest from major tech companies, has led to exponential growth in AI-related patent filings.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "href": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Evolution of Neural Network Models",
    "text": "Evolution of Neural Network Models\n\nIntroduction of Transformers\nIn 2017, transformers were introduced, revolutionizing AI and finding success in NLP, subsequently adopted in other domains.\n\n\nMachine Translation and Transformers\nA historical overview of machine translation, emphasizing the shift from IBM models to neural machine translation. The impact of sequence-to-sequence models (2014) and transformers (2017) is discussed.\n\n\nTransformer-based Models\nThe BERT model (2018) with a focus on pre-training, the evolution of models with increasing parameters from GPT-3 (175 billion) to 1.6 trillion parameters, and a comparison with human brain synapses provide perspective.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#transformers-in-vision",
    "href": "pages/DL/Week01_1.html#transformers-in-vision",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transformers in Vision",
    "text": "Transformers in Vision\n\nAdoption in Image\nClassification The evolution of image classification models, starting with AlexNet (2012), is traced. Transformers entered image classification and object detection in 2019, marking a paradigm shift towards transformers in state-of-the-art models.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#generative-models",
    "href": "pages/DL/Week01_1.html#generative-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Generative Models",
    "text": "Generative Models\n\nOverview\nAn introduction to generative models for image synthesis, covering the evolution from variation autoencoders to GANs (Generative Adversarial Networks). Recent developments in diffusion-based models overcoming GAN drawbacks are discussed.\n\n\nDALL-E and DALL-E 2\nDALL-E’s capability to generate realistic images based on text prompts is explored. The introduction of DALL-E 2, a diffusion-based model, exceeding expectations, is highlighted with examples of generated images showcasing photorealistic results.\n\n\nExciting Times in Generative Models\nThe exploration of generative models for realistic image generation is showcased, with examples of prompts generating photorealistic images illustrating field advancements.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction-1",
    "href": "pages/DL/Week01_1.html#introduction-1",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\nRapid advancements in deep learning have yielded powerful models trained on large datasets, showcasing impressive results. However, there is a growing need for sanity, interpretability, fairness, and responsibility in deploying these models.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "href": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Paradox of Deep Learning",
    "text": "Paradox of Deep Learning\nDespite the high capacity of deep learning models, they exhibit remarkable performance. Challenges include numerical instability, sharp minima, and susceptibility to adversarial examples.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#calls-for-sanity",
    "href": "pages/DL/Week01_1.html#calls-for-sanity",
    "title": "A Historical Overview of Deep Learning",
    "section": "Calls for Sanity",
    "text": "Calls for Sanity\nEmphasis is placed on explainability and interpretability to comprehend model decisions. Advances include workshops on human interpretability, tools like the Clever Hans toolkit to identify model reliance on cues, and benchmarking on adversarial examples.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "href": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "title": "A Historical Overview of Deep Learning",
    "section": "Fairness and Responsibility",
    "text": "Fairness and Responsibility\nIncreasing awareness of biases in AI models, particularly in facial recognition and criminal risk predictions, has led to concerns about fairness. Efforts such as the AI audit challenge at Stanford focus on building non-discriminatory models.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#green-ai",
    "href": "pages/DL/Week01_1.html#green-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Green AI",
    "text": "Green AI\nRising environmental concerns due to the high computational power and energy consumption of deep learning models have spurred calls for responsible AI, extending to the environmental impact. There is a push for more energy-efficient models.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "href": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Exciting Times in AI",
    "text": "Exciting Times in AI\nThe AI revolution is influencing scientific research, evident in DeepMind’s AlphaFold predicting protein folding. Applications in astronomy, predicting galaxy aging, and generating images for fundamental variables in experimental data are emerging. There is an emphasis on efficient deep learning for mobile devices, edge computing, and addressing constraints of power, storage, and real-time processing.",
    "crumbs": [
      "Deep Learning",
      "Week 1.1"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BS Degree Notes",
    "section": "",
    "text": "I put the course notes for my own easy perusal.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html",
    "href": "pages/DL/Week01_2.html",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units.\n\n\n\n\n\n\nDendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter.\n\n\n\n\n\nLayered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli.\n\n\n\n\n\n\nNeural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction",
    "href": "pages/DL/Week01_2.html#introduction",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#biological-neurons",
    "href": "pages/DL/Week01_2.html#biological-neurons",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Dendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#neural-network-architecture",
    "href": "pages/DL/Week01_2.html#neural-network-architecture",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Layered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "href": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Neural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-1",
    "href": "pages/DL/Week01_2.html#introduction-1",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\n\nObjective: Comprehend the McCulloch-Pitts neuron, a simplified computational model inspired by biological neurons.\nHistorical Context: Proposed in 1943 by McCulloch (neuroscientist) and Pitts (logician).\nPurpose: Emulate the brain’s complex processing for decision-making.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#neuron-structure",
    "href": "pages/DL/Week01_2.html#neuron-structure",
    "title": "Motivation from Biological Neuron",
    "section": "Neuron Structure",
    "text": "Neuron Structure\n\nComponents: Divided into two parts - g and f.\ng (Aggregation): Aggregates binary inputs via a simple summation process.\nf (Decision): Makes a binary decision based on the aggregation.\nExcitatory and Inhibitory Inputs: Inputs can be either excitatory (positive) or inhibitory (negative).",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#functionality",
    "href": "pages/DL/Week01_2.html#functionality",
    "title": "Motivation from Biological Neuron",
    "section": "Functionality",
    "text": "Functionality\n\nAggregation Function g(x):\n\nRepresents the sum of all inputs using the formula \\(g(x) = \\sum_{i=1}^{n} x_i\\), where \\(x_i\\) is a binary input (0 or 1).\n\nDecision Function f(g(x)):\n\nUtilizes a threshold parameter \\(\\theta\\) to determine firing.\nDecision is \\(f(g(x)) = \\begin{cases} 1 & \\text{if } g(x) \\geq \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\\).",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#boolean-function-implementation",
    "href": "pages/DL/Week01_2.html#boolean-function-implementation",
    "title": "Motivation from Biological Neuron",
    "section": "Boolean Function Implementation",
    "text": "Boolean Function Implementation\n\nExamples:\n\nImplemented using McCulloch-Pitts neuron for boolean functions like AND, OR, NOR, and NOT.\nExcitatory and inhibitory inputs utilized based on boolean function logic.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation",
    "href": "pages/DL/Week01_2.html#geometric-interpretation",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\nIn 2D:\n\nDraws a line to separate input space into two halves.\n\nIn 3D:\n\nUses a plane for separation.\n\nFor n Inputs:\n\nUtilizes a hyperplane for linear separation.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#linear-separability",
    "href": "pages/DL/Week01_2.html#linear-separability",
    "title": "Motivation from Biological Neuron",
    "section": "Linear Separability",
    "text": "Linear Separability\n\nDefinition: Boolean functions representable by a single McCulloch-Pitts neuron are linearly separable.\nImplication: Implies the existence of a plane (or hyperplane) separating points with output 0 and 1.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-2",
    "href": "pages/DL/Week01_2.html#introduction-2",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nPerceptrons, introduced by Frank Rosenblatt circa 1958, extend the concept of McCulloch-Pitts neurons with non-Boolean inputs, input weights, and a learning algorithm for weight adjustment.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-model",
    "href": "pages/DL/Week01_2.html#perceptron-model",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\n\nMathematical Representation\nThe perceptron is represented as \\(y = 1\\) if \\(\\sum_{i=1}^{n} w_i x_i \\geq \\text{threshold}\\); otherwise, \\(y = 0\\).\n\nNotable Differences\n\nInputs can be real, not just Boolean.\nIntroduction of weights, denoted by \\(w_i\\), indicating input importance.\nLearning algorithm to adapt weights based on data.\n\n\n\n\nNeater Formulation\nThe equation is rearranged for simplicity: \\(\\sum_{i=0}^{n} w_i x_i \\geq 0\\), where \\(x_0 = 1\\) and \\(w_0 = -\\text{threshold}\\).",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "href": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation for Boolean Functions",
    "text": "Motivation for Boolean Functions\nBoolean functions provide a foundation for understanding perceptrons. For instance, predicting movie preferences using Boolean inputs such as actor, director, and genre.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#importance-of-weights",
    "href": "pages/DL/Week01_2.html#importance-of-weights",
    "title": "Motivation from Biological Neuron",
    "section": "Importance of Weights",
    "text": "Importance of Weights\nWeights signify the importance of specific inputs in decision-making. Learning from data helps adjust weights, reflecting user preferences. For example, assigning a high weight to the director may heavily influence the decision to watch a movie.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#bias-w_0",
    "href": "pages/DL/Week01_2.html#bias-w_0",
    "title": "Motivation from Biological Neuron",
    "section": "Bias (\\(w_0\\))",
    "text": "Bias (\\(w_0\\))\n\\(w_0\\) acts as a bias or prior, influencing decision-making. It represents the initial bias or prejudice in decision-making. Adjusting \\(w_0\\) alters the decision threshold, accommodating user preferences.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "href": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Implementing Boolean Functions",
    "text": "Implementing Boolean Functions\nPerceptrons can implement Boolean functions with linear decision boundaries. For instance, implementing the OR function with a perceptron involves a geometric interpretation where a line separates positive and negative regions based on inputs.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-adjustments",
    "href": "pages/DL/Week01_2.html#errors-and-adjustments",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Adjustments",
    "text": "Errors and Adjustments\nErrors arise when the decision boundary misclassifies inputs. The learning algorithm adjusts weights iteratively to minimize errors and enhance accuracy. It’s an iterative process where weights are modified until the desired decision boundary is achieved.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-3",
    "href": "pages/DL/Week01_2.html#introduction-3",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThis section delves into errors within the context of perceptrons and introduces error surfaces as a recurring theme in the course, with a focus on understanding errors related to linear separability.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "href": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron for AND Function",
    "text": "Perceptron for AND Function\nConsideration of the AND function showcases an output of 1 for a specific input (green) and 0 for others (red). The decision is based on \\(w_0 + w_1x_1 + w_2x_2 \\geq 0\\), with \\(w_0\\) fixed at -1. Exploration of the impact of \\(w_1\\) and \\(w_2\\) on the decision boundary is undertaken.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "href": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Decision Boundaries",
    "text": "Errors and Decision Boundaries\nDemonstration of errors occurs with specific \\(w_1\\) and \\(w_2\\) values, showcasing misclassified points due to incorrect decision boundaries. Variability in errors is noted based on different weight values.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#error-function",
    "href": "pages/DL/Week01_2.html#error-function",
    "title": "Motivation from Biological Neuron",
    "section": "Error Function",
    "text": "Error Function\nViewing error as a function of \\(w_1\\) and \\(w_2\\) is introduced. The concept of error surfaces is brought in, where error is plotted against \\(w_1\\) and \\(w_2\\) values, each region on the surface corresponding to a distinct error level.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "href": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "title": "Motivation from Biological Neuron",
    "section": "Visualizing the Error Surface",
    "text": "Visualizing the Error Surface\nThe error surface is plotted for \\(w_1\\) and \\(w_2\\) values in the range -4 to +4. Each region on the surface corresponds to a distinct error level, highlighting the utility of visualizations in comprehending perceptron behavior.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "href": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Learning Algorithm",
    "text": "Perceptron Learning Algorithm\nExploration of the necessity for an algorithmic approach to finding optimal \\(w_1\\) and \\(w_2\\) values is undertaken. Limitations in visual inspection, especially in higher dimensions, are acknowledged. A teaser for the upcoming module on the perceptron learning algorithm is provided as a solution for finding suitable weight values algorithmically.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#overview",
    "href": "pages/DL/Week01_2.html#overview",
    "title": "Motivation from Biological Neuron",
    "section": "Overview",
    "text": "Overview\nThis module focuses on the Perceptron Learning Algorithm, building upon the perceptron’s concept and introducing a method to iteratively adjust weights for accurate binary classification.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation",
    "href": "pages/DL/Week01_2.html#motivation",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation",
    "text": "Motivation\nThe perceptron, initially designed for boolean functions, finds practical application in real-world scenarios. Consider a movie recommendation system based on past preferences, where features include both boolean and real-valued inputs. The goal is to learn weights that enable accurate predictions for new inputs.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#algorithm",
    "href": "pages/DL/Week01_2.html#algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Algorithm",
    "text": "Algorithm\n\nNotations\n\n\\(p\\): Inputs with label 1 (positive points)\n\\(n\\): Inputs with label 0 (negative points)\n\n\n\nConvergence\nConvergence is achieved when all positive points satisfy \\(\\sum w_i x_i &gt; 0\\) and all negative points satisfy \\(\\sum w_i x_i &lt; 0\\).\n\n\nSteps\n\nInitialization: Randomly initialize weights \\(w\\).\nIterative Update:\n\nWhile not converged:\n\nPick a random point \\(x\\) from \\(p \\cup n\\).\nIf \\(x\\) is in \\(p\\) and \\(w^T x &lt; 0\\), update \\(w = w + x\\).\nIf \\(x\\) is in \\(n\\) and \\(w^T x \\geq 0\\), update \\(w = w - x\\).",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "href": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nUnderstanding the geometric relationship involves recognizing that the angle between \\(w\\) and a point on the decision boundary is 90 degrees. Positive points’ angles should be acute (&lt; 90 degrees), and negative points’ angles should be obtuse (&gt; 90 degrees). Iteratively adjusting \\(w\\) aligns it better with correctly classified points.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-4",
    "href": "pages/DL/Week01_2.html#introduction-4",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this lecture is to present a formal proof establishing the convergence of the perceptron learning algorithm. The primary focus is to rigorously determine whether the algorithm exhibits convergence or continues weight updates indefinitely.",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#definitions",
    "href": "pages/DL/Week01_2.html#definitions",
    "title": "Motivation from Biological Neuron",
    "section": "Definitions",
    "text": "Definitions\n\nAbsolutely Linearly Separable Sets\n\nConsider two sets, \\(P\\) and \\(N\\), in an \\(n\\)-dimensional space. They are deemed absolutely linearly separable if there exist \\(n + 1\\) real numbers \\(w_0\\) to \\(w_n\\) such that the following conditions hold: \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n \\geq 0 \\quad \\text{for every } \\mathbf{x} \\in P\n\\] \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n &lt; 0 \\quad \\text{for every } \\mathbf{x} \\in N\n\\]\n\nPerceptron Learning Algorithm Convergence Theorem\n\nIf sets \\(P\\) and \\(N\\) are finite and linearly separable, the perceptron learning algorithm will update the weight vector a finite number of times. This implies that after a finite number of steps, the algorithm will find a weight vector \\(\\mathbf{w}\\) capable of separating sets \\(P\\) and \\(N\\).",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week01_2.html#proof",
    "href": "pages/DL/Week01_2.html#proof",
    "title": "Motivation from Biological Neuron",
    "section": "Proof",
    "text": "Proof\n\nSetup\nDefine \\(P'\\) as the union of \\(P\\) and the negation of \\(N\\). Normalize all inputs for convenience.\n\n\nAssumptions and Definitions\nAssume the existence of a normalized solution vector \\(\\mathbf{w^*}\\). Define the minimum dot product, \\(\\delta\\), as the minimum value obtained by dot products between \\(\\mathbf{w^*}\\) and points in \\(P'\\).\n\n\nPerceptron Learning Algorithm\nThe perceptron learning algorithm can be expressed as follows:\n\nInitialization:\n\nInitialize weight vector \\(\\mathbf{w}\\) randomly.\n\nIteration:\n\nAt each iteration, randomly select a point \\(\\mathbf{p}\\) from \\(P'\\).\nIf the condition \\(\\mathbf{w}^T\\mathbf{p} \\geq 0\\) is not satisfied, update \\(\\mathbf{w}\\) by \\(\\mathbf{w} = \\mathbf{w} + \\mathbf{p}\\).\n\n\n\n\nNormalization and Definitions\nNormalize all inputs, ensuring the norm of \\(\\mathbf{p}\\) is 1. Define the numerator of \\(\\cos \\beta\\) as the dot product between \\(\\mathbf{w^*}\\) and the updated weight vector at each iteration.\n\n\nNumerator Analysis\nShow that the numerator is greater than or equal to \\(\\delta\\) for each iteration.\nFor a randomly selected \\(\\mathbf{p}\\), if \\(\\mathbf{w}^T\\mathbf{p} &lt; 0\\) and an update is performed, the numerator is:\n\\[\n\\mathbf{w^*} \\cdot (\\mathbf{w} + \\mathbf{p}) \\geq \\delta\n\\]\n\n\nDenominator Analysis\nExpand the denominator, the square of the norm of the updated weight vector:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 = \\|\\mathbf{w}\\|^2 + 2\\mathbf{w}^T\\mathbf{p} + \\|\\mathbf{p}\\|^2\n\\]\nShow that the denominator is less than or equal to a value involving \\(k\\), the number of updates made:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 \\leq \\|\\mathbf{w^*}\\|^2 + k\n\\]\n\n\nCombining Numerator and Denominator\nUse the definition of \\(\\cos \\beta\\) to conclude that \\(\\cos \\beta\\) is greater than or equal to a certain quantity involving the square root of \\(k\\):\n\\[\n\\cos \\beta \\geq \\frac{\\delta}{\\sqrt{k}}\n\\]",
    "crumbs": [
      "Deep Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html",
    "href": "pages/DL/Week02.html",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Boolean functions, fundamental to computational logic, pose challenges when it comes to their linear separability. The perceptron learning algorithm, known for its guarantees with linearly separable data, encounters limitations when dealing with certain boolean functions. This module delves into the intricacies of these functions and explores the concept of linear separability.\n\n\n\n\n\nThe XOR function, denoted as \\(f(x_1, x_2)\\), outputs 1 when exactly one of its inputs is 1. It follows the logic: \\[f(0,0) \\rightarrow 0, \\, f(0,1) \\rightarrow 1, \\, f(1,0) \\rightarrow 1, \\, f(1,1) \\rightarrow 0\\]\n\n\n\nAttempting to implement XOR using a perceptron leads to a set of four inequalities. These conditions, when applied to weights (\\(w_0, w_1, w_2\\)), cannot be simultaneously satisfied. Geometrically, this signifies the inability to draw a line that separates positive and negative points in the XOR function.\n\n\n\n\nReal-world data often deviates from the assumption of linear separability. For instance, individuals with similar characteristics may exhibit diverse preferences, challenging the effectiveness of linear decision boundaries.\n\n\n\nRecognizing the limitations of a single perceptron in handling non-linearly separable data, a proposed solution involves using a network of perceptrons. This approach aims to extend the capability of handling complex, non-linearly separable boolean functions.\n\n\n\nBoolean functions with \\(n\\) inputs offer a wide range of possibilities, such as AND, OR, and others. The total number of boolean functions from \\(n\\) inputs is given by \\(2^{2^n}\\). The discussion extends to the linear separability of these boolean functions.\n\n\n\nOut of the \\(2^{2^n}\\) boolean functions, some are not linearly separable. The precise count of non-linearly separable functions remains an unsolved problem, highlighting the need for robust methods capable of handling such cases.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction",
    "href": "pages/DL/Week02.html#introduction",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Boolean functions, fundamental to computational logic, pose challenges when it comes to their linear separability. The perceptron learning algorithm, known for its guarantees with linearly separable data, encounters limitations when dealing with certain boolean functions. This module delves into the intricacies of these functions and explores the concept of linear separability.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#xor-function-analysis",
    "href": "pages/DL/Week02.html#xor-function-analysis",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "The XOR function, denoted as \\(f(x_1, x_2)\\), outputs 1 when exactly one of its inputs is 1. It follows the logic: \\[f(0,0) \\rightarrow 0, \\, f(0,1) \\rightarrow 1, \\, f(1,0) \\rightarrow 1, \\, f(1,1) \\rightarrow 0\\]\n\n\n\nAttempting to implement XOR using a perceptron leads to a set of four inequalities. These conditions, when applied to weights (\\(w_0, w_1, w_2\\)), cannot be simultaneously satisfied. Geometrically, this signifies the inability to draw a line that separates positive and negative points in the XOR function.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#implications-for-real-world-data",
    "href": "pages/DL/Week02.html#implications-for-real-world-data",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Real-world data often deviates from the assumption of linear separability. For instance, individuals with similar characteristics may exhibit diverse preferences, challenging the effectiveness of linear decision boundaries.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#network-of-perceptrons",
    "href": "pages/DL/Week02.html#network-of-perceptrons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Recognizing the limitations of a single perceptron in handling non-linearly separable data, a proposed solution involves using a network of perceptrons. This approach aims to extend the capability of handling complex, non-linearly separable boolean functions.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#boolean-functions-from-n-inputs",
    "href": "pages/DL/Week02.html#boolean-functions-from-n-inputs",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Boolean functions with \\(n\\) inputs offer a wide range of possibilities, such as AND, OR, and others. The total number of boolean functions from \\(n\\) inputs is given by \\(2^{2^n}\\). The discussion extends to the linear separability of these boolean functions.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#challenge-of-non-linear-separability",
    "href": "pages/DL/Week02.html#challenge-of-non-linear-separability",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "",
    "text": "Out of the \\(2^{2^n}\\) boolean functions, some are not linearly separable. The precise count of non-linearly separable functions remains an unsolved problem, highlighting the need for robust methods capable of handling such cases.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-to-multi-layer-perceptrons",
    "href": "pages/DL/Week02.html#introduction-to-multi-layer-perceptrons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction to Multi-Layer Perceptrons",
    "text": "Introduction to Multi-Layer Perceptrons\nMulti-Layer Perceptrons (MLPs) constitute a pivotal advancement in artificial neural networks. These networks boast a layered architecture, each layer serving a distinct role in processing information.\n\nLayers in an MLP\n\nInput Layer:\n\nComprising nodes representing input features (\\(x_1, x_2, ..., x_n\\)).\n\nHidden Layer:\n\nFeatures multiple perceptrons introducing non-linearities to the network.\n\nOutput Layer:\n\nHouses a single perceptron providing the final network output.\n\n\n\n\nWeights and Bias\n\nConnection Characteristics:\n\nWeights (\\(w\\)) and a bias term (\\(w_0\\)) define the connections between nodes.\n\nWeighted Sum and Activation:\n\nThe weighted sum of inputs, combined with the bias, influences perceptron activation.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#representation-of-boolean-functions-in-mlps",
    "href": "pages/DL/Week02.html#representation-of-boolean-functions-in-mlps",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Representation of Boolean Functions in MLPs",
    "text": "Representation of Boolean Functions in MLPs\n\nNetwork Structure for Boolean Functions\n\nHidden Layer Configuration:\n\nFor a boolean function with \\(n\\) inputs, the hidden layer consists of \\(2^n\\) perceptrons.\n\nWeight and Bias Adjustment:\n\nWeights and biases are adjusted to meet boolean logic conditions for accurate function representation.\n\n\n\n\nBoolean Function Implementation\n\nPerceptron Activation Conditions:\n\nEach perceptron in the hidden layer selectively fires based on specific input combinations.\n\nXOR Function Illustration:\n\nUsing the XOR function as an example, conditions on weights (\\(w_1, w_2, w_3, w_4\\)) are established for faithful representation.\n\nExtension to \\(n\\) Inputs:\n\nGeneralizing the approach to \\(n\\) inputs involves \\(2^n\\) perceptrons in the hidden layer.\nConditions for output layer weights are derived to ensure accurate representation.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#representation-power-and-implications",
    "href": "pages/DL/Week02.html#representation-power-and-implications",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Representation Power and Implications",
    "text": "Representation Power and Implications\n\nRepresentation Power Theorem\n\nTheorem Statement:\n\nAny boolean function of \\(n\\) inputs can be precisely represented by an MLP.\n\nSuggested MLP Structure:\n\nAn MLP with \\(2^n\\) perceptrons in the hidden layer and 1 perceptron in the output layer is deemed sufficient.\n\n\n\n\nPractical Considerations\n\nChallenges with Growing \\(n\\):\n\nThe exponential increase in perceptrons as \\(n\\) grows poses practical challenges.\n\nReal-World Applications:\n\nManaging and computing with a large number of perceptrons may be challenging in practical applications.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#transition-from-perceptrons-to-sigmoid-neurons",
    "href": "pages/DL/Week02.html#transition-from-perceptrons-to-sigmoid-neurons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Transition from Perceptrons to Sigmoid Neurons",
    "text": "Transition from Perceptrons to Sigmoid Neurons\n\nBinary Output Limitation\nPerceptrons, governed by binary output based on the weighted sum of inputs exceeding a threshold, exhibit a binary decision boundary. This rigid characteristic proves restrictive in scenarios where a more gradual decision-making process is preferred.\n\n\nReal-Valued Inputs and Outputs\nThe shift towards sigmoid neurons arises in the context of addressing arbitrary functions \\(Y = f(X)\\), wherein \\(X \\in \\mathbb{R}^n\\) and \\(Y \\in \\mathbb{R}\\). This entails the consideration of real numbers for both inputs and outputs. Examples include predicting oil quantity based on salinity, density, pressure, temperature, and marine diversity, as well as determining bank interest rates considering factors like salary, family size, previous loans, and defaults.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#objective",
    "href": "pages/DL/Week02.html#objective",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Objective",
    "text": "Objective\nThe primary objective is to construct a neural network capable of accurately approximating or representing real-valued functions, ensuring the proximity of the network’s output to actual values present in the training data.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-to-sigmoid-neurons",
    "href": "pages/DL/Week02.html#introduction-to-sigmoid-neurons",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction to Sigmoid Neurons",
    "text": "Introduction to Sigmoid Neurons\n\nSigmoid Function\nSigmoid neurons employ the sigmoid function (logistic function) to introduce smoothness in decision-making. Mathematically represented as: \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] where \\(z\\) denotes the weighted sum of inputs.\n\n\nSigmoid Function Properties\n\nAs \\(z\\) tends to positive infinity: \\(\\lim_{{z \\to \\infty}} \\sigma(z) = 1\\)\nAs \\(z\\) tends to negative infinity: \\(\\lim_{{z \\to -\\infty}} \\sigma(z) = 0\\)\nAt \\(W^T X = 0\\): \\(\\sigma(0) = \\frac{1}{2}\\)\n\nThe sigmoid function transforms outputs into the range [0, 1], facilitating a probabilistic interpretation.\n\n\nComparison with Perceptron\nContrasting with the perceptron function, the sigmoid function exhibits smoothness and continuity. The perceptron function lacks differentiability at the abrupt change in value, whereas the sigmoid function is differentiable.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#importance-of-differentiability",
    "href": "pages/DL/Week02.html#importance-of-differentiability",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Importance of Differentiability",
    "text": "Importance of Differentiability\nDifferentiability holds paramount importance for various machine learning algorithms, particularly in derivative-related operations. The application of calculus in neural network training and optimization is streamlined by the differentiability of the sigmoid neuron’s activation function.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#overview",
    "href": "pages/DL/Week02.html#overview",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Overview",
    "text": "Overview\nIn the realm of supervised machine learning, the fundamental objective is to comprehend the intricate structure of the setup, which encompasses various components crucial for effective model training. These components include the dataset, model representation, the learning algorithm, and the definition of an objective function.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#components",
    "href": "pages/DL/Week02.html#components",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Components",
    "text": "Components\n\nData Representation\nThe dataset, denoted as \\((x_i, y_i)\\), is pivotal to the learning process. Here, \\(x_i\\) signifies an \\(m\\)-dimensional input vector, while \\(y_i\\) represents a real-valued output associated with the given input. The dataset essentially comprises a collection of such input-output pairs.\n\n\nModel Assumption\nA critical assumption in this paradigm is that the output \\(y\\) is contingent upon the input \\(x\\), expressed as \\(y = f(x)\\). However, the specific form of the function \\(f\\) remains elusive, prompting the need for learning algorithms to discern it from the provided data.\n\nLearning Algorithm\nThe learning algorithm employed in this context is the Gradient Descent algorithm. This iterative approach facilitates the adjustment of model parameters, ensuring a continuous refinement of the model’s approximation.\n\n\n\nObjective Function (Loss Function)\nCentral to the learning process is the formulation of an objective function, commonly referred to as the Loss Function. Mathematically, it is defined as follows:\n\\[\\mathcal{L}(\\theta) = \\sum_{i=1}^{n} \\text{Difference}(y_{\\hat{i}}, y_i)\\]\nHere, \\(\\theta\\) denotes the parameters of the model, and \\(\\text{Difference}(y_{\\hat{i}}, y_i)\\) quantifies the dissimilarity between the predicted (\\(y_{\\hat{i}}\\)) and actual (\\(y_i\\)) values.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#objective-function-details",
    "href": "pages/DL/Week02.html#objective-function-details",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Objective Function Details",
    "text": "Objective Function Details\n\nDifference Function (Squared Error Loss)\nThe Difference Function, an integral component of the Loss Function, is expressed as:\n\\[\\text{Difference}(\\hat{y}, y) = (\\hat{y} - y)^2\\]\nThe squaring operation is implemented to ensure that both positive and negative errors contribute to the overall loss without canceling each other out.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#analogy-with-learning-trigonometry",
    "href": "pages/DL/Week02.html#analogy-with-learning-trigonometry",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Analogy with Learning Trigonometry",
    "text": "Analogy with Learning Trigonometry\n\nTraining Phase\nAnalogous to mastering a chapter in a textbook, the training phase strives for zero or minimal errors on the content encapsulated within the training dataset.\n\n\nValidation Phase\nResembling the solving of exercises at the end of a chapter, the validation phase allows for revisiting and enhancing comprehension based on additional exercises.\n\n\nTest Phase (Exam)\nThe test phase simulates a real-world scenario where the model encounters new data. Unlike the training and validation phases, there is no opportunity for revisiting and refining the learned information.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-1",
    "href": "pages/DL/Week02.html#introduction-1",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nSupervised machine learning involves the development of algorithms to learn parameters for a given model. This process aims to minimize the difference between predicted and actual values using a defined objective function. In this context, we explore a simplified model with one input, connected by weight (\\(w\\)), and a bias (\\(b\\)).\n\nModel Representation\nThe model is represented as \\(f(\\mathbf{x}) = -w \\mathbf{x} + b\\), where \\(\\mathbf{x}\\) is the input vector. The task is to determine an algorithm that learns the optimal values for \\(w\\) and \\(b\\) using training data.\n\n\nTraining Objective\nThe training objective involves minimizing the average difference between predicted values (\\(f(\\mathbf{x})\\)) and actual values (\\(y\\)) over all training points. The process requires finding the optimal \\(w\\) and \\(b\\) values that achieve this minimum loss.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#training-data",
    "href": "pages/DL/Week02.html#training-data",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Training Data",
    "text": "Training Data\nThe training data consists of pairs \\((\\mathbf{x}, y)\\), where \\(\\mathbf{x}\\) represents the input, and \\(y\\) corresponds to the output. The loss function is defined as the average difference between predicted and actual values across all training points.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#loss-function",
    "href": "pages/DL/Week02.html#loss-function",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Loss Function",
    "text": "Loss Function\nThe loss function is expressed as:\n\\[\\mathcal{L}(w, b) = \\frac{1}{N} \\sum_{i=1}^{N} \\left| f(\\mathbf{x}_i) - y_i \\right|\\]\nHere, \\(N\\) is the number of training points, \\(\\mathbf{x}_i\\) is the input for the \\(i\\)-th point, and \\(y_i\\) is the corresponding actual output.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#trial-and-error-approach",
    "href": "pages/DL/Week02.html#trial-and-error-approach",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Trial-and-Error Approach",
    "text": "Trial-and-Error Approach\nTo illustrate the concept, a trial-and-error approach is employed initially. Random values for \\(w\\) and \\(b\\) are chosen, and the loss is calculated. Adjustments are made iteratively to minimize the loss. This process involves systematically changing \\(w\\) and \\(b\\) values until an optimal solution is found.\n\nVisualization with Error Surface\nA 3D surface plot is used to visualize the loss in the \\(w-b\\) plane. This plot aids in identifying regions of low and high loss. However, the impracticality of exhaustively exploring this surface for large datasets is acknowledged due to computational constraints.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-2",
    "href": "pages/DL/Week02.html#introduction-2",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nThe transcript delves into the intricacies of parameter optimization, focusing on the goal of efficiently traversing the error surface to reach the minimum error. The parameters of interest, denoted as \\(\\theta\\), are expressed as vectors, specifically encompassing \\(W\\) and \\(B\\) in the context of a toy network.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#update-rule-with-conservative-movement",
    "href": "pages/DL/Week02.html#update-rule-with-conservative-movement",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Update Rule with Conservative Movement",
    "text": "Update Rule with Conservative Movement\nThe update rule for altering \\(\\theta\\) entails a meticulous adjustment of the parameters. The process involves taking a measured step, determined by a scalar \\(\\eta\\), in the direction of \\(\\Delta\\theta\\), which encapsulates the parameter changes. This introduces a level of conservatism in the parameter adjustments, promoting stability in the optimization process.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#taylor-series-for-function-approximation",
    "href": "pages/DL/Week02.html#taylor-series-for-function-approximation",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Taylor Series for Function Approximation",
    "text": "Taylor Series for Function Approximation\n\nOverview\nThe lecture introduces the Taylor series, a powerful mathematical tool for approximating functions that exhibit continuous differentiability. This method enables the representation of a function through polynomials, allowing for varying degrees of precision in the approximation.\n\n\nLinear Approximation\nLinear approximation entails the establishment of a tangent line at a specific point on the function. This approach provides an initial approximation, and the accuracy is contingent on the chosen neighborhood size, denoted as \\(\\varepsilon\\).\n\n\nQuadratic and Higher-Order Approximations\nQuadratic and higher-order approximations extend the accuracy of the approximation by incorporating additional terms. The lecture underscores the importance of selecting a small neighborhood for these approximations to maintain efficacy.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#extending-concepts-to-multiple-dimensions",
    "href": "pages/DL/Week02.html#extending-concepts-to-multiple-dimensions",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Extending Concepts to Multiple Dimensions",
    "text": "Extending Concepts to Multiple Dimensions\nThe discussion expands to functions with two variables, exemplifying how linear and quadratic approximations operate in multidimensional spaces. The lecture underscores the critical role of confined neighborhoods (\\(\\varepsilon\\)) in ensuring the precision of the Taylor series method across varying dimensions.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-3",
    "href": "pages/DL/Week02.html#introduction-3",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of optimization for machine learning models, the process of iteratively updating parameters to minimize a loss function is a fundamental concept. One key technique employed in this context is gradient descent. This discussion delves into the intricate mathematical foundations underpinning gradient descent, focusing on the decision criteria for parameter updates and the optimization of the update vector.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#taylor-series-expansion",
    "href": "pages/DL/Week02.html#taylor-series-expansion",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Taylor Series Expansion",
    "text": "Taylor Series Expansion\n\nObjective\nThe overarching objective is to determine an optimal change in parameters, denoted as \\(\\Delta\\theta\\) (represented as \\(\\mathbf{U}\\)), to minimize the loss function \\(\\mathcal{L}(\\theta)\\).\n\n\nLinear Approximation\nUtilizing the Taylor series, the loss function at a nearby point \\(\\theta + \\Delta\\theta\\) is approximated linearly as: \\[\\mathcal{L}(\\theta + \\Delta\\theta) \\approx \\mathcal{L}(\\theta) + \\eta\\mathbf{U}^T\\nabla \\mathcal{L}(\\theta)\\] Here, \\(\\eta\\) is a small positive scalar, ensuring a negligible difference.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#mathematical-aspects-of-gradient-descent",
    "href": "pages/DL/Week02.html#mathematical-aspects-of-gradient-descent",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Mathematical Aspects of Gradient Descent",
    "text": "Mathematical Aspects of Gradient Descent\n\nGradient\nThe gradient \\(\\nabla \\mathcal{L}(\\theta)\\) is introduced as a vector comprising partial derivatives of the loss function with respect to its parameters. For a function \\(y = W^2 + B^2\\) with two variables, the gradient is expressed as \\([2W, 2B]\\).\n\n\nSecond Order Derivative (Hessian)\nThe concept of the Hessian matrix, representing the second-order derivative, is introduced. This matrix provides insights into the curvature of the loss function. In the case of a two-variable function, the Hessian is illustrated as a \\(2\\times2\\) matrix.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#decision-criteria-for-parameter-updates",
    "href": "pages/DL/Week02.html#decision-criteria-for-parameter-updates",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Decision Criteria for Parameter Updates",
    "text": "Decision Criteria for Parameter Updates\n\nLinear Approximation and Criteria\nThe focus shifts to linear approximation, with higher-order terms neglected when \\(\\eta\\) is small. The decision criteria for a favorable parameter update is based on the condition: \\[\\eta\\mathbf{U}^T\\nabla \\mathcal{L}(\\theta) &lt; 0\\]",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#optimization-of-update-vector-mathbfu",
    "href": "pages/DL/Week02.html#optimization-of-update-vector-mathbfu",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Optimization of Update Vector \\(\\mathbf{U}\\)",
    "text": "Optimization of Update Vector \\(\\mathbf{U}\\)\n\nAngle \\(\\beta\\) and Cosine\nOptimizing the update vector involves considering the angle \\(\\beta\\) between \\(\\mathbf{U}\\) and the gradient vector. The cosine of \\(\\beta\\), denoted as \\(\\cos(\\beta)\\), is explored, and its range is discussed.\n\n\nOptimal Update for Maximum Descent\nIn the pursuit of maximum descent, the optimal scenario arises when \\(\\cos(\\beta) = -1\\), indicating that the angle \\(\\beta\\) is 180 degrees, signifying movement in the direction opposite to the gradient vector. This aligns with the well-known rule in gradient descent: “Move in the direction opposite to the gradient.”",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#overview-2",
    "href": "pages/DL/Week02.html#overview-2",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Overview",
    "text": "Overview\nIn the pursuit of optimizing the parameters of a sigmoid neuron, the lecture primarily delves into the application of the gradient descent algorithm. The primary objective is to minimize the associated loss function, thereby identifying optimal values for the neuron’s weights (\\(W\\)) and bias (\\(B\\)).",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#key-concepts",
    "href": "pages/DL/Week02.html#key-concepts",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Key Concepts",
    "text": "Key Concepts\n\n1. Gradient Descent Rule\nThe gradient descent rule serves as an iterative optimization technique employed to minimize the loss function. The core update rule is defined as follows:\n\\[\nW = W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}, \\quad B = B - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial B}\n\\]\nThis iterative process aims to iteratively refine the parameters (\\(W\\) and \\(B\\)) based on the computed partial derivatives of the loss function.\n\n\n2. Derivative Computation\n\n2.1 Derivative of Loss with Respect to \\(W\\)\nThe partial derivative of the loss function with respect to weights (\\(\\frac{\\partial \\mathcal{L}}{\\partial W}\\)) is computed through the application of the chain rule. In the context of the sigmoid function, the derivative is obtained as follows:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_i \\left(f(x_i) - y_i\\right) \\cdot f(x_i) \\cdot \\left(1 - f(x_i)\\right) \\cdot X_i\n\\]\nHere, \\(f(x_i)\\) represents the sigmoid function applied to the input \\(x_i\\) associated with data point \\(i\\).\n\n\n2.2 Derivative of Loss with Respect to \\(B\\)\nSimilarly, the partial derivative of the loss function with respect to bias (\\(\\frac{\\partial \\mathcal{L}}{\\partial B}\\)) is derived as:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial B} = \\sum_i \\left(f(x_i) - y_i\\right) \\cdot f(x_i) \\cdot \\left(1 - f(x_i)\\right)\n\\]\nThe introduction of \\(X_i\\) is omitted in this case, as it pertains to the bias term.\n\n\n\n3. Algorithm Execution\nThe algorithmic execution involves several key steps:\n\nInitialization:\n\nRandom initialization of weights (\\(W\\)) and bias (\\(B\\)).\nSetting the learning rate (\\(\\eta\\)) and maximum iterations.\n\nGradient Computation:\n\nIterating over all data points, computing the partial derivatives for \\(W\\) and \\(B\\) using the derived formulas.\n\nParameter Update:\n\nApplying the gradient descent update rule to iteratively adjust the weights and bias.\n\n\n\n\n4. Loss Surface Visualization\nThe lecture introduces the concept of visualizing the loss function surface in the \\(W-B\\) plane. This visual aid illustrates the algorithm’s movement along the surface, consistently reducing the loss.\n\n\n5. Observations\nThe lecture emphasizes crucial observations:\n\nLoss Reduction:\n\nEnsuring that at each iteration, the algorithm systematically decreases the loss.\n\nHyperparameter Impact:\n\nAcknowledging the influence of the learning rate (\\(\\eta\\)) on convergence and potential overshooting.\n\nExperimentation:\n\nEncouraging experimentation with diverse initializations and learning rates for a comprehensive understanding.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#introduction-4",
    "href": "pages/DL/Week02.html#introduction-4",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Introduction",
    "text": "Introduction\nThe representation power of a multi-layer network, particularly employing sigmoid neurons, is the focal point of this discussion. The objective is to establish a theorem analogous to the one developed for perceptrons, specifically emphasizing the network’s capability to approximate any continuous function.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#universal-approximation-theorem",
    "href": "pages/DL/Week02.html#universal-approximation-theorem",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\nThe Universal Approximation Theorem posits that a multi-layer network with a single hidden layer possesses the capacity to approximate any continuous function with precision. This approximation is achieved by manipulating the weights and biases associated with the sigmoid neurons within the hidden layer.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#tower-functions-illustration",
    "href": "pages/DL/Week02.html#tower-functions-illustration",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Tower Functions Illustration",
    "text": "Tower Functions Illustration\nTo illustrate the approximation process, the concept of towers of functions is introduced. This entails deconstructing an arbitrary function into a summation of tower functions, wherein each tower is represented by sigmoid neurons. The amalgamation of these towers serves to approximate the original function.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#tower-construction-process",
    "href": "pages/DL/Week02.html#tower-construction-process",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Tower Construction Process",
    "text": "Tower Construction Process\nThe construction of towers involves the utilization of sigmoid neurons with exceptionally high weights, approaching infinity. This strategic choice mimics step functions. By subtracting these step functions, a tower-like structure is formed. Notably, the width and position of the tower are modulated by adjusting the biases of the sigmoid neurons.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#tower-maker-neural-network",
    "href": "pages/DL/Week02.html#tower-maker-neural-network",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Tower Maker Neural Network",
    "text": "Tower Maker Neural Network\n\nArchitecture\nThe lecture introduces a neural network architecture termed the “Tower Maker.” This architecture comprises two sigmoid neurons characterized by high weights. The subtraction of their outputs yields a function resembling a tower.\n\n\nSigmoid Neuron Configuration\nThe sigmoid neurons within the Tower Maker are configured with exceedingly high weights, akin to infinity. This configuration transforms the sigmoid functions into step functions, pivotal in constructing tower-like shapes.\n\n\nBias Adjustment\nControl over the width and position of the tower is exercised through the manipulation of biases associated with the sigmoid neurons. Adjusting these biases ensures the customization of the tower function according to specific requirements.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#linear-function-integration",
    "href": "pages/DL/Week02.html#linear-function-integration",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Linear Function Integration",
    "text": "Linear Function Integration\nAn additional layer is incorporated into the Tower Maker architecture to integrate linear functions. This augmentation enhances the network’s ability to generate tower functions based on the input parameters.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#network-adjustment-for-precision",
    "href": "pages/DL/Week02.html#network-adjustment-for-precision",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Network Adjustment for Precision",
    "text": "Network Adjustment for Precision\nThe lecture underscores the correlation between the desired precision (represented by epsilon) and the network’s complexity. As the precision requirement increases, a more intricate network with an augmented number of neurons in the hidden layer becomes imperative. However, it is acknowledged that practical implementation may encounter challenges as the network’s size expands.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#single-input-function",
    "href": "pages/DL/Week02.html#single-input-function",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Single Input Function",
    "text": "Single Input Function\nConsider a function with a single input (\\(x\\)) plotted on the x-axis and corresponding output (\\(y\\)) on the y-axis. This introductory scenario involves the use of a sigmoid neuron function, denoted by:\n\\[f(x) = \\frac{1}{1 + e^{-(wx + b)}}\\]\nwhere:\n\n\\(w\\) represents the weight associated with the input.\n\\(b\\) is the bias term.\nThe sigmoid function smoothly transitions between 0 and 1.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#two-input-function",
    "href": "pages/DL/Week02.html#two-input-function",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Two Input Function",
    "text": "Two Input Function\nExpanding the scope to a two-input function, let’s consider an example related to oil mining, where salinity (\\(x_1\\)) and pressure (\\(x_2\\)) serve as inputs. The challenge is to establish a decision boundary separating points indicating the presence (orange) and absence (blue) of oil.\nA linear decision boundary proves inadequate, prompting the need for a more complex function.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#building-a-tower-in-2d",
    "href": "pages/DL/Week02.html#building-a-tower-in-2d",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Building a Tower in 2D",
    "text": "Building a Tower in 2D\nTo construct a tower-like structure, two sigmoid neurons are introduced, each handling one input (\\(x_1\\) and \\(x_2\\)). The sigmoid function takes the form:\n\\[f(x) = \\frac{1}{1 + e^{-(w_ix_i + b)}}\\]\nHere, \\(i\\) denotes the input index (1 or 2), \\(w_i\\) is the associated weight, and \\(b\\) is the bias term. Adjusting weights (\\(w_1\\) and \\(w_2\\)) results in step functions, dictating the slope of the tower in different directions.\nCombining these sigmoid neurons produces an open tower structure in one direction.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#closing-the-tower",
    "href": "pages/DL/Week02.html#closing-the-tower",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Closing the Tower",
    "text": "Closing the Tower\nTo enclose the tower from all sides, two additional sigmoid neurons (h13 and h14) are introduced. These neurons, with specific weight configurations, contribute to the formation of walls in different directions. Subtracting the outputs of these sigmoid neurons results in a structure with walls on all four sides but an open top.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#thresholding-to-get-a-closed-tower",
    "href": "pages/DL/Week02.html#thresholding-to-get-a-closed-tower",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Thresholding to Get a Closed Tower",
    "text": "Thresholding to Get a Closed Tower\nTo address the open top issue, thresholding is introduced. A sigmoid function with a switch-over point at 1 is applied to the structure’s output. This process retains only the portion of the structure above level 1, effectively closing the tower.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#extending-to-higher-dimensions",
    "href": "pages/DL/Week02.html#extending-to-higher-dimensions",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Extending to Higher Dimensions",
    "text": "Extending to Higher Dimensions\nGeneralizing this approach to n-dimensional inputs, the methodology remains consistent. For a single input (\\(x\\)), two neurons suffice; for two inputs (\\(x_1\\) and \\(x_2\\)), four neurons are necessary. The number of neurons in the middle layer increases with higher dimensions, extending the method to handle arbitrary functions.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#universal-approximation-theorem-1",
    "href": "pages/DL/Week02.html#universal-approximation-theorem-1",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Universal Approximation Theorem",
    "text": "Universal Approximation Theorem\nThis construction aligns with the Universal Approximation Theorem, asserting that a neural network, given a sufficient number of neurons, can approximate any arbitrary function to a desired precision.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#implications-for-deep-learning",
    "href": "pages/DL/Week02.html#implications-for-deep-learning",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Implications for Deep Learning",
    "text": "Implications for Deep Learning\nThis methodology underscores the flexibility of deep neural networks in approximating complex functions encountered in real-world applications. The ability to systematically construct networks capable of representing intricate relationships contributes to the effectiveness of deep learning models.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/DL/Week02.html#points-to-remember",
    "href": "pages/DL/Week02.html#points-to-remember",
    "title": "Deep Learning Foundations: From Boolean Functions to Universal Approximation",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nBoolean Functions and Linear Separability:\n\nPerceptrons face challenges with non-linearly separable boolean functions.\nMulti-layer perceptrons (MLPs) extend capabilities for handling complex functions.\n\nSigmoid Neurons and Differentiability:\n\nSigmoid neurons introduce smoothness in decision-making.\nDifferentiability is crucial for optimization in neural network training.\n\nGradient Descent: Mathematical Foundation:\n\nTaylor series expansion facilitates linear approximation in gradient descent.\nDecision criteria for parameter updates involve linear approximation conditions.\n\nTower Maker and Universal Approximation Theorem:\n\nThe Universal Approximation Theorem states that a single hidden layer in a neural network can approximate any continuous function.\nTower Maker architecture showcases the construction of towers using sigmoid neurons.\n\nDeep Learning Flexibility:\n\nDeep neural networks are flexible in approximating complex functions.\nThe Tower Maker architecture demonstrates the power of neural networks in constructing intricate representations.\n\n\nThis week’s exploration laid the groundwork for understanding the core principles and capabilities of neural networks, setting the stage for further exploration into advanced topics in deep learning.",
    "crumbs": [
      "Deep Learning",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html",
    "href": "pages/AI/Week01.html",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning.\n\n\n\n\nData Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward.\n\n\n\n\n\n\nThe foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers.\n\n\n\n\n\n\nThe training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology.\n\n\n\n\n\n\n\nThe dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification.\n\n\n\n\n\n\n\nNeural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains.\n\n\n\n\n\n\nThe triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "href": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "href": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Data Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#evolution-of-neural-networks",
    "href": "pages/AI/Week01.html#evolution-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#training-process-of-neural-networks",
    "href": "pages/AI/Week01.html#training-process-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "href": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "href": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Neural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "href": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#human-cognition-and-ai",
    "href": "pages/AI/Week01.html#human-cognition-and-ai",
    "title": "A Decade of Machine Learning",
    "section": "Human Cognition and AI",
    "text": "Human Cognition and AI\n\nCognitive Landscape\nHuman intelligence engages in a myriad of activities such as logic, representation, planning, reasoning, and search. The crux of these cognitive endeavors lies in symbolic reasoning, a substantial facet of the human cognitive load.\n\n\nSymbolic Reasoning\nSymbolic reasoning, integral to human cognition, involves the management of symbolic knowledge representation and intricate problem-solving processes.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "href": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "Distinguishing AI from Machine Learning",
    "text": "Distinguishing AI from Machine Learning\n\nAI Emphasis\nWithin the domain of Artificial Intelligence (AI), the spotlight is on symbolic knowledge representation and advanced problem-solving methodologies.\n\n\nMachine Learning Focus\nIn contrast, Machine Learning (ML) gravitates towards interpreting data, with applications ranging from recommender systems to predictive analytics and classification.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "href": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Knowledge Representation in AI",
    "text": "Knowledge Representation in AI\n\nDeclarative Knowledge\nAligning with the cognitive domain of humans, explicit symbolic representation, known as declarative knowledge, assumes a pivotal role. It encompasses the representation of the world and engages in reasoned deductions.\n\n\nInferences in AI\nAI agents showcase a spectrum of inferences, ranging from deductive reasoning based on logic to plausible or probabilistic inferences that incorporate an element of likelihood.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "href": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Symbolic Representation in AI",
    "text": "Symbolic Representation in AI\n\nDefining Symbols\nSymbols, representing abstract concepts, manifest in diverse forms. For example, the number (7) can be expressed in various ways, illustrating the distinction between the conceptualization of numbers and their symbolic representations.\n\n\nMeaning of Symbols\nThe significance of symbols is socially agreed upon, forming the foundation for semiotic systems. Whether in road signs or linguistic characters, symbols encapsulate shared meanings.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "href": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "title": "A Decade of Machine Learning",
    "section": "Semiotics and Biosemiotics",
    "text": "Semiotics and Biosemiotics\n\nSemiotics\nSemiotics, the scientific study of symbols in spoken and written languages, lays the groundwork for comprehending human communication and representation.\n\n\nBiosemiotics\nDelving deeper, Biosemiotics explores the emergence of complex behavior when simple systems engage in symbolic communication. This is exemplified by phenomena such as ant trails utilizing pheromones.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "href": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Reasoning Mechanisms in AI",
    "text": "Reasoning Mechanisms in AI\n\nFormal Reasoning\nIn the context of AI, reasoning involves the systematic manipulation of symbols in a meaningful manner. This encompasses algorithms for fundamental operations like addition and multiplication, extending to more intricate processes like the Fourier transform.\n\n\nConceptualizing Algorithms\nUnderstanding AI algorithms necessitates a conceptual grasp of symbolic manipulations. For instance, multiplication algorithms entail conceptualizing the multiplication of unit digits and the subsequent shifting of results.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#automation-vs.-ai",
    "href": "pages/AI/Week01.html#automation-vs.-ai",
    "title": "A Decade of Machine Learning",
    "section": "Automation vs. AI",
    "text": "Automation vs. AI\n\nNavigating Overlap\nWhile Automation and AI share common ground, not all automated systems integrate AI. For instance, implementations such as train reservation systems or basic online shopping may lack substantial AI components.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "href": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Machine Learning’s Role in AI",
    "text": "Machine Learning’s Role in AI\n\nML as a Component\nMachine Learning constitutes one facet of the multifaceted field of AI. Examples such as self-driving cars leverage ML for tasks including pattern recognition, speech processing, and object classification.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "href": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Clarifying Data Science in AI",
    "text": "Clarifying Data Science in AI\n\nData’s Multifaceted Role\nData science, encompassing elements of statistics, AI, and machine learning, plays a crucial role in the broader field of AI. It serves as a foundational component but does not encapsulate the entirety of AI.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "href": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to AI and Definitions:",
    "text": "Introduction to AI and Definitions:\n\n\nDefinitions of AI:\n\nHerbert Simon: Programs are considered intelligent if they display behaviors regarded as intelligent in humans.\nBar and Feigenbaum: AI seeks to comprehend the systematic behavior of information processing systems, analogous to physicists and biologists in their respective domains.\nElaine Rich: AI involves solving exponentially hard problems in polynomial time, leveraging domain-specific knowledge.\nJohn Hoagland: AI’s goal is to create machines with minds of their own, treating thinking and computing as fundamentally interconnected.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "href": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Fundamental Questions in AI:",
    "text": "Fundamental Questions in AI:\n\nQuestions about Intelligence:\nVarious perspectives exist on what constitutes intelligence, encompassing language use, reasoning, and learning. Ongoing debates revolve around whether machines can genuinely exhibit thinking, with insights from philosophers like Roger Penrose exploring quantum mechanics in the human brain.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#turing-test-and-challenges",
    "href": "pages/AI/Week01.html#turing-test-and-challenges",
    "title": "A Decade of Machine Learning",
    "section": "Turing Test and Challenges:",
    "text": "Turing Test and Challenges:\n\nAlan Turing’s Turing Test:\nThe evaluation of machine intelligence through its ability to convincingly engage in natural language conversations with a human judge forms the essence of the Turing Test. Associated challenges include situations where chatbots may impress but lack genuine intelligence. The Löbner Prize Competition attempts a similar test.\n\n\nHector Levesque’s Alternative: Vinograd Schemas\nAn alternate test proposed by Hector Levesque challenges a machine’s understanding through multiple-choice questions that require subject matter knowledge.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#vinograd-schemas-examples",
    "href": "pages/AI/Week01.html#vinograd-schemas-examples",
    "title": "A Decade of Machine Learning",
    "section": "Vinograd Schemas Examples:",
    "text": "Vinograd Schemas Examples:\n\nExample 1:\n\nOriginal Sentence: “The city council refused the demonstrators a permit because they feared violence.”\nAlternate Sentence: “The city council refused the demonstrators a permit because they advocated violence.”\nQuestion: What does “they” refer to? Options: Council, Demonstrators.\n\nExample 2:\n\nOriginal Sentence: “John took the water bottle out of the backpack so that it would be lighter.”\nAlternate Sentence: “John took the water bottle out of the backpack so that it would be handy.”\nQuestion: What does “it” refer to? Options: Backpack, Water Bottle.\n\nExample 3:\n\nOriginal Sentence: “The trophy would not fit into the brown suitcase because it was too small.”\nAlternate Sentence: “The trophy would not fit into the brown suitcase because it was too big.”\nQuestion: What does “it” refer to? Options: Trophy, Brown Suitcase.\n\nExample 4:\n\nOriginal Sentence: “The lawyer asked the witness a question but he was reluctant to repeat it.”\nAlternate Sentence: “The lawyer asked the witness a question but he was reluctant to answer it.”\nQuestion: Who was reluctant? Options: Lawyer, Witness.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "href": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Intelligence and AI Goals",
    "text": "Introduction to Intelligence and AI Goals\nIn the exploration of artificial intelligence (AI), the concept of intelligence takes center stage. AI endeavors to construct intelligent agents capable of complex problem-solving. A historical glimpse into European thinkers sheds light on the roots of AI ideologies.\n\nGalileo Galilei (1623)\nIn his 1623 publication, Galileo Galilei delves into the subjective nature of sensory experiences. He contends that taste, odors, and colors are subjective perceptions residing in consciousness. Galileo challenges the idea that these qualities exist inherently in external objects. Moreover, he posits that philosophy is expressed through the language of mathematics.\n\n\nThomas Hobbs\nThomas Hobbs, often referred to as the grandfather of AI, introduces the notion that thinking involves the manipulation of symbols. He associates reasoning with computation, not in the contemporary sense of computers, but as a form of mathematical operations. Hobbs views computation as the sum of many things added together or the determination of the remainder when one thing is subtracted from another.\n\n\nRené Descartes\nBuilding on Galileo’s ideas, Descartes extends the concept that animals are intricate machines, reserving acknowledgment of a mind solely for humans. He aligns thought with symbols and introduces the mind-body dualism, raising questions about the interaction between the mental world and the physical body.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "href": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "title": "A Decade of Machine Learning",
    "section": "Early Concepts of Thinking Machines",
    "text": "Early Concepts of Thinking Machines\nThe early stages of envisioning thinking machines were influenced by the use of punch cards in the textile industry’s Jacquard looms.\n\nJacquard Looms\nPunch cards were employed to control patterns in textile looms. This concept of punched cards was later adapted for programming early computers, emphasizing a transition from controlling patterns to controlling programs.\n\n\nCharles Babbage and Augusta Ada Byron\nCharles Babbage, a mathematician and inventor, conceptualized the Difference Engine and the Analytic Engine. Augusta Ada Byron, daughter of Lord Byron, collaborated with Babbage and is recognized as the world’s first programmer. She envisioned computers going beyond mere number crunching, foreseeing applications in music composition and AI-like capabilities.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "href": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "title": "A Decade of Machine Learning",
    "section": "Mechanical Calculators and Early Computers",
    "text": "Mechanical Calculators and Early Computers\nThe evolution of mechanical calculators and the emergence of early electronic computers marked significant progress in computational capabilities.\n\nPascal’s Calculator and Leibniz’s Step Drum\nPascal’s mechanical calculator incorporated Latin Lantern gears, performing basic arithmetic operations. Leibniz introduced the step drum, a mechanism for counting and representing numbers. Both contributed to the development of early calculating machines.\n\n\nENIAC (Electronic Numerical Integrator and Computer)\nENIAC, the first electronic computer, boasted over 17,000 vacuum tubes. Despite its immense size and weight, it laid the foundation for electronic computing. Augusta Ada Byron’s visionary insights into the potential of computers started to materialize with the advent of ENIAC.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#introduction",
    "href": "pages/AI/Week01.html#introduction",
    "title": "A Decade of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nThe course provides a comprehensive exploration of the evolution and fundamental principles of Artificial Intelligence (AI). With historical roots reaching back to the 1300s, early attempts by figures like Jazari and Ramon Llull set the stage for the development of AI.\n\nCoined Terminology\nThe term “Artificial Intelligence” was officially coined by John McCarthy during the Dartmouth Conference in 1956. This landmark event, organized alongside Marvin Minsky and Claude Shannon, aimed to investigate the potential for machines to simulate human intelligence through precise descriptions.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#key-figures",
    "href": "pages/AI/Week01.html#key-figures",
    "title": "A Decade of Machine Learning",
    "section": "Key Figures",
    "text": "Key Figures\n\n1. John McCarthy\n\nCredited with Naming AI\nAssistant Professor at Dartmouth\nDesigner of Lisp Programming Language\nContributions to Logic and Common Sense Reasoning\n\n\n\n2. Marvin Minsky\n\nCo-founder of MIT AI Lab\nNotable for Frame Systems (Foundation of OOP)\nAuthor of “The Society of the Mind” and “The Emotional Machine”\n\n\n\n3. Nathaniel Rochester\n\nIBM Engineer\nDesigner of IBM 701\nSupervised Arthur Samuel and the Checkers-playing Program\n\n\n\n4. Claude Shannon\n\nFather of Information Theory\nMathematician at Bell Labs\n\n\n\n5. Herbert Simon and Allen Newell\n\nDevelopers of Logic Theorist (LT) Program\nPioneers in Symbolic AI\nIntroduction of Physical Symbol Systems\nSimon’s Diverse Scholarship (Nobel Prize in Economics)",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#physical-symbol-systems",
    "href": "pages/AI/Week01.html#physical-symbol-systems",
    "title": "A Decade of Machine Learning",
    "section": "Physical Symbol Systems",
    "text": "Physical Symbol Systems\n\nSymbolic Representation\nSymbol systems represent perceptible entities and adhere to formal laws, mirroring the structure of the physical world. Simon and Newell’s hypothesis posits that a Physical Symbol System is both necessary and sufficient for general intelligent action, distinguishing it from sub-symbolic AI, where information is stored in weights without explicit symbols.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#philosophical-considerations",
    "href": "pages/AI/Week01.html#philosophical-considerations",
    "title": "A Decade of Machine Learning",
    "section": "Philosophical Considerations",
    "text": "Philosophical Considerations\n\nCopernican Shift\n\nGalileo’s Distinction Between Thought and Reality\nGalileo Galilei’s intellectual endeavors were marked by a profound separation between the realm of thought and the objective reality. This conceptual wedge laid the foundation for a nuanced understanding of how human cognition interfaces with the external world.\n\n\nCopernicus’ Challenge to the Geocentric Model, Emphasizing Subjectivity\nCopernicus, through his revolutionary heliocentric model, not only challenged the prevailing geocentric view but also underscored the subjectivity inherent in our interpretations of celestial motions. This shift forced a reconsideration of humanity’s position in the cosmos.\n\n\nHuman Creation of Mental Models; Reality Comprising Fundamental Particles\nHumans engage in the active creation of mental models to comprehend the intricacies of reality. The Copernican Shift extends to the microscopic realm, where the abundant nature of fundamental particles renders them unsuitable as standalone elements of representation. Instead, reality is approached through disciplined ontologies, focusing on entities like atoms, molecules, or cells based on the context of study.\n\n\nIllustration through the Powers of Ten Film\nThe Powers of Ten film serves as a captivating medium to illustrate the Copernican Shift, visually portraying the vastness and intricacies of the universe at different scales. This cinematic exploration emphasizes the dynamic interplay between our mental representations and the expansive reality they seek to capture.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#representation-and-reasoning",
    "href": "pages/AI/Week01.html#representation-and-reasoning",
    "title": "A Decade of Machine Learning",
    "section": "Representation and Reasoning",
    "text": "Representation and Reasoning\n\nHuman Reasoning\n\nHuman Reasoning Involves Symbolic Representations\nIn the realm of human cognition, symbolic representations play a pivotal role in the process of reasoning. These symbols serve as cognitive tools that humans manipulate to make sense of the world around them.\n\n\nFundamental Particles Unsuitable as Elements of Representation due to Abundance\nDespite the fundamental nature of particles, their sheer abundance makes them impractical as elemental units of representation. Human cognition necessitates a selective focus, leading to the adoption of more manageable entities like atoms, molecules, or cells, depending on the specific domain of study.\n\n\nRepresentation Depends on the Focus of Study (e.g., Atoms, Molecules, Cells)\nThe choice of representation is intricately tied to the focus of study. Whether delving into the microscopic realm of atoms or exploring the complexity of biological systems at the cellular level, the selection of representational units is driven by the demands of the specific discipline.\n\n\nDiscipline-specific Ontologies Define Level of Detail in Representations\nDiscipline-specific ontologies play a crucial role in determining the level of detail embedded in representations. These structured frameworks provide a systematic approach to capturing and organizing knowledge within distinct domains.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-problem-solving",
    "href": "pages/AI/Week01.html#introduction-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Problem Solving",
    "text": "Introduction to Problem Solving\nIn the expansive domain of Artificial Intelligence (AI), problem-solving emerges as the orchestrated actions of autonomous agents navigating predefined objectives within dynamic environments. This course delves into the intricacies of problem-solving, elucidating the diverse methodologies encapsulated within search methods.\n\nProblem-Solving Framework\n\nAgent and Environment:\n\nAutonomous agents operate within a world defined by specific objectives and a repertoire of actions. Decision-making unfolds in real-time, navigating challenges posed by incomplete knowledge and the concurrent activities of other agents.\n\nSimplifying Assumptions:\n\nInitial simplifications envision a static world with a solitary agent making decisions, providing foundational insights into fundamental problem-solving principles.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "href": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Two Approaches to Problem Solving",
    "text": "Two Approaches to Problem Solving\n\n1. Model-Based Reasoning (Search Methods)\n\nDefinition:\nModel-Based Reasoning involves grounded reasoning in first principles or search approaches, wherein agents experiment with diverse actions to discern their efficacy.\n\n\nAssumptions:\nThis approach assumes a static world, complete knowledge, and actions that never fail, forming the foundational basis for problem-solving methodologies.\n\n\n\n2. Knowledge-Based Approach\n\nCharacteristics:\nThe Knowledge-Based Approach draws upon a societal structure rich in stored experiences, leveraging accumulated knowledge for effective problem-solving. It encompasses memory-based reasoning, case-based reasoning, and machine learning paradigms.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#rubiks-cube-example",
    "href": "pages/AI/Week01.html#rubiks-cube-example",
    "title": "A Decade of Machine Learning",
    "section": "Rubik’s Cube Example",
    "text": "Rubik’s Cube Example\nThe Rubik’s Cube serves as an illustrative example, elucidating the dichotomy between knowledge-based and search-based problem-solving approaches.\n\nLearning Dynamics\n\nInitial Challenge:\n\nThe Rubik’s Cube presents an initial challenge devoid of a known solution, necessitating exploratory actions.\n\nEvolution of Knowledge:\n\nOver time, individuals develop efficient solving methods through experiential learning, showcasing the adaptive nature of human problem-solving.\n\nDeep Reinforcement Learning:\n\nThe introduction of deep reinforcement learning emphasizes autonomous learning without human guidance, mirroring aspects of artificial intelligence.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#sudoku-example",
    "href": "pages/AI/Week01.html#sudoku-example",
    "title": "A Decade of Machine Learning",
    "section": "Sudoku Example",
    "text": "Sudoku Example\nThe Sudoku puzzle exemplifies the synergy between search and reasoning in problem-solving, offering insights into the nuanced interplay of diverse problem-solving methodologies.\n\nCombined Approach\n\nSearch Methods:\n\nBasic search algorithms, such as depth-first search and breadth-first search, lay the foundation for problem-solving endeavors.\n\nReasoning:\n\nReasoning techniques refine available options, harmonizing search methodologies with informed decision-making for a holistic problem-solving strategy.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "href": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Role of Logic in Problem Solving",
    "text": "Role of Logic in Problem Solving\nLogic, particularly first-order logic, assumes a pivotal role in representing knowledge and facilitating deductive reasoning within the problem-solving paradigm.\n\nLogical Components\n\nDeductive Reasoning:\n\nLogic functions as a tool for deductive reasoning, employing principles such as deduction, induction, abduction, and plausible reasoning to navigate complex problem spaces.\n\nConstraint Processing:\n\nLogic, search methods, and other reasoning approaches converge under the umbrella of constraint processing, offering a comprehensive framework for addressing intricate problem scenarios.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#map-coloring-problem",
    "href": "pages/AI/Week01.html#map-coloring-problem",
    "title": "A Decade of Machine Learning",
    "section": "Map Coloring Problem",
    "text": "Map Coloring Problem\nThe Map Coloring Problem stands as an exemplary challenge within AI, involving the assignment of colors to regions while adhering to specific constraints.\n\nConstraint Graph Representation\n\nGraph Transformation:\n\nRegions and their color preferences undergo a transformative process, manifesting as a constraint graph that encapsulates the intricacies of the problem.\n\nAlgorithmic Solutions:\n\nConstraint processing algorithms come to the forefront as viable solutions to graph-related problems, showcasing the practical application of logical problem-solving methodologies.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week01.html#key-takeaways",
    "href": "pages/AI/Week01.html#key-takeaways",
    "title": "A Decade of Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDecade of Machine Learning:\n\nThe surge in machine learning over the past decade is attributed to increased data availability, enhanced computing power, and advancements in training algorithms, particularly within the domain of deep learning. \n\nNeural Network Evolution:\n\nFrom the foundational perceptron era to the transformative deep neural networks in computer vision, the evolution of neural networks has played a pivotal role in shaping the landscape of AI.\n\nTraining Process:\n\nSupervised training, especially in medical diagnosis and face recognition, showcases the practical applications of deep neural networks in real-world scenarios.\n\nMachine Learning in Internet Interaction:\n\nUsers’ dynamic interaction with the internet transforms them into valuable data points, shaping the customization of ads and optimizing user experiences.\n\nPerformance vs. Competence:\n\nNeural networks exhibit proficiency in specific tasks but lack a holistic understanding of the world, highlighting the need for specialized training.\n\nGame of Go and Reinforcement Learning:\n\nThe triumph of AlphaGo exemplifies the success achievable through reinforcement learning, showcasing the capacity for autonomous learning without human intervention.\n\nHuman Cognitive Architecture:\n\nUnderstanding human cognitive abilities, symbolic reasoning, and the distinction between AI and machine learning provides insights into the complex realm of intelligence.\n\nHistory and Philosophy:\n\nThe historical roots of AI, key figures in AI development, and philosophical considerations underscore the interdisciplinary nature of artificial intelligence.\n\nPhysical Symbol Systems:\n\nSymbolic representation, as proposed by Simon and Newell, forms the basis for general intelligent action, distinguishing it from sub-symbolic AI.\n\nProblem Solving:\n\nTwo approaches, model-based reasoning and knowledge-based approaches, along with the role of logic, contribute to nuanced problem-solving methodologies.\n\nMap Coloring Problem:\n\nThe Map Coloring Problem serves as a concrete example, highlighting the integration of graph theory, constraint processing, and algorithmic solutions in logical problem-solving.",
    "crumbs": [
      "AI: Search Methods",
      "Week 1"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html",
    "href": "pages/AI/Week03.html",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "In the realm of artificial intelligence, the quest for efficient problem-solving algorithms has led to the development of heuristic search methods. Unlike blind search algorithms, which explore the search space without any sense of direction, heuristic search algorithms leverage domain-specific knowledge to guide their exploration towards promising regions.\n\n\n\nBlind search algorithms, such as Depth First Search (DFS), Breadth First Search (BFS), and Depth First Iterative Deepening (DFID), navigate the search space without considering the location of the goal. These algorithms follow predetermined trajectories, regardless of the goal’s position.\n\n\n\nHeuristic search introduces a sense of direction by incorporating heuristic functions, which estimate the distance of each node from the goal. This allows the algorithm to prioritize nodes that are closer to the goal, leading to more efficient exploration of the search space.\n\n\nNature often provides inspiration for solving complex problems. In the case of heuristic search, the concept of gravity serves as a metaphor. Similar to how water flows downhill, guided by the pull of gravity, heuristic search algorithms aim to “flow” towards regions with lower estimated distances to the goal.\n\n\n\nA crucial component of heuristic search is the heuristic function, denoted as \\(h(N)\\), which assigns a numerical value to each node representing its estimated distance from the goal. These values guide the search algorithm in selecting the most promising nodes for exploration.\n\n\n\n\nHeuristic functions can vary based on the problem domain and the specific characteristics of the problem being solved. Two common types of heuristic functions are:\n\n\nThe Hamming distance heuristic, denoted as \\(h_1\\), counts the number of elements that are out of place in a given state compared to the goal state. It provides a simple measure of proximity to the goal, where lower values indicate states that are closer to the goal.\n\n\n\nThe Manhattan distance heuristic calculates the total distance that each element must move to reach its goal position. It is particularly useful for grid-based problems where movements are restricted to horizontal and vertical directions. The Manhattan distance is computed as follows:\n\\[\nh_{\\text{Manhattan}}(N) = \\sum_{i=1}^{n} \\left| x_i - x_{\\text{goal}} \\right| + \\left| y_i - y_{\\text{goal}} \\right|\n\\]\nwhere \\((x_i, y_i)\\) represents the coordinates of element \\(i\\) in the current state \\(N\\), and \\((x_{\\text{goal}}, y_{\\text{goal}})\\) represents the coordinates of the goal position.\n\n\n\n\nHeuristic search algorithms, such as Best First Search, find applications in various domains, including route finding, puzzle solving, and optimization problems. These algorithms leverage heuristic functions to efficiently navigate large search spaces and find optimal solutions.\n\n\nThe Best First Search algorithm prioritizes nodes for exploration based on their heuristic values, aiming to minimize the estimated distance to the goal. The algorithm operates as follows:\n\nOPEN &lt;- (S, null, h(S)) : []\nCLOSED &lt;- empty list\nwhile OPEN is not empty\n\nnodePair &lt;- head OPEN\n(N, _, _) &lt;- nodePair\nif GoalTest(N) = TRUE\n\nreturn RECONSTRUCTPATH(nodePair, CLOSED)\n\nelse CLOSED &lt;- nodePair : CLOSED\n\nneighbours &lt;- MoveGen(N)\nnewNodes &lt;- REMOVESEEN(neighbours, OPEN, CLOSED)\nnewPairs &lt;- MAKEPAIRS(newNodes, N)\n\nOPEN &lt;- sort_h(newPairs ++ tail OPEN)\n\nreturn empty list\n\n\n\n\n\nHeuristic search algorithms are applied to real-world problems to find optimal solutions efficiently. Two examples illustrate the application of heuristic search in different domains:\n\n\nIn geographical route finding, heuristic search aids in identifying optimal routes between locations. By considering factors such as distance and terrain, the algorithm navigates the search space to find the most efficient path from the start to the goal location.\n\n\n\nHeuristic search algorithms are commonly used to solve puzzles, such as the Eight Puzzle. By evaluating the heuristic value of each state, the algorithm explores the search space to find the shortest path to the goal state, minimizing the number of moves required to solve the puzzle.\n\n\n\n\nWhile heuristic search algorithms offer a principled approach to problem-solving, several considerations and limitations should be taken into account:\n\nEffectiveness of Heuristic Functions: The performance of heuristic search algorithms heavily depends on the accuracy of the heuristic functions employed. Imperfect heuristics may lead to suboptimal solutions or increased computational overhead.\nComplexity of the Environment: Heuristic search algorithms may struggle to navigate complex environments with obstacles or constraints that are not fully captured by the heuristic function. In such cases, the algorithm’s performance may be suboptimal.\nTrade-off between Efficiency and Optimality: Heuristic search algorithms aim to strike a balance between exploration efficiency and solution optimality. While these algorithms prioritize exploration towards promising regions, they may not always guarantee finding the shortest path to the goal.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#introduction",
    "href": "pages/AI/Week03.html#introduction",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "In the realm of artificial intelligence, the quest for efficient problem-solving algorithms has led to the development of heuristic search methods. Unlike blind search algorithms, which explore the search space without any sense of direction, heuristic search algorithms leverage domain-specific knowledge to guide their exploration towards promising regions.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#blind-search-algorithms",
    "href": "pages/AI/Week03.html#blind-search-algorithms",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "Blind search algorithms, such as Depth First Search (DFS), Breadth First Search (BFS), and Depth First Iterative Deepening (DFID), navigate the search space without considering the location of the goal. These algorithms follow predetermined trajectories, regardless of the goal’s position.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#heuristic-search-a-sense-of-direction",
    "href": "pages/AI/Week03.html#heuristic-search-a-sense-of-direction",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "Heuristic search introduces a sense of direction by incorporating heuristic functions, which estimate the distance of each node from the goal. This allows the algorithm to prioritize nodes that are closer to the goal, leading to more efficient exploration of the search space.\n\n\nNature often provides inspiration for solving complex problems. In the case of heuristic search, the concept of gravity serves as a metaphor. Similar to how water flows downhill, guided by the pull of gravity, heuristic search algorithms aim to “flow” towards regions with lower estimated distances to the goal.\n\n\n\nA crucial component of heuristic search is the heuristic function, denoted as \\(h(N)\\), which assigns a numerical value to each node representing its estimated distance from the goal. These values guide the search algorithm in selecting the most promising nodes for exploration.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#types-of-heuristic-functions",
    "href": "pages/AI/Week03.html#types-of-heuristic-functions",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "Heuristic functions can vary based on the problem domain and the specific characteristics of the problem being solved. Two common types of heuristic functions are:\n\n\nThe Hamming distance heuristic, denoted as \\(h_1\\), counts the number of elements that are out of place in a given state compared to the goal state. It provides a simple measure of proximity to the goal, where lower values indicate states that are closer to the goal.\n\n\n\nThe Manhattan distance heuristic calculates the total distance that each element must move to reach its goal position. It is particularly useful for grid-based problems where movements are restricted to horizontal and vertical directions. The Manhattan distance is computed as follows:\n\\[\nh_{\\text{Manhattan}}(N) = \\sum_{i=1}^{n} \\left| x_i - x_{\\text{goal}} \\right| + \\left| y_i - y_{\\text{goal}} \\right|\n\\]\nwhere \\((x_i, y_i)\\) represents the coordinates of element \\(i\\) in the current state \\(N\\), and \\((x_{\\text{goal}}, y_{\\text{goal}})\\) represents the coordinates of the goal position.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#application-of-heuristic-search",
    "href": "pages/AI/Week03.html#application-of-heuristic-search",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "Heuristic search algorithms, such as Best First Search, find applications in various domains, including route finding, puzzle solving, and optimization problems. These algorithms leverage heuristic functions to efficiently navigate large search spaces and find optimal solutions.\n\n\nThe Best First Search algorithm prioritizes nodes for exploration based on their heuristic values, aiming to minimize the estimated distance to the goal. The algorithm operates as follows:\n\nOPEN &lt;- (S, null, h(S)) : []\nCLOSED &lt;- empty list\nwhile OPEN is not empty\n\nnodePair &lt;- head OPEN\n(N, _, _) &lt;- nodePair\nif GoalTest(N) = TRUE\n\nreturn RECONSTRUCTPATH(nodePair, CLOSED)\n\nelse CLOSED &lt;- nodePair : CLOSED\n\nneighbours &lt;- MoveGen(N)\nnewNodes &lt;- REMOVESEEN(neighbours, OPEN, CLOSED)\nnewPairs &lt;- MAKEPAIRS(newNodes, N)\n\nOPEN &lt;- sort_h(newPairs ++ tail OPEN)\n\nreturn empty list",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#real-world-examples",
    "href": "pages/AI/Week03.html#real-world-examples",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "Heuristic search algorithms are applied to real-world problems to find optimal solutions efficiently. Two examples illustrate the application of heuristic search in different domains:\n\n\nIn geographical route finding, heuristic search aids in identifying optimal routes between locations. By considering factors such as distance and terrain, the algorithm navigates the search space to find the most efficient path from the start to the goal location.\n\n\n\nHeuristic search algorithms are commonly used to solve puzzles, such as the Eight Puzzle. By evaluating the heuristic value of each state, the algorithm explores the search space to find the shortest path to the goal state, minimizing the number of moves required to solve the puzzle.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#considerations-and-limitations",
    "href": "pages/AI/Week03.html#considerations-and-limitations",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "",
    "text": "While heuristic search algorithms offer a principled approach to problem-solving, several considerations and limitations should be taken into account:\n\nEffectiveness of Heuristic Functions: The performance of heuristic search algorithms heavily depends on the accuracy of the heuristic functions employed. Imperfect heuristics may lead to suboptimal solutions or increased computational overhead.\nComplexity of the Environment: Heuristic search algorithms may struggle to navigate complex environments with obstacles or constraints that are not fully captured by the heuristic function. In such cases, the algorithm’s performance may be suboptimal.\nTrade-off between Efficiency and Optimality: Heuristic search algorithms aim to strike a balance between exploration efficiency and solution optimality. While these algorithms prioritize exploration towards promising regions, they may not always guarantee finding the shortest path to the goal.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#overview",
    "href": "pages/AI/Week03.html#overview",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Overview",
    "text": "Overview\nHill climbing algorithms are designed to navigate through the search space of a problem by gradually improving upon the current solution. The algorithm begins with an initial solution and iteratively explores neighboring solutions, moving towards the one that maximizes (or minimizes) an objective function, also known as a heuristic evaluation function. The process continues until a local optimum (or maximum) is reached, where no neighboring solution yields a better result.\n\nAlgorithmic Framework\nThe basic framework of the hill climbing algorithm can be outlined as follows:\n\nInitialization: Start with an initial solution \\(S\\).\nMain Loop: Repeat the following steps until no better solution can be found:\n\nExploration: Generate neighboring solutions from the current solution \\(N\\).\nEvaluation: Evaluate each neighboring solution using a heuristic function \\(h(N)\\).\nSelection: Move to the neighboring solution \\(N\\) that maximizes (or minimizes) the heuristic function.\n\nTermination: Return the best solution found.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#hill-climbing-algorithm",
    "href": "pages/AI/Week03.html#hill-climbing-algorithm",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Hill-Climbing Algorithm",
    "text": "Hill-Climbing Algorithm\n\nPseudocode\nThe hill climbing algorithm can be represented in pseudocode as follows:\nN &lt;- S\ndo bestEver &lt;- N\nN &lt;- head(sort_h(MOVEGEN(bestEver)))\nwhile h(N) is better than h(bestEver)\n    bestEver &lt;- N\n    N &lt;- head(sort_h(MOVEGEN(bestEver)))\nreturn bestEver\nHere, \\(S\\) represents the initial solution, \\(N\\) represents the current solution, \\(h(N)\\) is the heuristic evaluation function, and \\(MOVEGEN\\) generates neighboring solutions. The algorithm iteratively updates the current solution to the best neighboring solution until no better solution can be found.\n\n\nDetailed Explanation\n\nInitialization: Set \\(N\\) to the initial solution \\(S\\).\nMain Loop:\n\nSet \\(\\text{bestEver}\\) to \\(N\\) to keep track of the best solution found so far.\nGenerate neighboring solutions from \\(\\text{bestEver}\\) using the \\(\\text{MOVEGEN}\\) function.\nSort the generated solutions based on the heuristic function \\(h(N)\\) and select the best one as the new current solution \\(N\\).\nRepeat the process until no better solution can be found.\n\nTermination: Return the best solution found, stored in \\(\\text{bestEver}\\).",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#advantages-and-disadvantages",
    "href": "pages/AI/Week03.html#advantages-and-disadvantages",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\n\nAdvantages\n\nEfficiency: Hill climbing is computationally efficient, especially in problems with a large search space, as it only explores neighboring solutions.\nSimplicity: The algorithm is straightforward to implement and understand, making it accessible for various optimization tasks.\nConstant Space Complexity: It requires constant memory space, making it suitable for resource-constrained environments.\n\n\n\nDisadvantages\n\nLocal Optima: Hill climbing algorithms are prone to getting stuck in local optima, failing to find the global optimum if present.\nLimited Scope: Due to its greedy nature, hill climbing may overlook better solutions that require moving away from the current solution.\nHeuristic Dependence: The effectiveness of hill climbing heavily relies on the quality of the heuristic function used, which may not always accurately guide the search.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#applications",
    "href": "pages/AI/Week03.html#applications",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Applications",
    "text": "Applications\nHill climbing algorithms find applications in various domains where optimization is required. Some common applications include:\n\nPuzzle Solving: In puzzles like the 8 puzzle or Rubik’s cube, hill climbing can be used to find solutions by navigating through the state space.\nOptimization Problems: Hill climbing is employed in optimization tasks such as scheduling, routing, and resource allocation to find near-optimal solutions within a limited time frame.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#extensions-and-alternatives",
    "href": "pages/AI/Week03.html#extensions-and-alternatives",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Extensions and Alternatives",
    "text": "Extensions and Alternatives\n\nDeterministic Methods\n\nSimulated Annealing: A probabilistic optimization technique that allows the algorithm to escape local optima by occasionally accepting worse solutions based on a temperature parameter.\nGenetic Algorithms: Inspired by the process of natural selection, genetic algorithms explore the search space through a population of candidate solutions, allowing for diversity and exploration.\n\n\n\nRandomized Methods\n\nRandom Restart Hill Climbing: A variant of hill climbing that periodically restarts the search from different initial solutions to overcome local optima.\nTabu Search: An iterative search method that uses memory structures to avoid revisiting previously explored solutions, enhancing exploration capabilities.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#solution-space-search-1",
    "href": "pages/AI/Week03.html#solution-space-search-1",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Solution Space Search",
    "text": "Solution Space Search\nIn the realm of solution space search, the focus is on formulating the problem in such a way that finding the goal node directly corresponds to discovering the solution. This approach simplifies the search process by eliminating the need for reconstructing the solution path.\n\nDefinition\nSolution space search involves defining the search problem in a manner where reaching the goal node signifies finding the solution. This formulation streamlines the search process, as each node in the search space represents a potential solution candidate.\n\n\nConfiguration Problems\nConfiguration problems align seamlessly with the concept of solution space search, as every node in the search space serves as a candidate solution. The evaluation of candidate solutions is based on their adherence to the goal description.\n\n\nPlanning Problems\nEven planning problems can be tackled using solution space search techniques, wherein each node represents a candidate plan. This approach, known as plan space planning, enables the exploration of various planning strategies to achieve the desired outcome.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#synthesis-vs.-perturbation",
    "href": "pages/AI/Week03.html#synthesis-vs.-perturbation",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Synthesis vs. Perturbation",
    "text": "Synthesis vs. Perturbation\nIn solution space search, two fundamental approaches are employed: synthesis and perturbation. These methods offer distinct strategies for generating and evaluating candidate solutions.\n\nSynthesis Methods\nSynthesis methods adopt a constructive approach, wherein the solution is built incrementally from an initial state. For instance, in problems like the N-Queen problem, the solution is constructed piece by piece, gradually moving towards the goal state.\n\n\nPerturbation Methods\nPerturbation methods involve modifying existing candidate solutions to explore alternative paths in the search space. By introducing changes such as shuffling arrays or altering solution representations, perturbation techniques generate new candidate solutions for evaluation.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#sat-problem-boolean-satisfiability-problem",
    "href": "pages/AI/Week03.html#sat-problem-boolean-satisfiability-problem",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "SAT Problem (Boolean Satisfiability Problem)",
    "text": "SAT Problem (Boolean Satisfiability Problem)\nThe Boolean Satisfiability Problem, commonly referred to as SAT, is a fundamental problem in computer science and artificial intelligence. It involves determining whether a given Boolean formula can be satisfied by assigning truth values to its variables.\n\nProblem Statement\nGiven a Boolean formula comprising propositional variables, the task is to find an assignment of truth values to these variables such that the formula evaluates to true. This problem is often studied in conjunctive normal form (CNF), where the formula consists of clauses connected by conjunctions.\n\n\nComplexity Analysis\nSAT is classified as NP-complete, indicating its high computational complexity. While verifying a solution can be done in polynomial time, finding the solution itself often requires exponential time, rendering brute force approaches impractical for large instances of the problem.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#traveling-salesperson-problem-tsp",
    "href": "pages/AI/Week03.html#traveling-salesperson-problem-tsp",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Traveling Salesperson Problem (TSP)",
    "text": "Traveling Salesperson Problem (TSP)\nThe Traveling Salesperson Problem is another classic problem in the realm of optimization and combinatorial optimization. It involves finding the shortest possible tour that visits each city exactly once and returns to the starting city.\n\nProblem Definition\nIn the TSP, a set of cities is given, along with the distances between each pair of cities. The objective is to determine the optimal tour that minimizes the total distance traveled while visiting each city exactly once.\n\n\nComplexity Analysis\nTSP is categorized as NP-hard, indicating its high computational complexity similar to SAT. The problem requires factorial time to solve, as the number of possible tours grows exponentially with the number of cities.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#greedy-constructive-methods-for-tsp",
    "href": "pages/AI/Week03.html#greedy-constructive-methods-for-tsp",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Greedy Constructive Methods for TSP",
    "text": "Greedy Constructive Methods for TSP\nIn tackling the TSP, various heuristic algorithms are employed to construct feasible solutions. Greedy constructive methods prioritize efficiency by iteratively adding elements to the solution based on certain criteria.\n\nNearest Neighbor Heuristic\nThe Nearest Neighbor Heuristic is a simple yet effective approach that starts from a chosen city and iteratively selects the nearest unvisited city as the next destination. While intuitive, this method may not always produce optimal solutions.\n\n\nGreedy Heuristic\nThe Greedy Heuristic operates similarly to Kruskal’s algorithm for finding minimum spanning trees. It selects edges with the shortest distance and adds them to the tour, avoiding the creation of smaller loops.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#savings-heuristic-for-tsp",
    "href": "pages/AI/Week03.html#savings-heuristic-for-tsp",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Savings Heuristic for TSP",
    "text": "Savings Heuristic for TSP\nThe Savings Heuristic is a popular approach for solving the TSP, particularly in scenarios where efficiency is paramount. This method leverages savings in cost to guide the construction of the tour.\n\nMethodology\nThe Savings Heuristic begins by creating tours of length 2 anchored on a base vertex. It then performs merge operations to combine these tours, optimizing the total cost while ensuring the connectivity of the tour.\n\n\nImplementation\nBy iteratively merging tours and maximizing cost savings, the Savings Heuristic generates feasible solutions that often exhibit competitive performance compared to other methods.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#perturbation-operators-for-tsp",
    "href": "pages/AI/Week03.html#perturbation-operators-for-tsp",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Perturbation Operators for TSP",
    "text": "Perturbation Operators for TSP\nIn addition to constructive methods, perturbation operators play a crucial role in exploring alternative solutions within the search space of the TSP. These operators facilitate the generation of diverse candidate solutions through systematic modifications.\n\nTour City Exchange\nThe Tour City Exchange operator involves swapping the positions of two cities in the tour sequence. By rearranging the order of cities, this operator explores different tour configurations within the search space.\n\n\nEdge Exchange\nAlternatively, the Edge Exchange operator focuses on modifying the edges in the tour rather than the cities themselves. By rearranging the connectivity between cities, this operator aims to improve the overall tour quality.\n\n\nThree Edge Exchange\nFor more significant modifications, the Three Edge Exchange operator removes three edges from the tour and reconstructs the tour based on the remaining connectivity. This operator enables the exploration of alternative tour structures.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#complexity-analysis-of-sat-and-tsp",
    "href": "pages/AI/Week03.html#complexity-analysis-of-sat-and-tsp",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Complexity Analysis of SAT and TSP",
    "text": "Complexity Analysis of SAT and TSP\nBoth SAT and TSP pose significant computational challenges due to their inherent complexity and large search spaces. Understanding the computational complexity of these problems is crucial for devising efficient solution approaches.\n\nSAT Complexity\nSAT is classified as NP-complete, indicating that it requires exponential time to solve in the worst case. Despite being verifiable in polynomial time, finding the solution itself often involves exhaustive search or heuristic methods.\n\n\nTSP Complexity\nSimilarly, TSP is categorized as NP-hard, implying that it requires factorial time to solve as\nthe problem size increases. The exponential growth in the number of possible tours presents a formidable challenge for exact solution techniques.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#time-complexity-analysis",
    "href": "pages/AI/Week03.html#time-complexity-analysis",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Time Complexity Analysis",
    "text": "Time Complexity Analysis\nThe time complexity of solving SAT and TSP instances is a critical consideration, particularly when dealing with large-scale problem instances. Understanding the computational limitations is essential for selecting appropriate solution strategies.\n\nSAT Time Complexity\nFor SAT instances, the time required to find a solution increases exponentially with the number of variables and clauses. Even with efficient algorithms, solving large instances of SAT may require significant computational resources.\n\n\nTSP Time Complexity\nSimilarly, the time complexity of solving TSP instances grows factorially with the number of cities. Despite the existence of heuristic methods, exact solution techniques for TSP remain impractical for instances with a large number of cities.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#exploration-in-search",
    "href": "pages/AI/Week03.html#exploration-in-search",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Exploration in Search",
    "text": "Exploration in Search\nWhile hill climbing efficiently exploits local gradients, it lacks the capability to explore diverse regions of the search space. To overcome this limitation, exploration becomes imperative. Exploration involves deviating from the current trajectory to uncover new paths that may lead to superior solutions.\n\nNeed for Exploration\n\nEscaping Local Optima: Exploration is necessary to escape local optima and discover potentially better solutions.\nHeuristic Limitations: Relying solely on heuristic functions may restrict the search to familiar regions, hindering exploration.\nBalancing Exploitation and Exploration: A balanced approach is required to ensure both exploitation and exploration are effectively utilized in the search process.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#beam-search",
    "href": "pages/AI/Week03.html#beam-search",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Beam Search",
    "text": "Beam Search\nBeam search represents a simple yet effective strategy to augment exploration in the search space. Instead of focusing solely on the best neighbor, beam search considers multiple options at each level of the search. By maintaining a beam width parameter, the algorithm keeps track of the best candidate solutions, increasing the likelihood of discovering the goal node.\n\nExploration Strategy\n\nConsideration of Multiple Candidates: Beam search diverges from the traditional approach by considering multiple candidate solutions simultaneously.\nBeam Width Parameter: The beam width parameter dictates the number of candidates retained at each level of the search.\nEnhanced Exploration: By maintaining multiple candidates, beam search explores diverse solution paths, fostering exploration in the search space.\n\n\n\nPseudocode\n\nOPEN ← S : []\nN ← S\ndo bestEver ← N\n\nif GOAL-TEST(OPEN) = TRUE\nthen return goal from OPEN\nelse neighbours ← MOVE-GEN(OPEN)\n\nOPEN ← take w (sort neighbours)\nN ← head OPEN ▷ best in new layer\n\n\nwhile h(N) is better than h(bestEver)\nreturn bestEver",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#variable-neighborhood-descent-vnd",
    "href": "pages/AI/Week03.html#variable-neighborhood-descent-vnd",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Variable Neighborhood Descent (VND)",
    "text": "Variable Neighborhood Descent (VND)\nVariable neighborhood descent offers a sophisticated approach to balance exploitation and exploration by sequentially employing different neighborhood functions. This adaptive strategy allows the algorithm to transition from sparse to denser neighborhoods as the search progresses, effectively navigating the search space while optimizing computational resources.\n\nAlgorithm\n\nSequential Neighborhood Exploration: VND iteratively explores different neighborhood functions to traverse the search space.\nAdaptive Strategy: The algorithm dynamically adjusts the neighborhood density based on the search progress.\nOptimizing Resource Usage: By varying neighborhood functions, VND optimizes computational resources while maintaining search efficiency.\n\n\n\nPseudocode\n\nMoveGenList ← MOVEGEN1 : MOVEGEN2 : ... : MOVEGENn : []\nbestNode ← S\nwhile MoveGenList is not empty\n\nbestNode ← HILL-CLIMBING(bestNode, head MoveGenList)\nMoveGenList ← tail MoveGenList\n\nreturn bestNode",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#best-neighbor-search",
    "href": "pages/AI/Week03.html#best-neighbor-search",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Best Neighbor Search",
    "text": "Best Neighbor Search\nIn contrast to traditional hill climbing, which moves only to better neighbors, the best neighbor search algorithm considers moving to the best neighbor regardless of improvement. This approach introduces variability in the search process, potentially leading to exploration of alternative solution paths.\n\nExploration Strategy\n\nDiverse Solution Paths: Best neighbor search explores diverse solution paths by considering the best neighbor at each step.\nVaried Movement: Unlike traditional hill climbing, which moves strictly to better neighbors, this algorithm allows for movement to any best neighbor, regardless of improvement.\n\n\n\nPseudocode\n\nN ← S\nbestSeen ← S\nuntil some termination condition\n\nN ← best MOVEGEN(N)\nif N is better than bestSeen\n\nbestSeen ← N\n\n\nreturn bestSeen",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#iterated-hill-climbing",
    "href": "pages/AI/Week03.html#iterated-hill-climbing",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Iterated Hill Climbing",
    "text": "Iterated Hill Climbing\nIterated hill climbing presents a randomized approach to local search, leveraging multiple iterations from randomly chosen starting points. By diversifying the starting points, this algorithm enhances exploration, increasing the likelihood of finding global optima.\n\nRandomized Exploration\n\nDiversified Starting Points: Iterated hill climbing initiates multiple search iterations from random starting points.\nExploration Enhancement: By exploring from different starting points, the algorithm increases the chances of discovering optimal solutions.\n\n\n\nPseudocode\n\nbestNode ← random candidate solution\nrepeat N times\ncurrentBest ← HILL-CLIMBING(new random candidate solution)\nif h(currentBest) is better than h(bestNode)\nbestNode ← currentBest\nreturn bestNode",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/AI/Week03.html#points-to-remember",
    "href": "pages/AI/Week03.html#points-to-remember",
    "title": "Exploring Heuristic Search and Optimization Techniques",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nHeuristic Search Methods:\n\nHeuristic search algorithms leverage domain-specific knowledge to guide exploration towards promising regions in the search space.\nUnlike blind search algorithms, heuristic search methods incorporate heuristic functions to estimate the distance to the goal and prioritize exploration accordingly.\n\nTypes of Heuristic Functions:\n\nHamming distance and Manhattan distance are common heuristic functions used in various problem domains.\nThese functions provide estimates of proximity to the goal, guiding the search algorithm towards optimal solutions.\n\nApplication of Heuristic Search:\n\nHeuristic search algorithms find applications in route finding, puzzle solving, optimization, and various other domains requiring efficient problem-solving techniques.\nBest First Search is a prominent heuristic search algorithm that prioritizes exploration based on heuristic values.\n\nHill Climbing Algorithm:\n\nHill climbing is a local search algorithm used for optimization problems, aiming to find the best possible solution by iteratively moving towards higher-elevation points in the search space.\nIt is prone to getting stuck in local optima and relies heavily on the quality of the heuristic function.\n\nSolution Space Search:\n\nSolution space search involves exploring potential solutions within a defined search space, with each node representing a candidate solution.\nConfiguration problems and planning problems can be addressed using solution space search techniques.\n\nComplexity Analysis:\n\nProblems like SAT and TSP are classified as NP-complete and NP-hard, respectively, indicating their high computational complexity.\nExact solution techniques for these problems often require exponential time, making heuristic and approximate methods essential.\n\nDeterministic Local Search:\n\nDeterministic local search methods, such as beam search, variable neighborhood descent, and iterated hill climbing, balance exploitation and exploration to navigate the search space efficiently.\nThese methods offer strategies to avoid local optima and enhance exploration by considering diverse solution paths.\n\nExploration in Search:\n\nExploration is crucial for escaping local optima and discovering superior solutions.\nMethods like beam search and iterated hill climbing introduce variability in the search process to explore alternative solution paths.\n\nPerturbation Operators:\n\nPerturbation operators, such as tour city exchange and edge exchange, facilitate the generation of diverse candidate solutions in optimization problems like the TSP.\n\nEfficiency and Optimality Trade-off:\n\nHeuristic search algorithms aim to strike a balance between exploration efficiency and solution optimality.\nWhile prioritizing exploration towards promising regions, these algorithms may not always guarantee finding the shortest path to the goal.",
    "crumbs": [
      "AI: Search Methods",
      "Week 3"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html",
    "href": "pages/SE/Week02.html",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "",
    "text": "In the realm of software engineering, meticulous attention to the identification and documentation of software requirements is paramount. Failure to allocate sufficient time and effort to this process can result in a substantial misalignment between user expectations and the actual deliverables produced by developers. The significance of requirement identification lies in its twofold purpose: firstly, to comprehend the desires of the customers and, secondly, to ensure a harmonious agreement between the developers’ understanding and the customers’ expectations. This mutual understanding is crucial, as any deviation may lead to a pronounced escalation in development costs.\n\n\n\nThe initiation of the requirement identification process involves a comprehensive understanding of the customers, who may range from internal to external stakeholders with diverse roles and profiles. Categorizing users into primary, secondary, and tertiary roles provides a structured approach:\n\nPrimary Users: Directly interact with the system. Examples include independent sellers, sales teams of consumer companies, authors, and publishers.\nSecondary Users: Utilize the system through an intermediary. For instance, sales managers who monitor sales numbers and profits.\nTertiary Users: Affected by the system without direct interaction. This category encompasses entities like logistics/shipping companies, banks, and buyers on platforms such as Amazon.\n\n\n\n\nThe process of gathering requirements necessitates a systematic approach, employing methods such as basic interviews, studying existing documentation, conducting focus groups, and observing user interactions. This systematic gathering ensures a holistic understanding of the diverse needs of users.\n\n\n\nThe intricacies of requirement gathering present various challenges, including:\n\nDiverse Stakeholder Contributions: Different stakeholders may contribute varied and potentially conflicting requirements, necessitating careful consideration and resolution.\nAmbiguity: Ambiguous requirements arise when terms, such as “manage,” are subject to diverse interpretations by users and developers. Resolving such ambiguity is vital for precision.\nInconsistency: Inconsistencies or contradictions in requirements, such as conflicting payment crediting frequencies, require resolution to maintain coherence in system development.\nIncompleteness: Some requirements may be incomplete, overlooking crucial aspects of implementation. This underscores the need for clarity and thoroughness in the gathering process.\n\n\n\n\nOnce requirements are gathered, a meticulous analysis is imperative. This involves:\n\nClarifying Ambiguities: Ensuring a shared understanding of terms and concepts to prevent diverse interpretations.\nResolving Inconsistencies: Addressing conflicts or contradictions in requirements to maintain coherence in the development process.\nCompleting Requirements: Ensuring that all aspects of implementation are considered to avoid oversights and incompleteness.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#importance-of-requirement-identification",
    "href": "pages/SE/Week02.html#importance-of-requirement-identification",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "",
    "text": "In the realm of software engineering, meticulous attention to the identification and documentation of software requirements is paramount. Failure to allocate sufficient time and effort to this process can result in a substantial misalignment between user expectations and the actual deliverables produced by developers. The significance of requirement identification lies in its twofold purpose: firstly, to comprehend the desires of the customers and, secondly, to ensure a harmonious agreement between the developers’ understanding and the customers’ expectations. This mutual understanding is crucial, as any deviation may lead to a pronounced escalation in development costs.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#process-of-requirement-identification",
    "href": "pages/SE/Week02.html#process-of-requirement-identification",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "",
    "text": "The initiation of the requirement identification process involves a comprehensive understanding of the customers, who may range from internal to external stakeholders with diverse roles and profiles. Categorizing users into primary, secondary, and tertiary roles provides a structured approach:\n\nPrimary Users: Directly interact with the system. Examples include independent sellers, sales teams of consumer companies, authors, and publishers.\nSecondary Users: Utilize the system through an intermediary. For instance, sales managers who monitor sales numbers and profits.\nTertiary Users: Affected by the system without direct interaction. This category encompasses entities like logistics/shipping companies, banks, and buyers on platforms such as Amazon.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#methods-for-requirement-gathering",
    "href": "pages/SE/Week02.html#methods-for-requirement-gathering",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "",
    "text": "The process of gathering requirements necessitates a systematic approach, employing methods such as basic interviews, studying existing documentation, conducting focus groups, and observing user interactions. This systematic gathering ensures a holistic understanding of the diverse needs of users.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#challenges-in-requirement-gathering",
    "href": "pages/SE/Week02.html#challenges-in-requirement-gathering",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "",
    "text": "The intricacies of requirement gathering present various challenges, including:\n\nDiverse Stakeholder Contributions: Different stakeholders may contribute varied and potentially conflicting requirements, necessitating careful consideration and resolution.\nAmbiguity: Ambiguous requirements arise when terms, such as “manage,” are subject to diverse interpretations by users and developers. Resolving such ambiguity is vital for precision.\nInconsistency: Inconsistencies or contradictions in requirements, such as conflicting payment crediting frequencies, require resolution to maintain coherence in system development.\nIncompleteness: Some requirements may be incomplete, overlooking crucial aspects of implementation. This underscores the need for clarity and thoroughness in the gathering process.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#requirement-analysis",
    "href": "pages/SE/Week02.html#requirement-analysis",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "",
    "text": "Once requirements are gathered, a meticulous analysis is imperative. This involves:\n\nClarifying Ambiguities: Ensuring a shared understanding of terms and concepts to prevent diverse interpretations.\nResolving Inconsistencies: Addressing conflicts or contradictions in requirements to maintain coherence in the development process.\nCompleting Requirements: Ensuring that all aspects of implementation are considered to avoid oversights and incompleteness.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#introduction",
    "href": "pages/SE/Week02.html#introduction",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software engineering, the process of requirement gathering is fundamental to understanding and delineating the needs of a diverse user base. In this module, we delve into the intricacies of identifying and analyzing requirements, using the Amazon Seller portal as a practical example.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#requirement-collection-techniques",
    "href": "pages/SE/Week02.html#requirement-collection-techniques",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Requirement Collection Techniques",
    "text": "Requirement Collection Techniques\n\nQuestionnaires\n\nDefinition\nA questionnaire is a systematic tool comprising a series of questions designed to extract precise information from users.\n\n\nApplication\nThis technique proves particularly effective when engaging with a broad user base dispersed across varying geographical locations. For instance, surveying sales team managers on their online selling experiences can yield valuable insights.\n\n\n\nInterviews\n\nDefinition\nInterviews involve posing a set of questions to users, categorized as either structured, unstructured, or semi-structured.\n\n\nPurpose\nStructured interviews adhere to a predefined set of questions, while unstructured interviews are more flexible, allowing for exploration based on user responses. These interviews serve the purpose of understanding user issues and eliciting diverse scenarios. For example, probing independent sellers on platform preferences elucidates crucial insights.\n\n\n\nFocus Groups\n\nDefinition\nFocus groups bring together stakeholders to engage in discussions concerning system issues and requirements.\n\n\nAdvantages\nThis technique facilitates consensus-building and identifies areas of conflict or disagreement. By conducting focus groups with stakeholders from different industries, varied expectations and requirements can be uncovered.\n\n\n\nObservations\n\nDefinition\nObservations entail spending time with stakeholders in their natural settings, shadowing them, and noting their day-to-day tasks.\n\n\nApplication\nBy observing how tasks are performed, such as the selling process in physical shops, requirements for the online setting can be extrapolated. For instance, understanding customer interactions in a physical shop aids in the design of online recommendation systems.\n\n\n\nDocumentation\n\nDefinition\nDocumentation refers to written procedures, rules, manuals, or regulations that provide guidelines for specific tasks.\n\n\nSignificance\nIn the context of a seller portal, compliance with bank regulations, such as adding seller accounts and handling monetary transactions, is crucial. Documentation aids in understanding and incorporating these requirements.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#summary-of-requirement-gathering-techniques",
    "href": "pages/SE/Week02.html#summary-of-requirement-gathering-techniques",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Summary of Requirement Gathering Techniques",
    "text": "Summary of Requirement Gathering Techniques\nIn summary, various requirement gathering techniques serve distinct purposes:\n\nQuestionnaires: Suited for obtaining specific answers.\nInterviews: Effective in exploring issues and scenarios.\nFocus Groups: Facilitate collection of multiple viewpoints.\nObservations: Provide insights into the context of user tasks.\nDocumentation: Offer guidelines through procedures, regulations, and standards.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#requirement-gathering-guidelines",
    "href": "pages/SE/Week02.html#requirement-gathering-guidelines",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Requirement Gathering Guidelines",
    "text": "Requirement Gathering Guidelines\n\nStakeholder Focus: Identify and address the needs of all stakeholder groups, encompassing primary, secondary, and tertiary users.\nTechnique Combination: Utilize a blend of requirement gathering techniques, each serving a unique purpose.\nPilot Sessions: Prioritize running pilot sessions to ensure the efficacy of data gathering techniques.\nResource Considerations: Acknowledge the expenses and time-intensive nature of the data gathering process.\nPragmatic Approach: Recognize the need for pragmatism in navigating complexities inherent in requirement gathering.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#introduction-1",
    "href": "pages/SE/Week02.html#introduction-1",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Introduction",
    "text": "Introduction\n\nIdentifying Requirements\nIn the realm of software engineering, the process of requirement identification is multifaceted. Employing techniques such as interviews, documentation scrutiny, and questionnaires aids in discerning the varied characteristics inherent in software requirements.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#functional-requirements",
    "href": "pages/SE/Week02.html#functional-requirements",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Functional Requirements",
    "text": "Functional Requirements\n\nDefinition\nFunctional requirements constitute the backbone of system functionalities. Analogous to mathematical functions \\(f: A \\rightarrow B\\), these requirements delineate the transformation of inputs from set \\(A\\) to corresponding outputs in set \\(B\\). A quintessential example encapsulates the notion: “A seller can add or delete items from their catalog.”\n\n\nCharacteristics\nFunctional requirements pivot on user actions and inputs, elucidating the dynamic interplay between user-driven commands and the ensuing system responses.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#non-functional-requirements",
    "href": "pages/SE/Week02.html#non-functional-requirements",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Non-Functional Requirements",
    "text": "Non-Functional Requirements\n\nDistinctive Nature\nIn stark contrast, non-functional requirements transcend specific functionalities, focusing on dictating the system’s behavior rather than delineating discrete functions. Consider the exemplar: “When a new product is added, it must show up on the user’s interface within five seconds.” Non-functional requirements, unlike their functional counterparts, do not manifest as explicit functions mapping inputs to outputs.\n\n\nExemplification\n\nReliability\nReliability surfaces as a cardinal non-functional requirement, quantifying the system’s consistency over time within a stable operating milieu. In the context of software, reliability assumes paramount importance, especially in critical operations like inventory management within a seller portal.\n\n\nRobustness\nRobustness augments the software system’s resilience by delineating its ability to rebound from errors and gracefully handle unexpected inputs. In the context of a seller portal, robustness guarantees the system’s adeptness at managing large data volumes, high traffic, and erratic user inputs.\n\n\n\nHolistic Consideration\nBeyond reliability and robustness, the software development landscape encompasses an array of additional non-functional requirements:\n\nPerformance: Dictating adherence to specified performance benchmarks.\nPortability: Ensuring adaptability across diverse platforms without necessitating modifications.\nSecurity: Safeguarding the system against unauthorized access and upholding data integrity.\nInteroperability: Ensuring seamless collaboration with other systems.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#introduction-2",
    "href": "pages/SE/Week02.html#introduction-2",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Introduction",
    "text": "Introduction\nThe previous module delved into the meticulous process of gathering and analyzing software requirements, distinguishing between functional and non-functional aspects. In this module, our focus shifts towards the crucial task of effectively organizing these requirements for streamlined software development.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#plan-and-document-model-in-software-engineering",
    "href": "pages/SE/Week02.html#plan-and-document-model-in-software-engineering",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Plan and Document Model in Software Engineering",
    "text": "Plan and Document Model in Software Engineering\nIn adherence to the plan and document model in software engineering, substantial emphasis is placed on planning and documenting the software development process. A pivotal role is played by the system analyst, who collaborates with the software team to gather and organize requirements. The culmination of this process is the creation of a Software Requirements Specification (SRS) document.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#structure-of-the-srs-document",
    "href": "pages/SE/Week02.html#structure-of-the-srs-document",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Structure of the SRS Document",
    "text": "Structure of the SRS Document\n\n1. Table of Contents\nThe SRS document features a comprehensive table of contents, outlining various sections and subsections.\n\n\n2. Sections 1 and 2: Broad System Overview\nThese sections provide a detailed overview of the software system, encompassing its purpose, scope, definitions, acronyms, abbreviations, perspective, functions, constraints, assumptions, and dependencies.\n\n\n3. Section 3 - Detailed Requirements\nThis pivotal section elaborates on the specific requirements of the software system.\n\n3.1 External Interface Requirements\n\nUser interfaces, including sample screen images, GUI standards, and screen layout.\nHardware interfaces detailing the interaction between hardware and software.\nSoftware interfaces outlining connections with other software components.\nCommunication interfaces specifying required software communication.\n\n3.2 System Features\n\nOutlining high-level functions (system features).\nInclusion of functional requirements for each system feature.\n\n3.3 to 3.6 - Non-Functional Requirements\n\nComprehensive details regarding non-functional requirements, such as performance, security, etc.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#significance-of-the-srs-document",
    "href": "pages/SE/Week02.html#significance-of-the-srs-document",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Significance of the SRS Document",
    "text": "Significance of the SRS Document\nThe Software Requirements Specification document holds paramount importance in the software development process.\n\n1. Agreement Facilitation\n\nFacilitates agreement between customers and developers.\nCustomers review and accept the SRS document, establishing mutual expectations.\n\n\n\n2. Reduction of Rework\n\nMandates stakeholders to rigorously consider requirements pre-design and development.\nResults in a reduction of changes in later stages of development.\n\n\n\n3. Cost and Schedule Estimation\n\nProvides a foundational basis for estimating costs and schedules.\nSize estimation derived from requirements aids in estimating effort and cost.\nEmpowers project managers to formulate a structured development schedule.\n\n\n\n4. Facilitation of Future Extensions\n\nServes as a foundational basis for planning future enhancements.\nEnables seamless adaptation and extension of the software system.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#drawback-of-srs-documentation-overload",
    "href": "pages/SE/Week02.html#drawback-of-srs-documentation-overload",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Drawback of SRS: Documentation Overload",
    "text": "Drawback of SRS: Documentation Overload\nDespite its merits, the SRS process necessitates a substantial volume of documentation, making it most practical when dealing with fixed requirements.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#introduction-to-bdd",
    "href": "pages/SE/Week02.html#introduction-to-bdd",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Introduction to BDD",
    "text": "Introduction to BDD\nBehavior-Driven Development (BDD) serves as a strategic approach in software engineering, particularly adept at handling dynamic and uncertain requirements within the development process. This methodology aligns seamlessly with the Agile perspective, emphasizing continuous stakeholder interaction and the iterative creation of functional prototypes over short development cycles.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#behavior-driven-design-bdd",
    "href": "pages/SE/Week02.html#behavior-driven-design-bdd",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Behavior-Driven Design (BDD)",
    "text": "Behavior-Driven Design (BDD)\nBDD centers its focus on understanding the behavioral intricacies of an application both prior to and during the development phase. This strategic emphasis aims to mitigate potential miscommunication pitfalls that often arise when dealing with evolving project requirements. In the realm of BDD, traditional Software Requirements Specification (SRS) documents make way for a more dynamic entity known as “user stories.”",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#user-stories",
    "href": "pages/SE/Week02.html#user-stories",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "User Stories",
    "text": "User Stories\nUser stories are succinct, plain-language representations of desired user interactions with a software product. These narratives adhere to the role-feature-benefit pattern, encapsulating the identity of the user, the desired action, and the ensuing value or benefit. This shift towards user stories provides a more agile and adaptable alternative to the conventional SRS documentation.\n\nUser Story Examples\n\nViewing Inventory:\n\nAs an independent seller, I want to view my inventory so that I can take stock of low-quantity products.\n\nTracking Customer Feedback:\n\nAs an independent seller, I want to view my customers’ feedback for each product so that I can identify pertinent issues.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#benefits-of-user-stories",
    "href": "pages/SE/Week02.html#benefits-of-user-stories",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Benefits of User Stories",
    "text": "Benefits of User Stories\n\nLightweight Requirements:\n\nUser stories offer a streamlined and lightweight alternative to the more cumbersome SRS documentation.\n\nPrioritization and Planning:\n\nStakeholders can strategically plan and prioritize development efforts based on the encapsulated user stories.\n\nReduced Misunderstanding:\n\nBy concentrating on behavioral expectations rather than detailed implementation specifics, user stories contribute to minimizing misunderstandings between stakeholders.\n\nFacilitates Conversations:\n\nThe adoption of user stories fosters interactive discussions between end-users and the development team. This collaborative approach often leads to the creation of simpler and more valuable solutions.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#guidelines-for-crafting-good-user-stories-smart",
    "href": "pages/SE/Week02.html#guidelines-for-crafting-good-user-stories-smart",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Guidelines for Crafting Good User Stories (SMART)",
    "text": "Guidelines for Crafting Good User Stories (SMART)\n\nSpecific:\n\nUser stories should exhibit specificity, providing clear and unambiguous details regarding the required implementation.\n\nMeasurable:\n\nEach user story should be designed with testability in mind, ensuring that measurable outcomes can be derived.\n\nAchievable:\n\nIdeal user stories should be implementable within a single agile iteration, typically spanning one to two weeks.\n\nRelevant:\n\nUser stories must align with the overall business objectives, offering tangible value to one or more stakeholders.\n\nTime-Boxed:\n\nImplementation efforts associated with a user story should cease if the allocated time surpasses the predefined limit. This necessitates a reassessment or potential subdivision of the user story.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#drawbacks-of-user-stories",
    "href": "pages/SE/Week02.html#drawbacks-of-user-stories",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Drawbacks of User Stories",
    "text": "Drawbacks of User Stories\n\nContinuous Customer Contact:\n\nSustaining continuous customer involvement throughout the development process may prove challenging or economically unfeasible.\n\nScaling Issues:\n\nBDD may encounter scalability challenges, particularly in the context of expansive software development projects or applications with stringent safety requirements. These scenarios often demand extensive pre-implementation planning and documentation, aspects that might not align seamlessly with the agile nature of BDD.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/SE/Week02.html#points-to-remember",
    "href": "pages/SE/Week02.html#points-to-remember",
    "title": "Software Requirements: Gathering, Analysis, and Development Methodologies",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nImportance of Requirement Identification:\n\nMeticulous attention to identifying and documenting software requirements is paramount to align user expectations with actual deliverables.\n\nMethods for Requirement Gathering:\n\nTechniques such as questionnaires, interviews, focus groups, observations, and documentation are employed to systematically gather diverse user needs.\n\nChallenges in Requirement Gathering:\n\nDiverse stakeholder contributions, ambiguity, inconsistency, and incompleteness pose challenges that need careful consideration and resolution.\n\nFunctional and Non-Functional Requirements:\n\nFunctional requirements focus on system functionalities, while non-functional requirements dictate the system’s behavior, encompassing aspects like reliability, robustness, performance, security, portability, and interoperability.\n\nSoftware Requirements Analysis:\n\nAnalysis involves clarifying ambiguities, resolving inconsistencies, and ensuring completeness of requirements.\n\nOrganizing Software Requirements:\n\nThe SRS document, following the plan and document model, plays a crucial role in organizing requirements for effective software development.\n\nBehavior-Driven Development (BDD):\n\nBDD offers an agile approach, replacing traditional SRS with user stories for better adaptability to dynamic requirements.\n\nUser Stories:\n\nUser stories are lightweight, plain-language representations of desired user interactions, fostering prioritization, reduced misunderstanding, and interactive discussions.\n\nGuidelines for Crafting Good User Stories (SMART):\n\nUser stories should be Specific, Measurable, Achievable, Relevant, and Time-Boxed, ensuring clarity and testability.\n\nDrawbacks of User Stories:\n\nContinuous customer contact may be challenging, and scalability issues may arise in expansive projects or applications with stringent safety requirements.",
    "crumbs": [
      "Software Engineering",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html",
    "href": "pages/ST/Week02.html",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "Graphs, as fundamental data structures, play a crucial role in software testing. The inception of graph theory dates back to 1736, when Leonard Euler addressed the “Seven Bridges of Konigsberg” problem. Graphs find applications not only in computer science and data science but also in diverse fields such as sociology, economics, chemistry, and biology.\n\n\n\nA graph comprises vertices (nodes) denoted by the set \\(V\\) and edges denoted by the set \\(E\\), where \\(E\\) is a subset of \\(V \\times V\\) (the cartesian product of \\(V\\) with itself). Graphs can be classified as undirected (lacking arrows on edges) or directed (with edges having directions). Self-loops, edges connecting a vertex to itself, add a special characteristic. While graphs can be finite or infinite, finite graphs are preferred for testing purposes.\n\n\nThe degree of a vertex is defined as the number of edges incident to it. In directed graphs, the degree is further categorized into in-degree (count of incoming edges) and out-degree (count of outgoing edges).\n\n\n\n\nControl flow graphs are essential in software testing for modeling program control flow. An illustrative example includes a control flow graph for an if-else statement.\n\n\n\n\nPath: A path represents a sequence of vertices connected by edges.\nLength of a Path: It corresponds to the number of edges in a given path.\nReachability: This concept determines whether a vertex or edge is reachable from another within the graph.\n\n\n\n\nDFS and BFS are algorithms crucial for reachability analysis in graphs.\n\nDFS (Depth First Search): This algorithm explores as far as possible before backtracking.\nBFS (Breadth First Search): BFS explores level by level in a graph.\n\nThese algorithms are instrumental in solving various reachability problems in graph theory.\n\n\n\nA test path is a sequence of vertices and edges starting from an initial vertex and ending at a final vertex. Feasible test paths are executable with valid test cases, while infeasible ones cannot be achieved.\n\n\n\n\nA test path visits a vertex or edge when it includes them in the sequence.\nTouring is an equivalent concept for vertices and edges.\n\n\n\n\n\nTest Requirements: These specifications define properties to be tested, such as covering every if statement or loop.\nTest Criteria: Rules outlining how test requirements should be satisfied.\n\n\n\n\nStructural coverage criteria concentrate on graph structure without considering variables. An example is Branch Coverage, aiming to cover all branches in a graph.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#introduction-to-graphs",
    "href": "pages/ST/Week02.html#introduction-to-graphs",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "Graphs, as fundamental data structures, play a crucial role in software testing. The inception of graph theory dates back to 1736, when Leonard Euler addressed the “Seven Bridges of Konigsberg” problem. Graphs find applications not only in computer science and data science but also in diverse fields such as sociology, economics, chemistry, and biology.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#graph-components",
    "href": "pages/ST/Week02.html#graph-components",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "A graph comprises vertices (nodes) denoted by the set \\(V\\) and edges denoted by the set \\(E\\), where \\(E\\) is a subset of \\(V \\times V\\) (the cartesian product of \\(V\\) with itself). Graphs can be classified as undirected (lacking arrows on edges) or directed (with edges having directions). Self-loops, edges connecting a vertex to itself, add a special characteristic. While graphs can be finite or infinite, finite graphs are preferred for testing purposes.\n\n\nThe degree of a vertex is defined as the number of edges incident to it. In directed graphs, the degree is further categorized into in-degree (count of incoming edges) and out-degree (count of outgoing edges).",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#control-flow-graphs",
    "href": "pages/ST/Week02.html#control-flow-graphs",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "Control flow graphs are essential in software testing for modeling program control flow. An illustrative example includes a control flow graph for an if-else statement.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#path-length-and-reachability",
    "href": "pages/ST/Week02.html#path-length-and-reachability",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "Path: A path represents a sequence of vertices connected by edges.\nLength of a Path: It corresponds to the number of edges in a given path.\nReachability: This concept determines whether a vertex or edge is reachable from another within the graph.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#depth-first-search-dfs-and-breadth-first-search-bfs",
    "href": "pages/ST/Week02.html#depth-first-search-dfs-and-breadth-first-search-bfs",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "DFS and BFS are algorithms crucial for reachability analysis in graphs.\n\nDFS (Depth First Search): This algorithm explores as far as possible before backtracking.\nBFS (Breadth First Search): BFS explores level by level in a graph.\n\nThese algorithms are instrumental in solving various reachability problems in graph theory.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#test-path-and-feasibility",
    "href": "pages/ST/Week02.html#test-path-and-feasibility",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "A test path is a sequence of vertices and edges starting from an initial vertex and ending at a final vertex. Feasible test paths are executable with valid test cases, while infeasible ones cannot be achieved.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#visiting-and-touring",
    "href": "pages/ST/Week02.html#visiting-and-touring",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "A test path visits a vertex or edge when it includes them in the sequence.\nTouring is an equivalent concept for vertices and edges.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#test-requirements-and-criteria",
    "href": "pages/ST/Week02.html#test-requirements-and-criteria",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "Test Requirements: These specifications define properties to be tested, such as covering every if statement or loop.\nTest Criteria: Rules outlining how test requirements should be satisfied.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#structural-coverage-criteria",
    "href": "pages/ST/Week02.html#structural-coverage-criteria",
    "title": "Graph Theory Fundamentals",
    "section": "",
    "text": "Structural coverage criteria concentrate on graph structure without considering variables. An example is Branch Coverage, aiming to cover all branches in a graph.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#introduction-to-graph-representation",
    "href": "pages/ST/Week02.html#introduction-to-graph-representation",
    "title": "Graph Theory Fundamentals",
    "section": "Introduction to Graph Representation",
    "text": "Introduction to Graph Representation\n\nGraph Data Structures\nIn the context of software testing, graphs serve as fundamental data structures for implementing algorithms related to test case design. The lecture emphasizes the significance of representing graphs using matrices and lists, specifically, the adjacency matrix and the adjacency list.\n\n\nRepresentation Methods\nGraphs can be represented using two primary methods: matrices and lists.\n\nAdjacency List Representation\nFor each vertex in the graph, an array of lists is employed. This array contains lists corresponding to each vertex, enumerating its adjacent vertices. This representation proves advantageous for sparse graphs where not all vertices have extensive connections.\n\nExample\nConsider vertices \\(u\\), \\(v\\), and \\(w\\). The adjacency list representation could be:\n\n\\(u\\): {\\(v\\), \\(w\\)}\n\\(v\\): {\\(u\\), \\(w\\)}\n\\(w\\): {\\(u\\)}\n\n\n\n\nAdjacency Matrix Representation\nThis method utilizes an \\(n \\times n\\) matrix, where \\(n\\) is the number of vertices. A 0 or 1 is assigned to each matrix entry based on the presence or absence of an edge between the corresponding vertices.\n\nExample\nFor the graph with vertices \\(u\\), \\(v\\), and \\(w\\): \\[\n\\begin{matrix}\n& u & v & w \\\\\nu & 0 & 1 & 1 \\\\\nv & 1 & 0 & 1 \\\\\nw & 1 & 1 & 0 \\\\\n\\end{matrix}\n\\]",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#breadth-first-search-bfs",
    "href": "pages/ST/Week02.html#breadth-first-search-bfs",
    "title": "Graph Theory Fundamentals",
    "section": "Breadth-First Search (BFS)",
    "text": "Breadth-First Search (BFS)\n\nAlgorithm Overview\nBFS is a traversal algorithm used to explore a graph in a breadth-first manner. The algorithm commences by assigning colors, distances, and parent pointers to vertices. It then employs a queue for traversing the graph, exploring adjacency lists, enqueuing adjacent vertices, and updating attributes.\n\nBFS Tree\nThe algorithm constructs a BFS tree, representing the shortest paths from the source vertex. Each edge in the tree corresponds to the shortest path between vertices.\n\n\nQueue Operations\nThe BFS algorithm relies on two fundamental operations: enqueue (insert) and dequeue (remove). These operations manage the vertices during the traversal process.\n\n\nVertex Attributes\n\nColors: Vertices are initially white (unexplored), turn blue when enqueued, and finally black when explored.\nDistance Attribute (\\(d\\)): Represents the length of the shortest path from the source.\nParent Attribute (\\(\\pi\\)): Points to the predecessor vertex in the BFS tree.\n\n\n\n\nExample Execution of BFS\nThe lecture provides a step-by-step illustration of BFS execution using a sample graph. It outlines the process of enqueueing, dequeuing, and updating vertex attributes, resulting in the construction of the BFS tree.\n\n\nAnalysis of BFS\nThe efficiency of BFS is analyzed in terms of its running time, which is linear, \\(O(v + e)\\), where \\(v\\) is the number of vertices and \\(e\\) is the number of edges. BFS guarantees the identification of shortest paths in unweighted graphs.\n\nCorrectness Theorem\nA correctness theorem is presented, asserting that BFS correctly explores all reachable vertices from the source and returns the shortest paths.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#overview-of-dfs",
    "href": "pages/ST/Week02.html#overview-of-dfs",
    "title": "Graph Theory Fundamentals",
    "section": "Overview of DFS",
    "text": "Overview of DFS\nDFS operates by systematically exploring edges out of the most recently discovered vertex with unexplored edges. The algorithm assigns colors to vertices to track their exploration status:\n\nWhite: Undiscovered\nGray: Discovered but not fully explored\nBlack: Fully explored\n\nAdditionally, timestamps in the form of discovery and finish times are assigned to vertices during the process, offering further information about the graph.\n\nPseudocode for DFS\nDFS(G):\n  for each vertex u in G:\n    color[u] = \\text{white}\n    parent[u] = \\text{nil}\n  time = 0\n  for each vertex u in G:\n    if color[u] is \\text{white}:\n      DFS-Visit(u)\n\nDFS-Visit(u):\n  time = time + 1\n  discovery[u] = time\n  color[u] = \\text{gray}\n  for each vertex v adjacent to u:\n    if color[v] is \\text{white}:\n      parent[v] = u\n      DFS-Visit(v)\n  color[u] = \\text{black}\n  time = time + 1\n  finish[u] = time\n\n\nProperties of DFS\n\nParenthesis Theorem:\n\nDiscovery times are always less than finish times, creating nested parenthesis intervals.\n\nWhite Path Theorem:\n\nWhen first encountering a vertex, there exists a path of white-colored vertices leading to it.\n\nEdge Classification:\n\nTree Edges: Form the DFS tree.\nForward Edges: Connect descendants to ancestors.\nBackward Edges: Connect ancestors to descendants.\nCross Edges: Connect vertices unrelated in the DFS tree.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#algorithm-for-scc",
    "href": "pages/ST/Week02.html#algorithm-for-scc",
    "title": "Graph Theory Fundamentals",
    "section": "Algorithm for SCC",
    "text": "Algorithm for SCC\n\nRun DFS on the graph to compute finish times.\n\nThe finish times denote the order in which vertices complete their exploration.\n\nCompute the transpose of the graph.\n\nReverse the direction of edges in the graph.\n\nRun DFS on the transpose graph in reverse finish time order.\n\nExplore vertices in the order of decreasing finish times obtained in step 1.\n\nIdentify SCCs based on DFS trees in the second run.\n\nEach DFS tree represents a strongly connected component.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#introduction",
    "href": "pages/ST/Week02.html#introduction",
    "title": "Graph Theory Fundamentals",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software testing, structural coverage criteria play a pivotal role in ensuring the thorough examination of software artifacts. This lecture delves into various structural coverage criteria applied to graphs, elucidating their significance in the testing process. The focus lies on node coverage, edge coverage, edge pair coverage, and prime path coverage. Additionally, the challenges associated with achieving complete path coverage, especially in the presence of loops, are explored.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#nodes-edges-and-paths",
    "href": "pages/ST/Week02.html#nodes-edges-and-paths",
    "title": "Graph Theory Fundamentals",
    "section": "Nodes, Edges, and Paths",
    "text": "Nodes, Edges, and Paths\nA graph modeling a software artifact comprises nodes (or vertices) and edges, representing the structural entities. Paths in the graph manifest as sequences of nodes and edges, forming the basis for coverage criteria.\n\nNode Coverage\nDefinition: The test requirement for node coverage entails generating test cases that visit every node in the graph at least once. A test set, denoted as \\(T\\), satisfies node coverage if, for every reachable node, there exists a test path in \\(T\\) that visits that node.\n\n\nEdge Coverage\nDefinition: Edge coverage necessitates visiting every edge in the graph at least once. The test requirement can be expressed as executing each reachable path of length up to 1. It aims to subsume node coverage, ensuring that the paths of length 0 (nodes) and length 1 (edges) are covered.\n\n\nEdge Pair Coverage\nDefinition: This criterion extends coverage to pairs of edges. Test paths of length 2 (pairs of edges) are considered, ensuring coverage of all possible edge pairs. Edge pair coverage aims to encompass both edge and node coverage.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#prime-path-coverage",
    "href": "pages/ST/Week02.html#prime-path-coverage",
    "title": "Graph Theory Fundamentals",
    "section": "Prime Path Coverage",
    "text": "Prime Path Coverage\nPrime paths are maximal simple paths within a graph, devoid of internal loops. Enumerating prime paths provides an effective coverage criterion, addressing challenges associated with loops in control flow graphs.\nDefinition: A prime path is a simple path that is not a proper subpath of any other simple path. It serves as a maximal simple path within the graph.\n\nTouring with Side Trips and Detours\nTo address scenarios where prime paths might necessitate traversing loops, two concepts are introduced:\n\nSide Trips:\n\nDefinition: A test path \\(p\\) is considered to have a side trip towards a subpath \\(q\\) if every edge in \\(q\\) appears in \\(p\\) in the same order.\nExplanation: Side trips allow for the traversal of a subpath \\(q\\) within the main test path \\(p\\), ensuring that the edges in \\(q\\) are followed in the same sequence as they appear in \\(p\\).\nPurpose: The concept of side trips is particularly useful when dealing with loops in control flow graphs. It allows for the inclusion of loop-related paths within the main test path, contributing to a more practical and feasible testing scenario.\n\nDetours:\n\nDefinition: A test path \\(p\\) is considered to have a detour towards a subpath \\(q\\) if every node in \\(q\\) appears in \\(p\\) in the same order.\nExplanation: Detours enable the traversal of a subpath \\(q\\) within the main test path \\(p\\), ensuring that the nodes in \\(q\\) are visited in the same sequence as they appear in \\(p\\).\nPurpose: Similar to side trips, detours offer a mechanism to accommodate loops in control flow graphs during testing. They provide flexibility by allowing the inclusion of paths related to loops, contributing to a more realistic testing approach.\n\n\nThese concepts help mitigate infeasibility concerns, allowing for more practical testing scenarios.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#round-trip-coverage",
    "href": "pages/ST/Week02.html#round-trip-coverage",
    "title": "Graph Theory Fundamentals",
    "section": "Round Trip Coverage",
    "text": "Round Trip Coverage\nRound trips are prime paths that commence and culminate at the same node. Coverage criteria for round trips include:\n\nSimple Round Trip Coverage: Ensures at least one round trip for each reachable node.\nComplete Round Trip Coverage: Requires coverage of all possible round trip paths within the graph.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#overview",
    "href": "pages/ST/Week02.html#overview",
    "title": "Graph Theory Fundamentals",
    "section": "Overview",
    "text": "Overview\nThis section delves into the meticulous process of deriving test requirements and paths to achieve structural coverage criteria within software testing. The primary focus is on graphs representing software artifacts.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#structural-coverage-criteria-2",
    "href": "pages/ST/Week02.html#structural-coverage-criteria-2",
    "title": "Graph Theory Fundamentals",
    "section": "Structural Coverage Criteria",
    "text": "Structural Coverage Criteria\n\n1. Node Coverage and Edge Coverage\n\nTest Requirements\n\nNode Coverage: Set of nodes in the graph.\nEdge Coverage: Set of edges in the graph.\n\n\n\nTest Paths\nUtilize Breadth-First Search (BFS) from an initial node to cover reachable nodes and edges systematically.\n\n\n\n2. Edge Pair Coverage\n\nTest Requirements\n\nAll paths of length \\(2\\) in the graph.\n\n\n\nAlgorithm\nEnumerate pairs of edges by traversing nodes and adjacency lists. This involves considering nodes \\(u\\) and \\(v\\), exploring their adjacency lists, and forming pairs \\(u \\to v \\to w\\), where \\(w\\) is in the adjacency list of \\(v\\).\n\n\n\n3. Specified Path Coverage\n\nTest Requirements\n\nSet of specified paths provided by a tester.\n\n\n\nAlgorithm\nModify BFS for graphs without loops to achieve specified path coverage. This entails adapting BFS to include specified paths in the traversal.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#prime-path-coverage-1",
    "href": "pages/ST/Week02.html#prime-path-coverage-1",
    "title": "Graph Theory Fundamentals",
    "section": "Prime Path Coverage",
    "text": "Prime Path Coverage\n\nPrime Paths\nPrime paths are defined as maximal simple paths in a graph.\n\n\nTest Requirements\nEnumerate all prime paths in the graph.\n\n\nPrime Path Enumeration Algorithm\n\n1. Algorithm Overview\n\nEnumerate simple paths in ascending order of length.\nChoose prime paths among the enumerated paths.\n\n\n\n2. Enumeration Process\n\nPaths of length \\(0\\) (vertices) are considered, marking unextendable paths with “!”.\nPaths of length \\(1\\) (edges) are enumerated, marking unextendable and simple cycle paths with “!” and “*“.\nExtension of paths to obtain length \\(2\\) paths is performed, and paths are marked accordingly.\nThe process continues until paths of length \\(\\text{mod } v - 1\\) are reached, with markings indicating path characteristics.\n\n\n\n3. Result\nObtain all prime paths as test requirements.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#test-paths-for-prime-path-coverage",
    "href": "pages/ST/Week02.html#test-paths-for-prime-path-coverage",
    "title": "Graph Theory Fundamentals",
    "section": "Test Paths for Prime Path Coverage",
    "text": "Test Paths for Prime Path Coverage\n\nAlgorithm Overview\n\nStart with the longest prime path.\nExtend each path to the initial and final vertices.\nUtilize traversal algorithms to extend paths systematically.\n\n\n\nExample\nFor a graph with multiple loops, initiate the process with the longest prime path and extend it to cover all instances of loops.\n\n\nOptimality Challenge\nAchieving optimal test paths is generally intractable. Symbolic execution, an advanced technique, can be explored for improved test path generation.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/ST/Week02.html#points-to-remember",
    "href": "pages/ST/Week02.html#points-to-remember",
    "title": "Graph Theory Fundamentals",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nGraph Basics:\n\nGraphs are fundamental data structures with applications in various fields.\nGraphs consist of vertices and edges, and their components include degree, control flow graphs, paths, and reachability.\n\nDFS and BFS:\n\nDFS and BFS are essential algorithms for reachability analysis in graphs.\nDFS explores as far as possible before backtracking, while BFS explores level by level.\nThey play a crucial role in solving reachability problems.\n\nGraph Representation:\n\nGraphs can be represented using adjacency matrices or lists.\nAdjacency list representation is advantageous for sparse graphs.\n\nBreadth-First Search (BFS):\n\nBFS explores a graph in a breadth-first manner, constructing a BFS tree.\nIt guarantees the identification of shortest paths in unweighted graphs.\n\nDepth First Search (DFS):\n\nDFS explores graphs systematically, assigning colors to vertices.\nIt provides insights into the structure and connectivity of a graph.\n\nStrongly Connected Components (SCC):\n\nSCCs are subsets of vertices in a directed graph where every pair of vertices is reachable from each other.\nDFS is used to efficiently identify SCCs.\n\nStructural Coverage Criteria:\n\nNode coverage, edge coverage, edge pair coverage, prime path coverage, best effort touring, and round trip coverage are discussed.\nThese criteria ensure thorough evaluation of software artifacts in testing.\n\nTest Paths for Prime Path Coverage:\n\nEnumerating prime paths involves considering simple paths in ascending order of length.\nThe algorithm systematically extends paths to cover initial and final vertices.\n\nOptimality Challenge:\n\nAchieving optimal test paths is generally intractable.\nSymbolic execution is an advanced technique that can be explored for improved test path generation.",
    "crumbs": [
      "Software Testing",
      "Week 2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html",
    "href": "pages/RL/Week01_2.html",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Reinforcement Learning (RL) is a paradigm within machine learning that focuses on trial-and-error learning. It involves learning from the evaluation of actions taken, rather than receiving explicit instructional feedback. In RL, agents explore various actions to determine their effectiveness through evaluative feedback. This differentiates RL from other learning approaches.\n\n\nIn the realm of RL applications, various domains utilize this learning paradigm. From controlling robots to playing games like tic-tac-toe, RL finds its application in scenarios where learning from experience is crucial.\n\n\n\n\nTo understand the learning mechanism in RL, let’s consider a straightforward example: playing tic-tac-toe. In a traditional supervised learning setting, an expert labels optimal moves for different board positions. However, RL takes a different approach.\n\n\nIn a supervised learning setup for tic-tac-toe, experts label correct moves for specific board positions. The computer is then trained using this labeled dataset to predict the right move for any given position.\n\n\n\nIn RL, the agent is simply told to play the game without explicit instructions on moves. The agent receives points based on the game outcome: +1 for a win, -1 for a loss, and 0 for a draw. The crucial aspect is that the agent is not informed about the winning conditions; it learns solely from playing the game repeatedly.\n\n\n\nA historical example of RL in the form of a simple tic-tac-toe learning system is Menace (Matchbox Educable Noughts and Crosses Engine). This system, developed in the 1960s, used matchboxes with colored beads to learn optimal moves. Each matchbox represented a board position, and colored beads denoted possible moves.\n\nLearning Process\n\nOpen matchbox for the current position.\nSelect a bead representing a move.\nPlay the move on the board.\nUpdate bead counts based on game outcome.\nRepeat the process for subsequent games.\n\nOutcome Influence\n\nWinning increased the probability of selecting specific moves.\nLosing decreased the likelihood of choosing certain moves.\n\n\n\n\n\nUnderstanding RL involves examining the game tree, representing possible moves and outcomes. Temporal Difference (TD) Learning plays a crucial role in RL.\n\nGame Tree\n\nDescribes possible moves and outcomes.\nEach path represents a sequence of moves leading to a win, draw, or loss.\n\nTemporal Difference Learning\n\nCompares predicted outcomes at successive time steps.\nUpdates move probabilities based on the difference in predicted outcomes.\n\n\n\n\nObservations of dopamine activity in monkeys during a reward-based task mirror TD learning predictions. The brain’s dopamine response shifts from the actual reward to the predictive stimulus, showcasing the alignment between computational models and biological learning. \n\n\n\n\n\n\n\nDeep Reinforcement Learning (DRL) merges RL principles with deep learning for enhanced function approximation. DRL has revolutionized the field, enabling solutions to complex problems.\n\nGrowing Excitement\n\nSignificant increase in publications mentioning reinforcement learning.\nDRL has sparked renewed interest and excitement in the RL community.\n\n\n\n\n\n\nReinforcement Learning remains an active area of research with ongoing exploration of fundamental questions. The ultimate goal is to develop omnivorous learning systems capable of consuming diverse information for improved learning.\n\nReinforcement Learning with Human Feedback\n\nIncorporating human feedback into RL processes.\nAiming for more versatile and powerful learning systems.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#introduction-to-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#introduction-to-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Reinforcement Learning (RL) is a paradigm within machine learning that focuses on trial-and-error learning. It involves learning from the evaluation of actions taken, rather than receiving explicit instructional feedback. In RL, agents explore various actions to determine their effectiveness through evaluative feedback. This differentiates RL from other learning approaches.\n\n\nIn the realm of RL applications, various domains utilize this learning paradigm. From controlling robots to playing games like tic-tac-toe, RL finds its application in scenarios where learning from experience is crucial.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#learning-mechanisms-a-simple-example-with-tic-tac-toe",
    "href": "pages/RL/Week01_2.html#learning-mechanisms-a-simple-example-with-tic-tac-toe",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "To understand the learning mechanism in RL, let’s consider a straightforward example: playing tic-tac-toe. In a traditional supervised learning setting, an expert labels optimal moves for different board positions. However, RL takes a different approach.\n\n\nIn a supervised learning setup for tic-tac-toe, experts label correct moves for specific board positions. The computer is then trained using this labeled dataset to predict the right move for any given position.\n\n\n\nIn RL, the agent is simply told to play the game without explicit instructions on moves. The agent receives points based on the game outcome: +1 for a win, -1 for a loss, and 0 for a draw. The crucial aspect is that the agent is not informed about the winning conditions; it learns solely from playing the game repeatedly.\n\n\n\nA historical example of RL in the form of a simple tic-tac-toe learning system is Menace (Matchbox Educable Noughts and Crosses Engine). This system, developed in the 1960s, used matchboxes with colored beads to learn optimal moves. Each matchbox represented a board position, and colored beads denoted possible moves.\n\nLearning Process\n\nOpen matchbox for the current position.\nSelect a bead representing a move.\nPlay the move on the board.\nUpdate bead counts based on game outcome.\nRepeat the process for subsequent games.\n\nOutcome Influence\n\nWinning increased the probability of selecting specific moves.\nLosing decreased the likelihood of choosing certain moves.\n\n\n\n\n\nUnderstanding RL involves examining the game tree, representing possible moves and outcomes. Temporal Difference (TD) Learning plays a crucial role in RL.\n\nGame Tree\n\nDescribes possible moves and outcomes.\nEach path represents a sequence of moves leading to a win, draw, or loss.\n\nTemporal Difference Learning\n\nCompares predicted outcomes at successive time steps.\nUpdates move probabilities based on the difference in predicted outcomes.\n\n\n\n\nObservations of dopamine activity in monkeys during a reward-based task mirror TD learning predictions. The brain’s dopamine response shifts from the actual reward to the predictive stimulus, showcasing the alignment between computational models and biological learning.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#deep-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#deep-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Deep Reinforcement Learning (DRL) merges RL principles with deep learning for enhanced function approximation. DRL has revolutionized the field, enabling solutions to complex problems.\n\nGrowing Excitement\n\nSignificant increase in publications mentioning reinforcement learning.\nDRL has sparked renewed interest and excitement in the RL community.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#future-directions-in-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#future-directions-in-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "",
    "text": "Reinforcement Learning remains an active area of research with ongoing exploration of fundamental questions. The ultimate goal is to develop omnivorous learning systems capable of consuming diverse information for improved learning.\n\nReinforcement Learning with Human Feedback\n\nIncorporating human feedback into RL processes.\nAiming for more versatile and powerful learning systems.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#reinforcement-learning-framework",
    "href": "pages/RL/Week01_2.html#reinforcement-learning-framework",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Reinforcement Learning Framework",
    "text": "Reinforcement Learning Framework\nReinforcement learning is characterized by learning through interactions with an environment. The learner receives feedback based on its actions, necessitating a strategic approach to explore different possibilities (exploration) and exploit known optimal actions for favorable outcomes.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#immediate-reinforcement-learning",
    "href": "pages/RL/Week01_2.html#immediate-reinforcement-learning",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Immediate Reinforcement Learning",
    "text": "Immediate Reinforcement Learning\nIn the immediate reinforcement learning problem, actions yield immediate payoffs, eliminating the need for a sequence of moves or temporal considerations. This simplification directs attention to the exploration-exploitation dilemma, a critical aspect of reinforcement learning.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#exploration-vs.-exploitation-dilemma",
    "href": "pages/RL/Week01_2.html#exploration-vs.-exploitation-dilemma",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Exploration vs. Exploitation Dilemma",
    "text": "Exploration vs. Exploitation Dilemma\nThe core challenge revolves around determining the optimal trade-off between exploring various actions and exploiting the known best action. Excessive exploration may impede performance, while premature exploitation might lead to suboptimal outcomes.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  },
  {
    "objectID": "pages/RL/Week01_2.html#points-to-remember",
    "href": "pages/RL/Week01_2.html#points-to-remember",
    "title": "Exploring Reinforcement Learning: From Foundations to Future Challenges",
    "section": "Points to Remember",
    "text": "Points to Remember\n\nReinforcement Learning Fundamentals\n\nRL focuses on trial-and-error learning, distinguishing it from other machine learning approaches.\nApplications span diverse domains, from robotics to game playing.\n\nLearning Mechanisms in Tic-Tac-Toe\n\nSupervised learning relies on labeled datasets of optimal moves.\nRL involves trial and error, with agents learning from the game’s outcome.\n\nHistorical Example: Menace\n\nMenace used a matchbox system with colored beads to learn optimal moves.\nLearning process involved updating bead counts based on game outcomes.\n\nGame Tree and Temporal Difference Learning\n\nGame tree represents possible moves and outcomes.\nTemporal Difference (TD) Learning updates move probabilities based on predicted outcomes.\n\nDeep Reinforcement Learning (DRL)\n\nIntegration of RL principles with deep learning for enhanced function approximation.\nDRL has led to a significant increase in publications and excitement in the RL community.\n\nFuture Directions in RL\n\nOngoing research aims at developing versatile learning systems.\nIncorporating human feedback for more powerful learning systems.\n\nImmediate Reinforcement Learning: Multi-Arm Bandit Problem\n\nImmediate RL focuses on actions yielding immediate payoffs.\nThe exploration-exploitation dilemma is crucial, requiring a strategic balance.",
    "crumbs": [
      "Reinforcement Learning",
      "Week 1.2"
    ]
  }
]