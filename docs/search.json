[
  {
    "objectID": "pages/ST/Week01.html",
    "href": "pages/ST/Week01.html",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes.\n\n\n\n\n\nThe initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary.\n\n\n\n\n\n\nThe V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints.\n\n\n\n\nBeyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances.\n\n\n\n\n\nIntegral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "href": "pages/ST/Week01.html#introduction-to-software-development-life-cycle-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "In the dynamic realm of the software industry, the Software Development Life Cycle (SDLC) emerges as a pivotal and systematic process encompassing various stages: designing, developing, testing, and releasing software. Its ultimate objective is to deliver software of the highest quality, aligning with customer expectations. Guiding this intricate process is the ISO/IEC standard 10207, which meticulously defines software lifecycle processes."
  },
  {
    "objectID": "pages/ST/Week01.html#phases-of-sdlc",
    "href": "pages/ST/Week01.html#phases-of-sdlc",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The initial phase involves a meticulous identification of development goals, stakeholders, and feasibility studies. Rigorous analysis, validation, and documentation of requirements take precedence. A comprehensive project plan is then crafted, incorporating timelines and resource allocation.\n\n\n\nThis phase delves into the intricate details of software modules and internals. Designing identifies these modules, while architecture defines module connections, operating systems, databases, and user interface aspects. Feasibility studies and system-level test cases are conducted, culminating in the creation of design and architecture documents.\n\n\n\nImplementation of low-level design in adherence to coding guidelines takes center stage in this phase. Developers, in turn, conduct unit testing, while project management tools meticulously track progress. The output comprises executable code, comprehensive documentation, and meticulously crafted unit test cases.\n\n\n\nThe testing phase is a critical juncture where software undergoes thorough examination for defects. This includes integration testing, system testing, and acceptance testing. The iterative process of defect identification, rectification, and retesting continues until all functionalities meet the defined criteria. The output comprises detailed test cases and comprehensive test documentation.\n\n\n\nPost-deployment, the maintenance phase kicks in, addressing errors post-release and accommodating customer feature requests. Regression testing ensures continued software integrity, with both reusing and creating new test cases as necessary."
  },
  {
    "objectID": "pages/ST/Week01.html#sdlc-models",
    "href": "pages/ST/Week01.html#sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "The V Model stands out for its emphasis on testing, incorporating both verification and validation. It follows a traditional waterfall model, mapping testing phases directly to corresponding development phases. This model places a premium on thorough testing practices.\n\n\n\nAn amalgamation of methodologies, Agile Software Development prioritizes adaptability and rapid development. This involves developing in small, manageable subsets with incremental releases, fostering quick delivery, customer interactions, and rapid response. Agile models often include iterations or sprints."
  },
  {
    "objectID": "pages/ST/Week01.html#other-sdlc-models",
    "href": "pages/ST/Week01.html#other-sdlc-models",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Beyond the V Model and Agile, the software industry features a myriad of other SDLC models, each with its unique approach. Models like Big Bang, Rapid Application Development, Incremental Model, and the Waterfall Model cater to diverse project requirements and circumstances."
  },
  {
    "objectID": "pages/ST/Week01.html#umbrella-activities",
    "href": "pages/ST/Week01.html#umbrella-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "",
    "text": "Integral to SDLC are umbrella activities, including project management. This involves team management, task delegation, resource planning, duration estimation, intermediate releases, and overall project planning.\n\n\n\nDocumentation forms the backbone of SDLC, with essential artifacts encompassing code, test cases, and various documents. The Requirements Traceability Matrix (RTM) emerges as a crucial tool, linking artifacts across different phases and ensuring a seamless flow of information.\n\n\n\nEnsuring the readiness of software for the market involves dedicated efforts from software quality auditors, inspection teams, and certification and accreditation teams. Quality Assurance activities play a vital role in maintaining the overall integrity and reliability of the software product."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction",
    "href": "pages/ST/Week01.html#introduction",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\n\nDefinition of Software Testing\nSoftware testing is a comprehensive process involving the scrutiny of various artifacts, including code, design, architecture documents, and requirements documents. The core objective is to validate and verify these artifacts, ensuring the software’s reliability and functionality.\n\n\nGoals of Software Testing\nThe overarching goals encompass providing an unbiased, independent assessment of the software, verifying its compliance with business capabilities, and evaluating associated risks that may impact its performance."
  },
  {
    "objectID": "pages/ST/Week01.html#standard-glossary",
    "href": "pages/ST/Week01.html#standard-glossary",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Standard Glossary",
    "text": "Standard Glossary\n\nVerification: This process determines whether the products meet specified requirements at various stages of the software development life cycle.\nValidation: Evaluation of the software at the end of the development phase, ensuring it aligns with standards and intended usage.\nFault: A static defect within the software, often originating from a mistake made during development.\nFailure: The visible, external manifestation of incorrect behavior resulting from a fault.\nError: The incorrect state of the program when a failure occurs, indicating a deviation from the intended behavior."
  },
  {
    "objectID": "pages/ST/Week01.html#historical-perspective",
    "href": "pages/ST/Week01.html#historical-perspective",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Historical Perspective",
    "text": "Historical Perspective\nDrawing from the historical lens, luminaries like Edison and Lovelace utilized terms such as “bug” and “error” to emphasize the iterative process of identifying and rectifying faults and difficulties in inventions."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-terminology",
    "href": "pages/ST/Week01.html#testing-terminology",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Terminology",
    "text": "Testing Terminology\n\nTest Case: A comprehensive entity comprising test inputs and expected outputs, evaluated by executing the test case on the code.\nTest Case ID: An identifier crucial for retrieval and management of test cases.\nTraceability: The establishment of links connecting test cases to specific requirements, ensuring thorough validation."
  },
  {
    "objectID": "pages/ST/Week01.html#types-of-testing",
    "href": "pages/ST/Week01.html#types-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Types of Testing",
    "text": "Types of Testing\n\nUnit Testing: A meticulous examination carried out by developers during the coding phase to test individual methods.\nIntegration Testing: An evaluation of the interaction between diverse software components.\nSystem Testing: A holistic examination of the entire system to ensure alignment with design requirements.\nAcceptance Testing: Conducted by end customers to validate that the delivered software meets all committed requirements."
  },
  {
    "objectID": "pages/ST/Week01.html#quality-parameters-testing",
    "href": "pages/ST/Week01.html#quality-parameters-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Quality Parameters Testing",
    "text": "Quality Parameters Testing\n\nFunctional Testing: Ensures the software functions precisely as intended.\nStress Testing: Evaluates software performance under extreme conditions to assess its robustness.\nPerformance Testing: Verifies if the software responds within specified time limits under varying conditions.\nUsability Testing: Ensures the software offers a user-friendly interface, enhancing the overall user experience.\nRegression Testing: Validates that existing functionalities continue to work seamlessly after software changes."
  },
  {
    "objectID": "pages/ST/Week01.html#methods-of-testing",
    "href": "pages/ST/Week01.html#methods-of-testing",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Methods of Testing",
    "text": "Methods of Testing\n\nBlack Box Testing: A method that evaluates the software without delving into its internal structure, relying solely on inputs and requirements.\nWhite Box Testing: Testing carried out with a comprehensive understanding of the software’s internal structure, design, and code.\nGray Box Testing: An intermediate approach that combines elements of both black box and white box testing."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-activities",
    "href": "pages/ST/Week01.html#testing-activities",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Activities",
    "text": "Testing Activities\n\nTest Case Design:\n\nCritical for efficiently identifying defects.\nRequires a blend of computer science expertise, domain knowledge, and mathematical proficiency.\nEmphasis on the development of effective test case design algorithms. \n\nTest Automation:\n\nInvolves the conversion of test cases into executable scripts.\nAddresses preparatory steps and incorporates concepts of observability and controllability.\nUtilizes both open-source and proprietary test automation tools.\n\nExecution:\n\nAutomated process involving the execution of test cases.\nUtilizes a selection of open-source or proprietary tools chosen by the organization.\n\nEvaluation:\n\nThe critical analysis of test results to determine correctness.\nManual intervention may be required for fault isolation.\nCrucial for drawing inferences about the software’s quality."
  },
  {
    "objectID": "pages/ST/Week01.html#introduction-1",
    "href": "pages/ST/Week01.html#introduction-1",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Introduction",
    "text": "Introduction\nIn the realm of software testing, the pursuit of testing goals is intricately tied to the specificities of the software product in question and the maturity of an organization’s quality processes. This diversity in objectives and approaches underscores the importance of comprehending the nuanced landscape of testing process levels, which range from the rudimentary Level 0 to the pinnacle of maturity at Level 4."
  },
  {
    "objectID": "pages/ST/Week01.html#testing-process-maturity-levels",
    "href": "pages/ST/Week01.html#testing-process-maturity-levels",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Testing Process Maturity Levels",
    "text": "Testing Process Maturity Levels\n\nLevel 0: Low Maturity\nAt this embryonic stage, there is an absence of a clear demarcation between testing and debugging activities. The predominant focus revolves around expedient product releases, potentially at the expense of a rigorous testing regimen.\nLevel 1: Testing for Correctness\nThe next tier witnesses a paradigm shift as testing endeavors to validate software correctness. However, a common misunderstanding prevails — an attempt to prove complete correctness through testing, an inherently unattainable feat.\nLevel 2: Finding Errors\nAs organizations ascend to Level 2, there is a conscious recognition of testing as a mechanism to unearth errors by actively showcasing failures. However, a resistance lingers when it comes to acknowledging and addressing errors identified in the code.\nLevel 3: Sophisticated Testing\nLevel 3 marks a watershed moment where testing is not merely a reactive measure but is embraced as a robust technique for both identifying and eliminating errors. A collaborative ethos emerges, with a collective effort to mitigate risks in software development.\nLevel 4: Mature Process-Oriented Testing\nAt the pinnacle of maturity, testing transcends mere procedural activities; it metamorphoses into a mental discipline. Integrated seamlessly into mainstream development, the focus is on continuous quality improvement. Here, test engineers and developers synergize their efforts to deliver software of the highest quality."
  },
  {
    "objectID": "pages/ST/Week01.html#significance-for-the-course",
    "href": "pages/ST/Week01.html#significance-for-the-course",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Significance for the Course",
    "text": "Significance for the Course\nUnderstanding the nuances of testing process levels assumes paramount importance as it serves as the bedrock for tailoring testing approaches. The focus of this course is strategically directed towards the technical intricacies relevant to Levels 3 and 4, where testing is not just a process but an integral aspect of the software development mindset."
  },
  {
    "objectID": "pages/ST/Week01.html#controllability-and-observability",
    "href": "pages/ST/Week01.html#controllability-and-observability",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Controllability and Observability",
    "text": "Controllability and Observability\n\nControllability: This pertains to the ability to provide inputs and execute the software module. It underscores the necessity of having a structured approach to govern the input parameters and execution environment.\nObservability: The study and recording of outputs form the crux of observability. This involves a meticulous examination of the software’s responses, contributing significantly to the overall understanding of its behavior."
  },
  {
    "objectID": "pages/ST/Week01.html#illustration",
    "href": "pages/ST/Week01.html#illustration",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Illustration",
    "text": "Illustration\nThe challenges of controllability and observability find illustration in real-world scenarios. Designing effective test cases becomes paramount to ensure both the reachability of various modules and the meticulous observation of their outputs. This practical application reinforces the theoretical concepts discussed in the course."
  },
  {
    "objectID": "pages/ST/Week01.html#test-automation-tool-junit",
    "href": "pages/ST/Week01.html#test-automation-tool-junit",
    "title": "Software Development Life Cycle (SDLC)",
    "section": "Test Automation Tool: JUnit",
    "text": "Test Automation Tool: JUnit\nThe course introduces JUnit as the designated test automation tool. JUnit’s utility is elucidated through a discussion of its prefix and postfix annotations, providing a structured approach to manage controllability and observability. Subsequent classes delve into both the theoretical underpinnings and the hands-on application of JUnit, ensuring a comprehensive understanding of its role in the testing process."
  },
  {
    "objectID": "pages/DL/Week01_1.html",
    "href": "pages/DL/Week01_1.html",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain.\n\n\n\nIn 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969.\n\n\n\nThe period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape.\n\n\n\nThe Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation.\n\n\n\nA disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy."
  },
  {
    "objectID": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "href": "pages/DL/Week01_1.html#biological-neurons-and-theories",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In the early stages of understanding neural networks, Joseph von Gerlach’s 1871 proposition of the Reticular theory posited a continuous network for the nervous system. Supporting evidence came from Golgi’s staining technique. The debate shifted with Santiago Ramón y Cajal’s 1891 Neuron doctrine, proposing discrete individual cells forming a network. This sparked the Nobel Prize conflict in 1906, ultimately resolved through electron microscopy. The ensuing discourse revolved around the balance between localized and distributed processing in the brain."
  },
  {
    "objectID": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "href": "pages/DL/Week01_1.html#artificial-neurons-and-the-perceptron",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "In 1943, McCulloch and Pitts presented a model of the neuron, laying the groundwork for artificial neurons. A significant stride occurred in 1957 when Frank Rosenblatt introduced the perceptron model, featuring weighted inputs. However, the limitations of a single perceptron were identified by Minsky and Papert in 1969."
  },
  {
    "objectID": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "href": "pages/DL/Week01_1.html#spring-to-winter-of-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The period from 1957 to 1969 marked the “Spring of AI,” characterized by optimism, funding, and interest. Yet, Minsky and Papert’s critique ushered in the “Winter of AI.” The emergence of backpropagation in 1986, popularized by Rumelhart and Hinton, and the acknowledgment of gradient descent (discovered by Cauchy in the 19th century) marked a shift in the AI landscape."
  },
  {
    "objectID": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "href": "pages/DL/Week01_1.html#universal-approximation-theorem",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "The Universal Approximation Theorem, introduced in 1989, elucidates how a multi-layered neural network can approximate any function. Emphasis is placed on the significance of the number of neurons for achieving superior approximation."
  },
  {
    "objectID": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "href": "pages/DL/Week01_1.html#practical-challenges-and-progress",
    "title": "A Historical Overview of Deep Learning",
    "section": "",
    "text": "A disparity between theoretical knowledge and practical challenges in training deep neural networks emerged. Stability and convergence issues with backpropagation were identified in practice. However, progress in convolutional neural networks over two decades has been noteworthy."
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction",
    "href": "pages/DL/Week01_1.html#introduction",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\n\nHistorical Perspective\nDeep learning encountered challenges in training via backpropagation. Jeff Hinton’s group proposed a crucial weight initialization idea in 2016, fostering stable training. The improved availability of computing power and data around 2006 laid the foundation for success."
  },
  {
    "objectID": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "href": "pages/DL/Week01_1.html#early-challenges-and-solutions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Early Challenges and Solutions",
    "text": "Early Challenges and Solutions\n\nUnsupervised Pre-training\nBetween 2007 and 2009, investigations into the effectiveness of unsupervised pre-training led to insights that shaped optimization and regularization algorithms. The course will delve into topics such as initializations, regularizations, and optimizations."
  },
  {
    "objectID": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "href": "pages/DL/Week01_1.html#emergence-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Emergence of Deep Learning",
    "text": "Emergence of Deep Learning\n\nPractical Utility\nDeep learning applications started winning competitions, including handwriting recognition on the MNIST dataset, speech recognition, and visual pattern recognition like traffic sign data.\n\n\nImageNet Challenge (2012-2016)\nThe ImageNet challenge, a pivotal turning point, witnessed the evolution from ZFNet to ResNet (152 layers), achieving a remarkable 3.6% error rate in 2016, surpassing human performance."
  },
  {
    "objectID": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "href": "pages/DL/Week01_1.html#transition-period-2012-2016",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transition Period (2012-2016)",
    "text": "Transition Period (2012-2016)\n\nGolden Period of Deep Learning\nThe universal acceptance of deep learning marked its golden period, with convolutional neural networks dominating image-related problems. Similar trends were observed in natural language processing (NLP) and speech processing."
  },
  {
    "objectID": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "href": "pages/DL/Week01_1.html#from-cats-to-convolutional-neural-networks",
    "title": "A Historical Overview of Deep Learning",
    "section": "From Cats to Convolutional Neural Networks",
    "text": "From Cats to Convolutional Neural Networks\n\nMotivation from Neural Science (1959)\nAn experiment with a cat’s brain in 1959 revealed different parts activated for different stick positions, motivating the concept of receptive fields in convolutional neural networks (CNNs).\n\n\nNeocognitron Model (1980)\nInspired by distributed processing observed in the cat experiment, the Neocognitron model utilized receptive fields for different parts of the network.\n\n\nLeNet Model (1989)\nJan Lecun’s contribution to deep learning, the LeNet model, was employed for recognizing handwritten digits, finding applications in postal services for automated sorting of letters.\n\n\nLeNet-5 Model (1998)\nFurther improvements on the LeNet model, introducing the MNIST dataset for testing CNNs."
  },
  {
    "objectID": "pages/DL/Week01_1.html#history-of-deep-learning",
    "href": "pages/DL/Week01_1.html#history-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "History of Deep Learning",
    "text": "History of Deep Learning\n\n1950s: Enthusiasm in AI.\n1990s: Convolutional Neural Networks (CNNs) used for real-world problems, challenges with large networks and training.\n2006-2012: Advances in deep learning, successful training for ImageNet challenges.\n2016 onwards: Acceleration with better optimization methods (Nesterov’s method), leading to faster convergence.\nOptimization Algorithms: Adagrad, RMSprop, Adam, AdamW, etc., focus on faster and better convergence."
  },
  {
    "objectID": "pages/DL/Week01_1.html#activation-functions",
    "href": "pages/DL/Week01_1.html#activation-functions",
    "title": "A Historical Overview of Deep Learning",
    "section": "Activation Functions",
    "text": "Activation Functions\nThe evolution from the logistic function to various activation functions (ReLU, Leaky ReLU, Parametric ReLU, Tanh, etc.) aimed at stabilizing training, achieving better performance, and faster convergence. The use of improved activation functions contributed to enhanced stability and performance."
  },
  {
    "objectID": "pages/DL/Week01_1.html#sequence-processing",
    "href": "pages/DL/Week01_1.html#sequence-processing",
    "title": "A Historical Overview of Deep Learning",
    "section": "Sequence Processing",
    "text": "Sequence Processing\nIntroduction to problems involving sequences in deep learning, featuring Recurrent Neural Networks (RNNs) proposed in 1982 for sequence processing. Long Short-Term Memory Cells (LSTMs) were introduced in 1997 to address the vanishing gradient problem. By 2014, RNNs and LSTMs dominated natural language processing (NLP) and speech applications. In 2017, Transformer networks started replacing RNNs and LSTMs in sequence learning."
  },
  {
    "objectID": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "href": "pages/DL/Week01_1.html#game-playing-with-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Game Playing with Deep Learning",
    "text": "Game Playing with Deep Learning\n\n2015: Deep Reinforcement Learning (DRL) agents beat humans in Atari games.\nBreakthrough in Go game playing using DRL in 2015.\n2016: DRL-based agents beat professional poker players.\nComplex strategy games like Dota 2 mastered by DRL agents.\nIntroduction of OpenAI Gym as a toolkit for developing and comparing reinforcement learning algorithms.\nEmergence of AlphaStar and MuZero for mastering multiple games and tasks."
  },
  {
    "objectID": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "href": "pages/DL/Week01_1.html#general-trends-in-deep-reinforcement-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "General Trends in Deep Reinforcement Learning",
    "text": "General Trends in Deep Reinforcement Learning\nDeep RL agents consistently outperforming humans in various complex games, progressing from simple environments to mastering complex strategy games. The trend is towards developing “master of all” models (e.g., MuZero) for general intelligence in multiple tasks."
  },
  {
    "objectID": "pages/DL/Week01_1.html#overview",
    "href": "pages/DL/Week01_1.html#overview",
    "title": "A Historical Overview of Deep Learning",
    "section": "Overview",
    "text": "Overview\n\nRevival and Advances\nA recap of deep learning’s revival and recent advances, reflecting an increasing interest in real-world problem-solving and challenges.\n\n\nAI Publications Growth\nThe Stanford AI Index Report highlights a significant increase in AI publications, indicating exponential growth across machine learning, computer vision, and NLP.\n\n\nFunding and Startups\nThe rise of AI startups, coupled with the interest from major tech companies, has led to exponential growth in AI-related patent filings."
  },
  {
    "objectID": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "href": "pages/DL/Week01_1.html#evolution-of-neural-network-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Evolution of Neural Network Models",
    "text": "Evolution of Neural Network Models\n\nIntroduction of Transformers\nIn 2017, transformers were introduced, revolutionizing AI and finding success in NLP, subsequently adopted in other domains.\n\n\nMachine Translation and Transformers\nA historical overview of machine translation, emphasizing the shift from IBM models to neural machine translation. The impact of sequence-to-sequence models (2014) and transformers (2017) is discussed.\n\n\nTransformer-based Models\nThe BERT model (2018) with a focus on pre-training, the evolution of models with increasing parameters from GPT-3 (175 billion) to 1.6 trillion parameters, and a comparison with human brain synapses provide perspective."
  },
  {
    "objectID": "pages/DL/Week01_1.html#transformers-in-vision",
    "href": "pages/DL/Week01_1.html#transformers-in-vision",
    "title": "A Historical Overview of Deep Learning",
    "section": "Transformers in Vision",
    "text": "Transformers in Vision\n\nAdoption in Image\nClassification The evolution of image classification models, starting with AlexNet (2012), is traced. Transformers entered image classification and object detection in 2019, marking a paradigm shift towards transformers in state-of-the-art models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#generative-models",
    "href": "pages/DL/Week01_1.html#generative-models",
    "title": "A Historical Overview of Deep Learning",
    "section": "Generative Models",
    "text": "Generative Models\n\nOverview\nAn introduction to generative models for image synthesis, covering the evolution from variation autoencoders to GANs (Generative Adversarial Networks). Recent developments in diffusion-based models overcoming GAN drawbacks are discussed.\n\n\nDALL-E and DALL-E 2\nDALL-E’s capability to generate realistic images based on text prompts is explored. The introduction of DALL-E 2, a diffusion-based model, exceeding expectations, is highlighted with examples of generated images showcasing photorealistic results.\n\n\nExciting Times in Generative Models\nThe exploration of generative models for realistic image generation is showcased, with examples of prompts generating photorealistic images illustrating field advancements."
  },
  {
    "objectID": "pages/DL/Week01_1.html#introduction-1",
    "href": "pages/DL/Week01_1.html#introduction-1",
    "title": "A Historical Overview of Deep Learning",
    "section": "Introduction",
    "text": "Introduction\nRapid advancements in deep learning have yielded powerful models trained on large datasets, showcasing impressive results. However, there is a growing need for sanity, interpretability, fairness, and responsibility in deploying these models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "href": "pages/DL/Week01_1.html#paradox-of-deep-learning",
    "title": "A Historical Overview of Deep Learning",
    "section": "Paradox of Deep Learning",
    "text": "Paradox of Deep Learning\nDespite the high capacity of deep learning models, they exhibit remarkable performance. Challenges include numerical instability, sharp minima, and susceptibility to adversarial examples."
  },
  {
    "objectID": "pages/DL/Week01_1.html#calls-for-sanity",
    "href": "pages/DL/Week01_1.html#calls-for-sanity",
    "title": "A Historical Overview of Deep Learning",
    "section": "Calls for Sanity",
    "text": "Calls for Sanity\nEmphasis is placed on explainability and interpretability to comprehend model decisions. Advances include workshops on human interpretability, tools like the Clever Hans toolkit to identify model reliance on cues, and benchmarking on adversarial examples."
  },
  {
    "objectID": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "href": "pages/DL/Week01_1.html#fairness-and-responsibility",
    "title": "A Historical Overview of Deep Learning",
    "section": "Fairness and Responsibility",
    "text": "Fairness and Responsibility\nIncreasing awareness of biases in AI models, particularly in facial recognition and criminal risk predictions, has led to concerns about fairness. Efforts such as the AI audit challenge at Stanford focus on building non-discriminatory models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#green-ai",
    "href": "pages/DL/Week01_1.html#green-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Green AI",
    "text": "Green AI\nRising environmental concerns due to the high computational power and energy consumption of deep learning models have spurred calls for responsible AI, extending to the environmental impact. There is a push for more energy-efficient models."
  },
  {
    "objectID": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "href": "pages/DL/Week01_1.html#exciting-times-in-ai",
    "title": "A Historical Overview of Deep Learning",
    "section": "Exciting Times in AI",
    "text": "Exciting Times in AI\nThe AI revolution is influencing scientific research, evident in DeepMind’s AlphaFold predicting protein folding. Applications in astronomy, predicting galaxy aging, and generating images for fundamental variables in experimental data are emerging. There is an emphasis on efficient deep learning for mobile devices, edge computing, and addressing constraints of power, storage, and real-time processing."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BS Degree Notes",
    "section": "",
    "text": "I put the course notes for my own easy perusal."
  },
  {
    "objectID": "pages/DL/Week01_2.html",
    "href": "pages/DL/Week01_2.html",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units.\n\n\n\n\n\n\nDendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter.\n\n\n\n\n\nLayered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli.\n\n\n\n\n\n\nNeural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction",
    "href": "pages/DL/Week01_2.html#introduction",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units."
  },
  {
    "objectID": "pages/DL/Week01_2.html#biological-neurons",
    "href": "pages/DL/Week01_2.html#biological-neurons",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Dendrite: Functions as a signal receiver from other neurons.\nSynapse: The connection point between neurons.\nSoma: The central processing unit for information.\nAxon: Transmits processed information to other neurons.\n\n\n\n\nIn a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter."
  },
  {
    "objectID": "pages/DL/Week01_2.html#neural-network-architecture",
    "href": "pages/DL/Week01_2.html#neural-network-architecture",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Layered Structure: Neurons are organized into layers.\nInterconnected Network: The human brain comprises approximately 100 billion neurons.\nDivision of Work: Neurons may specialize in processing specific information types.\nExample: Neurons responding to visual, auditory, or textual stimuli."
  },
  {
    "objectID": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "href": "pages/DL/Week01_2.html#multi-layer-perceptrons-mlps",
    "title": "Motivation from Biological Neuron",
    "section": "",
    "text": "Neural networks with multiple layers.\n\n\n\nInitial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.\n\n\n\nUsing a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.\n\n\n\n\nLayer 1: Detects edges and corners.\nSubsequent Layers: Organize information into features and recognize complex objects.\n\n\n\n\nEach layer processes more abstract representations of the input.\n\n\n\nInput traverses through layers, resulting in a physical response."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-1",
    "href": "pages/DL/Week01_2.html#introduction-1",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\n\nObjective: Comprehend the McCulloch-Pitts neuron, a simplified computational model inspired by biological neurons.\nHistorical Context: Proposed in 1943 by McCulloch (neuroscientist) and Pitts (logician).\nPurpose: Emulate the brain’s complex processing for decision-making."
  },
  {
    "objectID": "pages/DL/Week01_2.html#neuron-structure",
    "href": "pages/DL/Week01_2.html#neuron-structure",
    "title": "Motivation from Biological Neuron",
    "section": "Neuron Structure",
    "text": "Neuron Structure\n\nComponents: Divided into two parts - g and f.\ng (Aggregation): Aggregates binary inputs via a simple summation process.\nf (Decision): Makes a binary decision based on the aggregation.\nExcitatory and Inhibitory Inputs: Inputs can be either excitatory (positive) or inhibitory (negative)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#functionality",
    "href": "pages/DL/Week01_2.html#functionality",
    "title": "Motivation from Biological Neuron",
    "section": "Functionality",
    "text": "Functionality\n\nAggregation Function g(x):\n\nRepresents the sum of all inputs using the formula \\(g(x) = \\sum_{i=1}^{n} x_i\\), where \\(x_i\\) is a binary input (0 or 1).\n\nDecision Function f(g(x)):\n\nUtilizes a threshold parameter \\(\\theta\\) to determine firing.\nDecision is \\(f(g(x)) = \\begin{cases} 1 & \\text{if } g(x) \\geq \\theta \\\\ 0 & \\text{otherwise} \\end{cases}\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#boolean-function-implementation",
    "href": "pages/DL/Week01_2.html#boolean-function-implementation",
    "title": "Motivation from Biological Neuron",
    "section": "Boolean Function Implementation",
    "text": "Boolean Function Implementation\n\nExamples:\n\nImplemented using McCulloch-Pitts neuron for boolean functions like AND, OR, NOR, and NOT.\nExcitatory and inhibitory inputs utilized based on boolean function logic."
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation",
    "href": "pages/DL/Week01_2.html#geometric-interpretation",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\n\nIn 2D:\n\nDraws a line to separate input space into two halves.\n\nIn 3D:\n\nUses a plane for separation.\n\nFor n Inputs:\n\nUtilizes a hyperplane for linear separation."
  },
  {
    "objectID": "pages/DL/Week01_2.html#linear-separability",
    "href": "pages/DL/Week01_2.html#linear-separability",
    "title": "Motivation from Biological Neuron",
    "section": "Linear Separability",
    "text": "Linear Separability\n\nDefinition: Boolean functions representable by a single McCulloch-Pitts neuron are linearly separable.\nImplication: Implies the existence of a plane (or hyperplane) separating points with output 0 and 1."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-2",
    "href": "pages/DL/Week01_2.html#introduction-2",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nPerceptrons, introduced by Frank Rosenblatt circa 1958, extend the concept of McCulloch-Pitts neurons with non-Boolean inputs, input weights, and a learning algorithm for weight adjustment."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-model",
    "href": "pages/DL/Week01_2.html#perceptron-model",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\n\nMathematical Representation\nThe perceptron is represented as \\(y = 1\\) if \\(\\sum_{i=1}^{n} w_i x_i \\geq \\text{threshold}\\); otherwise, \\(y = 0\\).\n\nNotable Differences\n\nInputs can be real, not just Boolean.\nIntroduction of weights, denoted by \\(w_i\\), indicating input importance.\nLearning algorithm to adapt weights based on data.\n\n\n\n\nNeater Formulation\nThe equation is rearranged for simplicity: \\(\\sum_{i=0}^{n} w_i x_i \\geq 0\\), where \\(x_0 = 1\\) and \\(w_0 = -\\text{threshold}\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "href": "pages/DL/Week01_2.html#motivation-for-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation for Boolean Functions",
    "text": "Motivation for Boolean Functions\nBoolean functions provide a foundation for understanding perceptrons. For instance, predicting movie preferences using Boolean inputs such as actor, director, and genre."
  },
  {
    "objectID": "pages/DL/Week01_2.html#importance-of-weights",
    "href": "pages/DL/Week01_2.html#importance-of-weights",
    "title": "Motivation from Biological Neuron",
    "section": "Importance of Weights",
    "text": "Importance of Weights\nWeights signify the importance of specific inputs in decision-making. Learning from data helps adjust weights, reflecting user preferences. For example, assigning a high weight to the director may heavily influence the decision to watch a movie."
  },
  {
    "objectID": "pages/DL/Week01_2.html#bias-w_0",
    "href": "pages/DL/Week01_2.html#bias-w_0",
    "title": "Motivation from Biological Neuron",
    "section": "Bias (\\(w_0\\))",
    "text": "Bias (\\(w_0\\))\n\\(w_0\\) acts as a bias or prior, influencing decision-making. It represents the initial bias or prejudice in decision-making. Adjusting \\(w_0\\) alters the decision threshold, accommodating user preferences."
  },
  {
    "objectID": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "href": "pages/DL/Week01_2.html#implementing-boolean-functions",
    "title": "Motivation from Biological Neuron",
    "section": "Implementing Boolean Functions",
    "text": "Implementing Boolean Functions\nPerceptrons can implement Boolean functions with linear decision boundaries. For instance, implementing the OR function with a perceptron involves a geometric interpretation where a line separates positive and negative regions based on inputs."
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-adjustments",
    "href": "pages/DL/Week01_2.html#errors-and-adjustments",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Adjustments",
    "text": "Errors and Adjustments\nErrors arise when the decision boundary misclassifies inputs. The learning algorithm adjusts weights iteratively to minimize errors and enhance accuracy. It’s an iterative process where weights are modified until the desired decision boundary is achieved."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-3",
    "href": "pages/DL/Week01_2.html#introduction-3",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThis section delves into errors within the context of perceptrons and introduces error surfaces as a recurring theme in the course, with a focus on understanding errors related to linear separability."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "href": "pages/DL/Week01_2.html#perceptron-for-and-function",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron for AND Function",
    "text": "Perceptron for AND Function\nConsideration of the AND function showcases an output of 1 for a specific input (green) and 0 for others (red). The decision is based on \\(w_0 + w_1x_1 + w_2x_2 \\geq 0\\), with \\(w_0\\) fixed at -1. Exploration of the impact of \\(w_1\\) and \\(w_2\\) on the decision boundary is undertaken."
  },
  {
    "objectID": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "href": "pages/DL/Week01_2.html#errors-and-decision-boundaries",
    "title": "Motivation from Biological Neuron",
    "section": "Errors and Decision Boundaries",
    "text": "Errors and Decision Boundaries\nDemonstration of errors occurs with specific \\(w_1\\) and \\(w_2\\) values, showcasing misclassified points due to incorrect decision boundaries. Variability in errors is noted based on different weight values."
  },
  {
    "objectID": "pages/DL/Week01_2.html#error-function",
    "href": "pages/DL/Week01_2.html#error-function",
    "title": "Motivation from Biological Neuron",
    "section": "Error Function",
    "text": "Error Function\nViewing error as a function of \\(w_1\\) and \\(w_2\\) is introduced. The concept of error surfaces is brought in, where error is plotted against \\(w_1\\) and \\(w_2\\) values, each region on the surface corresponding to a distinct error level."
  },
  {
    "objectID": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "href": "pages/DL/Week01_2.html#visualizing-the-error-surface",
    "title": "Motivation from Biological Neuron",
    "section": "Visualizing the Error Surface",
    "text": "Visualizing the Error Surface\nThe error surface is plotted for \\(w_1\\) and \\(w_2\\) values in the range -4 to +4. Each region on the surface corresponds to a distinct error level, highlighting the utility of visualizations in comprehending perceptron behavior."
  },
  {
    "objectID": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "href": "pages/DL/Week01_2.html#perceptron-learning-algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Perceptron Learning Algorithm",
    "text": "Perceptron Learning Algorithm\nExploration of the necessity for an algorithmic approach to finding optimal \\(w_1\\) and \\(w_2\\) values is undertaken. Limitations in visual inspection, especially in higher dimensions, are acknowledged. A teaser for the upcoming module on the perceptron learning algorithm is provided as a solution for finding suitable weight values algorithmically."
  },
  {
    "objectID": "pages/DL/Week01_2.html#overview",
    "href": "pages/DL/Week01_2.html#overview",
    "title": "Motivation from Biological Neuron",
    "section": "Overview",
    "text": "Overview\nThis module focuses on the Perceptron Learning Algorithm, building upon the perceptron’s concept and introducing a method to iteratively adjust weights for accurate binary classification."
  },
  {
    "objectID": "pages/DL/Week01_2.html#motivation",
    "href": "pages/DL/Week01_2.html#motivation",
    "title": "Motivation from Biological Neuron",
    "section": "Motivation",
    "text": "Motivation\nThe perceptron, initially designed for boolean functions, finds practical application in real-world scenarios. Consider a movie recommendation system based on past preferences, where features include both boolean and real-valued inputs. The goal is to learn weights that enable accurate predictions for new inputs."
  },
  {
    "objectID": "pages/DL/Week01_2.html#algorithm",
    "href": "pages/DL/Week01_2.html#algorithm",
    "title": "Motivation from Biological Neuron",
    "section": "Algorithm",
    "text": "Algorithm\n\nNotations\n\n\\(p\\): Inputs with label 1 (positive points)\n\\(n\\): Inputs with label 0 (negative points)\n\n\n\nConvergence\nConvergence is achieved when all positive points satisfy \\(\\sum w_i x_i &gt; 0\\) and all negative points satisfy \\(\\sum w_i x_i &lt; 0\\).\n\n\nSteps\n\nInitialization: Randomly initialize weights \\(w\\).\nIterative Update:\n\nWhile not converged:\n\nPick a random point \\(x\\) from \\(p \\cup n\\).\nIf \\(x\\) is in \\(p\\) and \\(w^T x &lt; 0\\), update \\(w = w + x\\).\nIf \\(x\\) is in \\(n\\) and \\(w^T x \\geq 0\\), update \\(w = w - x\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "href": "pages/DL/Week01_2.html#geometric-interpretation-1",
    "title": "Motivation from Biological Neuron",
    "section": "Geometric Interpretation",
    "text": "Geometric Interpretation\nUnderstanding the geometric relationship involves recognizing that the angle between \\(w\\) and a point on the decision boundary is 90 degrees. Positive points’ angles should be acute (&lt; 90 degrees), and negative points’ angles should be obtuse (&gt; 90 degrees). Iteratively adjusting \\(w\\) aligns it better with correctly classified points."
  },
  {
    "objectID": "pages/DL/Week01_2.html#introduction-4",
    "href": "pages/DL/Week01_2.html#introduction-4",
    "title": "Motivation from Biological Neuron",
    "section": "Introduction",
    "text": "Introduction\nThe objective of this lecture is to present a formal proof establishing the convergence of the perceptron learning algorithm. The primary focus is to rigorously determine whether the algorithm exhibits convergence or continues weight updates indefinitely."
  },
  {
    "objectID": "pages/DL/Week01_2.html#definitions",
    "href": "pages/DL/Week01_2.html#definitions",
    "title": "Motivation from Biological Neuron",
    "section": "Definitions",
    "text": "Definitions\n\nAbsolutely Linearly Separable Sets\n\nConsider two sets, \\(P\\) and \\(N\\), in an \\(n\\)-dimensional space. They are deemed absolutely linearly separable if there exist \\(n + 1\\) real numbers \\(w_0\\) to \\(w_n\\) such that the following conditions hold: \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n \\geq 0 \\quad \\text{for every } \\mathbf{x} \\in P\n\\] \\[\nw_0x_0 + w_1x_1 + \\ldots + w_nx_n &lt; 0 \\quad \\text{for every } \\mathbf{x} \\in N\n\\]\n\nPerceptron Learning Algorithm Convergence Theorem\n\nIf sets \\(P\\) and \\(N\\) are finite and linearly separable, the perceptron learning algorithm will update the weight vector a finite number of times. This implies that after a finite number of steps, the algorithm will find a weight vector \\(\\mathbf{w}\\) capable of separating sets \\(P\\) and \\(N\\)."
  },
  {
    "objectID": "pages/DL/Week01_2.html#proof",
    "href": "pages/DL/Week01_2.html#proof",
    "title": "Motivation from Biological Neuron",
    "section": "Proof",
    "text": "Proof\n\nSetup\nDefine \\(P'\\) as the union of \\(P\\) and the negation of \\(N\\). Normalize all inputs for convenience.\n\n\nAssumptions and Definitions\nAssume the existence of a normalized solution vector \\(\\mathbf{w^*}\\). Define the minimum dot product, \\(\\delta\\), as the minimum value obtained by dot products between \\(\\mathbf{w^*}\\) and points in \\(P'\\).\n\n\nPerceptron Learning Algorithm\nThe perceptron learning algorithm can be expressed as follows:\n\nInitialization:\n\nInitialize weight vector \\(\\mathbf{w}\\) randomly.\n\nIteration:\n\nAt each iteration, randomly select a point \\(\\mathbf{p}\\) from \\(P'\\).\nIf the condition \\(\\mathbf{w}^T\\mathbf{p} \\geq 0\\) is not satisfied, update \\(\\mathbf{w}\\) by \\(\\mathbf{w} = \\mathbf{w} + \\mathbf{p}\\).\n\n\n\n\nNormalization and Definitions\nNormalize all inputs, ensuring the norm of \\(\\mathbf{p}\\) is 1. Define the numerator of \\(\\cos \\beta\\) as the dot product between \\(\\mathbf{w^*}\\) and the updated weight vector at each iteration.\n\n\nNumerator Analysis\nShow that the numerator is greater than or equal to \\(\\delta\\) for each iteration.\nFor a randomly selected \\(\\mathbf{p}\\), if \\(\\mathbf{w}^T\\mathbf{p} &lt; 0\\) and an update is performed, the numerator is:\n\\[\n\\mathbf{w^*} \\cdot (\\mathbf{w} + \\mathbf{p}) \\geq \\delta\n\\]\n\n\nDenominator Analysis\nExpand the denominator, the square of the norm of the updated weight vector:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 = \\|\\mathbf{w}\\|^2 + 2\\mathbf{w}^T\\mathbf{p} + \\|\\mathbf{p}\\|^2\n\\]\nShow that the denominator is less than or equal to a value involving \\(k\\), the number of updates made:\n\\[\n\\|\\mathbf{w} + \\mathbf{p}\\|^2 \\leq \\|\\mathbf{w^*}\\|^2 + k\n\\]\n\n\nCombining Numerator and Denominator\nUse the definition of \\(\\cos \\beta\\) to conclude that \\(\\cos \\beta\\) is greater than or equal to a certain quantity involving the square root of \\(k\\):\n\\[\n\\cos \\beta \\geq \\frac{\\delta}{\\sqrt{k}}\n\\]"
  },
  {
    "objectID": "pages/AI/Week01.html",
    "href": "pages/AI/Week01.html",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning.\n\n\n\n\nData Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward.\n\n\n\n\n\n\nThe foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers.\n\n\n\n\n\n\nThe training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology.\n\n\n\n\n\n\n\nThe dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification.\n\n\n\n\n\n\n\nNeural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains.\n\n\n\n\n\n\nThe triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously."
  },
  {
    "objectID": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "href": "pages/AI/Week01.html#overview-of-the-past-decade-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The preceding decade has marked a significant upswing in interest and advancements within the realm of machine learning (ML). This surge is attributable to the confluence of increased data availability facilitated by the ubiquity of the internet and the simultaneous enhancement of computational power. Central to this transformation has been the evolution of sophisticated training algorithms, particularly within the domain of deep learning."
  },
  {
    "objectID": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "href": "pages/AI/Week01.html#key-drivers-of-ml-advancements",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Data Explosion: The pervasive nature of the internet has ushered in an unparalleled era of data abundance, fundamentally reshaping the landscape of machine learning.\nIncreased Computing Power: Strides in computing capabilities have substantially amplified the processing capabilities for handling vast datasets, a crucial enabler for ML progress.\nNeural Network Advancements: Noteworthy progress in training algorithms, especially those tailored for neural networks, has played a pivotal role in propelling the field forward."
  },
  {
    "objectID": "pages/AI/Week01.html#evolution-of-neural-networks",
    "href": "pages/AI/Week01.html#evolution-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The foundational era witnessed the inception of the perceptron, a single-layered neural network devised as a binary classifier by McCulloch and Pitts in 1943. However, the limitation of this era lay in the perceptron’s ability to only classify linearly separable classes.\n\n\n\nThe subsequent evolution involved the introduction of the multi-layer perceptron by Rumelhart, Hinton, and Williams. This innovation addressed the limitations associated with linear separability, with the popularization of the backpropagation algorithm for training feedforward networks.\n\n\n\nDeep neural networks, characterized by numerous hidden layers, emerged as a game-changer in computer vision tasks. The breakthrough in 2012 by Hinton, LeCun, and Bengio underscored the efficacy of deep neural networks in recognizing diverse object types. The general architecture encompasses input layers, hidden layers, and output layers."
  },
  {
    "objectID": "pages/AI/Week01.html#training-process-of-neural-networks",
    "href": "pages/AI/Week01.html#training-process-of-neural-networks",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The training process predominantly involves supervised learning, wherein images are presented alongside their corresponding expected outputs. The iterative application of the backpropagation algorithm facilitates weight adjustments based on the disparity between predicted and expected outputs. Consequently, neural networks acquire the ability to classify and distinguish input data through repetitive exposure.\n\n\n\n\n\nDeep neural networks exhibit excellence in medical diagnosis, particularly in discerning diseases from images, as evidenced in the domain of breast cancer detection.\n\n\n\nThe instrumental role of deep neural networks in face recognition is noteworthy, aiding in the identification of individuals within images.\n\n\n\nThe Face2Gene app serves as a tangible manifestation of the successful application of deep neural networks. It aids medical professionals in diagnosing genetic disorders based on facial features, showcasing the practical impact of this technology."
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "href": "pages/AI/Week01.html#machine-learning-in-internet-interaction",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The dynamic interaction of users with the internet inadvertently transforms them into valuable data points for machine learning algorithms. These algorithms, wielded by major tech entities, classify users to customize ads and optimize overall user experiences.\n\n\n\n\n\nMachine learning, particularly in pattern recognition, demonstrates capabilities akin to those observed in the animal kingdom. However, it falls short of encompassing the comprehensive cognitive functions characteristic of human intelligence.\n\n\n\nHuman cognitive abilities span goal-directed, autonomous action, and a capacity for collective approaches. Distinctive human attributes include planning, wealth accumulation, home-building, and fostering societal diversification."
  },
  {
    "objectID": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "href": "pages/AI/Week01.html#performance-vs.-competence-in-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "Neural networks showcase proficiency in specific tasks but lack a holistic understanding of the world. The inherent brittleness of machine learning necessitates meticulous preparation, coding, and specialized training for diverse problem domains."
  },
  {
    "objectID": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "href": "pages/AI/Week01.html#game-of-go-and-reinforcement-learning",
    "title": "A Decade of Machine Learning",
    "section": "",
    "text": "The triumph of Alphago, developed by DeepMind, stands out as a testament to the success achievable through reinforcement learning. This approach played a pivotal role in training the program for strategic decision-making. Subsequent iterations, such as Alphago Zero and AlphaZero, demonstrated the capacity to learn autonomously without human intervention and master multiple games simultaneously."
  },
  {
    "objectID": "pages/AI/Week01.html#human-cognition-and-ai",
    "href": "pages/AI/Week01.html#human-cognition-and-ai",
    "title": "A Decade of Machine Learning",
    "section": "Human Cognition and AI",
    "text": "Human Cognition and AI\n\nCognitive Landscape\nHuman intelligence engages in a myriad of activities such as logic, representation, planning, reasoning, and search. The crux of these cognitive endeavors lies in symbolic reasoning, a substantial facet of the human cognitive load.\n\n\nSymbolic Reasoning\nSymbolic reasoning, integral to human cognition, involves the management of symbolic knowledge representation and intricate problem-solving processes."
  },
  {
    "objectID": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "href": "pages/AI/Week01.html#distinguishing-ai-from-machine-learning",
    "title": "A Decade of Machine Learning",
    "section": "Distinguishing AI from Machine Learning",
    "text": "Distinguishing AI from Machine Learning\n\nAI Emphasis\nWithin the domain of Artificial Intelligence (AI), the spotlight is on symbolic knowledge representation and advanced problem-solving methodologies.\n\n\nMachine Learning Focus\nIn contrast, Machine Learning (ML) gravitates towards interpreting data, with applications ranging from recommender systems to predictive analytics and classification."
  },
  {
    "objectID": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "href": "pages/AI/Week01.html#knowledge-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Knowledge Representation in AI",
    "text": "Knowledge Representation in AI\n\nDeclarative Knowledge\nAligning with the cognitive domain of humans, explicit symbolic representation, known as declarative knowledge, assumes a pivotal role. It encompasses the representation of the world and engages in reasoned deductions.\n\n\nInferences in AI\nAI agents showcase a spectrum of inferences, ranging from deductive reasoning based on logic to plausible or probabilistic inferences that incorporate an element of likelihood."
  },
  {
    "objectID": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "href": "pages/AI/Week01.html#symbolic-representation-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Symbolic Representation in AI",
    "text": "Symbolic Representation in AI\n\nDefining Symbols\nSymbols, representing abstract concepts, manifest in diverse forms. For example, the number (7) can be expressed in various ways, illustrating the distinction between the conceptualization of numbers and their symbolic representations.\n\n\nMeaning of Symbols\nThe significance of symbols is socially agreed upon, forming the foundation for semiotic systems. Whether in road signs or linguistic characters, symbols encapsulate shared meanings."
  },
  {
    "objectID": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "href": "pages/AI/Week01.html#semiotics-and-biosemiotics",
    "title": "A Decade of Machine Learning",
    "section": "Semiotics and Biosemiotics",
    "text": "Semiotics and Biosemiotics\n\nSemiotics\nSemiotics, the scientific study of symbols in spoken and written languages, lays the groundwork for comprehending human communication and representation.\n\n\nBiosemiotics\nDelving deeper, Biosemiotics explores the emergence of complex behavior when simple systems engage in symbolic communication. This is exemplified by phenomena such as ant trails utilizing pheromones."
  },
  {
    "objectID": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "href": "pages/AI/Week01.html#reasoning-mechanisms-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Reasoning Mechanisms in AI",
    "text": "Reasoning Mechanisms in AI\n\nFormal Reasoning\nIn the context of AI, reasoning involves the systematic manipulation of symbols in a meaningful manner. This encompasses algorithms for fundamental operations like addition and multiplication, extending to more intricate processes like the Fourier transform.\n\n\nConceptualizing Algorithms\nUnderstanding AI algorithms necessitates a conceptual grasp of symbolic manipulations. For instance, multiplication algorithms entail conceptualizing the multiplication of unit digits and the subsequent shifting of results."
  },
  {
    "objectID": "pages/AI/Week01.html#automation-vs.-ai",
    "href": "pages/AI/Week01.html#automation-vs.-ai",
    "title": "A Decade of Machine Learning",
    "section": "Automation vs. AI",
    "text": "Automation vs. AI\n\nNavigating Overlap\nWhile Automation and AI share common ground, not all automated systems integrate AI. For instance, implementations such as train reservation systems or basic online shopping may lack substantial AI components."
  },
  {
    "objectID": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "href": "pages/AI/Week01.html#machine-learnings-role-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Machine Learning’s Role in AI",
    "text": "Machine Learning’s Role in AI\n\nML as a Component\nMachine Learning constitutes one facet of the multifaceted field of AI. Examples such as self-driving cars leverage ML for tasks including pattern recognition, speech processing, and object classification."
  },
  {
    "objectID": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "href": "pages/AI/Week01.html#clarifying-data-science-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Clarifying Data Science in AI",
    "text": "Clarifying Data Science in AI\n\nData’s Multifaceted Role\nData science, encompassing elements of statistics, AI, and machine learning, plays a crucial role in the broader field of AI. It serves as a foundational component but does not encapsulate the entirety of AI."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "href": "pages/AI/Week01.html#introduction-to-ai-and-definitions",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to AI and Definitions:",
    "text": "Introduction to AI and Definitions:\n\n\nDefinitions of AI:\n\nHerbert Simon: Programs are considered intelligent if they display behaviors regarded as intelligent in humans.\nBar and Feigenbaum: AI seeks to comprehend the systematic behavior of information processing systems, analogous to physicists and biologists in their respective domains.\nElaine Rich: AI involves solving exponentially hard problems in polynomial time, leveraging domain-specific knowledge.\nJohn Hoagland: AI’s goal is to create machines with minds of their own, treating thinking and computing as fundamentally interconnected."
  },
  {
    "objectID": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "href": "pages/AI/Week01.html#fundamental-questions-in-ai",
    "title": "A Decade of Machine Learning",
    "section": "Fundamental Questions in AI:",
    "text": "Fundamental Questions in AI:\n\nQuestions about Intelligence:\nVarious perspectives exist on what constitutes intelligence, encompassing language use, reasoning, and learning. Ongoing debates revolve around whether machines can genuinely exhibit thinking, with insights from philosophers like Roger Penrose exploring quantum mechanics in the human brain."
  },
  {
    "objectID": "pages/AI/Week01.html#turing-test-and-challenges",
    "href": "pages/AI/Week01.html#turing-test-and-challenges",
    "title": "A Decade of Machine Learning",
    "section": "Turing Test and Challenges:",
    "text": "Turing Test and Challenges:\n\nAlan Turing’s Turing Test:\nThe evaluation of machine intelligence through its ability to convincingly engage in natural language conversations with a human judge forms the essence of the Turing Test. Associated challenges include situations where chatbots may impress but lack genuine intelligence. The Löbner Prize Competition attempts a similar test.\n\n\nHector Levesque’s Alternative: Vinograd Schemas\nAn alternate test proposed by Hector Levesque challenges a machine’s understanding through multiple-choice questions that require subject matter knowledge."
  },
  {
    "objectID": "pages/AI/Week01.html#vinograd-schemas-examples",
    "href": "pages/AI/Week01.html#vinograd-schemas-examples",
    "title": "A Decade of Machine Learning",
    "section": "Vinograd Schemas Examples:",
    "text": "Vinograd Schemas Examples:\n\nExample 1:\n\nOriginal Sentence: “The city council refused the demonstrators a permit because they feared violence.”\nAlternate Sentence: “The city council refused the demonstrators a permit because they advocated violence.”\nQuestion: What does “they” refer to? Options: Council, Demonstrators.\n\nExample 2:\n\nOriginal Sentence: “John took the water bottle out of the backpack so that it would be lighter.”\nAlternate Sentence: “John took the water bottle out of the backpack so that it would be handy.”\nQuestion: What does “it” refer to? Options: Backpack, Water Bottle.\n\nExample 3:\n\nOriginal Sentence: “The trophy would not fit into the brown suitcase because it was too small.”\nAlternate Sentence: “The trophy would not fit into the brown suitcase because it was too big.”\nQuestion: What does “it” refer to? Options: Trophy, Brown Suitcase.\n\nExample 4:\n\nOriginal Sentence: “The lawyer asked the witness a question but he was reluctant to repeat it.”\nAlternate Sentence: “The lawyer asked the witness a question but he was reluctant to answer it.”\nQuestion: Who was reluctant? Options: Lawyer, Witness."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "href": "pages/AI/Week01.html#introduction-to-intelligence-and-ai-goals",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Intelligence and AI Goals",
    "text": "Introduction to Intelligence and AI Goals\nIn the exploration of artificial intelligence (AI), the concept of intelligence takes center stage. AI endeavors to construct intelligent agents capable of complex problem-solving. A historical glimpse into European thinkers sheds light on the roots of AI ideologies.\n\nGalileo Galilei (1623)\nIn his 1623 publication, Galileo Galilei delves into the subjective nature of sensory experiences. He contends that taste, odors, and colors are subjective perceptions residing in consciousness. Galileo challenges the idea that these qualities exist inherently in external objects. Moreover, he posits that philosophy is expressed through the language of mathematics.\n\n\nThomas Hobbs\nThomas Hobbs, often referred to as the grandfather of AI, introduces the notion that thinking involves the manipulation of symbols. He associates reasoning with computation, not in the contemporary sense of computers, but as a form of mathematical operations. Hobbs views computation as the sum of many things added together or the determination of the remainder when one thing is subtracted from another.\n\n\nRené Descartes\nBuilding on Galileo’s ideas, Descartes extends the concept that animals are intricate machines, reserving acknowledgment of a mind solely for humans. He aligns thought with symbols and introduces the mind-body dualism, raising questions about the interaction between the mental world and the physical body."
  },
  {
    "objectID": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "href": "pages/AI/Week01.html#early-concepts-of-thinking-machines",
    "title": "A Decade of Machine Learning",
    "section": "Early Concepts of Thinking Machines",
    "text": "Early Concepts of Thinking Machines\nThe early stages of envisioning thinking machines were influenced by the use of punch cards in the textile industry’s Jacquard looms.\n\nJacquard Looms\nPunch cards were employed to control patterns in textile looms. This concept of punched cards was later adapted for programming early computers, emphasizing a transition from controlling patterns to controlling programs.\n\n\nCharles Babbage and Augusta Ada Byron\nCharles Babbage, a mathematician and inventor, conceptualized the Difference Engine and the Analytic Engine. Augusta Ada Byron, daughter of Lord Byron, collaborated with Babbage and is recognized as the world’s first programmer. She envisioned computers going beyond mere number crunching, foreseeing applications in music composition and AI-like capabilities."
  },
  {
    "objectID": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "href": "pages/AI/Week01.html#mechanical-calculators-and-early-computers",
    "title": "A Decade of Machine Learning",
    "section": "Mechanical Calculators and Early Computers",
    "text": "Mechanical Calculators and Early Computers\nThe evolution of mechanical calculators and the emergence of early electronic computers marked significant progress in computational capabilities.\n\nPascal’s Calculator and Leibniz’s Step Drum\nPascal’s mechanical calculator incorporated Latin Lantern gears, performing basic arithmetic operations. Leibniz introduced the step drum, a mechanism for counting and representing numbers. Both contributed to the development of early calculating machines.\n\n\nENIAC (Electronic Numerical Integrator and Computer)\nENIAC, the first electronic computer, boasted over 17,000 vacuum tubes. Despite its immense size and weight, it laid the foundation for electronic computing. Augusta Ada Byron’s visionary insights into the potential of computers started to materialize with the advent of ENIAC."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction",
    "href": "pages/AI/Week01.html#introduction",
    "title": "A Decade of Machine Learning",
    "section": "Introduction",
    "text": "Introduction\nThe course provides a comprehensive exploration of the evolution and fundamental principles of Artificial Intelligence (AI). With historical roots reaching back to the 1300s, early attempts by figures like Jazari and Ramon Llull set the stage for the development of AI.\n\nCoined Terminology\nThe term “Artificial Intelligence” was officially coined by John McCarthy during the Dartmouth Conference in 1956. This landmark event, organized alongside Marvin Minsky and Claude Shannon, aimed to investigate the potential for machines to simulate human intelligence through precise descriptions."
  },
  {
    "objectID": "pages/AI/Week01.html#key-figures",
    "href": "pages/AI/Week01.html#key-figures",
    "title": "A Decade of Machine Learning",
    "section": "Key Figures",
    "text": "Key Figures\n\n1. John McCarthy\n\nCredited with Naming AI\nAssistant Professor at Dartmouth\nDesigner of Lisp Programming Language\nContributions to Logic and Common Sense Reasoning\n\n\n\n2. Marvin Minsky\n\nCo-founder of MIT AI Lab\nNotable for Frame Systems (Foundation of OOP)\nAuthor of “The Society of the Mind” and “The Emotional Machine”\n\n\n\n3. Nathaniel Rochester\n\nIBM Engineer\nDesigner of IBM 701\nSupervised Arthur Samuel and the Checkers-playing Program\n\n\n\n4. Claude Shannon\n\nFather of Information Theory\nMathematician at Bell Labs\n\n\n\n5. Herbert Simon and Allen Newell\n\nDevelopers of Logic Theorist (LT) Program\nPioneers in Symbolic AI\nIntroduction of Physical Symbol Systems\nSimon’s Diverse Scholarship (Nobel Prize in Economics)"
  },
  {
    "objectID": "pages/AI/Week01.html#physical-symbol-systems",
    "href": "pages/AI/Week01.html#physical-symbol-systems",
    "title": "A Decade of Machine Learning",
    "section": "Physical Symbol Systems",
    "text": "Physical Symbol Systems\n\nSymbolic Representation\nSymbol systems represent perceptible entities and adhere to formal laws, mirroring the structure of the physical world. Simon and Newell’s hypothesis posits that a Physical Symbol System is both necessary and sufficient for general intelligent action, distinguishing it from sub-symbolic AI, where information is stored in weights without explicit symbols."
  },
  {
    "objectID": "pages/AI/Week01.html#philosophical-considerations",
    "href": "pages/AI/Week01.html#philosophical-considerations",
    "title": "A Decade of Machine Learning",
    "section": "Philosophical Considerations",
    "text": "Philosophical Considerations\n\nCopernican Shift\n\nGalileo’s Distinction Between Thought and Reality\nGalileo Galilei’s intellectual endeavors were marked by a profound separation between the realm of thought and the objective reality. This conceptual wedge laid the foundation for a nuanced understanding of how human cognition interfaces with the external world.\n\n\nCopernicus’ Challenge to the Geocentric Model, Emphasizing Subjectivity\nCopernicus, through his revolutionary heliocentric model, not only challenged the prevailing geocentric view but also underscored the subjectivity inherent in our interpretations of celestial motions. This shift forced a reconsideration of humanity’s position in the cosmos.\n\n\nHuman Creation of Mental Models; Reality Comprising Fundamental Particles\nHumans engage in the active creation of mental models to comprehend the intricacies of reality. The Copernican Shift extends to the microscopic realm, where the abundant nature of fundamental particles renders them unsuitable as standalone elements of representation. Instead, reality is approached through disciplined ontologies, focusing on entities like atoms, molecules, or cells based on the context of study.\n\n\nIllustration through the Powers of Ten Film\nThe Powers of Ten film serves as a captivating medium to illustrate the Copernican Shift, visually portraying the vastness and intricacies of the universe at different scales. This cinematic exploration emphasizes the dynamic interplay between our mental representations and the expansive reality they seek to capture."
  },
  {
    "objectID": "pages/AI/Week01.html#representation-and-reasoning",
    "href": "pages/AI/Week01.html#representation-and-reasoning",
    "title": "A Decade of Machine Learning",
    "section": "Representation and Reasoning",
    "text": "Representation and Reasoning\n\nHuman Reasoning\n\nHuman Reasoning Involves Symbolic Representations\nIn the realm of human cognition, symbolic representations play a pivotal role in the process of reasoning. These symbols serve as cognitive tools that humans manipulate to make sense of the world around them.\n\n\nFundamental Particles Unsuitable as Elements of Representation due to Abundance\nDespite the fundamental nature of particles, their sheer abundance makes them impractical as elemental units of representation. Human cognition necessitates a selective focus, leading to the adoption of more manageable entities like atoms, molecules, or cells, depending on the specific domain of study.\n\n\nRepresentation Depends on the Focus of Study (e.g., Atoms, Molecules, Cells)\nThe choice of representation is intricately tied to the focus of study. Whether delving into the microscopic realm of atoms or exploring the complexity of biological systems at the cellular level, the selection of representational units is driven by the demands of the specific discipline.\n\n\nDiscipline-specific Ontologies Define Level of Detail in Representations\nDiscipline-specific ontologies play a crucial role in determining the level of detail embedded in representations. These structured frameworks provide a systematic approach to capturing and organizing knowledge within distinct domains."
  },
  {
    "objectID": "pages/AI/Week01.html#introduction-to-problem-solving",
    "href": "pages/AI/Week01.html#introduction-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Introduction to Problem Solving",
    "text": "Introduction to Problem Solving\nIn the expansive domain of Artificial Intelligence (AI), problem-solving emerges as the orchestrated actions of autonomous agents navigating predefined objectives within dynamic environments. This course delves into the intricacies of problem-solving, elucidating the diverse methodologies encapsulated within search methods.\n\nProblem-Solving Framework\n\nAgent and Environment:\n\nAutonomous agents operate within a world defined by specific objectives and a repertoire of actions. Decision-making unfolds in real-time, navigating challenges posed by incomplete knowledge and the concurrent activities of other agents.\n\nSimplifying Assumptions:\n\nInitial simplifications envision a static world with a solitary agent making decisions, providing foundational insights into fundamental problem-solving principles."
  },
  {
    "objectID": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "href": "pages/AI/Week01.html#two-approaches-to-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Two Approaches to Problem Solving",
    "text": "Two Approaches to Problem Solving\n\n1. Model-Based Reasoning (Search Methods)\n\nDefinition:\nModel-Based Reasoning involves grounded reasoning in first principles or search approaches, wherein agents experiment with diverse actions to discern their efficacy.\n\n\nAssumptions:\nThis approach assumes a static world, complete knowledge, and actions that never fail, forming the foundational basis for problem-solving methodologies.\n\n\n\n2. Knowledge-Based Approach\n\nCharacteristics:\nThe Knowledge-Based Approach draws upon a societal structure rich in stored experiences, leveraging accumulated knowledge for effective problem-solving. It encompasses memory-based reasoning, case-based reasoning, and machine learning paradigms."
  },
  {
    "objectID": "pages/AI/Week01.html#rubiks-cube-example",
    "href": "pages/AI/Week01.html#rubiks-cube-example",
    "title": "A Decade of Machine Learning",
    "section": "Rubik’s Cube Example",
    "text": "Rubik’s Cube Example\nThe Rubik’s Cube serves as an illustrative example, elucidating the dichotomy between knowledge-based and search-based problem-solving approaches.\n\nLearning Dynamics\n\nInitial Challenge:\n\nThe Rubik’s Cube presents an initial challenge devoid of a known solution, necessitating exploratory actions.\n\nEvolution of Knowledge:\n\nOver time, individuals develop efficient solving methods through experiential learning, showcasing the adaptive nature of human problem-solving.\n\nDeep Reinforcement Learning:\n\nThe introduction of deep reinforcement learning emphasizes autonomous learning without human guidance, mirroring aspects of artificial intelligence."
  },
  {
    "objectID": "pages/AI/Week01.html#sudoku-example",
    "href": "pages/AI/Week01.html#sudoku-example",
    "title": "A Decade of Machine Learning",
    "section": "Sudoku Example",
    "text": "Sudoku Example\nThe Sudoku puzzle exemplifies the synergy between search and reasoning in problem-solving, offering insights into the nuanced interplay of diverse problem-solving methodologies.\n\nCombined Approach\n\nSearch Methods:\n\nBasic search algorithms, such as depth-first search and breadth-first search, lay the foundation for problem-solving endeavors.\n\nReasoning:\n\nReasoning techniques refine available options, harmonizing search methodologies with informed decision-making for a holistic problem-solving strategy."
  },
  {
    "objectID": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "href": "pages/AI/Week01.html#role-of-logic-in-problem-solving",
    "title": "A Decade of Machine Learning",
    "section": "Role of Logic in Problem Solving",
    "text": "Role of Logic in Problem Solving\nLogic, particularly first-order logic, assumes a pivotal role in representing knowledge and facilitating deductive reasoning within the problem-solving paradigm.\n\nLogical Components\n\nDeductive Reasoning:\n\nLogic functions as a tool for deductive reasoning, employing principles such as deduction, induction, abduction, and plausible reasoning to navigate complex problem spaces.\n\nConstraint Processing:\n\nLogic, search methods, and other reasoning approaches converge under the umbrella of constraint processing, offering a comprehensive framework for addressing intricate problem scenarios."
  },
  {
    "objectID": "pages/AI/Week01.html#map-coloring-problem",
    "href": "pages/AI/Week01.html#map-coloring-problem",
    "title": "A Decade of Machine Learning",
    "section": "Map Coloring Problem",
    "text": "Map Coloring Problem\nThe Map Coloring Problem stands as an exemplary challenge within AI, involving the assignment of colors to regions while adhering to specific constraints.\n\nConstraint Graph Representation\n\nGraph Transformation:\n\nRegions and their color preferences undergo a transformative process, manifesting as a constraint graph that encapsulates the intricacies of the problem.\n\nAlgorithmic Solutions:\n\nConstraint processing algorithms come to the forefront as viable solutions to graph-related problems, showcasing the practical application of logical problem-solving methodologies."
  },
  {
    "objectID": "pages/AI/Week01.html#key-takeaways",
    "href": "pages/AI/Week01.html#key-takeaways",
    "title": "A Decade of Machine Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDecade of Machine Learning:\n\nThe surge in machine learning over the past decade is attributed to increased data availability, enhanced computing power, and advancements in training algorithms, particularly within the domain of deep learning. \n\nNeural Network Evolution:\n\nFrom the foundational perceptron era to the transformative deep neural networks in computer vision, the evolution of neural networks has played a pivotal role in shaping the landscape of AI.\n\nTraining Process:\n\nSupervised training, especially in medical diagnosis and face recognition, showcases the practical applications of deep neural networks in real-world scenarios.\n\nMachine Learning in Internet Interaction:\n\nUsers’ dynamic interaction with the internet transforms them into valuable data points, shaping the customization of ads and optimizing user experiences.\n\nPerformance vs. Competence:\n\nNeural networks exhibit proficiency in specific tasks but lack a holistic understanding of the world, highlighting the need for specialized training.\n\nGame of Go and Reinforcement Learning:\n\nThe triumph of AlphaGo exemplifies the success achievable through reinforcement learning, showcasing the capacity for autonomous learning without human intervention.\n\nHuman Cognitive Architecture:\n\nUnderstanding human cognitive abilities, symbolic reasoning, and the distinction between AI and machine learning provides insights into the complex realm of intelligence.\n\nHistory and Philosophy:\n\nThe historical roots of AI, key figures in AI development, and philosophical considerations underscore the interdisciplinary nature of artificial intelligence.\n\nPhysical Symbol Systems:\n\nSymbolic representation, as proposed by Simon and Newell, forms the basis for general intelligent action, distinguishing it from sub-symbolic AI.\n\nProblem Solving:\n\nTwo approaches, model-based reasoning and knowledge-based approaches, along with the role of logic, contribute to nuanced problem-solving methodologies.\n\nMap Coloring Problem:\n\nThe Map Coloring Problem serves as a concrete example, highlighting the integration of graph theory, constraint processing, and algorithmic solutions in logical problem-solving."
  }
]