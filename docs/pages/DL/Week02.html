<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>BS Degree Notes - Deep Learning Foundations: From Boolean Functions to Universal Approximation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-sans.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/DL/Week01_1.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../pages/DL/Week02.html">Week 2</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week02.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 12</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#boolean-functions-and-linear-separability" id="toc-boolean-functions-and-linear-separability" class="nav-link active" data-scroll-target="#boolean-functions-and-linear-separability">Boolean Functions and Linear Separability</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#xor-function-analysis" id="toc-xor-function-analysis" class="nav-link" data-scroll-target="#xor-function-analysis">XOR Function Analysis</a>
  <ul class="collapse">
  <li><a href="#xor-function-definition" id="toc-xor-function-definition" class="nav-link" data-scroll-target="#xor-function-definition">XOR Function Definition</a></li>
  <li><a href="#perceptron-implementation-challenges" id="toc-perceptron-implementation-challenges" class="nav-link" data-scroll-target="#perceptron-implementation-challenges">Perceptron Implementation Challenges</a></li>
  </ul></li>
  <li><a href="#implications-for-real-world-data" id="toc-implications-for-real-world-data" class="nav-link" data-scroll-target="#implications-for-real-world-data">Implications for Real-World Data</a></li>
  <li><a href="#network-of-perceptrons" id="toc-network-of-perceptrons" class="nav-link" data-scroll-target="#network-of-perceptrons">Network of Perceptrons</a></li>
  <li><a href="#boolean-functions-from-n-inputs" id="toc-boolean-functions-from-n-inputs" class="nav-link" data-scroll-target="#boolean-functions-from-n-inputs">Boolean Functions from N Inputs</a></li>
  <li><a href="#challenge-of-non-linear-separability" id="toc-challenge-of-non-linear-separability" class="nav-link" data-scroll-target="#challenge-of-non-linear-separability">Challenge of Non-Linear Separability</a></li>
  </ul></li>
  <li><a href="#multi-layer-perceptrons-mlps-and-boolean-function-representation" id="toc-multi-layer-perceptrons-mlps-and-boolean-function-representation" class="nav-link" data-scroll-target="#multi-layer-perceptrons-mlps-and-boolean-function-representation">Multi-Layer Perceptrons (MLPs) and Boolean Function Representation</a>
  <ul class="collapse">
  <li><a href="#introduction-to-multi-layer-perceptrons" id="toc-introduction-to-multi-layer-perceptrons" class="nav-link" data-scroll-target="#introduction-to-multi-layer-perceptrons">Introduction to Multi-Layer Perceptrons</a>
  <ul class="collapse">
  <li><a href="#layers-in-an-mlp" id="toc-layers-in-an-mlp" class="nav-link" data-scroll-target="#layers-in-an-mlp">Layers in an MLP</a></li>
  <li><a href="#weights-and-bias" id="toc-weights-and-bias" class="nav-link" data-scroll-target="#weights-and-bias">Weights and Bias</a></li>
  </ul></li>
  <li><a href="#representation-of-boolean-functions-in-mlps" id="toc-representation-of-boolean-functions-in-mlps" class="nav-link" data-scroll-target="#representation-of-boolean-functions-in-mlps">Representation of Boolean Functions in MLPs</a>
  <ul class="collapse">
  <li><a href="#network-structure-for-boolean-functions" id="toc-network-structure-for-boolean-functions" class="nav-link" data-scroll-target="#network-structure-for-boolean-functions">Network Structure for Boolean Functions</a></li>
  <li><a href="#boolean-function-implementation" id="toc-boolean-function-implementation" class="nav-link" data-scroll-target="#boolean-function-implementation">Boolean Function Implementation</a></li>
  </ul></li>
  <li><a href="#representation-power-and-implications" id="toc-representation-power-and-implications" class="nav-link" data-scroll-target="#representation-power-and-implications">Representation Power and Implications</a>
  <ul class="collapse">
  <li><a href="#representation-power-theorem" id="toc-representation-power-theorem" class="nav-link" data-scroll-target="#representation-power-theorem">Representation Power Theorem</a></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations">Practical Considerations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#introduction-to-sigmoid-neurons-and-the-sigmoid-function" id="toc-introduction-to-sigmoid-neurons-and-the-sigmoid-function" class="nav-link" data-scroll-target="#introduction-to-sigmoid-neurons-and-the-sigmoid-function">Introduction to Sigmoid Neurons and the Sigmoid Function</a>
  <ul class="collapse">
  <li><a href="#transition-from-perceptrons-to-sigmoid-neurons" id="toc-transition-from-perceptrons-to-sigmoid-neurons" class="nav-link" data-scroll-target="#transition-from-perceptrons-to-sigmoid-neurons">Transition from Perceptrons to Sigmoid Neurons</a>
  <ul class="collapse">
  <li><a href="#binary-output-limitation" id="toc-binary-output-limitation" class="nav-link" data-scroll-target="#binary-output-limitation">Binary Output Limitation</a></li>
  <li><a href="#real-valued-inputs-and-outputs" id="toc-real-valued-inputs-and-outputs" class="nav-link" data-scroll-target="#real-valued-inputs-and-outputs">Real-Valued Inputs and Outputs</a></li>
  </ul></li>
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective">Objective</a></li>
  <li><a href="#introduction-to-sigmoid-neurons" id="toc-introduction-to-sigmoid-neurons" class="nav-link" data-scroll-target="#introduction-to-sigmoid-neurons">Introduction to Sigmoid Neurons</a>
  <ul class="collapse">
  <li><a href="#sigmoid-function" id="toc-sigmoid-function" class="nav-link" data-scroll-target="#sigmoid-function">Sigmoid Function</a></li>
  <li><a href="#sigmoid-function-properties" id="toc-sigmoid-function-properties" class="nav-link" data-scroll-target="#sigmoid-function-properties">Sigmoid Function Properties</a></li>
  <li><a href="#comparison-with-perceptron" id="toc-comparison-with-perceptron" class="nav-link" data-scroll-target="#comparison-with-perceptron">Comparison with Perceptron</a></li>
  </ul></li>
  <li><a href="#importance-of-differentiability" id="toc-importance-of-differentiability" class="nav-link" data-scroll-target="#importance-of-differentiability">Importance of Differentiability</a></li>
  </ul></li>
  <li><a href="#supervised-machine-learning-setup" id="toc-supervised-machine-learning-setup" class="nav-link" data-scroll-target="#supervised-machine-learning-setup">Supervised Machine Learning Setup</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#components" id="toc-components" class="nav-link" data-scroll-target="#components">Components</a>
  <ul class="collapse">
  <li><a href="#data-representation" id="toc-data-representation" class="nav-link" data-scroll-target="#data-representation">Data Representation</a></li>
  <li><a href="#model-assumption" id="toc-model-assumption" class="nav-link" data-scroll-target="#model-assumption">Model Assumption</a></li>
  <li><a href="#objective-function-loss-function" id="toc-objective-function-loss-function" class="nav-link" data-scroll-target="#objective-function-loss-function">Objective Function (Loss Function)</a></li>
  </ul></li>
  <li><a href="#objective-function-details" id="toc-objective-function-details" class="nav-link" data-scroll-target="#objective-function-details">Objective Function Details</a>
  <ul class="collapse">
  <li><a href="#difference-function-squared-error-loss" id="toc-difference-function-squared-error-loss" class="nav-link" data-scroll-target="#difference-function-squared-error-loss">Difference Function (Squared Error Loss)</a></li>
  </ul></li>
  <li><a href="#analogy-with-learning-trigonometry" id="toc-analogy-with-learning-trigonometry" class="nav-link" data-scroll-target="#analogy-with-learning-trigonometry">Analogy with Learning Trigonometry</a>
  <ul class="collapse">
  <li><a href="#training-phase" id="toc-training-phase" class="nav-link" data-scroll-target="#training-phase">Training Phase</a></li>
  <li><a href="#validation-phase" id="toc-validation-phase" class="nav-link" data-scroll-target="#validation-phase">Validation Phase</a></li>
  <li><a href="#test-phase-exam" id="toc-test-phase-exam" class="nav-link" data-scroll-target="#test-phase-exam">Test Phase (Exam)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#learning-parameters-infeasible-guess-work" id="toc-learning-parameters-infeasible-guess-work" class="nav-link" data-scroll-target="#learning-parameters-infeasible-guess-work">Learning Parameters: (Infeasible) guess work</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1">Introduction</a>
  <ul class="collapse">
  <li><a href="#model-representation" id="toc-model-representation" class="nav-link" data-scroll-target="#model-representation">Model Representation</a></li>
  <li><a href="#training-objective" id="toc-training-objective" class="nav-link" data-scroll-target="#training-objective">Training Objective</a></li>
  </ul></li>
  <li><a href="#training-data" id="toc-training-data" class="nav-link" data-scroll-target="#training-data">Training Data</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  <li><a href="#trial-and-error-approach" id="toc-trial-and-error-approach" class="nav-link" data-scroll-target="#trial-and-error-approach">Trial-and-Error Approach</a>
  <ul class="collapse">
  <li><a href="#visualization-with-error-surface" id="toc-visualization-with-error-surface" class="nav-link" data-scroll-target="#visualization-with-error-surface">Visualization with Error Surface</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#learning-parameters-taylor-series-approximation" id="toc-learning-parameters-taylor-series-approximation" class="nav-link" data-scroll-target="#learning-parameters-taylor-series-approximation">Learning Parameters: Taylor series approximation</a>
  <ul class="collapse">
  <li><a href="#introduction-2" id="toc-introduction-2" class="nav-link" data-scroll-target="#introduction-2">Introduction</a></li>
  <li><a href="#update-rule-with-conservative-movement" id="toc-update-rule-with-conservative-movement" class="nav-link" data-scroll-target="#update-rule-with-conservative-movement">Update Rule with Conservative Movement</a></li>
  <li><a href="#taylor-series-for-function-approximation" id="toc-taylor-series-for-function-approximation" class="nav-link" data-scroll-target="#taylor-series-for-function-approximation">Taylor Series for Function Approximation</a>
  <ul class="collapse">
  <li><a href="#overview-1" id="toc-overview-1" class="nav-link" data-scroll-target="#overview-1">Overview</a></li>
  <li><a href="#linear-approximation" id="toc-linear-approximation" class="nav-link" data-scroll-target="#linear-approximation">Linear Approximation</a></li>
  <li><a href="#quadratic-and-higher-order-approximations" id="toc-quadratic-and-higher-order-approximations" class="nav-link" data-scroll-target="#quadratic-and-higher-order-approximations">Quadratic and Higher-Order Approximations</a></li>
  </ul></li>
  <li><a href="#extending-concepts-to-multiple-dimensions" id="toc-extending-concepts-to-multiple-dimensions" class="nav-link" data-scroll-target="#extending-concepts-to-multiple-dimensions">Extending Concepts to Multiple Dimensions</a></li>
  </ul></li>
  <li><a href="#gradient-descent-mathematical-foundation" id="toc-gradient-descent-mathematical-foundation" class="nav-link" data-scroll-target="#gradient-descent-mathematical-foundation">Gradient Descent: Mathematical Foundation</a>
  <ul class="collapse">
  <li><a href="#introduction-3" id="toc-introduction-3" class="nav-link" data-scroll-target="#introduction-3">Introduction</a></li>
  <li><a href="#taylor-series-expansion" id="toc-taylor-series-expansion" class="nav-link" data-scroll-target="#taylor-series-expansion">Taylor Series Expansion</a>
  <ul class="collapse">
  <li><a href="#objective-1" id="toc-objective-1" class="nav-link" data-scroll-target="#objective-1">Objective</a></li>
  <li><a href="#linear-approximation-1" id="toc-linear-approximation-1" class="nav-link" data-scroll-target="#linear-approximation-1">Linear Approximation</a></li>
  </ul></li>
  <li><a href="#mathematical-aspects-of-gradient-descent" id="toc-mathematical-aspects-of-gradient-descent" class="nav-link" data-scroll-target="#mathematical-aspects-of-gradient-descent">Mathematical Aspects of Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#gradient" id="toc-gradient" class="nav-link" data-scroll-target="#gradient">Gradient</a></li>
  <li><a href="#second-order-derivative-hessian" id="toc-second-order-derivative-hessian" class="nav-link" data-scroll-target="#second-order-derivative-hessian">Second Order Derivative (Hessian)</a></li>
  </ul></li>
  <li><a href="#decision-criteria-for-parameter-updates" id="toc-decision-criteria-for-parameter-updates" class="nav-link" data-scroll-target="#decision-criteria-for-parameter-updates">Decision Criteria for Parameter Updates</a>
  <ul class="collapse">
  <li><a href="#linear-approximation-and-criteria" id="toc-linear-approximation-and-criteria" class="nav-link" data-scroll-target="#linear-approximation-and-criteria">Linear Approximation and Criteria</a></li>
  </ul></li>
  <li><a href="#optimization-of-update-vector-mathbfu" id="toc-optimization-of-update-vector-mathbfu" class="nav-link" data-scroll-target="#optimization-of-update-vector-mathbfu">Optimization of Update Vector <span class="math inline">\(\mathbf{U}\)</span></a>
  <ul class="collapse">
  <li><a href="#angle-beta-and-cosine" id="toc-angle-beta-and-cosine" class="nav-link" data-scroll-target="#angle-beta-and-cosine">Angle <span class="math inline">\(\beta\)</span> and Cosine</a></li>
  <li><a href="#optimal-update-for-maximum-descent" id="toc-optimal-update-for-maximum-descent" class="nav-link" data-scroll-target="#optimal-update-for-maximum-descent">Optimal Update for Maximum Descent</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#gradient-descent-for-sigmoid-neuron-optimization" id="toc-gradient-descent-for-sigmoid-neuron-optimization" class="nav-link" data-scroll-target="#gradient-descent-for-sigmoid-neuron-optimization">Gradient Descent for Sigmoid Neuron Optimization</a>
  <ul class="collapse">
  <li><a href="#overview-2" id="toc-overview-2" class="nav-link" data-scroll-target="#overview-2">Overview</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a>
  <ul class="collapse">
  <li><a href="#gradient-descent-rule" id="toc-gradient-descent-rule" class="nav-link" data-scroll-target="#gradient-descent-rule">1. Gradient Descent Rule</a></li>
  <li><a href="#derivative-computation" id="toc-derivative-computation" class="nav-link" data-scroll-target="#derivative-computation">2. Derivative Computation</a></li>
  <li><a href="#algorithm-execution" id="toc-algorithm-execution" class="nav-link" data-scroll-target="#algorithm-execution">3. Algorithm Execution</a></li>
  <li><a href="#loss-surface-visualization" id="toc-loss-surface-visualization" class="nav-link" data-scroll-target="#loss-surface-visualization">4. Loss Surface Visualization</a></li>
  <li><a href="#observations" id="toc-observations" class="nav-link" data-scroll-target="#observations">5. Observations</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#representation-power-of-multi-layer-networks" id="toc-representation-power-of-multi-layer-networks" class="nav-link" data-scroll-target="#representation-power-of-multi-layer-networks">Representation Power of Multi-Layer Networks</a>
  <ul class="collapse">
  <li><a href="#introduction-4" id="toc-introduction-4" class="nav-link" data-scroll-target="#introduction-4">Introduction</a></li>
  <li><a href="#universal-approximation-theorem" id="toc-universal-approximation-theorem" class="nav-link" data-scroll-target="#universal-approximation-theorem">Universal Approximation Theorem</a></li>
  <li><a href="#tower-functions-illustration" id="toc-tower-functions-illustration" class="nav-link" data-scroll-target="#tower-functions-illustration">Tower Functions Illustration</a></li>
  <li><a href="#tower-construction-process" id="toc-tower-construction-process" class="nav-link" data-scroll-target="#tower-construction-process">Tower Construction Process</a></li>
  <li><a href="#tower-maker-neural-network" id="toc-tower-maker-neural-network" class="nav-link" data-scroll-target="#tower-maker-neural-network">Tower Maker Neural Network</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  <li><a href="#sigmoid-neuron-configuration" id="toc-sigmoid-neuron-configuration" class="nav-link" data-scroll-target="#sigmoid-neuron-configuration">Sigmoid Neuron Configuration</a></li>
  <li><a href="#bias-adjustment" id="toc-bias-adjustment" class="nav-link" data-scroll-target="#bias-adjustment">Bias Adjustment</a></li>
  </ul></li>
  <li><a href="#linear-function-integration" id="toc-linear-function-integration" class="nav-link" data-scroll-target="#linear-function-integration">Linear Function Integration</a></li>
  <li><a href="#network-adjustment-for-precision" id="toc-network-adjustment-for-precision" class="nav-link" data-scroll-target="#network-adjustment-for-precision">Network Adjustment for Precision</a></li>
  <li><a href="#single-input-function" id="toc-single-input-function" class="nav-link" data-scroll-target="#single-input-function">Single Input Function</a></li>
  <li><a href="#two-input-function" id="toc-two-input-function" class="nav-link" data-scroll-target="#two-input-function">Two Input Function</a></li>
  <li><a href="#building-a-tower-in-2d" id="toc-building-a-tower-in-2d" class="nav-link" data-scroll-target="#building-a-tower-in-2d">Building a Tower in 2D</a></li>
  <li><a href="#closing-the-tower" id="toc-closing-the-tower" class="nav-link" data-scroll-target="#closing-the-tower">Closing the Tower</a></li>
  <li><a href="#thresholding-to-get-a-closed-tower" id="toc-thresholding-to-get-a-closed-tower" class="nav-link" data-scroll-target="#thresholding-to-get-a-closed-tower">Thresholding to Get a Closed Tower</a></li>
  <li><a href="#extending-to-higher-dimensions" id="toc-extending-to-higher-dimensions" class="nav-link" data-scroll-target="#extending-to-higher-dimensions">Extending to Higher Dimensions</a></li>
  <li><a href="#universal-approximation-theorem-1" id="toc-universal-approximation-theorem-1" class="nav-link" data-scroll-target="#universal-approximation-theorem-1">Universal Approximation Theorem</a></li>
  <li><a href="#implications-for-deep-learning" id="toc-implications-for-deep-learning" class="nav-link" data-scroll-target="#implications-for-deep-learning">Implications for Deep Learning</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#points-to-remember" id="toc-points-to-remember" class="nav-link" data-scroll-target="#points-to-remember">Points to Remember</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/DL/Week01_1.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../pages/DL/Week02.html">Week 2</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Deep Learning Foundations: From Boolean Functions to Universal Approximation</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="boolean-functions-and-linear-separability" class="level1">
<h1>Boolean Functions and Linear Separability</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Boolean functions, fundamental to computational logic, pose challenges when it comes to their linear separability. The perceptron learning algorithm, known for its guarantees with linearly separable data, encounters limitations when dealing with certain boolean functions. This module delves into the intricacies of these functions and explores the concept of linear separability.</p>
</section>
<section id="xor-function-analysis" class="level2">
<h2 class="anchored" data-anchor-id="xor-function-analysis">XOR Function Analysis</h2>
<section id="xor-function-definition" class="level3">
<h3 class="anchored" data-anchor-id="xor-function-definition">XOR Function Definition</h3>
<p>The XOR function, denoted as <span class="math inline">\(f(x_1, x_2)\)</span>, outputs 1 when exactly one of its inputs is 1. It follows the logic: <span class="math display">\[f(0,0) \rightarrow 0, \, f(0,1) \rightarrow 1, \, f(1,0) \rightarrow 1, \, f(1,1) \rightarrow 0\]</span></p>
</section>
<section id="perceptron-implementation-challenges" class="level3">
<h3 class="anchored" data-anchor-id="perceptron-implementation-challenges">Perceptron Implementation Challenges</h3>
<p>Attempting to implement XOR using a perceptron leads to a set of four inequalities. These conditions, when applied to weights (<span class="math inline">\(w_0, w_1, w_2\)</span>), cannot be simultaneously satisfied. Geometrically, this signifies the inability to draw a line that separates positive and negative points in the XOR function.</p>
</section>
</section>
<section id="implications-for-real-world-data" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-real-world-data">Implications for Real-World Data</h2>
<p>Real-world data often deviates from the assumption of linear separability. For instance, individuals with similar characteristics may exhibit diverse preferences, challenging the effectiveness of linear decision boundaries.</p>
</section>
<section id="network-of-perceptrons" class="level2">
<h2 class="anchored" data-anchor-id="network-of-perceptrons">Network of Perceptrons</h2>
<p>Recognizing the limitations of a single perceptron in handling non-linearly separable data, a proposed solution involves using a network of perceptrons. This approach aims to extend the capability of handling complex, non-linearly separable boolean functions.</p>
</section>
<section id="boolean-functions-from-n-inputs" class="level2">
<h2 class="anchored" data-anchor-id="boolean-functions-from-n-inputs">Boolean Functions from N Inputs</h2>
<p>Boolean functions with <span class="math inline">\(n\)</span> inputs offer a wide range of possibilities, such as AND, OR, and others. The total number of boolean functions from <span class="math inline">\(n\)</span> inputs is given by <span class="math inline">\(2^{2^n}\)</span>. The discussion extends to the linear separability of these boolean functions.</p>
</section>
<section id="challenge-of-non-linear-separability" class="level2">
<h2 class="anchored" data-anchor-id="challenge-of-non-linear-separability">Challenge of Non-Linear Separability</h2>
<p>Out of the <span class="math inline">\(2^{2^n}\)</span> boolean functions, some are not linearly separable. The precise count of non-linearly separable functions remains an unsolved problem, highlighting the need for robust methods capable of handling such cases.</p>
</section>
</section>
<section id="multi-layer-perceptrons-mlps-and-boolean-function-representation" class="level1">
<h1>Multi-Layer Perceptrons (MLPs) and Boolean Function Representation</h1>
<section id="introduction-to-multi-layer-perceptrons" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-multi-layer-perceptrons">Introduction to Multi-Layer Perceptrons</h2>
<p>Multi-Layer Perceptrons (MLPs) constitute a pivotal advancement in artificial neural networks. These networks boast a layered architecture, each layer serving a distinct role in processing information.</p>
<section id="layers-in-an-mlp" class="level3">
<h3 class="anchored" data-anchor-id="layers-in-an-mlp">Layers in an MLP</h3>
<ol type="1">
<li><strong>Input Layer:</strong>
<ul>
<li>Comprising nodes representing input features (<span class="math inline">\(x_1, x_2, ..., x_n\)</span>).</li>
</ul></li>
<li><strong>Hidden Layer:</strong>
<ul>
<li>Features multiple perceptrons introducing non-linearities to the network.</li>
</ul></li>
<li><strong>Output Layer:</strong>
<ul>
<li>Houses a single perceptron providing the final network output.</li>
</ul></li>
</ol>
</section>
<section id="weights-and-bias" class="level3">
<h3 class="anchored" data-anchor-id="weights-and-bias">Weights and Bias</h3>
<ol type="1">
<li><strong>Connection Characteristics:</strong>
<ul>
<li>Weights (<span class="math inline">\(w\)</span>) and a bias term (<span class="math inline">\(w_0\)</span>) define the connections between nodes.</li>
</ul></li>
<li><strong>Weighted Sum and Activation:</strong>
<ul>
<li>The weighted sum of inputs, combined with the bias, influences perceptron activation.</li>
</ul></li>
</ol>
</section>
</section>
<section id="representation-of-boolean-functions-in-mlps" class="level2">
<h2 class="anchored" data-anchor-id="representation-of-boolean-functions-in-mlps">Representation of Boolean Functions in MLPs</h2>
<section id="network-structure-for-boolean-functions" class="level3">
<h3 class="anchored" data-anchor-id="network-structure-for-boolean-functions">Network Structure for Boolean Functions</h3>
<ol type="1">
<li><strong>Hidden Layer Configuration:</strong>
<ul>
<li>For a boolean function with <span class="math inline">\(n\)</span> inputs, the hidden layer consists of <span class="math inline">\(2^n\)</span> perceptrons.</li>
</ul></li>
<li><strong>Weight and Bias Adjustment:</strong>
<ul>
<li>Weights and biases are adjusted to meet boolean logic conditions for accurate function representation.</li>
</ul></li>
</ol>
</section>
<section id="boolean-function-implementation" class="level3">
<h3 class="anchored" data-anchor-id="boolean-function-implementation">Boolean Function Implementation</h3>
<ol type="1">
<li><strong>Perceptron Activation Conditions:</strong>
<ul>
<li>Each perceptron in the hidden layer selectively fires based on specific input combinations.</li>
</ul></li>
<li><strong>XOR Function Illustration:</strong>
<ul>
<li>Using the XOR function as an example, conditions on weights (<span class="math inline">\(w_1, w_2, w_3, w_4\)</span>) are established for faithful representation.</li>
</ul></li>
<li><strong>Extension to <span class="math inline">\(n\)</span> Inputs:</strong>
<ul>
<li>Generalizing the approach to <span class="math inline">\(n\)</span> inputs involves <span class="math inline">\(2^n\)</span> perceptrons in the hidden layer.</li>
<li>Conditions for output layer weights are derived to ensure accurate representation.</li>
</ul></li>
</ol>
</section>
</section>
<section id="representation-power-and-implications" class="level2">
<h2 class="anchored" data-anchor-id="representation-power-and-implications">Representation Power and Implications</h2>
<section id="representation-power-theorem" class="level3">
<h3 class="anchored" data-anchor-id="representation-power-theorem">Representation Power Theorem</h3>
<ol type="1">
<li><strong>Theorem Statement:</strong>
<ul>
<li>Any boolean function of <span class="math inline">\(n\)</span> inputs can be precisely represented by an MLP.</li>
</ul></li>
<li><strong>Suggested MLP Structure:</strong>
<ul>
<li>An MLP with <span class="math inline">\(2^n\)</span> perceptrons in the hidden layer and 1 perceptron in the output layer is deemed sufficient.</li>
</ul></li>
</ol>
</section>
<section id="practical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="practical-considerations">Practical Considerations</h3>
<ol type="1">
<li><strong>Challenges with Growing <span class="math inline">\(n\)</span>:</strong>
<ul>
<li>The exponential increase in perceptrons as <span class="math inline">\(n\)</span> grows poses practical challenges.</li>
</ul></li>
<li><strong>Real-World Applications:</strong>
<ul>
<li>Managing and computing with a large number of perceptrons may be challenging in practical applications.</li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="introduction-to-sigmoid-neurons-and-the-sigmoid-function" class="level1">
<h1>Introduction to Sigmoid Neurons and the Sigmoid Function</h1>
<section id="transition-from-perceptrons-to-sigmoid-neurons" class="level2">
<h2 class="anchored" data-anchor-id="transition-from-perceptrons-to-sigmoid-neurons">Transition from Perceptrons to Sigmoid Neurons</h2>
<section id="binary-output-limitation" class="level3">
<h3 class="anchored" data-anchor-id="binary-output-limitation">Binary Output Limitation</h3>
<p>Perceptrons, governed by binary output based on the weighted sum of inputs exceeding a threshold, exhibit a binary decision boundary. This rigid characteristic proves restrictive in scenarios where a more gradual decision-making process is preferred.</p>
</section>
<section id="real-valued-inputs-and-outputs" class="level3">
<h3 class="anchored" data-anchor-id="real-valued-inputs-and-outputs">Real-Valued Inputs and Outputs</h3>
<p>The shift towards sigmoid neurons arises in the context of addressing arbitrary functions <span class="math inline">\(Y = f(X)\)</span>, wherein <span class="math inline">\(X \in \mathbb{R}^n\)</span> and <span class="math inline">\(Y \in \mathbb{R}\)</span>. This entails the consideration of real numbers for both inputs and outputs. Examples include predicting oil quantity based on salinity, density, pressure, temperature, and marine diversity, as well as determining bank interest rates considering factors like salary, family size, previous loans, and defaults.</p>
</section>
</section>
<section id="objective" class="level2">
<h2 class="anchored" data-anchor-id="objective">Objective</h2>
<p>The primary objective is to construct a neural network capable of accurately approximating or representing real-valued functions, ensuring the proximity of the network’s output to actual values present in the training data.</p>
</section>
<section id="introduction-to-sigmoid-neurons" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-sigmoid-neurons">Introduction to Sigmoid Neurons</h2>
<section id="sigmoid-function" class="level3">
<h3 class="anchored" data-anchor-id="sigmoid-function">Sigmoid Function</h3>
<p>Sigmoid neurons employ the sigmoid function (logistic function) to introduce smoothness in decision-making. Mathematically represented as: <span class="math display">\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</span> where <span class="math inline">\(z\)</span> denotes the weighted sum of inputs.</p>
</section>
<section id="sigmoid-function-properties" class="level3">
<h3 class="anchored" data-anchor-id="sigmoid-function-properties">Sigmoid Function Properties</h3>
<ol type="1">
<li>As <span class="math inline">\(z\)</span> tends to positive infinity: <span class="math inline">\(\lim_{{z \to \infty}} \sigma(z) = 1\)</span></li>
<li>As <span class="math inline">\(z\)</span> tends to negative infinity: <span class="math inline">\(\lim_{{z \to -\infty}} \sigma(z) = 0\)</span></li>
<li>At <span class="math inline">\(W^T X = 0\)</span>: <span class="math inline">\(\sigma(0) = \frac{1}{2}\)</span></li>
</ol>
<p>The sigmoid function transforms outputs into the range [0, 1], facilitating a probabilistic interpretation.</p>
</section>
<section id="comparison-with-perceptron" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-perceptron">Comparison with Perceptron</h3>
<p>Contrasting with the perceptron function, the sigmoid function exhibits smoothness and continuity. The perceptron function lacks differentiability at the abrupt change in value, whereas the sigmoid function is differentiable.</p>
</section>
</section>
<section id="importance-of-differentiability" class="level2">
<h2 class="anchored" data-anchor-id="importance-of-differentiability">Importance of Differentiability</h2>
<p>Differentiability holds paramount importance for various machine learning algorithms, particularly in derivative-related operations. The application of calculus in neural network training and optimization is streamlined by the differentiability of the sigmoid neuron’s activation function.</p>
</section>
</section>
<section id="supervised-machine-learning-setup" class="level1">
<h1>Supervised Machine Learning Setup</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>In the realm of supervised machine learning, the fundamental objective is to comprehend the intricate structure of the setup, which encompasses various components crucial for effective model training. These components include the dataset, model representation, the learning algorithm, and the definition of an objective function.</p>
</section>
<section id="components" class="level2">
<h2 class="anchored" data-anchor-id="components">Components</h2>
<section id="data-representation" class="level3">
<h3 class="anchored" data-anchor-id="data-representation">Data Representation</h3>
<p>The dataset, denoted as <span class="math inline">\((x_i, y_i)\)</span>, is pivotal to the learning process. Here, <span class="math inline">\(x_i\)</span> signifies an <span class="math inline">\(m\)</span>-dimensional input vector, while <span class="math inline">\(y_i\)</span> represents a real-valued output associated with the given input. The dataset essentially comprises a collection of such input-output pairs.</p>
</section>
<section id="model-assumption" class="level3">
<h3 class="anchored" data-anchor-id="model-assumption">Model Assumption</h3>
<p>A critical assumption in this paradigm is that the output <span class="math inline">\(y\)</span> is contingent upon the input <span class="math inline">\(x\)</span>, expressed as <span class="math inline">\(y = f(x)\)</span>. However, the specific form of the function <span class="math inline">\(f\)</span> remains elusive, prompting the need for learning algorithms to discern it from the provided data.</p>
<section id="learning-algorithm" class="level4">
<h4 class="anchored" data-anchor-id="learning-algorithm">Learning Algorithm</h4>
<p>The learning algorithm employed in this context is the Gradient Descent algorithm. This iterative approach facilitates the adjustment of model parameters, ensuring a continuous refinement of the model’s approximation.</p>
</section>
</section>
<section id="objective-function-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="objective-function-loss-function">Objective Function (Loss Function)</h3>
<p>Central to the learning process is the formulation of an objective function, commonly referred to as the Loss Function. Mathematically, it is defined as follows:</p>
<p><span class="math display">\[\mathcal{L}(\theta) = \sum_{i=1}^{n} \text{Difference}(\hat{y_{i}}, y_i)\]</span></p>
<p>Here, <span class="math inline">\(\theta\)</span> denotes the parameters of the model, and <span class="math inline">\(\text{Difference}(\hat{y_{i}}, y_i)\)</span> quantifies the dissimilarity between the predicted (<span class="math inline">\(\hat{y_{i}}\)</span>) and actual (<span class="math inline">\(y_i\)</span>) values.</p>
</section>
</section>
<section id="objective-function-details" class="level2">
<h2 class="anchored" data-anchor-id="objective-function-details">Objective Function Details</h2>
<section id="difference-function-squared-error-loss" class="level3">
<h3 class="anchored" data-anchor-id="difference-function-squared-error-loss">Difference Function (Squared Error Loss)</h3>
<p>The Difference Function, an integral component of the Loss Function, is expressed as:</p>
<p><span class="math display">\[\text{Difference}(\hat{y}, y) = (\hat{y} - y)^2\]</span></p>
<p>The squaring operation is implemented to ensure that both positive and negative errors contribute to the overall loss without canceling each other out.</p>
</section>
</section>
<section id="analogy-with-learning-trigonometry" class="level2">
<h2 class="anchored" data-anchor-id="analogy-with-learning-trigonometry">Analogy with Learning Trigonometry</h2>
<section id="training-phase" class="level3">
<h3 class="anchored" data-anchor-id="training-phase">Training Phase</h3>
<p>Analogous to mastering a chapter in a textbook, the training phase strives for zero or minimal errors on the content encapsulated within the training dataset.</p>
</section>
<section id="validation-phase" class="level3">
<h3 class="anchored" data-anchor-id="validation-phase">Validation Phase</h3>
<p>Resembling the solving of exercises at the end of a chapter, the validation phase allows for revisiting and enhancing comprehension based on additional exercises.</p>
</section>
<section id="test-phase-exam" class="level3">
<h3 class="anchored" data-anchor-id="test-phase-exam">Test Phase (Exam)</h3>
<p>The test phase simulates a real-world scenario where the model encounters new data. Unlike the training and validation phases, there is no opportunity for revisiting and refining the learned information.</p>
</section>
</section>
</section>
<section id="learning-parameters-infeasible-guess-work" class="level1">
<h1>Learning Parameters: (Infeasible) guess work</h1>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">Introduction</h2>
<p>Supervised machine learning involves the development of algorithms to learn parameters for a given model. This process aims to minimize the difference between predicted and actual values using a defined objective function. In this context, we explore a simplified model with one input, connected by weight (<span class="math inline">\(w\)</span>), and a bias (<span class="math inline">\(b\)</span>).</p>
<section id="model-representation" class="level3">
<h3 class="anchored" data-anchor-id="model-representation">Model Representation</h3>
<p>The model is represented as <span class="math inline">\(f(\mathbf{x}) = -w \mathbf{x} + b\)</span>, where <span class="math inline">\(\mathbf{x}\)</span> is the input vector. The task is to determine an algorithm that learns the optimal values for <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> using training data.</p>
</section>
<section id="training-objective" class="level3">
<h3 class="anchored" data-anchor-id="training-objective">Training Objective</h3>
<p>The training objective involves minimizing the average difference between predicted values (<span class="math inline">\(f(\mathbf{x})\)</span>) and actual values (<span class="math inline">\(y\)</span>) over all training points. The process requires finding the optimal <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> values that achieve this minimum loss.</p>
</section>
</section>
<section id="training-data" class="level2">
<h2 class="anchored" data-anchor-id="training-data">Training Data</h2>
<p>The training data consists of pairs <span class="math inline">\((\mathbf{x}, y)\)</span>, where <span class="math inline">\(\mathbf{x}\)</span> represents the input, and <span class="math inline">\(y\)</span> corresponds to the output. The loss function is defined as the average difference between predicted and actual values across all training points.</p>
</section>
<section id="loss-function" class="level2">
<h2 class="anchored" data-anchor-id="loss-function">Loss Function</h2>
<p>The loss function is expressed as:</p>
<p><span class="math display">\[\mathcal{L}(w, b) = \frac{1}{N} \sum_{i=1}^{N} \left| f(\mathbf{x}_i) - y_i \right|\]</span></p>
<p>Here, <span class="math inline">\(N\)</span> is the number of training points, <span class="math inline">\(\mathbf{x}_i\)</span> is the input for the <span class="math inline">\(i\)</span>-th point, and <span class="math inline">\(y_i\)</span> is the corresponding actual output.</p>
</section>
<section id="trial-and-error-approach" class="level2">
<h2 class="anchored" data-anchor-id="trial-and-error-approach">Trial-and-Error Approach</h2>
<p>To illustrate the concept, a trial-and-error approach is employed initially. Random values for <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> are chosen, and the loss is calculated. Adjustments are made iteratively to minimize the loss. This process involves systematically changing <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> values until an optimal solution is found.</p>
<section id="visualization-with-error-surface" class="level3">
<h3 class="anchored" data-anchor-id="visualization-with-error-surface">Visualization with Error Surface</h3>
<p>A 3D surface plot is used to visualize the loss in the <span class="math inline">\(w-b\)</span> plane. This plot aids in identifying regions of low and high loss. However, the impracticality of exhaustively exploring this surface for large datasets is acknowledged due to computational constraints.</p>
</section>
</section>
</section>
<section id="learning-parameters-taylor-series-approximation" class="level1">
<h1>Learning Parameters: Taylor series approximation</h1>
<section id="introduction-2" class="level2">
<h2 class="anchored" data-anchor-id="introduction-2">Introduction</h2>
<p>The transcript delves into the intricacies of parameter optimization, focusing on the goal of efficiently traversing the error surface to reach the minimum error. The parameters of interest, denoted as <span class="math inline">\(\theta\)</span>, are expressed as vectors, specifically encompassing <span class="math inline">\(W\)</span> and <span class="math inline">\(B\)</span> in the context of a toy network.</p>
</section>
<section id="update-rule-with-conservative-movement" class="level2">
<h2 class="anchored" data-anchor-id="update-rule-with-conservative-movement">Update Rule with Conservative Movement</h2>
<p>The update rule for altering <span class="math inline">\(\theta\)</span> entails a meticulous adjustment of the parameters. The process involves taking a measured step, determined by a scalar <span class="math inline">\(\eta\)</span>, in the direction of <span class="math inline">\(\Delta\theta\)</span>, which encapsulates the parameter changes. This introduces a level of conservatism in the parameter adjustments, promoting stability in the optimization process.</p>
</section>
<section id="taylor-series-for-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="taylor-series-for-function-approximation">Taylor Series for Function Approximation</h2>
<section id="overview-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-1">Overview</h3>
<p>The lecture introduces the Taylor series, a powerful mathematical tool for approximating functions that exhibit continuous differentiability. This method enables the representation of a function through polynomials, allowing for varying degrees of precision in the approximation.</p>
</section>
<section id="linear-approximation" class="level3">
<h3 class="anchored" data-anchor-id="linear-approximation">Linear Approximation</h3>
<p>Linear approximation entails the establishment of a tangent line at a specific point on the function. This approach provides an initial approximation, and the accuracy is contingent on the chosen neighborhood size, denoted as <span class="math inline">\(\varepsilon\)</span>.</p>
</section>
<section id="quadratic-and-higher-order-approximations" class="level3">
<h3 class="anchored" data-anchor-id="quadratic-and-higher-order-approximations">Quadratic and Higher-Order Approximations</h3>
<p>Quadratic and higher-order approximations extend the accuracy of the approximation by incorporating additional terms. The lecture underscores the importance of selecting a small neighborhood for these approximations to maintain efficacy.</p>
</section>
</section>
<section id="extending-concepts-to-multiple-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="extending-concepts-to-multiple-dimensions">Extending Concepts to Multiple Dimensions</h2>
<p>The discussion expands to functions with two variables, exemplifying how linear and quadratic approximations operate in multidimensional spaces. The lecture underscores the critical role of confined neighborhoods (<span class="math inline">\(\varepsilon\)</span>) in ensuring the precision of the Taylor series method across varying dimensions.</p>
</section>
</section>
<section id="gradient-descent-mathematical-foundation" class="level1">
<h1>Gradient Descent: Mathematical Foundation</h1>
<section id="introduction-3" class="level2">
<h2 class="anchored" data-anchor-id="introduction-3">Introduction</h2>
<p>In the realm of optimization for machine learning models, the process of iteratively updating parameters to minimize a loss function is a fundamental concept. One key technique employed in this context is <strong>gradient descent</strong>. This discussion delves into the intricate mathematical foundations underpinning gradient descent, focusing on the decision criteria for parameter updates and the optimization of the update vector.</p>
</section>
<section id="taylor-series-expansion" class="level2">
<h2 class="anchored" data-anchor-id="taylor-series-expansion">Taylor Series Expansion</h2>
<section id="objective-1" class="level3">
<h3 class="anchored" data-anchor-id="objective-1">Objective</h3>
<p>The overarching objective is to determine an optimal change in parameters, denoted as <span class="math inline">\(\Delta\theta\)</span> (represented as <span class="math inline">\(\mathbf{U}\)</span>), to minimize the loss function <span class="math inline">\(\mathcal{L}(\theta)\)</span>.</p>
</section>
<section id="linear-approximation-1" class="level3">
<h3 class="anchored" data-anchor-id="linear-approximation-1">Linear Approximation</h3>
<p>Utilizing the Taylor series, the loss function at a nearby point <span class="math inline">\(\theta + \Delta\theta\)</span> is approximated linearly as: <span class="math display">\[\mathcal{L}(\theta + \Delta\theta) \approx \mathcal{L}(\theta) + \eta\mathbf{U}^T\nabla \mathcal{L}(\theta)\]</span> Here, <span class="math inline">\(\eta\)</span> is a small positive scalar, ensuring a negligible difference.</p>
</section>
</section>
<section id="mathematical-aspects-of-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-aspects-of-gradient-descent">Mathematical Aspects of Gradient Descent</h2>
<section id="gradient" class="level3">
<h3 class="anchored" data-anchor-id="gradient">Gradient</h3>
<p>The gradient <span class="math inline">\(\nabla \mathcal{L}(\theta)\)</span> is introduced as a vector comprising partial derivatives of the loss function with respect to its parameters. For a function <span class="math inline">\(y = W^2 + B^2\)</span> with two variables, the gradient is expressed as <span class="math inline">\([2W, 2B]\)</span>.</p>
</section>
<section id="second-order-derivative-hessian" class="level3">
<h3 class="anchored" data-anchor-id="second-order-derivative-hessian">Second Order Derivative (Hessian)</h3>
<p>The concept of the Hessian matrix, representing the second-order derivative, is introduced. This matrix provides insights into the curvature of the loss function. In the case of a two-variable function, the Hessian is illustrated as a <span class="math inline">\(2\times2\)</span> matrix.</p>
</section>
</section>
<section id="decision-criteria-for-parameter-updates" class="level2">
<h2 class="anchored" data-anchor-id="decision-criteria-for-parameter-updates">Decision Criteria for Parameter Updates</h2>
<section id="linear-approximation-and-criteria" class="level3">
<h3 class="anchored" data-anchor-id="linear-approximation-and-criteria">Linear Approximation and Criteria</h3>
<p>The focus shifts to linear approximation, with higher-order terms neglected when <span class="math inline">\(\eta\)</span> is small. The decision criteria for a favorable parameter update is based on the condition: <span class="math display">\[\eta\mathbf{U}^T\nabla \mathcal{L}(\theta) &lt; 0\]</span></p>
</section>
</section>
<section id="optimization-of-update-vector-mathbfu" class="level2">
<h2 class="anchored" data-anchor-id="optimization-of-update-vector-mathbfu">Optimization of Update Vector <span class="math inline">\(\mathbf{U}\)</span></h2>
<section id="angle-beta-and-cosine" class="level3">
<h3 class="anchored" data-anchor-id="angle-beta-and-cosine">Angle <span class="math inline">\(\beta\)</span> and Cosine</h3>
<p>Optimizing the update vector involves considering the angle <span class="math inline">\(\beta\)</span> between <span class="math inline">\(\mathbf{U}\)</span> and the gradient vector. The cosine of <span class="math inline">\(\beta\)</span>, denoted as <span class="math inline">\(\cos(\beta)\)</span>, is explored, and its range is discussed.</p>
</section>
<section id="optimal-update-for-maximum-descent" class="level3">
<h3 class="anchored" data-anchor-id="optimal-update-for-maximum-descent">Optimal Update for Maximum Descent</h3>
<p>In the pursuit of maximum descent, the optimal scenario arises when <span class="math inline">\(\cos(\beta) = -1\)</span>, indicating that the angle <span class="math inline">\(\beta\)</span> is 180 degrees, signifying movement in the direction opposite to the gradient vector. This aligns with the well-known rule in gradient descent: “Move in the direction opposite to the gradient.”</p>
</section>
</section>
</section>
<section id="gradient-descent-for-sigmoid-neuron-optimization" class="level1">
<h1>Gradient Descent for Sigmoid Neuron Optimization</h1>
<section id="overview-2" class="level2">
<h2 class="anchored" data-anchor-id="overview-2">Overview</h2>
<p>In the pursuit of optimizing the parameters of a sigmoid neuron, the lecture primarily delves into the application of the gradient descent algorithm. The primary objective is to minimize the associated loss function, thereby identifying optimal values for the neuron’s weights (<span class="math inline">\(W\)</span>) and bias (<span class="math inline">\(B\)</span>).</p>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<section id="gradient-descent-rule" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent-rule">1. Gradient Descent Rule</h3>
<p>The <strong>gradient descent rule</strong> serves as an iterative optimization technique employed to minimize the loss function. The core update rule is defined as follows:</p>
<p><span class="math display">\[
W = W - \eta \cdot \frac{\partial \mathcal{L}}{\partial W}, \quad B = B - \eta \cdot \frac{\partial \mathcal{L}}{\partial B}
\]</span></p>
<p>This iterative process aims to iteratively refine the parameters (<span class="math inline">\(W\)</span> and <span class="math inline">\(B\)</span>) based on the computed partial derivatives of the loss function.</p>
</section>
<section id="derivative-computation" class="level3">
<h3 class="anchored" data-anchor-id="derivative-computation">2. Derivative Computation</h3>
<section id="derivative-of-loss-with-respect-to-w" class="level4">
<h4 class="anchored" data-anchor-id="derivative-of-loss-with-respect-to-w">2.1 Derivative of Loss with Respect to <span class="math inline">\(W\)</span></h4>
<p>The <strong>partial derivative of the loss function with respect to weights (<span class="math inline">\(\frac{\partial \mathcal{L}}{\partial W}\)</span>)</strong> is computed through the application of the chain rule. In the context of the sigmoid function, the derivative is obtained as follows:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial W} = \sum_i \left(f(x_i) - y_i\right) \cdot f(x_i) \cdot \left(1 - f(x_i)\right) \cdot X_i
\]</span></p>
<p>Here, <span class="math inline">\(f(x_i)\)</span> represents the sigmoid function applied to the input <span class="math inline">\(x_i\)</span> associated with data point <span class="math inline">\(i\)</span>.</p>
</section>
<section id="derivative-of-loss-with-respect-to-b" class="level4">
<h4 class="anchored" data-anchor-id="derivative-of-loss-with-respect-to-b">2.2 Derivative of Loss with Respect to <span class="math inline">\(B\)</span></h4>
<p>Similarly, the <strong>partial derivative of the loss function with respect to bias (<span class="math inline">\(\frac{\partial \mathcal{L}}{\partial B}\)</span>)</strong> is derived as:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial B} = \sum_i \left(f(x_i) - y_i\right) \cdot f(x_i) \cdot \left(1 - f(x_i)\right)
\]</span></p>
<p>The introduction of <span class="math inline">\(X_i\)</span> is omitted in this case, as it pertains to the bias term.</p>
</section>
</section>
<section id="algorithm-execution" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-execution">3. Algorithm Execution</h3>
<p>The algorithmic execution involves several key steps:</p>
<ol type="1">
<li><strong>Initialization:</strong>
<ul>
<li>Random initialization of weights (<span class="math inline">\(W\)</span>) and bias (<span class="math inline">\(B\)</span>).</li>
<li>Setting the learning rate (<span class="math inline">\(\eta\)</span>) and maximum iterations.</li>
</ul></li>
<li><strong>Gradient Computation:</strong>
<ul>
<li>Iterating over all data points, computing the partial derivatives for <span class="math inline">\(W\)</span> and <span class="math inline">\(B\)</span> using the derived formulas.</li>
</ul></li>
<li><strong>Parameter Update:</strong>
<ul>
<li>Applying the gradient descent update rule to iteratively adjust the weights and bias.</li>
</ul></li>
</ol>
</section>
<section id="loss-surface-visualization" class="level3">
<h3 class="anchored" data-anchor-id="loss-surface-visualization">4. Loss Surface Visualization</h3>
<p>The lecture introduces the concept of visualizing the <strong>loss function surface</strong> in the <span class="math inline">\(W-B\)</span> plane. This visual aid illustrates the algorithm’s movement along the surface, consistently reducing the loss.</p>
</section>
<section id="observations" class="level3">
<h3 class="anchored" data-anchor-id="observations">5. Observations</h3>
<p>The lecture emphasizes crucial observations:</p>
<ul>
<li><strong>Loss Reduction:</strong>
<ul>
<li>Ensuring that at each iteration, the algorithm systematically decreases the loss.</li>
</ul></li>
<li><strong>Hyperparameter Impact:</strong>
<ul>
<li>Acknowledging the influence of the learning rate (<span class="math inline">\(\eta\)</span>) on convergence and potential overshooting.</li>
</ul></li>
<li><strong>Experimentation:</strong>
<ul>
<li>Encouraging experimentation with diverse initializations and learning rates for a comprehensive understanding.</li>
</ul></li>
</ul>
</section>
</section>
</section>
<section id="representation-power-of-multi-layer-networks" class="level1">
<h1>Representation Power of Multi-Layer Networks</h1>
<section id="introduction-4" class="level2">
<h2 class="anchored" data-anchor-id="introduction-4">Introduction</h2>
<p>The representation power of a multi-layer network, particularly employing sigmoid neurons, is the focal point of this discussion. The objective is to establish a theorem analogous to the one developed for perceptrons, specifically emphasizing the network’s capability to approximate any continuous function.</p>
</section>
<section id="universal-approximation-theorem" class="level2">
<h2 class="anchored" data-anchor-id="universal-approximation-theorem">Universal Approximation Theorem</h2>
<p>The Universal Approximation Theorem posits that a multi-layer network with a single hidden layer possesses the capacity to approximate any continuous function with precision. This approximation is achieved by manipulating the weights and biases associated with the sigmoid neurons within the hidden layer.</p>
</section>
<section id="tower-functions-illustration" class="level2">
<h2 class="anchored" data-anchor-id="tower-functions-illustration">Tower Functions Illustration</h2>
<p>To illustrate the approximation process, the concept of towers of functions is introduced. This entails deconstructing an arbitrary function into a summation of tower functions, wherein each tower is represented by sigmoid neurons. The amalgamation of these towers serves to approximate the original function.</p>
</section>
<section id="tower-construction-process" class="level2">
<h2 class="anchored" data-anchor-id="tower-construction-process">Tower Construction Process</h2>
<p>The construction of towers involves the utilization of sigmoid neurons with exceptionally high weights, approaching infinity. This strategic choice mimics step functions. By subtracting these step functions, a tower-like structure is formed. Notably, the width and position of the tower are modulated by adjusting the biases of the sigmoid neurons.</p>
</section>
<section id="tower-maker-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="tower-maker-neural-network">Tower Maker Neural Network</h2>
<section id="architecture" class="level3">
<h3 class="anchored" data-anchor-id="architecture">Architecture</h3>
<p>The lecture introduces a neural network architecture termed the “Tower Maker.” This architecture comprises two sigmoid neurons characterized by high weights. The subtraction of their outputs yields a function resembling a tower.</p>
</section>
<section id="sigmoid-neuron-configuration" class="level3">
<h3 class="anchored" data-anchor-id="sigmoid-neuron-configuration">Sigmoid Neuron Configuration</h3>
<p>The sigmoid neurons within the Tower Maker are configured with exceedingly high weights, akin to infinity. This configuration transforms the sigmoid functions into step functions, pivotal in constructing tower-like shapes.</p>
</section>
<section id="bias-adjustment" class="level3">
<h3 class="anchored" data-anchor-id="bias-adjustment">Bias Adjustment</h3>
<p>Control over the width and position of the tower is exercised through the manipulation of biases associated with the sigmoid neurons. Adjusting these biases ensures the customization of the tower function according to specific requirements.</p>
</section>
</section>
<section id="linear-function-integration" class="level2">
<h2 class="anchored" data-anchor-id="linear-function-integration">Linear Function Integration</h2>
<p>An additional layer is incorporated into the Tower Maker architecture to integrate linear functions. This augmentation enhances the network’s ability to generate tower functions based on the input parameters.</p>
</section>
<section id="network-adjustment-for-precision" class="level2">
<h2 class="anchored" data-anchor-id="network-adjustment-for-precision">Network Adjustment for Precision</h2>
<p>The lecture underscores the correlation between the desired precision (represented by epsilon) and the network’s complexity. As the precision requirement increases, a more intricate network with an augmented number of neurons in the hidden layer becomes imperative. However, it is acknowledged that practical implementation may encounter challenges as the network’s size expands.</p>
</section>
<section id="single-input-function" class="level2">
<h2 class="anchored" data-anchor-id="single-input-function">Single Input Function</h2>
<p>Consider a function with a single input (<span class="math inline">\(x\)</span>) plotted on the x-axis and corresponding output (<span class="math inline">\(y\)</span>) on the y-axis. This introductory scenario involves the use of a sigmoid neuron function, denoted by:</p>
<p><span class="math display">\[f(x) = \frac{1}{1 + e^{-(wx + b)}}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(w\)</span> represents the weight associated with the input.</li>
<li><span class="math inline">\(b\)</span> is the bias term.</li>
<li>The sigmoid function smoothly transitions between 0 and 1.</li>
</ul>
</section>
<section id="two-input-function" class="level2">
<h2 class="anchored" data-anchor-id="two-input-function">Two Input Function</h2>
<p>Expanding the scope to a two-input function, let’s consider an example related to oil mining, where salinity (<span class="math inline">\(x_1\)</span>) and pressure (<span class="math inline">\(x_2\)</span>) serve as inputs. The challenge is to establish a decision boundary separating points indicating the presence (orange) and absence (blue) of oil.</p>
<p>A linear decision boundary proves inadequate, prompting the need for a more complex function.</p>
</section>
<section id="building-a-tower-in-2d" class="level2">
<h2 class="anchored" data-anchor-id="building-a-tower-in-2d">Building a Tower in 2D</h2>
<p>To construct a tower-like structure, two sigmoid neurons are introduced, each handling one input (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>). The sigmoid function takes the form:</p>
<p><span class="math display">\[f(x) = \frac{1}{1 + e^{-(w_ix_i + b)}}\]</span></p>
<p>Here, <span class="math inline">\(i\)</span> denotes the input index (1 or 2), <span class="math inline">\(w_i\)</span> is the associated weight, and <span class="math inline">\(b\)</span> is the bias term. Adjusting weights (<span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>) results in step functions, dictating the slope of the tower in different directions.</p>
<p>Combining these sigmoid neurons produces an open tower structure in one direction.</p>
</section>
<section id="closing-the-tower" class="level2">
<h2 class="anchored" data-anchor-id="closing-the-tower">Closing the Tower</h2>
<p>To enclose the tower from all sides, two additional sigmoid neurons (h13 and h14) are introduced. These neurons, with specific weight configurations, contribute to the formation of walls in different directions. Subtracting the outputs of these sigmoid neurons results in a structure with walls on all four sides but an open top.</p>
</section>
<section id="thresholding-to-get-a-closed-tower" class="level2">
<h2 class="anchored" data-anchor-id="thresholding-to-get-a-closed-tower">Thresholding to Get a Closed Tower</h2>
<p>To address the open top issue, thresholding is introduced. A sigmoid function with a switch-over point at 1 is applied to the structure’s output. This process retains only the portion of the structure above level 1, effectively closing the tower.</p>
</section>
<section id="extending-to-higher-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="extending-to-higher-dimensions">Extending to Higher Dimensions</h2>
<p>Generalizing this approach to n-dimensional inputs, the methodology remains consistent. For a single input (<span class="math inline">\(x\)</span>), two neurons suffice; for two inputs (<span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>), four neurons are necessary. The number of neurons in the middle layer increases with higher dimensions, extending the method to handle arbitrary functions.</p>
</section>
<section id="universal-approximation-theorem-1" class="level2">
<h2 class="anchored" data-anchor-id="universal-approximation-theorem-1">Universal Approximation Theorem</h2>
<p>This construction aligns with the Universal Approximation Theorem, asserting that a neural network, given a sufficient number of neurons, can approximate any arbitrary function to a desired precision.</p>
</section>
<section id="implications-for-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-deep-learning">Implications for Deep Learning</h2>
<p>This methodology underscores the flexibility of deep neural networks in approximating complex functions encountered in real-world applications. The ability to systematically construct networks capable of representing intricate relationships contributes to the effectiveness of deep learning models.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this week’s deep learning lectures, we delved into fundamental concepts, challenges, and advancements in the field. We explored the intricacies of boolean functions and their linear separability, shedding light on the limitations perceptrons face when dealing with complex functions like XOR. The introduction of multi-layer perceptrons (MLPs) provided a solution, extending the capability to handle non-linearly separable data.</p>
<p>The transition from perceptrons to sigmoid neurons marked a crucial shift, addressing the binary output limitation and introducing real-valued inputs and outputs. We explored the importance of the sigmoid function’s differentiability in machine learning algorithms, particularly in the context of neural network training.</p>
<p>Supervised machine learning setups, learning parameters through trial-and-error and Taylor series approximation, and the mathematical foundations of gradient descent were thoroughly discussed. The optimization process for sigmoid neurons through gradient descent provided insights into updating weights and biases iteratively, aiming to minimize the loss function.</p>
<p>The representation power of multi-layer networks, illustrated through the Universal Approximation Theorem, showcased the ability of neural networks to approximate any continuous function. The Tower Maker architecture exemplified the construction of towers of functions using sigmoid neurons, highlighting the flexibility and power of deep neural networks.</p>
<p>The week concluded with an exploration of the Universal Approximation Theorem’s implications for deep learning, emphasizing the adaptability of neural networks in approximating complex functions encountered in real-world applications.</p>
<section id="points-to-remember" class="level2">
<h2 class="anchored" data-anchor-id="points-to-remember">Points to Remember</h2>
<ol type="1">
<li><strong>Boolean Functions and Linear Separability:</strong>
<ul>
<li>Perceptrons face challenges with non-linearly separable boolean functions.</li>
<li>Multi-layer perceptrons (MLPs) extend capabilities for handling complex functions.</li>
</ul></li>
<li><strong>Sigmoid Neurons and Differentiability:</strong>
<ul>
<li>Sigmoid neurons introduce smoothness in decision-making.</li>
<li>Differentiability is crucial for optimization in neural network training.</li>
</ul></li>
<li><strong>Gradient Descent: Mathematical Foundation:</strong>
<ul>
<li>Taylor series expansion facilitates linear approximation in gradient descent.</li>
<li>Decision criteria for parameter updates involve linear approximation conditions.</li>
</ul></li>
<li><strong>Tower Maker and Universal Approximation Theorem:</strong>
<ul>
<li>The Universal Approximation Theorem states that a single hidden layer in a neural network can approximate any continuous function.</li>
<li>Tower Maker architecture showcases the construction of towers using sigmoid neurons.</li>
</ul></li>
<li><strong>Deep Learning Flexibility:</strong>
<ul>
<li>Deep neural networks are flexible in approximating complex functions.</li>
<li>The Tower Maker architecture demonstrates the power of neural networks in constructing intricate representations.</li>
</ul></li>
</ol>
<p>This week’s exploration laid the groundwork for understanding the core principles and capabilities of neural networks, setting the stage for further exploration into advanced topics in deep learning.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>