<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>BS Degree Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-sans.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/DL/Week01_1.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../pages/DL/Week05.html">Week 5</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week05.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 12</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#adaptive-learning-rates" id="toc-adaptive-learning-rates" class="nav-link active" data-scroll-target="#adaptive-learning-rates">Adaptive Learning Rates</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#adaptive-learning-rate-concepts" id="toc-adaptive-learning-rate-concepts" class="nav-link" data-scroll-target="#adaptive-learning-rate-concepts">Adaptive Learning Rate Concepts</a>
  <ul class="collapse">
  <li><a href="#importance-of-adaptive-learning-rates" id="toc-importance-of-adaptive-learning-rates" class="nav-link" data-scroll-target="#importance-of-adaptive-learning-rates">Importance of Adaptive Learning Rates</a></li>
  </ul></li>
  <li><a href="#neural-network-representation" id="toc-neural-network-representation" class="nav-link" data-scroll-target="#neural-network-representation">Neural Network Representation</a>
  <ul class="collapse">
  <li><a href="#weight-matrices-and-bias-vectors" id="toc-weight-matrices-and-bias-vectors" class="nav-link" data-scroll-target="#weight-matrices-and-bias-vectors">Weight Matrices and Bias Vectors</a></li>
  <li><a href="#loss-function-and-optimization" id="toc-loss-function-and-optimization" class="nav-link" data-scroll-target="#loss-function-and-optimization">Loss Function and Optimization</a></li>
  </ul></li>
  <li><a href="#derivation-of-adaptive-learning-rates" id="toc-derivation-of-adaptive-learning-rates" class="nav-link" data-scroll-target="#derivation-of-adaptive-learning-rates">Derivation of Adaptive Learning Rates</a>
  <ul class="collapse">
  <li><a href="#derivative-calculation-with-sparse-features" id="toc-derivative-calculation-with-sparse-features" class="nav-link" data-scroll-target="#derivative-calculation-with-sparse-features">Derivative Calculation with Sparse Features</a></li>
  <li><a href="#impact-of-sparse-features-on-gradient-descent" id="toc-impact-of-sparse-features-on-gradient-descent" class="nav-link" data-scroll-target="#impact-of-sparse-features-on-gradient-descent">Impact of Sparse Features on Gradient Descent</a></li>
  <li><a href="#importance-of-adaptive-learning-rates-for-sparse-features" id="toc-importance-of-adaptive-learning-rates-for-sparse-features" class="nav-link" data-scroll-target="#importance-of-adaptive-learning-rates-for-sparse-features">Importance of Adaptive Learning Rates for Sparse Features</a></li>
  </ul></li>
  <li><a href="#implementing-adaptive-learning-rates" id="toc-implementing-adaptive-learning-rates" class="nav-link" data-scroll-target="#implementing-adaptive-learning-rates">Implementing Adaptive Learning Rates</a>
  <ul class="collapse">
  <li><a href="#mathematical-formulation" id="toc-mathematical-formulation" class="nav-link" data-scroll-target="#mathematical-formulation">Mathematical Formulation</a></li>
  <li><a href="#benefits-of-adaptive-learning-rates" id="toc-benefits-of-adaptive-learning-rates" class="nav-link" data-scroll-target="#benefits-of-adaptive-learning-rates">Benefits of Adaptive Learning Rates</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#adagrad" id="toc-adagrad" class="nav-link" data-scroll-target="#adagrad">AdaGrad</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1">Introduction</a></li>
  <li><a href="#update-rule-for-adagrad" id="toc-update-rule-for-adagrad" class="nav-link" data-scroll-target="#update-rule-for-adagrad">Update Rule for AdaGrad</a></li>
  <li><a href="#implementation-in-code" id="toc-implementation-in-code" class="nav-link" data-scroll-target="#implementation-in-code">Implementation in Code</a></li>
  <li><a href="#experimental-results-and-analysis" id="toc-experimental-results-and-analysis" class="nav-link" data-scroll-target="#experimental-results-and-analysis">Experimental Results and Analysis</a></li>
  <li><a href="#visual-analysis-of-adagrad-behavior" id="toc-visual-analysis-of-adagrad-behavior" class="nav-link" data-scroll-target="#visual-analysis-of-adagrad-behavior">Visual Analysis of AdaGrad Behavior</a></li>
  <li><a href="#challenges-and-limitations" id="toc-challenges-and-limitations" class="nav-link" data-scroll-target="#challenges-and-limitations">Challenges and Limitations</a></li>
  <li><a href="#potential-improvements" id="toc-potential-improvements" class="nav-link" data-scroll-target="#potential-improvements">Potential Improvements</a></li>
  </ul></li>
  <li><a href="#rmsprop" id="toc-rmsprop" class="nav-link" data-scroll-target="#rmsprop">RMSprop</a>
  <ul class="collapse">
  <li><a href="#introduction-2" id="toc-introduction-2" class="nav-link" data-scroll-target="#introduction-2">Introduction</a></li>
  <li><a href="#motivation-for-rmsprop" id="toc-motivation-for-rmsprop" class="nav-link" data-scroll-target="#motivation-for-rmsprop">Motivation for RMSprop</a></li>
  <li><a href="#formulation-of-rmsprop" id="toc-formulation-of-rmsprop" class="nav-link" data-scroll-target="#formulation-of-rmsprop">Formulation of RMSprop</a></li>
  <li><a href="#key-insights-into-rmsprop" id="toc-key-insights-into-rmsprop" class="nav-link" data-scroll-target="#key-insights-into-rmsprop">Key Insights into RMSprop</a>
  <ul class="collapse">
  <li><a href="#exponentially-decaying-average" id="toc-exponentially-decaying-average" class="nav-link" data-scroll-target="#exponentially-decaying-average">Exponentially Decaying Average</a></li>
  <li><a href="#control-over-learning-rate-decay" id="toc-control-over-learning-rate-decay" class="nav-link" data-scroll-target="#control-over-learning-rate-decay">Control over Learning Rate Decay</a></li>
  <li><a href="#comparison-with-adagrad" id="toc-comparison-with-adagrad" class="nav-link" data-scroll-target="#comparison-with-adagrad">Comparison with AdaGrad</a></li>
  </ul></li>
  <li><a href="#behavior-of-rmsprop-during-training" id="toc-behavior-of-rmsprop-during-training" class="nav-link" data-scroll-target="#behavior-of-rmsprop-during-training">Behavior of RMSprop during Training</a>
  <ul class="collapse">
  <li><a href="#effect-on-learning-rate" id="toc-effect-on-learning-rate" class="nav-link" data-scroll-target="#effect-on-learning-rate">Effect on Learning Rate</a></li>
  <li><a href="#sensitivity-to-initial-learning-rate" id="toc-sensitivity-to-initial-learning-rate" class="nav-link" data-scroll-target="#sensitivity-to-initial-learning-rate">Sensitivity to Initial Learning Rate</a></li>
  <li><a href="#oscillation-phenomenon" id="toc-oscillation-phenomenon" class="nav-link" data-scroll-target="#oscillation-phenomenon">Oscillation Phenomenon</a></li>
  </ul></li>
  <li><a href="#addressing-sensitivity-to-initial-learning-rate" id="toc-addressing-sensitivity-to-initial-learning-rate" class="nav-link" data-scroll-target="#addressing-sensitivity-to-initial-learning-rate">Addressing Sensitivity to Initial Learning Rate</a>
  <ul class="collapse">
  <li><a href="#adaptive-learning-rate-adjustment" id="toc-adaptive-learning-rate-adjustment" class="nav-link" data-scroll-target="#adaptive-learning-rate-adjustment">Adaptive Learning Rate Adjustment</a></li>
  <li><a href="#experimental-exploration" id="toc-experimental-exploration" class="nav-link" data-scroll-target="#experimental-exploration">Experimental Exploration</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#adadelta" id="toc-adadelta" class="nav-link" data-scroll-target="#adadelta">AdaDelta</a>
  <ul class="collapse">
  <li><a href="#introduction-3" id="toc-introduction-3" class="nav-link" data-scroll-target="#introduction-3">Introduction</a></li>
  <li><a href="#overview-of-adadelta" id="toc-overview-of-adadelta" class="nav-link" data-scroll-target="#overview-of-adadelta">Overview of AdaDelta</a></li>
  <li><a href="#mathematical-formulation-1" id="toc-mathematical-formulation-1" class="nav-link" data-scroll-target="#mathematical-formulation-1">Mathematical Formulation</a></li>
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a>
  <ul class="collapse">
  <li><a href="#exponential-moving-averages" id="toc-exponential-moving-averages" class="nav-link" data-scroll-target="#exponential-moving-averages">Exponential Moving Averages</a></li>
  <li><a href="#ratio-of-updates" id="toc-ratio-of-updates" class="nav-link" data-scroll-target="#ratio-of-updates">Ratio of Updates</a></li>
  <li><a href="#adaptive-learning-rate" id="toc-adaptive-learning-rate" class="nav-link" data-scroll-target="#adaptive-learning-rate">Adaptive Learning Rate</a></li>
  </ul></li>
  <li><a href="#algorithm-workflow" id="toc-algorithm-workflow" class="nav-link" data-scroll-target="#algorithm-workflow">Algorithm Workflow</a></li>
  <li><a href="#advantages-of-adadelta" id="toc-advantages-of-adadelta" class="nav-link" data-scroll-target="#advantages-of-adadelta">Advantages of AdaDelta</a></li>
  </ul></li>
  <li><a href="#adam" id="toc-adam" class="nav-link" data-scroll-target="#adam">Adam</a>
  <ul class="collapse">
  <li><a href="#components-of-adam-algorithm" id="toc-components-of-adam-algorithm" class="nav-link" data-scroll-target="#components-of-adam-algorithm">Components of Adam Algorithm</a></li>
  <li><a href="#update-equations-for-adam" id="toc-update-equations-for-adam" class="nav-link" data-scroll-target="#update-equations-for-adam">Update Equations for Adam</a></li>
  <li><a href="#rationale-behind-bias-correction" id="toc-rationale-behind-bias-correction" class="nav-link" data-scroll-target="#rationale-behind-bias-correction">Rationale behind Bias Correction</a></li>
  <li><a href="#comparison-with-other-adaptive-algorithms" id="toc-comparison-with-other-adaptive-algorithms" class="nav-link" data-scroll-target="#comparison-with-other-adaptive-algorithms">Comparison with Other Adaptive Algorithms</a>
  <ul class="collapse">
  <li><a href="#experimental-results" id="toc-experimental-results" class="nav-link" data-scroll-target="#experimental-results">Experimental Results</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#lp-norms-and-optimization" id="toc-lp-norms-and-optimization" class="nav-link" data-scroll-target="#lp-norms-and-optimization">LP Norms and Optimization</a>
  <ul class="collapse">
  <li><a href="#introduction-4" id="toc-introduction-4" class="nav-link" data-scroll-target="#introduction-4">Introduction</a></li>
  <li><a href="#lp-norms" id="toc-lp-norms" class="nav-link" data-scroll-target="#lp-norms">LP Norms</a>
  <ul class="collapse">
  <li><a href="#l2-norm" id="toc-l2-norm" class="nav-link" data-scroll-target="#l2-norm">L2 Norm</a></li>
  <li><a href="#l-infinity-norm" id="toc-l-infinity-norm" class="nav-link" data-scroll-target="#l-infinity-norm">L Infinity Norm</a></li>
  </ul></li>
  <li><a href="#optimization-with-lp-norms" id="toc-optimization-with-lp-norms" class="nav-link" data-scroll-target="#optimization-with-lp-norms">Optimization with LP Norms</a>
  <ul class="collapse">
  <li><a href="#adam-optimizer-with-exponentially-weighted-l2-norm" id="toc-adam-optimizer-with-exponentially-weighted-l2-norm" class="nav-link" data-scroll-target="#adam-optimizer-with-exponentially-weighted-l2-norm">Adam Optimizer with Exponentially Weighted L2 Norm</a></li>
  <li><a href="#adam-max-introducing-max-norm" id="toc-adam-max-introducing-max-norm" class="nav-link" data-scroll-target="#adam-max-introducing-max-norm">Adam Max: Introducing Max Norm</a></li>
  <li><a href="#benefits-of-using-max-norm" id="toc-benefits-of-using-max-norm" class="nav-link" data-scroll-target="#benefits-of-using-max-norm">Benefits of Using Max Norm</a></li>
  <li><a href="#update-rule-for-adam-max" id="toc-update-rule-for-adam-max" class="nav-link" data-scroll-target="#update-rule-for-adam-max">Update Rule for Adam Max</a></li>
  </ul></li>
  <li><a href="#comparison-with-l2-norm" id="toc-comparison-with-l2-norm" class="nav-link" data-scroll-target="#comparison-with-l2-norm">Comparison with L2 Norm</a>
  <ul class="collapse">
  <li><a href="#scenario-1-sparse-gradients" id="toc-scenario-1-sparse-gradients" class="nav-link" data-scroll-target="#scenario-1-sparse-gradients">Scenario 1: Sparse Gradients</a></li>
  <li><a href="#scenario-2-zero-inputs" id="toc-scenario-2-zero-inputs" class="nav-link" data-scroll-target="#scenario-2-zero-inputs">Scenario 2: Zero Inputs</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#nadam" id="toc-nadam" class="nav-link" data-scroll-target="#nadam">NADAM</a>
  <ul class="collapse">
  <li><a href="#introduction-5" id="toc-introduction-5" class="nav-link" data-scroll-target="#introduction-5">Introduction</a></li>
  <li><a href="#rewriting-nag-equations" id="toc-rewriting-nag-equations" class="nav-link" data-scroll-target="#rewriting-nag-equations">Rewriting NAG Equations</a>
  <ul class="collapse">
  <li><a href="#original-nag-update-rule" id="toc-original-nag-update-rule" class="nav-link" data-scroll-target="#original-nag-update-rule">Original NAG Update Rule</a></li>
  <li><a href="#simplifying-nag-equations" id="toc-simplifying-nag-equations" class="nav-link" data-scroll-target="#simplifying-nag-equations">Simplifying NAG Equations</a></li>
  </ul></li>
  <li><a href="#modified-nag-equations" id="toc-modified-nag-equations" class="nav-link" data-scroll-target="#modified-nag-equations">Modified NAG Equations</a>
  <ul class="collapse">
  <li><a href="#update-rule-for-nesterov-accelerated-gradient-descent" id="toc-update-rule-for-nesterov-accelerated-gradient-descent" class="nav-link" data-scroll-target="#update-rule-for-nesterov-accelerated-gradient-descent">Update Rule for Nesterov Accelerated Gradient Descent</a></li>
  <li><a href="#mathematical-formulation-of-nag" id="toc-mathematical-formulation-of-nag" class="nav-link" data-scroll-target="#mathematical-formulation-of-nag">Mathematical Formulation of NAG</a></li>
  </ul></li>
  <li><a href="#practical-considerations-and-conclusion" id="toc-practical-considerations-and-conclusion" class="nav-link" data-scroll-target="#practical-considerations-and-conclusion">Practical Considerations and Conclusion</a>
  <ul class="collapse">
  <li><a href="#choosing-the-optimizer" id="toc-choosing-the-optimizer" class="nav-link" data-scroll-target="#choosing-the-optimizer">Choosing the Optimizer</a></li>
  <li><a href="#learning-rate-schedules" id="toc-learning-rate-schedules" class="nav-link" data-scroll-target="#learning-rate-schedules">Learning Rate Schedules</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#learning-rate-schedules-1" id="toc-learning-rate-schedules-1" class="nav-link" data-scroll-target="#learning-rate-schedules-1">Learning Rate Schedules</a>
  <ul class="collapse">
  <li><a href="#introduction-to-learning-rate-schedules" id="toc-introduction-to-learning-rate-schedules" class="nav-link" data-scroll-target="#introduction-to-learning-rate-schedules">Introduction to Learning Rate Schedules</a></li>
  <li><a href="#epoch-based-learning-rate-schemes" id="toc-epoch-based-learning-rate-schemes" class="nav-link" data-scroll-target="#epoch-based-learning-rate-schemes">Epoch-Based Learning Rate Schemes</a>
  <ul class="collapse">
  <li><a href="#step-decay" id="toc-step-decay" class="nav-link" data-scroll-target="#step-decay">Step Decay</a></li>
  <li><a href="#exponential-decay" id="toc-exponential-decay" class="nav-link" data-scroll-target="#exponential-decay">Exponential Decay</a></li>
  </ul></li>
  <li><a href="#adaptive-learning-rate-schemes" id="toc-adaptive-learning-rate-schemes" class="nav-link" data-scroll-target="#adaptive-learning-rate-schemes">Adaptive Learning Rate Schemes</a>
  <ul class="collapse">
  <li><a href="#adagrad-1" id="toc-adagrad-1" class="nav-link" data-scroll-target="#adagrad-1">Adagrad</a></li>
  <li><a href="#rmsprop-1" id="toc-rmsprop-1" class="nav-link" data-scroll-target="#rmsprop-1">RMSProp</a></li>
  <li><a href="#ada-delta" id="toc-ada-delta" class="nav-link" data-scroll-target="#ada-delta">ADA Delta</a></li>
  <li><a href="#adam-1" id="toc-adam-1" class="nav-link" data-scroll-target="#adam-1">Adam</a></li>
  <li><a href="#adamax" id="toc-adamax" class="nav-link" data-scroll-target="#adamax">Adamax</a></li>
  </ul></li>
  <li><a href="#cyclic-learning-rate-schedules" id="toc-cyclic-learning-rate-schedules" class="nav-link" data-scroll-target="#cyclic-learning-rate-schedules">Cyclic Learning Rate Schedules</a>
  <ul class="collapse">
  <li><a href="#triangular-schedule" id="toc-triangular-schedule" class="nav-link" data-scroll-target="#triangular-schedule">Triangular Schedule</a></li>
  <li><a href="#cosine-annealing" id="toc-cosine-annealing" class="nav-link" data-scroll-target="#cosine-annealing">Cosine Annealing</a></li>
  <li><a href="#warm-restart" id="toc-warm-restart" class="nav-link" data-scroll-target="#warm-restart">Warm Restart</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="adaptive-learning-rates" class="level1">
<h1>Adaptive Learning Rates</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In the realm of deep learning, optimization algorithms play a crucial role in training neural networks. One such algorithm is gradient descent, which aims to minimize a loss function by iteratively updating the parameters of the network. However, traditional gradient descent algorithms often struggle with finding an appropriate learning rate that balances convergence speed and stability across different regions of the optimization landscape. This is where adaptive learning rates come into play.</p>
</section>
<section id="adaptive-learning-rate-concepts" class="level2">
<h2 class="anchored" data-anchor-id="adaptive-learning-rate-concepts">Adaptive Learning Rate Concepts</h2>
<p>Adaptive learning rate algorithms dynamically adjust the learning rate during training based on the history of gradients and the current position in the optimization landscape. This adaptive behavior allows for faster convergence in regions with gentle gradients and more cautious updates in steep regions to prevent overshooting.</p>
<section id="importance-of-adaptive-learning-rates" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-adaptive-learning-rates">Importance of Adaptive Learning Rates</h3>
<p>Traditional gradient descent algorithms use a fixed learning rate, which may lead to suboptimal convergence behavior, especially in scenarios where the optimization landscape is highly variable. Adaptive learning rates address this issue by dynamically adjusting the learning rate based on the gradient’s magnitude and direction, resulting in improved convergence and training efficiency.</p>
</section>
</section>
<section id="neural-network-representation" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-representation">Neural Network Representation</h2>
<p>To understand the application of adaptive learning rates, let’s first revisit the representation of a neural network. Consider a neural network with <span class="math inline">\(L\)</span> layers, where each layer is composed of neurons. The input to the network is represented by <span class="math inline">\(\mathbf{x}\)</span>, and the output is represented by <span class="math inline">\(\mathbf{y}\)</span>.</p>
<section id="weight-matrices-and-bias-vectors" class="level3">
<h3 class="anchored" data-anchor-id="weight-matrices-and-bias-vectors">Weight Matrices and Bias Vectors</h3>
<p>At each layer <span class="math inline">\(i\)</span>, the network applies a set of weights <span class="math inline">\(\mathbf{W}^{(i)}\)</span> and biases <span class="math inline">\(\mathbf{b}^{(i)}\)</span> to transform the input into a pre-activation vector <span class="math inline">\(\mathbf{a}^{(i)}\)</span>. The pre-activation vector is then passed through an activation function <span class="math inline">\(\mathbf{g}^{(i)}(\cdot)\)</span> to produce the activation vector <span class="math inline">\(\mathbf{h}^{(i)}\)</span>. This process is repeated for each subsequent layer until the final output is obtained.</p>
</section>
<section id="loss-function-and-optimization" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-and-optimization">Loss Function and Optimization</h3>
<p>During training, the network’s parameters, including weights and biases, are updated iteratively to minimize a loss function <span class="math inline">\(\mathcal{L}(\theta_t)\)</span>, where <span class="math inline">\(\theta_t\)</span> represents the parameters at iteration <span class="math inline">\(t\)</span>. The optimization process involves computing the gradient of the loss function with respect to the parameters and adjusting the parameters in the direction that minimizes the loss.</p>
</section>
</section>
<section id="derivation-of-adaptive-learning-rates" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-adaptive-learning-rates">Derivation of Adaptive Learning Rates</h2>
<p>Now, let’s delve into the derivation of adaptive learning rates and their significance in optimizing neural networks.</p>
<section id="derivative-calculation-with-sparse-features" class="level3">
<h3 class="anchored" data-anchor-id="derivative-calculation-with-sparse-features">Derivative Calculation with Sparse Features</h3>
<p>Consider a scenario where the input features <span class="math inline">\(\mathbf{x}\)</span> include sparse features, i.e., features that are often zero across many training instances. In such cases, the derivative of the loss function with respect to the weights corresponding to sparse features tends to be small due to the frequent occurrence of zero values.</p>
<p>Mathematically, let’s denote the derivative of the loss function <span class="math inline">\(\mathcal{L}\)</span> with respect to a weight <span class="math inline">\(\mathbf{W}_j^{(i)}\)</span> as <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{W}_j^{(i)}}\)</span>. If a feature <span class="math inline">\(x_j\)</span> is sparse, the derivative can be expressed as:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial \mathbf{W}_j^{(i)}} = \sum_{k=1}^{m} \frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(i)}} \cdot x_j
\]</span></p>
<p>Where <span class="math inline">\(m\)</span> represents the total number of training instances, and <span class="math inline">\(x_j\)</span> is the value of the sparse feature.</p>
</section>
<section id="impact-of-sparse-features-on-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="impact-of-sparse-features-on-gradient-descent">Impact of Sparse Features on Gradient Descent</h3>
<p>Sparse features lead to sparse updates during gradient descent, as the derivatives associated with these features are small. Consequently, the weights corresponding to sparse features experience minimal changes during optimization, potentially impeding the network’s ability to learn from these features effectively.</p>
</section>
<section id="importance-of-adaptive-learning-rates-for-sparse-features" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-adaptive-learning-rates-for-sparse-features">Importance of Adaptive Learning Rates for Sparse Features</h3>
<p>To address the issue of sparse updates for weights associated with sparse features, adaptive learning rates offer a solution. By dynamically adjusting the learning rate based on the sparsity of features, adaptive learning rates ensure that weights corresponding to sparse features receive meaningful updates, allowing the network to effectively leverage the information provided by these features.</p>
</section>
</section>
<section id="implementing-adaptive-learning-rates" class="level2">
<h2 class="anchored" data-anchor-id="implementing-adaptive-learning-rates">Implementing Adaptive Learning Rates</h2>
<p>The implementation of adaptive learning rates involves designing algorithms that automatically adjust the learning rate based on the sparsity of features. This requires a systematic approach to ensure efficient optimization across millions of features without manual intervention.</p>
<section id="mathematical-formulation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h3>
<p>Let’s denote the learning rate at iteration <span class="math inline">\(t\)</span> as <span class="math inline">\(\eta_t\)</span>. To adaptively adjust the learning rate based on the sparsity of features, we can define a function <span class="math inline">\(\eta_t = f(\mathbf{x}_t)\)</span>, where <span class="math inline">\(\mathbf{x}_t\)</span> represents the input data at iteration <span class="math inline">\(t\)</span>.</p>
<p>One approach to defining the adaptive learning rate function is to incorporate a measure of feature sparsity into the learning rate calculation. For example, we can define <span class="math inline">\(\eta_t\)</span> as follows:</p>
<p><span class="math display">\[
\eta_t = \eta_0 \cdot \text{sparsity\_factor}(\mathbf{x}_t)
\]</span></p>
<p>Where <span class="math inline">\(\eta_0\)</span> represents the initial learning rate, and <span class="math inline">\(\text{sparsity\_factor}(\mathbf{x}_t)\)</span> is a function that quantifies the sparsity of the input data at iteration <span class="math inline">\(t\)</span>.</p>
</section>
<section id="benefits-of-adaptive-learning-rates" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-adaptive-learning-rates">Benefits of Adaptive Learning Rates</h3>
<p>By incorporating adaptive learning rates into the optimization process, neural networks can effectively leverage sparse features for improved performance. Adaptive learning rates ensure that weights associated with sparse features receive sufficient updates, allowing the network to learn meaningful representations from sparse data.</p>
</section>
</section>
</section>
<section id="adagrad" class="level1">
<h1>AdaGrad</h1>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">Introduction</h2>
<p>AdaGrad is a powerful optimization algorithm used in deep learning to adaptively adjust the learning rate during training. It addresses the challenge of selecting an appropriate learning rate for different features in the input data. This method ensures that features with frequent updates receive smaller learning rates, while features with sparse updates receive larger learning rates, leading to more effective and efficient training of neural networks.</p>
</section>
<section id="update-rule-for-adagrad" class="level2">
<h2 class="anchored" data-anchor-id="update-rule-for-adagrad">Update Rule for AdaGrad</h2>
<p>The core idea behind AdaGrad is to adjust the learning rate for each feature based on its update history. This is achieved by maintaining a history of the squared gradients for each feature and dividing the learning rate by the square root of this history. Mathematically, the update rule for AdaGrad can be expressed as follows:</p>
<p><span class="math display">\[
\mathbf{v}_t = \mathbf{v}_{t-1} + \left( \frac{\partial \mathcal{L}(\theta_t)}{\partial \mathbf{W}_t} \right)^2
\]</span></p>
<p><span class="math display">\[
\mathbf{W}_{t+1} = \mathbf{W}_t - \frac{\eta}{\sqrt{\mathbf{v}_t + \epsilon}} \frac{\partial \mathcal{L}(\theta_t)}{\partial \mathbf{W}_t}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\mathbf{v}_t\)</span> is the accumulated squared gradients.</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate.</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant added to the denominator for numerical stability.</li>
<li><span class="math inline">\(\frac{\partial \mathcal{L}(\theta_t)}{\partial \mathbf{W}_t}\)</span> is the gradient of the loss function with respect to the weights at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\mathbf{W}_t\)</span> represents the weights at iteration <span class="math inline">\(t\)</span>.</li>
</ul>
<p>Similarly, the update rule for the bias terms <span class="math inline">\(\mathbf{b}_t\)</span> can be derived using the same principle.</p>
</section>
<section id="implementation-in-code" class="level2">
<h2 class="anchored" data-anchor-id="implementation-in-code">Implementation in Code</h2>
<p>In code, the AdaGrad algorithm involves accumulating the squared gradients and updating the weights and biases accordingly. The update equations for weights and biases can be implemented as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>v_W <span class="op">+=</span> (dW <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>W <span class="op">-=</span> (learning_rate <span class="op">/</span> np.sqrt(v_W <span class="op">+</span> eps)) <span class="op">*</span> dW</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>v_b <span class="op">+=</span> (db <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">-=</span> (learning_rate <span class="op">/</span> np.sqrt(v_b <span class="op">+</span> eps)) <span class="op">*</span> db</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Where:</p>
<ul>
<li><code>v_W</code> and <code>v_b</code> are the accumulated squared gradients for weights and biases, respectively.</li>
<li><code>dW</code> and <code>db</code> are the gradients of the loss function with respect to the weights and biases.</li>
<li><code>learning_rate</code> is the learning rate.</li>
<li><code>eps</code> is a small constant for numerical stability.</li>
</ul>
</section>
<section id="experimental-results-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="experimental-results-and-analysis">Experimental Results and Analysis</h2>
<p>In an experiment, AdaGrad was applied to training data with both sparse and dense features. It demonstrated the ability to make proportionate movements in the direction of sparse features, despite their small updates. However, one observation was that as the training progressed, AdaGrad’s effective learning rate decreased significantly, potentially causing slow convergence near the minimum.</p>
</section>
<section id="visual-analysis-of-adagrad-behavior" class="level2">
<h2 class="anchored" data-anchor-id="visual-analysis-of-adagrad-behavior">Visual Analysis of AdaGrad Behavior</h2>
<p>Visual analysis of AdaGrad’s behavior revealed that it was able to move proportionately in the direction of both sparse and dense features. However, as the training progressed, AdaGrad’s effective learning rate decreased exponentially due to the accumulation of update history.</p>
</section>
<section id="challenges-and-limitations" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-limitations">Challenges and Limitations</h2>
<p>While AdaGrad successfully adapted the learning rate for both sparse and dense features, it faced challenges as training progressed. The effective learning rate for dense features became so small that it hindered movement in the direction of sparse features, leading to slower convergence. The accumulation of update history posed a challenge near the minimum, where gradients became small but the history remained large, causing the effective learning rate to decrease excessively.</p>
</section>
<section id="potential-improvements" class="level2">
<h2 class="anchored" data-anchor-id="potential-improvements">Potential Improvements</h2>
<p>One potential improvement could be to explore variations of AdaGrad that address the issue of excessively small effective learning rates. Adding a momentum term to AdaGrad could potentially combine the advantages of momentum-based algorithms with adaptive learning rates, improving convergence speed and performance. However, further research and experimentation are needed to explore these possibilities and address the limitations of AdaGrad effectively.</p>
</section>
</section>
<section id="rmsprop" class="level1">
<h1>RMSprop</h1>
<section id="introduction-2" class="level2">
<h2 class="anchored" data-anchor-id="introduction-2">Introduction</h2>
<p>In the realm of deep learning, optimization algorithms play a crucial role in training neural networks effectively. One such algorithm is RMSprop, short for Root Mean Square Propagation, which addresses some of the limitations of previous optimization methods like AdaGrad. In this lecture module, we delve into the intricacies of RMSprop, its formulation, and its behavior during the training process.</p>
</section>
<section id="motivation-for-rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="motivation-for-rmsprop">Motivation for RMSprop</h2>
<p>The motivation behind RMSprop stems from the need to address the aggressive decay of learning rates in optimization algorithms as the training progresses. In AdaGrad, for instance, the denominator in the update rule accumulates the squares of past gradients, causing the learning rate to diminish rapidly, especially for frequently updated parameters. This phenomenon inhibits the convergence of the optimization process, leading to suboptimal solutions.</p>
</section>
<section id="formulation-of-rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="formulation-of-rmsprop">Formulation of RMSprop</h2>
<p>To mitigate the rapid growth of the denominator in AdaGrad, RMSprop introduces a scaling mechanism by modifying the update rule. Instead of accumulating the squares of gradients indiscriminately, RMSprop employs an exponentially decaying average of past squared gradients. This is achieved by introducing a decay factor, typically denoted as <span class="math inline">\(\beta\)</span>, which controls the rate at which the history is accumulated. The modified update rule for the denominator <span class="math inline">\(v_t\)</span> in RMSprop is given by:</p>
<p><span class="math display">\[ v_t = \beta v_{t-1} + (1 - \beta) (\nabla \mathcal{L})^2 \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(v_t\)</span> represents the accumulated history of squared gradients at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\beta\)</span> is a hyperparameter controlling the exponential decay rate.</li>
<li><span class="math inline">\(\nabla \mathcal{L}\)</span> denotes the gradient of the loss function.</li>
</ul>
</section>
<section id="key-insights-into-rmsprop" class="level2">
<h2 class="anchored" data-anchor-id="key-insights-into-rmsprop">Key Insights into RMSprop</h2>
<section id="exponentially-decaying-average" class="level3">
<h3 class="anchored" data-anchor-id="exponentially-decaying-average">Exponentially Decaying Average</h3>
<p>The crux of RMSprop lies in the use of an exponentially decaying average for accumulating the history of squared gradients. This approach ensures that the denominator <span class="math inline">\(v_t\)</span> grows less aggressively compared to AdaGrad, thereby stabilizing the effective learning rate.</p>
</section>
<section id="control-over-learning-rate-decay" class="level3">
<h3 class="anchored" data-anchor-id="control-over-learning-rate-decay">Control over Learning Rate Decay</h3>
<p>By scaling down the growth of the denominator with the help of the decay factor <span class="math inline">\(\beta\)</span>, RMSprop prevents the rapid decline of the effective learning rate. This control over the learning rate decay allows for smoother convergence during optimization.</p>
</section>
<section id="comparison-with-adagrad" class="level3">
<h3 class="anchored" data-anchor-id="comparison-with-adagrad">Comparison with AdaGrad</h3>
<p>In contrast to AdaGrad, where the denominator accumulates gradients without any decay, RMSprop offers a more controlled approach by incorporating an exponentially decaying average. This modification alleviates the issue of overly aggressive learning rate decay encountered in AdaGrad.</p>
</section>
</section>
<section id="behavior-of-rmsprop-during-training" class="level2">
<h2 class="anchored" data-anchor-id="behavior-of-rmsprop-during-training">Behavior of RMSprop during Training</h2>
<section id="effect-on-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="effect-on-learning-rate">Effect on Learning Rate</h3>
<p>During the training process, RMSprop dynamically adjusts the effective learning rate based on the magnitude of gradients encountered. In regions with steep gradients, the learning rate decreases gradually to prevent overshooting, while in flatter regions, it increases to expedite convergence.</p>
</section>
<section id="sensitivity-to-initial-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="sensitivity-to-initial-learning-rate">Sensitivity to Initial Learning Rate</h3>
<p>One notable aspect of RMSprop is its sensitivity to the initial learning rate (<span class="math inline">\(\eta_0\)</span>). The choice of <span class="math inline">\(\eta_0\)</span> can significantly impact the convergence behavior of the algorithm, leading to variations in convergence speed and stability.</p>
</section>
<section id="oscillation-phenomenon" class="level3">
<h3 class="anchored" data-anchor-id="oscillation-phenomenon">Oscillation Phenomenon</h3>
<p>In some scenarios, RMSprop may exhibit oscillations around the minima during optimization. These oscillations stem from the interplay between the learning rate and the curvature of the loss surface. If the learning rate becomes constant and the curvature allows for symmetric oscillations, the optimization process may oscillate between different points on the loss surface.</p>
</section>
</section>
<section id="addressing-sensitivity-to-initial-learning-rate" class="level2">
<h2 class="anchored" data-anchor-id="addressing-sensitivity-to-initial-learning-rate">Addressing Sensitivity to Initial Learning Rate</h2>
<section id="adaptive-learning-rate-adjustment" class="level3">
<h3 class="anchored" data-anchor-id="adaptive-learning-rate-adjustment">Adaptive Learning Rate Adjustment</h3>
<p>To mitigate the sensitivity to the initial learning rate, researchers have proposed adaptive techniques that dynamically adjust the learning rate during training. These methods aim to alleviate the reliance on manually tuning the initial learning rate, thereby improving the robustness of optimization algorithms like RMSprop.</p>
</section>
<section id="experimental-exploration" class="level3">
<h3 class="anchored" data-anchor-id="experimental-exploration">Experimental Exploration</h3>
<p>Empirical studies have shown that the choice of initial learning rate (<span class="math inline">\(\eta_0\)</span>) can significantly impact the convergence behavior of RMSprop. Researchers often conduct experiments with different values of <span class="math inline">\(\eta_0\)</span> to determine the optimal setting for specific datasets and network architectures.</p>
</section>
</section>
</section>
<section id="adadelta" class="level1">
<h1>AdaDelta</h1>
<section id="introduction-3" class="level2">
<h2 class="anchored" data-anchor-id="introduction-3">Introduction</h2>
<p>In the domain of deep learning, optimization algorithms play a crucial role in training neural networks effectively. One such algorithm is AdaDelta, which is designed to address challenges such as choosing an appropriate learning rate and dealing with varying magnitudes of gradients during training. This algorithm dynamically adapts the learning rate based on past gradients, allowing for smoother convergence and improved performance. In this section, we delve into the details of the AdaDelta algorithm, its key components, and its application in optimizing neural network parameters.</p>
</section>
<section id="overview-of-adadelta" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-adadelta">Overview of AdaDelta</h2>
<p>AdaDelta is an extension of the RMSprop optimization algorithm, which aims to mitigate its dependency on an initial learning rate. Unlike traditional methods that require manual tuning of hyperparameters like the learning rate, AdaDelta automatically adjusts the learning rate during training based on past gradients and accumulated updates.</p>
</section>
<section id="mathematical-formulation-1" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-formulation-1">Mathematical Formulation</h2>
<p>Let’s define some key variables and equations used in AdaDelta:</p>
<p><strong>Variables:</strong></p>
<ul>
<li><span class="math inline">\(\mathbf{u}_t\)</span>: Velocity at iteration <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(v_t\)</span>: Accumulated history of squared gradients at iteration <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\beta\)</span>: Hyperparameter controlling the exponential decay rate</li>
</ul>
<p><strong>Equations:</strong></p>
<ol type="1">
<li><p><strong>Update Rule</strong>: <span class="math display">\[ \Delta \mathbf{W}_t = - \frac{\sqrt{\mathbf{u}_{t-1} + \epsilon}}{\sqrt{v_t + \epsilon}} \cdot \nabla \mathcal{L} \]</span></p></li>
<li><p><strong>Update Velocity</strong>: <span class="math display">\[ \mathbf{u}_t = \beta \cdot \mathbf{u}_{t-1} + (1 - \beta) \cdot (\Delta \mathbf{W}_t)^2 \]</span></p></li>
<li><p><strong>Parameter Update</strong>: <span class="math display">\[ \mathbf{W}_{t+1} = \mathbf{W}_t + \Delta \mathbf{W}_t \]</span></p></li>
</ol>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts</h2>
<section id="exponential-moving-averages" class="level3">
<h3 class="anchored" data-anchor-id="exponential-moving-averages">Exponential Moving Averages</h3>
<p>AdaDelta utilizes exponential moving averages to compute the update velocity and accumulated history of squared gradients. This involves maintaining a running average of past gradients and squared gradients, weighted by the decay factor <span class="math inline">\(\beta\)</span>. By doing so, AdaDelta can adaptively adjust the learning rate based on the magnitude and variance of gradients encountered during training.</p>
</section>
<section id="ratio-of-updates" class="level3">
<h3 class="anchored" data-anchor-id="ratio-of-updates">Ratio of Updates</h3>
<p>The AdaDelta algorithm calculates the update as a ratio of two variables: <span class="math inline">\(\mathbf{u}_t\)</span> and <span class="math inline">\(v_t\)</span>. This ratio serves as a scaling factor for the gradient, allowing AdaDelta to effectively modulate the learning rate based on the historical behavior of gradients.</p>
</section>
<section id="adaptive-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="adaptive-learning-rate">Adaptive Learning Rate</h3>
<p>Unlike traditional optimization algorithms that rely on a fixed learning rate, AdaDelta dynamically adjusts the learning rate based on the accumulated history of gradients. This adaptive nature enables AdaDelta to navigate complex optimization landscapes more efficiently and converge to optimal solutions with fewer iterations.</p>
</section>
</section>
<section id="algorithm-workflow" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-workflow">Algorithm Workflow</h2>
<p>Now, let’s outline the step-by-step workflow of the AdaDelta algorithm:</p>
<ol type="1">
<li><strong>Initialization</strong>:
<ul>
<li>Initialize parameters and variables, including <span class="math inline">\(\mathbf{W}\)</span>, <span class="math inline">\(\mathbf{u}\)</span>, and <span class="math inline">\(v\)</span>.</li>
<li>Set hyperparameters such as <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\epsilon\)</span>.</li>
</ul></li>
<li><strong>Compute Gradient</strong>:
<ul>
<li>Calculate the gradient of the loss function with respect to the model parameters (<span class="math inline">\(\nabla \mathcal{L}\)</span>).</li>
</ul></li>
<li><strong>Update Velocity</strong>:
<ul>
<li>Update the velocity (<span class="math inline">\(\mathbf{u}_t\)</span>) using the current gradient and the decay factor <span class="math inline">\(\beta\)</span>.</li>
</ul></li>
<li><strong>Compute Update</strong>:
<ul>
<li>Compute the update (<span class="math inline">\(\Delta \mathbf{W}_t\)</span>) using the ratio of <span class="math inline">\(\mathbf{u}_t\)</span> and <span class="math inline">\(v_t\)</span>.</li>
</ul></li>
<li><strong>Parameter Update</strong>:
<ul>
<li>Update the model parameters (<span class="math inline">\(\mathbf{W}\)</span>) using the computed update.</li>
</ul></li>
<li><strong>Repeat</strong>:
<ul>
<li>Iterate through steps 2-5 for multiple epochs or until convergence criteria are met.</li>
</ul></li>
</ol>
</section>
<section id="advantages-of-adadelta" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-adadelta">Advantages of AdaDelta</h2>
<p>AdaDelta offers several advantages over traditional optimization algorithms:</p>
<ol type="1">
<li><strong>Automatic Learning Rate Adjustment</strong>:
<ul>
<li>AdaDelta eliminates the need for manually tuning the learning rate by adapting it dynamically based on past gradients.</li>
</ul></li>
<li><strong>Improved Convergence</strong>:
<ul>
<li>By adjusting the learning rate according to the historical behavior of gradients, AdaDelta can converge more smoothly and efficiently.</li>
</ul></li>
<li><strong>Robustness to Hyperparameters</strong>:
<ul>
<li>AdaDelta’s reliance on only a few hyperparameters, such as <span class="math inline">\(\beta\)</span>, makes it more robust and easier to use compared to algorithms with additional tuning parameters.</li>
</ul></li>
</ol>
</section>
</section>
<section id="adam" class="level1">
<h1>Adam</h1>
<p>Adam, short for Adaptive Moments, combines elements of RMSprop and momentum-based optimization techniques to achieve adaptive learning rates. The algorithm maintains exponentially weighted averages of past gradients and squared gradients to adjust the effective learning rate for each parameter.</p>
<section id="components-of-adam-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="components-of-adam-algorithm">Components of Adam Algorithm</h3>
<p>Adam algorithm consists of the following key components:</p>
<ol type="1">
<li><p><strong>Exponentially Weighted Averages</strong>: Adam maintains two exponentially weighted moving averages: <span class="math inline">\(\mathbf{m}_t\)</span> for the gradients and <span class="math inline">\(\mathbf{v}_t\)</span> for the squared gradients.</p></li>
<li><p><strong>Bias Correction</strong>: Adam incorporates bias correction terms to mitigate the initialization bias, ensuring smoother updates during the initial training phases.</p></li>
<li><p><strong>Effective Learning Rate</strong>: The effective learning rate in Adam is computed based on the moving averages of gradients and squared gradients, adjusted by bias correction factors.</p></li>
</ol>
</section>
<section id="update-equations-for-adam" class="level3">
<h3 class="anchored" data-anchor-id="update-equations-for-adam">Update Equations for Adam</h3>
<p>The update equations for Adam algorithm are as follows:</p>
<p><span class="math display">\[
\begin{align*}
\mathbf{m}_t &amp;= \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \nabla \mathcal{L}_t \\
\mathbf{v}_t &amp;= \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) (\nabla \mathcal{L}_t)^2 \\
\hat{\mathbf{m}}_t &amp;= \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
\hat{\mathbf{v}}_t &amp;= \frac{\mathbf{v}_t}{1 - \beta_2^t} \\
\mathbf{W}_t &amp;= \mathbf{W}_{t-1} - \eta \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}
\end{align*}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters controlling the exponential decay rates.</li>
<li><span class="math inline">\(\nabla \mathcal{L}_t\)</span> denotes the gradient of the loss function at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate.</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant to prevent division by zero.</li>
</ul>
</section>
<section id="rationale-behind-bias-correction" class="level3">
<h3 class="anchored" data-anchor-id="rationale-behind-bias-correction">Rationale behind Bias Correction</h3>
<p>The bias correction term in Adam addresses the initialization bias observed in the early stages of training. By dividing the moving averages by the bias correction factors, Adam ensures that the effective learning rate remains stable across different iterations, preventing erratic updates during the initial training phase.</p>
</section>
<section id="comparison-with-other-adaptive-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-other-adaptive-algorithms">Comparison with Other Adaptive Algorithms</h2>
<p>Adam algorithm exhibits favorable convergence properties compared to other adaptive algorithms such as AdaDelta and RMSprop. By incorporating both momentum and adaptive learning rate mechanisms, Adam achieves faster convergence while avoiding the learning rate decay issues encountered in RMSprop.</p>
<section id="experimental-results" class="level3">
<h3 class="anchored" data-anchor-id="experimental-results">Experimental Results</h3>
<p>Empirical studies have demonstrated the superior performance of Adam in terms of convergence speed and generalization ability. By dynamically adjusting the learning rate based on past gradients and squared gradients, Adam effectively navigates the loss landscape, leading to faster convergence and improved model performance.</p>
</section>
</section>
</section>
<section id="lp-norms-and-optimization" class="level1">
<h1>LP Norms and Optimization</h1>
<section id="introduction-4" class="level2">
<h2 class="anchored" data-anchor-id="introduction-4">Introduction</h2>
<p>In deep learning, optimization algorithms play a crucial role in training neural networks efficiently. Understanding different norms and their implications on optimization is essential for designing effective optimization techniques. In this discussion, we delve into LP Norms and their significance in optimization algorithms, particularly focusing on the Adam optimizer.</p>
</section>
<section id="lp-norms" class="level2">
<h2 class="anchored" data-anchor-id="lp-norms">LP Norms</h2>
<p>LP Norm is a mathematical concept used to measure the size of a vector in a space. It is defined by the following formula:</p>
<p><span class="math display">\[
\| \mathbf{x} \|_p = \left( \sum_{i=1}^{n} |x_i|^p \right)^{1/p}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{x}\)</span> is the input vector, <span class="math inline">\(p\)</span> is a parameter, and <span class="math inline">\(n\)</span> is the dimensionality of the vector.</p>
<section id="l2-norm" class="level3">
<h3 class="anchored" data-anchor-id="l2-norm">L2 Norm</h3>
<p>The L2 Norm, also known as the Euclidean Norm, is a special case of the LP Norm where <span class="math inline">\(p = 2\)</span>. It is calculated by taking the square root of the sum of squares of the vector components:</p>
<p><span class="math display">\[
\| \mathbf{x} \|_2 = \sqrt{\sum_{i=1}^{n} |x_i|^2}
\]</span></p>
<p>The L2 Norm is widely used in deep learning for regularization and optimization purposes.</p>
</section>
<section id="l-infinity-norm" class="level3">
<h3 class="anchored" data-anchor-id="l-infinity-norm">L Infinity Norm</h3>
<p>The L Infinity Norm, denoted as <span class="math inline">\(\| \mathbf{x} \|_{\infty}\)</span>, represents the maximum absolute value of the vector components:</p>
<p><span class="math display">\[
\| \mathbf{x} \|_{\infty} = \max_{i} |x_i|
\]</span></p>
<p>It simplifies computations and is particularly useful in scenarios where the maximum magnitude of the elements is of interest.</p>
</section>
</section>
<section id="optimization-with-lp-norms" class="level2">
<h2 class="anchored" data-anchor-id="optimization-with-lp-norms">Optimization with LP Norms</h2>
<p>Optimization algorithms in deep learning often involve computing gradients and updating model parameters iteratively. The choice of norm used in these algorithms can have significant implications on convergence and performance.</p>
<section id="adam-optimizer-with-exponentially-weighted-l2-norm" class="level3">
<h3 class="anchored" data-anchor-id="adam-optimizer-with-exponentially-weighted-l2-norm">Adam Optimizer with Exponentially Weighted L2 Norm</h3>
<p>The Adam optimizer is a popular choice for training neural networks due to its adaptive learning rate mechanism. It incorporates an exponentially weighted L2 Norm of gradients to adaptively adjust the learning rate for each parameter.</p>
<p>The update rule for the Adam optimizer involves maintaining two exponentially decaying moving averages: <span class="math inline">\(m_t\)</span> for the first moment (mean) and <span class="math inline">\(v_t\)</span> for the second moment (uncentered variance) of the gradients.</p>
<p><span class="math display">\[
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla \mathcal{L}_t
\]</span></p>
<p><span class="math display">\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla \mathcal{L}_t)^2
\]</span></p>
<p>where <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters controlling the exponential decay rates, <span class="math inline">\(\nabla \mathcal{L}_t\)</span> is the gradient of the loss function at iteration <span class="math inline">\(t\)</span>.</p>
</section>
<section id="adam-max-introducing-max-norm" class="level3">
<h3 class="anchored" data-anchor-id="adam-max-introducing-max-norm">Adam Max: Introducing Max Norm</h3>
<p>In the context of the Adam optimizer, the use of L2 Norm for computing the gradient’s magnitude may lead to numerical instability, especially when dealing with large values of <span class="math inline">\(p\)</span>. To address this issue, we explore the possibility of using the L Infinity Norm (Max Norm) instead.</p>
<p>The Max Norm, defined as <span class="math inline">\(\| \mathbf{x} \|_{\infty} = \max_{i} |x_i|\)</span>, simplifies to selecting the maximum absolute value from the vector components.</p>
</section>
<section id="benefits-of-using-max-norm" class="level3">
<h3 class="anchored" data-anchor-id="benefits-of-using-max-norm">Benefits of Using Max Norm</h3>
<ol type="1">
<li><p><strong>Simplicity</strong>: The Max Norm computation is straightforward and does not involve complex mathematical operations.</p></li>
<li><p><strong>Stability</strong>: Max Norm avoids numerical instability issues associated with large values of <span class="math inline">\(p\)</span> in LP Norms, making it a robust choice for optimization algorithms.</p></li>
</ol>
</section>
<section id="update-rule-for-adam-max" class="level3">
<h3 class="anchored" data-anchor-id="update-rule-for-adam-max">Update Rule for Adam Max</h3>
<p>The update rule for Adam Max, a variant of the Adam optimizer using Max Norm, is derived by replacing the L2 Norm computation with the Max Norm for computing the second moment <span class="math inline">\(v_t\)</span>.</p>
<p><span class="math display">\[
v_t = \beta_2 v_{t-1} + (1 - \beta_2) \| \nabla \mathcal{L}_t \|_{\infty}^2
\]</span></p>
<p>This modification simplifies the computation and enhances the stability of the optimization process.</p>
</section>
</section>
<section id="comparison-with-l2-norm" class="level2">
<h2 class="anchored" data-anchor-id="comparison-with-l2-norm">Comparison with L2 Norm</h2>
<p>To understand the practical implications of using Max Norm in optimization, let’s compare its performance with the traditional L2 Norm approach.</p>
<section id="scenario-1-sparse-gradients" class="level3">
<h3 class="anchored" data-anchor-id="scenario-1-sparse-gradients">Scenario 1: Sparse Gradients</h3>
<p>In scenarios where gradients alternate between high and zero values, the Max Norm maintains a more consistent learning rate compared to the L2 Norm. This stability ensures smoother convergence during training, especially when dealing with sparse features.</p>
</section>
<section id="scenario-2-zero-inputs" class="level3">
<h3 class="anchored" data-anchor-id="scenario-2-zero-inputs">Scenario 2: Zero Inputs</h3>
<p>When encountering zero inputs, the Max Norm prevents unnecessary fluctuations in the learning rate. Unlike the L2 Norm, which may amplify changes even with zero gradients, the Max Norm remains stable and preserves the learning rate effectively.</p>
</section>
</section>
</section>
<section id="nadam" class="level1">
<h1>NADAM</h1>
<section id="introduction-5" class="level2">
<h2 class="anchored" data-anchor-id="introduction-5">Introduction</h2>
<p>Nesterov Accelerated Gradient Descent (NAG) is an optimization algorithm used in training neural networks. It is an extension of the standard momentum-based gradient descent method. The key idea behind NAG is to improve upon the momentum-based approach by incorporating the Nesterov’s accelerated gradient (NAG) concept, also known as the lookahead effect. In this module, we explore how to integrate Nesterov Accelerated Gradient Descent into the Adam optimizer.</p>
</section>
<section id="rewriting-nag-equations" class="level2">
<h2 class="anchored" data-anchor-id="rewriting-nag-equations">Rewriting NAG Equations</h2>
<section id="original-nag-update-rule" class="level3">
<h3 class="anchored" data-anchor-id="original-nag-update-rule">Original NAG Update Rule</h3>
<p>The original NAG update rule involves computing the gradient at a lookahead value and then updating the parameters based on this lookahead gradient. This approach involves cumbersome computations and redundant calculations.</p>
</section>
<section id="simplifying-nag-equations" class="level3">
<h3 class="anchored" data-anchor-id="simplifying-nag-equations">Simplifying NAG Equations</h3>
<p>To simplify the NAG equations and integrate them into the Adam optimizer, we need to rewrite the update rule in a more compact and efficient manner. The goal is to eliminate redundant computations and express all equations in terms of the current time step (<span class="math inline">\(t\)</span>) and the next time step (<span class="math inline">\(t + 1\)</span>).</p>
</section>
</section>
<section id="modified-nag-equations" class="level2">
<h2 class="anchored" data-anchor-id="modified-nag-equations">Modified NAG Equations</h2>
<section id="update-rule-for-nesterov-accelerated-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="update-rule-for-nesterov-accelerated-gradient-descent">Update Rule for Nesterov Accelerated Gradient Descent</h3>
<p>The update rule for Nesterov Accelerated Gradient Descent (NAG) involves the following steps:</p>
<ol type="1">
<li>Compute the gradient at the current parameter values.</li>
<li>Compute the lookahead gradient at the next parameter values using the gradient computed in step 1.</li>
<li>Update the parameters using a combination of the current gradient and the lookahead gradient.</li>
</ol>
</section>
<section id="mathematical-formulation-of-nag" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-formulation-of-nag">Mathematical Formulation of NAG</h3>
<p>The Nesterov Accelerated Gradient Descent update rule can be expressed as follows:</p>
<p><span class="math display">\[
\mathbf{u}_{t+1} = \beta \mathbf{u}_t + \eta \nabla \mathcal{L}(\theta_t - \beta \mathbf{u}_t)
\]</span></p>
<p><span class="math display">\[
v_{t+1} = \beta_2 v_t + (1 - \beta_2) (\nabla \mathcal{L}_t)^2
\]</span></p>
<p><span class="math display">\[
\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_{t+1}} + \epsilon} \left( \beta_1 \mathbf{u}_{t+1} + (1 - \beta_1) \nabla \mathcal{L}_t \right)
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{u}_t\)</span> represents the velocity at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(v_t\)</span> represents the accumulated history of squared gradients at iteration <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(\beta\)</span> is a hyperparameter controlling the exponential decay rate for the velocity.</li>
<li><span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters controlling the exponential decay rates for the velocity and squared gradients, respectively.</li>
<li><span class="math inline">\(\eta\)</span> is the learning rate.</li>
<li><span class="math inline">\(\epsilon\)</span> is a small constant to prevent division by zero.</li>
</ul>
</section>
</section>
<section id="practical-considerations-and-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="practical-considerations-and-conclusion">Practical Considerations and Conclusion</h2>
<section id="choosing-the-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="choosing-the-optimizer">Choosing the Optimizer</h3>
<p>In practical applications, choosing the right optimizer is crucial for achieving good performance in training neural networks. While there are various optimization algorithms available, Adam optimizer is widely used due to its effectiveness in many scenarios. However, other variants such as Nesterov Accelerated Gradient Descent (NAG) can also be considered, especially when dealing with specific optimization challenges.</p>
</section>
<section id="learning-rate-schedules" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate-schedules">Learning Rate Schedules</h3>
<p>In addition to selecting the optimizer, tuning the learning rate schedule is another important aspect of training deep learning models. Proper adjustment of the learning rate can significantly impact the convergence and stability of the optimization process. Experimenting with different learning rate schedules and monitoring the training process can help determine the optimal settings for achieving desired performance.</p>
</section>
</section>
</section>
<section id="learning-rate-schedules-1" class="level1">
<h1>Learning Rate Schedules</h1>
<p>In deep learning, the choice of learning rate schedule plays a crucial role in optimizing neural network models. This section explores various learning rate schemes, including epoch-based and adaptive approaches, as well as cyclic learning rate schedules such as cyclical and cosine annealing.</p>
<section id="introduction-to-learning-rate-schedules" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-learning-rate-schedules">Introduction to Learning Rate Schedules</h2>
<p>In neural network training, the learning rate (<span class="math inline">\(\eta\)</span>) determines the step size during gradient descent optimization. Choosing an appropriate learning rate schedule can significantly impact the convergence and performance of the model. Different learning rate schedules adjust the learning rate over time to facilitate effective optimization.</p>
</section>
<section id="epoch-based-learning-rate-schemes" class="level2">
<h2 class="anchored" data-anchor-id="epoch-based-learning-rate-schemes">Epoch-Based Learning Rate Schemes</h2>
<p>Epoch-based learning rate schemes adjust the learning rate based on the number of training epochs. Common approaches include step decay and exponential decay.</p>
<section id="step-decay" class="level3">
<h3 class="anchored" data-anchor-id="step-decay">Step Decay</h3>
<p>In step decay, the learning rate is reduced by a factor (<span class="math inline">\(\gamma\)</span>) after a fixed number of epochs (<span class="math inline">\(\tau\)</span>). Mathematically, it can be expressed as:</p>
<p><span class="math display">\[ \eta_t = \eta_0 \times \gamma^{\lfloor \frac{t}{\tau} \rfloor} \]</span></p>
<p>where <span class="math inline">\(\eta_t\)</span> is the learning rate at iteration <span class="math inline">\(t\)</span>, <span class="math inline">\(\eta_0\)</span> is the initial learning rate, <span class="math inline">\(\gamma\)</span> is the decay factor, and <span class="math inline">\(\tau\)</span> is the step size.</p>
</section>
<section id="exponential-decay" class="level3">
<h3 class="anchored" data-anchor-id="exponential-decay">Exponential Decay</h3>
<p>Exponential decay reduces the learning rate exponentially over time. The learning rate at iteration <span class="math inline">\(t\)</span> is given by:</p>
<p><span class="math display">\[ \eta_t = \eta_0 \times e^{-\lambda t} \]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> controls the rate of decay.</p>
</section>
</section>
<section id="adaptive-learning-rate-schemes" class="level2">
<h2 class="anchored" data-anchor-id="adaptive-learning-rate-schemes">Adaptive Learning Rate Schemes</h2>
<p>Adaptive learning rate schemes dynamically adjust the learning rate based on past gradients or other parameters. Examples include Adagrad, RMSProp, ADA Delta, Adam, and Adamax.</p>
<section id="adagrad-1" class="level3">
<h3 class="anchored" data-anchor-id="adagrad-1">Adagrad</h3>
<p>Adagrad adapts the learning rate for each parameter based on the magnitude of its gradients. It scales the learning rate inversely proportional to the square root of the sum of squared gradients.</p>
<p><span class="math display">\[ \eta_t = \frac{\eta_0}{\sqrt{v_t + \epsilon}} \]</span></p>
<p>where <span class="math inline">\(v_t\)</span> represents the accumulated history of squared gradients at iteration <span class="math inline">\(t\)</span> and <span class="math inline">\(\epsilon\)</span> is a small constant to prevent division by zero.</p>
</section>
<section id="rmsprop-1" class="level3">
<h3 class="anchored" data-anchor-id="rmsprop-1">RMSProp</h3>
<p>RMSProp improves upon Adagrad by using a moving average of squared gradients for scaling the learning rate. It addresses the diminishing learning rate problem in Adagrad by using a decay rate <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[ v_t = \beta v_{t-1} + (1 - \beta) (\nabla \mathcal{L}_t)^2 \]</span></p>
<p>where <span class="math inline">\(\nabla \mathcal{L}_t\)</span> denotes the gradient of the loss function at iteration <span class="math inline">\(t\)</span>.</p>
</section>
<section id="ada-delta" class="level3">
<h3 class="anchored" data-anchor-id="ada-delta">ADA Delta</h3>
<p>ADA Delta further enhances RMSProp by replacing the learning rate with the root mean square (RMS) of parameter updates.</p>
<p><span class="math display">\[ \eta_t = \sqrt{\frac{v_{t-1} + \epsilon}{v_t + \epsilon}} \]</span></p>
</section>
<section id="adam-1" class="level3">
<h3 class="anchored" data-anchor-id="adam-1">Adam</h3>
<p>Adam combines the advantages of both RMSProp and momentum optimization. It maintains two moving averages for gradients and squared gradients.</p>
<p><span class="math display">\[ \mathbf{u}_t = \beta_1 \mathbf{u}_{t-1} + (1 - \beta_1) \nabla \mathcal{L}_t \]</span> <span class="math display">\[ v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla \mathcal{L}_t)^2 \]</span> <span class="math display">\[ \eta_t = \frac{\eta}{\sqrt{v_t + \epsilon}} \]</span></p>
<p>where <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are hyperparameters controlling the exponential decay rates.</p>
</section>
<section id="adamax" class="level3">
<h3 class="anchored" data-anchor-id="adamax">Adamax</h3>
<p>Adamax is a variant of Adam that replaces the <span class="math inline">\(L_2\)</span> norm with the <span class="math inline">\(L_{\infty}\)</span> norm.</p>
</section>
</section>
<section id="cyclic-learning-rate-schedules" class="level2">
<h2 class="anchored" data-anchor-id="cyclic-learning-rate-schedules">Cyclic Learning Rate Schedules</h2>
<p>Cyclic learning rate schedules alternate between increasing and decreasing the learning rate over a predefined range.</p>
<section id="triangular-schedule" class="level3">
<h3 class="anchored" data-anchor-id="triangular-schedule">Triangular Schedule</h3>
<p>The triangular schedule cyclically increases the learning rate from a minimum to maximum value and back. It helps escape saddle points by periodically increasing the learning rate.</p>
<p><span class="math display">\[ \eta_t = \eta_{\text{min}} + (\eta_{\text{max}} - \eta_{\text{min}}) \times \text{max}(0, 1 - |\frac{T}{\mu} - 2 \lfloor \frac{T}{\mu} \rfloor - 1|) \]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the period of the cycle.</p>
</section>
<section id="cosine-annealing" class="level3">
<h3 class="anchored" data-anchor-id="cosine-annealing">Cosine Annealing</h3>
<p>Cosine annealing smoothly decreases the learning rate using a cosine function. It converges faster compared to fixed learning rates.</p>
<p><span class="math display">\[ \eta_t = \eta_{\text{min}} + \frac{1}{2} (\eta_{\text{max}} - \eta_{\text{min}}) (1 + \cos(\frac{T}{T_{\text{max}}} \pi)) \]</span></p>
<p>where <span class="math inline">\(T\)</span> is the current epoch and <span class="math inline">\(T_{\text{max}}\)</span> is the restart interval.</p>
</section>
<section id="warm-restart" class="level3">
<h3 class="anchored" data-anchor-id="warm-restart">Warm Restart</h3>
<p>Warm restart involves quickly jumping from the minimum to maximum learning rate and then decaying. It is popular in Transformer architectures.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>