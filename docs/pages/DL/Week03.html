<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>week03 â€“ BS Degree Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-sans.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/DL/Week01_1.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../pages/DL/Week03.html">Week 3</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 12</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">NLP</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">LLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#feed-forward-neural-networks-and-back-propagation" id="toc-feed-forward-neural-networks-and-back-propagation" class="nav-link active" data-scroll-target="#feed-forward-neural-networks-and-back-propagation">Feed Forward Neural Networks and Back Propagation</a>
  <ul class="collapse">
  <li><a href="#components-of-a-feed-forward-neural-network" id="toc-components-of-a-feed-forward-neural-network" class="nav-link" data-scroll-target="#components-of-a-feed-forward-neural-network">Components of a Feed Forward Neural Network</a>
  <ul class="collapse">
  <li><a href="#input-layer" id="toc-input-layer" class="nav-link" data-scroll-target="#input-layer">Input Layer</a></li>
  <li><a href="#hidden-layers" id="toc-hidden-layers" class="nav-link" data-scroll-target="#hidden-layers">Hidden Layers</a></li>
  <li><a href="#output-layer" id="toc-output-layer" class="nav-link" data-scroll-target="#output-layer">Output Layer</a></li>
  </ul></li>
  <li><a href="#computing-pre-activation-and-activation" id="toc-computing-pre-activation-and-activation" class="nav-link" data-scroll-target="#computing-pre-activation-and-activation">Computing Pre-activation and Activation</a>
  <ul class="collapse">
  <li><a href="#pre-activation" id="toc-pre-activation" class="nav-link" data-scroll-target="#pre-activation">Pre-activation</a></li>
  <li><a href="#activation" id="toc-activation" class="nav-link" data-scroll-target="#activation">Activation</a></li>
  </ul></li>
  <li><a href="#output-activation-and-function-approximation" id="toc-output-activation-and-function-approximation" class="nav-link" data-scroll-target="#output-activation-and-function-approximation">Output Activation and Function Approximation</a>
  <ul class="collapse">
  <li><a href="#output-activation-function" id="toc-output-activation-function" class="nav-link" data-scroll-target="#output-activation-function">Output Activation Function</a></li>
  <li><a href="#function-approximation" id="toc-function-approximation" class="nav-link" data-scroll-target="#function-approximation">Function Approximation</a></li>
  </ul></li>
  <li><a href="#parameter-learning-and-loss-function" id="toc-parameter-learning-and-loss-function" class="nav-link" data-scroll-target="#parameter-learning-and-loss-function">Parameter Learning and Loss Function</a>
  <ul class="collapse">
  <li><a href="#parameter-optimization" id="toc-parameter-optimization" class="nav-link" data-scroll-target="#parameter-optimization">Parameter Optimization</a></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a></li>
  </ul></li>
  <li><a href="#back-propagation-algorithm" id="toc-back-propagation-algorithm" class="nav-link" data-scroll-target="#back-propagation-algorithm">Back Propagation Algorithm</a>
  <ul class="collapse">
  <li><a href="#forward-pass" id="toc-forward-pass" class="nav-link" data-scroll-target="#forward-pass">Forward Pass</a></li>
  <li><a href="#backward-pass" id="toc-backward-pass" class="nav-link" data-scroll-target="#backward-pass">Backward Pass</a></li>
  <li><a href="#update-rule" id="toc-update-rule" class="nav-link" data-scroll-target="#update-rule">Update Rule</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#intuition-in-feedforward-neural-networks" id="toc-intuition-in-feedforward-neural-networks" class="nav-link" data-scroll-target="#intuition-in-feedforward-neural-networks">Intuition in Feedforward Neural Networks</a>
  <ul class="collapse">
  <li><a href="#gradient-descent-revisited" id="toc-gradient-descent-revisited" class="nav-link" data-scroll-target="#gradient-descent-revisited">Gradient Descent Revisited</a></li>
  <li><a href="#transition-to-feedforward-neural-networks" id="toc-transition-to-feedforward-neural-networks" class="nav-link" data-scroll-target="#transition-to-feedforward-neural-networks">Transition to Feedforward Neural Networks</a></li>
  <li><a href="#parameter-representation" id="toc-parameter-representation" class="nav-link" data-scroll-target="#parameter-representation">Parameter Representation</a></li>
  <li><a href="#complexity-of-parameters" id="toc-complexity-of-parameters" class="nav-link" data-scroll-target="#complexity-of-parameters">Complexity of Parameters</a></li>
  <li><a href="#algorithm-adaptation" id="toc-algorithm-adaptation" class="nav-link" data-scroll-target="#algorithm-adaptation">Algorithm Adaptation</a></li>
  <li><a href="#challenges-in-parameter-learning" id="toc-challenges-in-parameter-learning" class="nav-link" data-scroll-target="#challenges-in-parameter-learning">Challenges in Parameter Learning</a></li>
  <li><a href="#choice-of-loss-function" id="toc-choice-of-loss-function" class="nav-link" data-scroll-target="#choice-of-loss-function">Choice of Loss Function</a></li>
  <li><a href="#efficient-computation-of-gradients" id="toc-efficient-computation-of-gradients" class="nav-link" data-scroll-target="#efficient-computation-of-gradients">Efficient Computation of Gradients</a></li>
  </ul></li>
  <li><a href="#output-and-loss-functions" id="toc-output-and-loss-functions" class="nav-link" data-scroll-target="#output-and-loss-functions">Output and Loss Functions</a>
  <ul class="collapse">
  <li><a href="#regression-problems" id="toc-regression-problems" class="nav-link" data-scroll-target="#regression-problems">Regression Problems</a>
  <ul class="collapse">
  <li><a href="#loss-function-mean-squared-error-mse" id="toc-loss-function-mean-squared-error-mse" class="nav-link" data-scroll-target="#loss-function-mean-squared-error-mse">Loss Function: Mean Squared Error (MSE)</a></li>
  <li><a href="#output-function-linear-activation" id="toc-output-function-linear-activation" class="nav-link" data-scroll-target="#output-function-linear-activation">Output Function: Linear Activation</a></li>
  </ul></li>
  <li><a href="#classification-problems" id="toc-classification-problems" class="nav-link" data-scroll-target="#classification-problems">Classification Problems</a>
  <ul class="collapse">
  <li><a href="#output-function-softmax-activation" id="toc-output-function-softmax-activation" class="nav-link" data-scroll-target="#output-function-softmax-activation">Output Function: Softmax Activation</a></li>
  <li><a href="#loss-function-cross-entropy" id="toc-loss-function-cross-entropy" class="nav-link" data-scroll-target="#loss-function-cross-entropy">Loss Function: Cross Entropy</a></li>
  </ul></li>
  <li><a href="#softmax-function" id="toc-softmax-function" class="nav-link" data-scroll-target="#softmax-function">Softmax Function</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss">Cross Entropy Loss</a></li>
  </ul></li>
  <li><a href="#understanding-back-propagation-algorithm" id="toc-understanding-back-propagation-algorithm" class="nav-link" data-scroll-target="#understanding-back-propagation-algorithm">Understanding Back Propagation Algorithm</a>
  <ul class="collapse">
  <li><a href="#derivatives-and-chain-rule" id="toc-derivatives-and-chain-rule" class="nav-link" data-scroll-target="#derivatives-and-chain-rule">Derivatives and Chain Rule</a>
  <ul class="collapse">
  <li><a href="#derivative-calculation" id="toc-derivative-calculation" class="nav-link" data-scroll-target="#derivative-calculation">Derivative Calculation</a></li>
  <li><a href="#challenges-in-deep-neural-networks" id="toc-challenges-in-deep-neural-networks" class="nav-link" data-scroll-target="#challenges-in-deep-neural-networks">Challenges in Deep Neural Networks</a></li>
  <li><a href="#leveraging-the-chain-rule" id="toc-leveraging-the-chain-rule" class="nav-link" data-scroll-target="#leveraging-the-chain-rule">Leveraging the Chain Rule</a></li>
  </ul></li>
  <li><a href="#chain-rule-intuition" id="toc-chain-rule-intuition" class="nav-link" data-scroll-target="#chain-rule-intuition">Chain Rule Intuition</a>
  <ul class="collapse">
  <li><a href="#step-by-step-derivative-calculation" id="toc-step-by-step-derivative-calculation" class="nav-link" data-scroll-target="#step-by-step-derivative-calculation">Step-by-Step Derivative Calculation</a></li>
  <li><a href="#reusability-of-computations" id="toc-reusability-of-computations" class="nav-link" data-scroll-target="#reusability-of-computations">Reusability of Computations</a></li>
  <li><a href="#generalization-across-layers" id="toc-generalization-across-layers" class="nav-link" data-scroll-target="#generalization-across-layers">Generalization Across Layers</a></li>
  </ul></li>
  <li><a href="#responsibilities-in-back-propagation" id="toc-responsibilities-in-back-propagation" class="nav-link" data-scroll-target="#responsibilities-in-back-propagation">Responsibilities in Back Propagation</a>
  <ul class="collapse">
  <li><a href="#error-propagation" id="toc-error-propagation" class="nav-link" data-scroll-target="#error-propagation">Error Propagation</a></li>
  <li><a href="#influence-of-weights-and-biases" id="toc-influence-of-weights-and-biases" class="nav-link" data-scroll-target="#influence-of-weights-and-biases">Influence of Weights and Biases</a></li>
  <li><a href="#derivatives-as-indicators-of-influence" id="toc-derivatives-as-indicators-of-influence" class="nav-link" data-scroll-target="#derivatives-as-indicators-of-influence">Derivatives as Indicators of Influence</a></li>
  </ul></li>
  <li><a href="#mathematical-realization" id="toc-mathematical-realization" class="nav-link" data-scroll-target="#mathematical-realization">Mathematical Realization</a>
  <ul class="collapse">
  <li><a href="#derivatives-and-responsibilities" id="toc-derivatives-and-responsibilities" class="nav-link" data-scroll-target="#derivatives-and-responsibilities">Derivatives and Responsibilities</a></li>
  <li><a href="#partial-derivatives" id="toc-partial-derivatives" class="nav-link" data-scroll-target="#partial-derivatives">Partial Derivatives</a></li>
  <li><a href="#objective-of-back-propagation" id="toc-objective-of-back-propagation" class="nav-link" data-scroll-target="#objective-of-back-propagation">Objective of Back Propagation</a></li>
  <li><a href="#emphasis-on-cross-entropy" id="toc-emphasis-on-cross-entropy" class="nav-link" data-scroll-target="#emphasis-on-cross-entropy">Emphasis on Cross Entropy</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#gradient-w.r.t-output-units" id="toc-gradient-w.r.t-output-units" class="nav-link" data-scroll-target="#gradient-w.r.t-output-units">Gradient w.r.t output units</a>
  <ul class="collapse">
  <li><a href="#talking-to-the-output-layer" id="toc-talking-to-the-output-layer" class="nav-link" data-scroll-target="#talking-to-the-output-layer">Talking to the Output Layer</a>
  <ul class="collapse">
  <li><a href="#goal" id="toc-goal" class="nav-link" data-scroll-target="#goal">Goal</a></li>
  <li><a href="#loss-function-1" id="toc-loss-function-1" class="nav-link" data-scroll-target="#loss-function-1">Loss Function</a></li>
  <li><a href="#derivative-calculation-1" id="toc-derivative-calculation-1" class="nav-link" data-scroll-target="#derivative-calculation-1">Derivative Calculation</a></li>
  <li><a href="#gradient-vector" id="toc-gradient-vector" class="nav-link" data-scroll-target="#gradient-vector">Gradient Vector</a></li>
  </ul></li>
  <li><a href="#talking-to-the-hidden-layers" id="toc-talking-to-the-hidden-layers" class="nav-link" data-scroll-target="#talking-to-the-hidden-layers">Talking to the Hidden Layers</a>
  <ul class="collapse">
  <li><a href="#objective" id="toc-objective" class="nav-link" data-scroll-target="#objective">Objective</a></li>
  <li><a href="#chain-rule-application" id="toc-chain-rule-application" class="nav-link" data-scroll-target="#chain-rule-application">Chain Rule Application</a></li>
  <li><a href="#pre-activation-to-activation" id="toc-pre-activation-to-activation" class="nav-link" data-scroll-target="#pre-activation-to-activation">Pre-Activation to Activation</a></li>
  <li><a href="#derivative-calculation-2" id="toc-derivative-calculation-2" class="nav-link" data-scroll-target="#derivative-calculation-2">Derivative Calculation</a></li>
  <li><a href="#gradient-flow" id="toc-gradient-flow" class="nav-link" data-scroll-target="#gradient-flow">Gradient Flow</a></li>
  </ul></li>
  <li><a href="#talking-to-the-weights" id="toc-talking-to-the-weights" class="nav-link" data-scroll-target="#talking-to-the-weights">Talking to the Weights</a>
  <ul class="collapse">
  <li><a href="#objective-1" id="toc-objective-1" class="nav-link" data-scroll-target="#objective-1">Objective</a></li>
  <li><a href="#chain-rule-application-1" id="toc-chain-rule-application-1" class="nav-link" data-scroll-target="#chain-rule-application-1">Chain Rule Application</a></li>
  <li><a href="#derivative-calculation-3" id="toc-derivative-calculation-3" class="nav-link" data-scroll-target="#derivative-calculation-3">Derivative Calculation</a></li>
  <li><a href="#weight-update" id="toc-weight-update" class="nav-link" data-scroll-target="#weight-update">Weight Update</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#computing-gradients-with-respect-to-hidden-units" id="toc-computing-gradients-with-respect-to-hidden-units" class="nav-link" data-scroll-target="#computing-gradients-with-respect-to-hidden-units">Computing Gradients with Respect to Hidden Units</a>
  <ul class="collapse">
  <li><a href="#introduction-to-hidden-units" id="toc-introduction-to-hidden-units" class="nav-link" data-scroll-target="#introduction-to-hidden-units">Introduction to Hidden Units</a></li>
  <li><a href="#chain-rule-for-gradient-computation" id="toc-chain-rule-for-gradient-computation" class="nav-link" data-scroll-target="#chain-rule-for-gradient-computation">Chain Rule for Gradient Computation</a></li>
  <li><a href="#deriving-the-formula" id="toc-deriving-the-formula" class="nav-link" data-scroll-target="#deriving-the-formula">Deriving the Formula</a></li>
  <li><a href="#computing-gradients-for-hidden-units" id="toc-computing-gradients-for-hidden-units" class="nav-link" data-scroll-target="#computing-gradients-for-hidden-units">Computing Gradients for Hidden Units</a></li>
  <li><a href="#generalizing-the-formula" id="toc-generalizing-the-formula" class="nav-link" data-scroll-target="#generalizing-the-formula">Generalizing the Formula</a></li>
  </ul></li>
  <li><a href="#derivatives-with-respect-to-parameters" id="toc-derivatives-with-respect-to-parameters" class="nav-link" data-scroll-target="#derivatives-with-respect-to-parameters">Derivatives with Respect to Parameters</a>
  <ul class="collapse">
  <li><a href="#computing-derivatives-of-loss-function" id="toc-computing-derivatives-of-loss-function" class="nav-link" data-scroll-target="#computing-derivatives-of-loss-function">Computing Derivatives of Loss Function</a>
  <ul class="collapse">
  <li><a href="#iterative-approach" id="toc-iterative-approach" class="nav-link" data-scroll-target="#iterative-approach">Iterative Approach</a></li>
  <li><a href="#derivative-with-respect-to-weight-matrix-element" id="toc-derivative-with-respect-to-weight-matrix-element" class="nav-link" data-scroll-target="#derivative-with-respect-to-weight-matrix-element">Derivative with Respect to Weight Matrix Element</a></li>
  <li><a href="#outer-product-representation" id="toc-outer-product-representation" class="nav-link" data-scroll-target="#outer-product-representation">Outer Product Representation</a></li>
  <li><a href="#efficient-computation" id="toc-efficient-computation" class="nav-link" data-scroll-target="#efficient-computation">Efficient Computation</a></li>
  </ul></li>
  <li><a href="#derivative-with-respect-to-bias" id="toc-derivative-with-respect-to-bias" class="nav-link" data-scroll-target="#derivative-with-respect-to-bias">Derivative with Respect to Bias</a>
  <ul class="collapse">
  <li><a href="#splitting-into-two-parts" id="toc-splitting-into-two-parts" class="nav-link" data-scroll-target="#splitting-into-two-parts">Splitting into Two Parts</a></li>
  <li><a href="#gradient-vector-1" id="toc-gradient-vector-1" class="nav-link" data-scroll-target="#gradient-vector-1">Gradient Vector</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#backpropagation-algorithm" id="toc-backpropagation-algorithm" class="nav-link" data-scroll-target="#backpropagation-algorithm">Backpropagation Algorithm</a>
  <ul class="collapse">
  <li><a href="#forward-propagation" id="toc-forward-propagation" class="nav-link" data-scroll-target="#forward-propagation">Forward Propagation</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#computation-of-pre-activations-and-activations" id="toc-computation-of-pre-activations-and-activations" class="nav-link" data-scroll-target="#computation-of-pre-activations-and-activations">Computation of Pre-activations and Activations</a></li>
  </ul></li>
  <li><a href="#loss-computation" id="toc-loss-computation" class="nav-link" data-scroll-target="#loss-computation">Loss Computation</a>
  <ul class="collapse">
  <li><a href="#loss-function-2" id="toc-loss-function-2" class="nav-link" data-scroll-target="#loss-function-2">Loss Function</a></li>
  <li><a href="#loss-computation-1" id="toc-loss-computation-1" class="nav-link" data-scroll-target="#loss-computation-1">Loss Computation</a></li>
  </ul></li>
  <li><a href="#backward-propagation" id="toc-backward-propagation" class="nav-link" data-scroll-target="#backward-propagation">Backward Propagation</a>
  <ul class="collapse">
  <li><a href="#overview-1" id="toc-overview-1" class="nav-link" data-scroll-target="#overview-1">Overview</a></li>
  <li><a href="#gradient-computation" id="toc-gradient-computation" class="nav-link" data-scroll-target="#gradient-computation">Gradient Computation</a></li>
  <li><a href="#chain-rule" id="toc-chain-rule" class="nav-link" data-scroll-target="#chain-rule">Chain Rule</a></li>
  <li><a href="#update-rule-1" id="toc-update-rule-1" class="nav-link" data-scroll-target="#update-rule-1">Update Rule</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a>
  <ul class="collapse">
  <li><a href="#points-to-remember" id="toc-points-to-remember" class="nav-link" data-scroll-target="#points-to-remember">Points to Remember</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<section id="feed-forward-neural-networks-and-back-propagation" class="level1">
<h1>Feed Forward Neural Networks and Back Propagation</h1>
<p>Feed forward neural networks are a fundamental architecture in the realm of artificial neural networks (ANNs), designed to process data in a forward direction, from input to output. This architecture is crucial for various machine learning tasks, including classification, regression, and pattern recognition. In this section, we delve into the intricacies of feed forward neural networks, including their components, computation processes, parameter learning techniques, and the crucial back propagation algorithm.</p>
<section id="components-of-a-feed-forward-neural-network" class="level2">
<h2 class="anchored" data-anchor-id="components-of-a-feed-forward-neural-network">Components of a Feed Forward Neural Network</h2>
<section id="input-layer" class="level3">
<h3 class="anchored" data-anchor-id="input-layer">Input Layer</h3>
<p>The input layer serves as the entry point for data into the neural network. It consists of an <span class="math inline">\(n\)</span>-dimensional vector, where each element represents a feature or attribute of the input data. Mathematically, the input layer can be represented as:</p>
<p><span class="math display">\[
\mathbf{x} \in \mathbb{R}^n
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{x}\)</span> denotes the input vector, and <span class="math inline">\(n\)</span> represents the number of input features.</p>
</section>
<section id="hidden-layers" class="level3">
<h3 class="anchored" data-anchor-id="hidden-layers">Hidden Layers</h3>
<p>Hidden layers form the core computational units of a feed forward neural network. These layers, typically denoted as <span class="math inline">\(L - 1\)</span>, are responsible for processing and transforming the input data through a series of non-linear transformations. Each hidden layer comprises a set of neurons, with each neuron connected to every neuron in the previous layer. Mathematically, the <span class="math inline">\(i\)</span>-th hidden layer can be represented as:</p>
<p><span class="math display">\[
\text{Layer } i : \mathbf{a}^{(i)} = \mathbf{g}^{(i)}(\mathbf{z}^{(i)})
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{a}^{(i)}\)</span> represents the activation vector of the <span class="math inline">\(i\)</span>-th layer, <span class="math inline">\(\mathbf{z}^{(i)}\)</span> denotes the pre-activation vector, and <span class="math inline">\(\mathbf{g}^{(i)}\)</span> represents the activation function applied element-wise to <span class="math inline">\(\mathbf{z}^{(i)}\)</span>.</p>
</section>
<section id="output-layer" class="level3">
<h3 class="anchored" data-anchor-id="output-layer">Output Layer</h3>
<p>The output layer is the final layer of the neural network, responsible for generating the networkâ€™s predictions or outputs. The number of neurons in the output layer depends on the nature of the task (e.g., binary classification, multi-class classification, regression). Mathematically, the output layer can be represented as:</p>
<p><span class="math display">\[
\text{Output Layer: } \mathbf{y} = \mathbf{f}(\mathbf{a}^{(L)})
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{y}\)</span> represents the output vector, and <span class="math inline">\(\mathbf{f}\)</span> denotes the output activation function.</p>
</section>
</section>
<section id="computing-pre-activation-and-activation" class="level2">
<h2 class="anchored" data-anchor-id="computing-pre-activation-and-activation">Computing Pre-activation and Activation</h2>
<p>The computation process in a feed forward neural network involves computing the pre-activation and activation values for each neuron in the network.</p>
<section id="pre-activation" class="level3">
<h3 class="anchored" data-anchor-id="pre-activation">Pre-activation</h3>
<p>Pre-activation refers to the linear transformation applied to the input data, followed by the addition of a bias term. Mathematically, the pre-activation for the <span class="math inline">\(i\)</span>-th layer can be expressed as:</p>
<p><span class="math display">\[
\mathbf{z}^{(i)} = \mathbf{W}^{(i)} \mathbf{a}^{(i-1)} + \mathbf{b}^{(i)}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{W}^{(i)}\)</span> represents the weight matrix connecting the <span class="math inline">\((i-1)\)</span>-th and <span class="math inline">\(i\)</span>-th layers, <span class="math inline">\(\mathbf{a}^{(i-1)}\)</span> denotes the activation vector of the previous layer, and <span class="math inline">\(\mathbf{b}^{(i)}\)</span> represents the bias vector for the <span class="math inline">\(i\)</span>-th layer.</p>
</section>
<section id="activation" class="level3">
<h3 class="anchored" data-anchor-id="activation">Activation</h3>
<p>Activation involves applying a non-linear function to the pre-activation values, introducing non-linearity into the networkâ€™s computations. Common activation functions include sigmoid, tanh, ReLU, and softmax. Mathematically, the activation for the <span class="math inline">\(i\)</span>-th layer can be expressed as:</p>
<p><span class="math display">\[
\mathbf{a}^{(i)} = \mathbf{g}^{(i)}(\mathbf{z}^{(i)})
\]</span></p>
<p>Where <span class="math inline">\(\mathbf{g}^{(i)}\)</span> represents the activation function applied element-wise to <span class="math inline">\(\mathbf{z}^{(i)}\)</span>.</p>
</section>
</section>
<section id="output-activation-and-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="output-activation-and-function-approximation">Output Activation and Function Approximation</h2>
<p>The output activation function plays a crucial role in determining the nature of the networkâ€™s predictions. Depending on the task at hand, different activation functions may be employed to ensure appropriate output scaling and behavior.</p>
<section id="output-activation-function" class="level3">
<h3 class="anchored" data-anchor-id="output-activation-function">Output Activation Function</h3>
<p>The output activation function governs the transformation of the final layerâ€™s pre-activation values into the networkâ€™s outputs. Common choices include softmax for multi-class classification tasks and linear functions for regression tasks. Mathematically, the output activation function can be expressed as:</p>
<p><span class="math display">\[
\mathbf{y} = \mathbf{f}(\mathbf{a}^{(L)})
\]</span></p>
<p>Where <span class="math inline">\(\mathbf{f}\)</span> denotes the output activation function.</p>
</section>
<section id="function-approximation" class="level3">
<h3 class="anchored" data-anchor-id="function-approximation">Function Approximation</h3>
<p>The feed forward neural network serves as a powerful function approximator, capable of capturing complex relationships between inputs and outputs. By iteratively adjusting the networkâ€™s parameters through training, the network learns to approximate the underlying function mapping inputs to outputs. Mathematically, the networkâ€™s output (<span class="math inline">\(\hat{\mathbf{y}}\)</span>) can be expressed as:</p>
<p><span class="math display">\[
\hat{\mathbf{y}} = \mathbf{f}(\mathbf{x}; \mathbf{W}, \mathbf{b})
\]</span></p>
<p>Where <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> represent the networkâ€™s parameters, and <span class="math inline">\(\mathbf{x}\)</span> denotes the input vector.</p>
</section>
</section>
<section id="parameter-learning-and-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="parameter-learning-and-loss-function">Parameter Learning and Loss Function</h2>
<p>The process of learning in a feed forward neural network involves optimizing the networkâ€™s parameters to minimize a predefined loss function. This optimization process typically utilizes gradient-based techniques, such as gradient descent, coupled with the back propagation algorithm.</p>
<section id="parameter-optimization" class="level3">
<h3 class="anchored" data-anchor-id="parameter-optimization">Parameter Optimization</h3>
<p>The parameters of a feed forward neural network, including weights (<span class="math inline">\(\mathbf{W}\)</span>) and biases (<span class="math inline">\(\mathbf{b}\)</span>), are learned through iterative optimization algorithms. The objective is to minimize the discrepancy between the networkâ€™s predictions and the true target values.</p>
</section>
<section id="loss-function" class="level3">
<h3 class="anchored" data-anchor-id="loss-function">Loss Function</h3>
<p>The loss function quantifies the disparity between the predicted outputs of the network and the actual target values. Common choices for the loss function include the squared error loss for regression tasks and the categorical cross-entropy loss for classification tasks.</p>
</section>
</section>
<section id="back-propagation-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="back-propagation-algorithm">Back Propagation Algorithm</h2>
<p>The back propagation algorithm serves as the cornerstone of parameter learning in feed forward neural networks. It facilitates the efficient computation of gradients with respect to network parameters, enabling gradient-based optimization techniques to adjust the parameters iteratively.</p>
<section id="forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass">Forward Pass</h3>
<p>During the forward pass, input data is propagated through the network, and pre-activation and activation values are computed for each layer.</p>
</section>
<section id="backward-pass" class="level3">
<h3 class="anchored" data-anchor-id="backward-pass">Backward Pass</h3>
<p>During the backward pass, gradients of the loss function with respect to network parameters are computed recursively using the chain rule of calculus. These gradients are then used to update the parameters in the direction that minimizes the loss function.</p>
</section>
<section id="update-rule" class="level3">
<h3 class="anchored" data-anchor-id="update-rule">Update Rule</h3>
<p>The update rule dictates how the network parameters are adjusted based on the computed gradients. Common choices include gradient descent, stochastic gradient descent, and variants such as Adam and RMSprop.</p>
</section>
</section>
</section>
<section id="intuition-in-feedforward-neural-networks" class="level1">
<h1>Intuition in Feedforward Neural Networks</h1>
<p>In this section, we delve into the intricacies of learning parameters for feedforward neural networks, elucidating the underlying principles and algorithms involved. We begin by revisiting the fundamental concepts of gradient descent and then extend our discussion to encompass the complexities introduced by the architecture of feedforward neural networks. Through meticulous examination, we elucidate the process of parameter learning, addressing key questions regarding the choice of loss function and efficient computation of partial derivatives.</p>
<section id="gradient-descent-revisited" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-revisited">Gradient Descent Revisited</h2>
<p>Gradient descent serves as a cornerstone algorithm in the realm of neural network training, facilitating the iterative adjustment of parameters to minimize the loss function. Mathematically, the process can be succinctly represented as follows:</p>
<p><span class="math display">\[
\theta_{t+1} = \theta_{t} - \alpha \cdot \nabla_{\theta} \mathcal{L}(\theta_t)
\]</span></p>
<p>Here, <span class="math inline">\(\theta\)</span> symbolizes the parameters of the neural network, <span class="math inline">\(\alpha\)</span> denotes the learning rate, and <span class="math inline">\(\nabla_{\theta} \mathcal{L}(\theta_t)\)</span> signifies the gradient of the loss function with respect to the parameters at iteration <span class="math inline">\(t\)</span>.</p>
</section>
<section id="transition-to-feedforward-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="transition-to-feedforward-neural-networks">Transition to Feedforward Neural Networks</h2>
<p>Moving beyond the realm of single neurons, we extend our focus to encompass feedforward neural networks, characterized by their layered architecture and interconnected nodes. In this context, the parameters of interest include weight matrices <span class="math inline">\(\mathbf{W}^{(i)}\)</span> and bias vectors <span class="math inline">\(\mathbf{b}^{(i)}\)</span> for each layer <span class="math inline">\(i\)</span>.</p>
</section>
<section id="parameter-representation" class="level2">
<h2 class="anchored" data-anchor-id="parameter-representation">Parameter Representation</h2>
<p>In contrast to the simplistic parameter representation in single neurons, where <span class="math inline">\(\theta\)</span> encapsulated only a handful of parameters, the scope expands significantly in feedforward neural networks. Now, <span class="math inline">\(\theta\)</span> encompasses a multitude of elements, incorporating the weights and biases across all layers of the network. Mathematically, we express this as:</p>
<p><span class="math display">\[
\theta = (\mathbf{W}^{(1)}, \mathbf{b}^{(1)}, \ldots, \mathbf{W}^{(L)}, \mathbf{b}^{(L)})
\]</span></p>
<p>Here, <span class="math inline">\(L\)</span> denotes the total number of layers in the network.</p>
</section>
<section id="complexity-of-parameters" class="level2">
<h2 class="anchored" data-anchor-id="complexity-of-parameters">Complexity of Parameters</h2>
<p>With the proliferation of layers and neurons in feedforward neural networks, the parameter space expands exponentially, posing computational challenges. Despite this complexity, the fundamental principles of gradient descent remain applicable, albeit with adaptations to accommodate the increased dimensionality of the parameter space.</p>
</section>
<section id="algorithm-adaptation" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-adaptation">Algorithm Adaptation</h2>
<p>The essence of gradient descent persists in the context of feedforward neural networks, albeit with modifications to accommodate the augmented parameter space. The core objective remains unchanged: iteratively updating parameters to minimize the loss function. Through meticulous computation of gradients, facilitated by techniques such as backpropagation, the network adjusts its parameters to optimize performance.</p>
</section>
<section id="challenges-in-parameter-learning" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-parameter-learning">Challenges in Parameter Learning</h2>
<p>The transition to feedforward neural networks introduces several challenges in the realm of parameter learning. Chief among these challenges is the computation of gradients, which necessitates the derivation of partial derivatives with respect to each parameter. In the context of complex architectures, this process can be computationally intensive and prone to errors.</p>
</section>
<section id="choice-of-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="choice-of-loss-function">Choice of Loss Function</h2>
<p>Central to the parameter learning process is the selection of an appropriate loss function, which quantifies the disparity between predicted and actual outputs. The choice of loss function is contingent upon the nature of the task at hand, with options ranging from mean squared error for regression tasks to cross-entropy loss for classification problems.</p>
</section>
<section id="efficient-computation-of-gradients" class="level2">
<h2 class="anchored" data-anchor-id="efficient-computation-of-gradients">Efficient Computation of Gradients</h2>
<p>Efficient computation of gradients is paramount in the realm of parameter learning, particularly in the context of feedforward neural networks with intricate architectures. Techniques such as vectorization and parallelization play a pivotal role in enhancing computational efficiency, enabling rapid convergence during training.</p>
</section>
</section>
<section id="output-and-loss-functions" class="level1">
<h1>Output and Loss Functions</h1>
<section id="regression-problems" class="level2">
<h2 class="anchored" data-anchor-id="regression-problems">Regression Problems</h2>
<p>Regression problems involve predicting continuous values based on input data. For instance, in predicting movie ratings, the goal is to estimate a numerical value (rating) for each input (movie).</p>
<section id="loss-function-mean-squared-error-mse" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-mean-squared-error-mse">Loss Function: Mean Squared Error (MSE)</h3>
<p>The mean squared error (MSE) is a common choice for regression tasks. It quantifies the average squared difference between the predicted and true values. Mathematically, MSE is expressed as:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{MSE}}(\theta) = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(N\)</span> is the number of training examples,</li>
<li><span class="math inline">\(\hat{y}_i\)</span> is the predicted value for the <span class="math inline">\(i\)</span>-th example,</li>
<li><span class="math inline">\(y_i\)</span> is the true value for the <span class="math inline">\(i\)</span>-th example.</li>
</ul>
</section>
<section id="output-function-linear-activation" class="level3">
<h3 class="anchored" data-anchor-id="output-function-linear-activation">Output Function: Linear Activation</h3>
<p>In regression tasks, a linear activation function is often employed at the output layer. This choice allows the model to produce unbounded output values, accommodating the natural range of the target variable. The output <span class="math inline">\(\hat{\mathbf{y}}\)</span> is computed as a linear transformation of the last hidden layer activations:</p>
<p><span class="math display">\[
\hat{\mathbf{y}} = \mathbf{W}^{(L)} \mathbf{a}^{(L-1)} + \mathbf{b}^{(L)}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{W}^{(L)}\)</span> and <span class="math inline">\(\mathbf{b}^{(L)}\)</span> are the weight matrix and bias vector of the output layer, respectively.</p>
</section>
</section>
<section id="classification-problems" class="level2">
<h2 class="anchored" data-anchor-id="classification-problems">Classification Problems</h2>
<p>Classification tasks involve assigning input data to discrete categories or classes. For example, in image classification, the aim is to categorize images into predefined classes.</p>
<section id="output-function-softmax-activation" class="level3">
<h3 class="anchored" data-anchor-id="output-function-softmax-activation">Output Function: Softmax Activation</h3>
<p>To obtain probabilities for each class in a classification problem, the softmax activation function is commonly used at the output layer. Softmax transforms the raw scores (logits) into a probability distribution over the classes. The softmax function is defined as:</p>
<p><span class="math display">\[
\text{Softmax}(\mathbf{z}^{(L)})_i = \frac{e^{z_i^{(L)}}}{\sum_{j=1}^{K} e^{z_j^{(L)}}}, \quad i = 1, 2, \ldots, K
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(K\)</span> is the number of classes,</li>
<li><span class="math inline">\(\mathbf{z}^{(L)}\)</span> is the pre-activation vector at the output layer.</li>
</ul>
</section>
<section id="loss-function-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-cross-entropy">Loss Function: Cross Entropy</h3>
<p>Cross entropy is a commonly used loss function for classification tasks. It measures the dissimilarity between the predicted probability distribution and the true distribution of class labels. The cross entropy loss is given by:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{CE}}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(N\)</span> is the number of training examples,</li>
<li><span class="math inline">\(K\)</span> is the number of classes,</li>
<li><span class="math inline">\(y_{i,k}\)</span> is the indicator function for the <span class="math inline">\(k\)</span>-th class of the <span class="math inline">\(i\)</span>-th example,</li>
<li><span class="math inline">\(\hat{y}_{i,k}\)</span> is the predicted probability of the <span class="math inline">\(k\)</span>-th class for the <span class="math inline">\(i\)</span>-th example.</li>
</ul>
<p>The cross entropy loss penalizes deviations between the predicted and true class probabilities, encouraging the model to assign high probabilities to the correct classes.</p>
</section>
</section>
<section id="softmax-function" class="level2">
<h2 class="anchored" data-anchor-id="softmax-function">Softmax Function</h2>
<p>The softmax function is employed to convert raw scores into probabilities for multiclass classification tasks. It ensures that the output represents a valid probability distribution over the classes, with values between 0 and 1 that sum up to 1. The softmax function is mathematically defined as:</p>
<p><span class="math display">\[
\text{Softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}, \quad i = 1, 2, \ldots, K
\]</span></p>
<p>where <span class="math inline">\(\mathbf{z}\)</span> is the input vector, and <span class="math inline">\(K\)</span> is the number of classes.</p>
</section>
<section id="cross-entropy-loss" class="level2">
<h2 class="anchored" data-anchor-id="cross-entropy-loss">Cross Entropy Loss</h2>
<p>Cross entropy loss quantifies the difference between the predicted and true distributions of class labels in classification tasks. It is a fundamental component in training neural networks for classification. The cross entropy loss is given by the formula:</p>
<p><span class="math display">\[
\mathcal{L}_{\text{CE}}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(N\)</span> is the number of training examples,</li>
<li><span class="math inline">\(K\)</span> is the number of classes,</li>
<li><span class="math inline">\(y_{i,k}\)</span> is the indicator function for the <span class="math inline">\(k\)</span>-th class of the <span class="math inline">\(i\)</span>-th example,</li>
<li><span class="math inline">\(\hat{y}_{i,k}\)</span> is the predicted probability of the <span class="math inline">\(k\)</span>-th class for the <span class="math inline">\(i\)</span>-th example.</li>
</ul>
<p>The cross entropy loss penalizes deviations between the predicted and true class probabilities. Minimizing this loss encourages the model to produce accurate probability distributions over the classes.</p>
</section>
</section>
<section id="understanding-back-propagation-algorithm" class="level1">
<h1>Understanding Back Propagation Algorithm</h1>
<section id="derivatives-and-chain-rule" class="level2">
<h2 class="anchored" data-anchor-id="derivatives-and-chain-rule">Derivatives and Chain Rule</h2>
<section id="derivative-calculation" class="level3">
<h3 class="anchored" data-anchor-id="derivative-calculation">Derivative Calculation</h3>
<p>In the context of neural networks, the derivative of the loss function with respect to the parameters (weights and biases) is essential for updating these parameters during the training process. This derivative quantifies how changes in the parameters affect the overall loss.</p>
</section>
<section id="challenges-in-deep-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-deep-neural-networks">Challenges in Deep Neural Networks</h3>
<p>Unlike simpler networks, deep neural networks entail a more complex structure with multiple layers and numerous parameters. Computing derivatives in such networks requires careful consideration and efficient algorithms.</p>
</section>
<section id="leveraging-the-chain-rule" class="level3">
<h3 class="anchored" data-anchor-id="leveraging-the-chain-rule">Leveraging the Chain Rule</h3>
<p>The chain rule of calculus provides a systematic approach to compute derivatives in composite functions. In the context of neural networks, it enables the computation of derivatives layer by layer, propagating the error from the output layer to the input layer.</p>
</section>
</section>
<section id="chain-rule-intuition" class="level2">
<h2 class="anchored" data-anchor-id="chain-rule-intuition">Chain Rule Intuition</h2>
<section id="step-by-step-derivative-calculation" class="level3">
<h3 class="anchored" data-anchor-id="step-by-step-derivative-calculation">Step-by-Step Derivative Calculation</h3>
<p>Visualizing the computation of derivatives as a chain of functions helps in understanding the iterative nature of back propagation. Each layer in the network contributes to the overall derivative calculation, with the chain rule facilitating this process.</p>
</section>
<section id="reusability-of-computations" class="level3">
<h3 class="anchored" data-anchor-id="reusability-of-computations">Reusability of Computations</h3>
<p>Once a segment of the derivative chain is computed, it can be reused for similar computations across different parameters. This reusability reduces redundancy and computational complexity, making the back propagation algorithm more efficient.</p>
</section>
<section id="generalization-across-layers" class="level3">
<h3 class="anchored" data-anchor-id="generalization-across-layers">Generalization Across Layers</h3>
<p>The principles of back propagation can be generalized across different layers and parameters in the network. By establishing a unified framework for derivative computation, the algorithm becomes more scalable and adaptable to varying network architectures.</p>
</section>
</section>
<section id="responsibilities-in-back-propagation" class="level2">
<h2 class="anchored" data-anchor-id="responsibilities-in-back-propagation">Responsibilities in Back Propagation</h2>
<section id="error-propagation" class="level3">
<h3 class="anchored" data-anchor-id="error-propagation">Error Propagation</h3>
<p>Back propagation involves tracing the propagation of errors from the output layer back to the input layer through the networkâ€™s connections. Each layer in the network bears responsibility for contributing to this error propagation process.</p>
</section>
<section id="influence-of-weights-and-biases" class="level3">
<h3 class="anchored" data-anchor-id="influence-of-weights-and-biases">Influence of Weights and Biases</h3>
<p>The weights and biases in the network play a crucial role in determining the magnitude of error propagation. Adjusting these parameters based on their influence on the loss function is key to optimizing the networkâ€™s performance.</p>
</section>
<section id="derivatives-as-indicators-of-influence" class="level3">
<h3 class="anchored" data-anchor-id="derivatives-as-indicators-of-influence">Derivatives as Indicators of Influence</h3>
<p>The derivatives of the loss function with respect to the parameters serve as indicators of their influence on the overall loss. Larger derivatives imply stronger influence, guiding the optimization process towards more effective parameter adjustments.</p>
</section>
</section>
<section id="mathematical-realization" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-realization">Mathematical Realization</h2>
<section id="derivatives-and-responsibilities" class="level3">
<h3 class="anchored" data-anchor-id="derivatives-and-responsibilities">Derivatives and Responsibilities</h3>
<p>Mathematically, derivatives quantify the sensitivity of the loss function to changes in the parameters. By computing these derivatives, the algorithm assigns responsibilities to each parameter based on its impact on the overall loss.</p>
</section>
<section id="partial-derivatives" class="level3">
<h3 class="anchored" data-anchor-id="partial-derivatives">Partial Derivatives</h3>
<p>Partial derivatives measure how the loss function changes with infinitesimal adjustments to individual parameters. This information guides the gradient-based optimization process, enabling efficient parameter updates.</p>
</section>
<section id="objective-of-back-propagation" class="level3">
<h3 class="anchored" data-anchor-id="objective-of-back-propagation">Objective of Back Propagation</h3>
<p>The primary objective of back propagation is to compute gradients with respect to various components of the network, including output, hidden units, weights, and biases. These gradients drive the optimization process towards minimizing the loss function.</p>
</section>
<section id="emphasis-on-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="emphasis-on-cross-entropy">Emphasis on Cross Entropy</h3>
<p>In classification problems, where the networkâ€™s output is represented using softmax activation, cross-entropy loss is commonly used. Back propagation algorithms are tailored to handle such loss functions efficiently, facilitating effective training of classification models.</p>
</section>
</section>
</section>
<section id="gradient-w.r.t-output-units" class="level1">
<h1>Gradient w.r.t output units</h1>
<section id="talking-to-the-output-layer" class="level2">
<h2 class="anchored" data-anchor-id="talking-to-the-output-layer">Talking to the Output Layer</h2>
<section id="goal" class="level3">
<h3 class="anchored" data-anchor-id="goal">Goal</h3>
<p>The primary objective in back propagation is to compute the derivative of the loss function with respect to the output layer activations. Letâ€™s denote the output vector as <span class="math inline">\(\mathbf{y}\)</span>, representing the networkâ€™s predictions or outputs.</p>
</section>
<section id="loss-function-1" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-1">Loss Function</h3>
<p>The loss function, denoted as <span class="math inline">\(\mathcal{L}(\theta_t)\)</span>, measures the discrepancy between the predicted output <span class="math inline">\(\hat{\mathbf{y}}\)</span> and the true labels <span class="math inline">\(\mathbf{y}\)</span>. It is often defined as the negative logarithm of the predicted probability of the true class.</p>
<p><span class="math display">\[
\mathcal{L}(\theta_t) = -\log(\hat{y}_l)
\]</span></p>
<p>where <span class="math inline">\(l\)</span> is the true class label.</p>
</section>
<section id="derivative-calculation-1" class="level3">
<h3 class="anchored" data-anchor-id="derivative-calculation-1">Derivative Calculation</h3>
<p>We aim to compute the derivative of the loss function with respect to each output neuron activation. This involves determining how a change in each output activation affects the overall loss.</p>
<p>The derivative can be expressed as follows:</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}(\theta_t)}{\partial a^{(L)}_i} =
\begin{cases}
-\frac{1}{\hat{y}_l} &amp; \text{if } i = l \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(a^{(L)}_i\)</span> represents the <span class="math inline">\(i\)</span>-th output neuron activation, and <span class="math inline">\(\hat{y}_l\)</span> is the predicted probability corresponding to the true class label.</p>
</section>
<section id="gradient-vector" class="level3">
<h3 class="anchored" data-anchor-id="gradient-vector">Gradient Vector</h3>
<p>The gradient of the loss function with respect to the output layer, denoted as <span class="math inline">\(\nabla_{\mathbf{y}} \mathcal{L}(\theta_t)\)</span>, is a vector containing the partial derivatives of the loss function with respect to each output neuron activation. It can be represented as:</p>
<p><span class="math display">\[
\nabla_{\mathbf{y}} \mathcal{L}(\theta_t) = \begin{bmatrix}
-\frac{1}{\hat{y}_1} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; -\frac{1}{\hat{y}_2} &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; -\frac{1}{\hat{y}_k}
\end{bmatrix}
\]</span></p>
<p>This gradient vector provides insights into how changes in the output layer activations affect the loss function.</p>
</section>
</section>
<section id="talking-to-the-hidden-layers" class="level2">
<h2 class="anchored" data-anchor-id="talking-to-the-hidden-layers">Talking to the Hidden Layers</h2>
<section id="objective" class="level3">
<h3 class="anchored" data-anchor-id="objective">Objective</h3>
<p>After understanding the derivatives at the output layer, the next step is to compute the derivatives with respect to the pre-activation values of the hidden layers. This involves understanding how changes in the pre-activations affect the output activations and, consequently, the loss function.</p>
</section>
<section id="chain-rule-application" class="level3">
<h3 class="anchored" data-anchor-id="chain-rule-application">Chain Rule Application</h3>
<p>To compute the derivative of the loss function with respect to the pre-activation values of the hidden layers, we apply the chain rule. This breaks down the computation into two steps:</p>
<ol type="1">
<li>Derivative of the loss function with respect to the output activations.</li>
<li>Derivative of the output activations with respect to the pre-activation values.</li>
</ol>
</section>
<section id="pre-activation-to-activation" class="level3">
<h3 class="anchored" data-anchor-id="pre-activation-to-activation">Pre-Activation to Activation</h3>
<p>The pre-activation values of the hidden layers are passed through an activation function to obtain the output activations. Mathematically, this can be expressed as:</p>
<p><span class="math display">\[
\mathbf{a}^{(i)} = \mathbf{g}^{(i)}(\mathbf{z}^{(i)})
\]</span></p>
<p>where <span class="math inline">\(\mathbf{z}^{(i)}\)</span> represents the pre-activation vector for the <span class="math inline">\(i\)</span>-th layer, and <span class="math inline">\(\mathbf{g}^{(i)}(\cdot)\)</span> is the activation function applied element-wise.</p>
</section>
<section id="derivative-calculation-2" class="level3">
<h3 class="anchored" data-anchor-id="derivative-calculation-2">Derivative Calculation</h3>
<p>The derivative of the output activations with respect to the pre-activation values depends on the choice of activation function. For commonly used activation functions like sigmoid, tanh, and ReLU, the derivatives can be computed analytically.</p>
</section>
<section id="gradient-flow" class="level3">
<h3 class="anchored" data-anchor-id="gradient-flow">Gradient Flow</h3>
<p>Understanding the gradient flow from the output layer to the hidden layers is crucial for parameter updates during training. The gradients propagate backward through the network, allowing for efficient computation of parameter updates.</p>
</section>
</section>
<section id="talking-to-the-weights" class="level2">
<h2 class="anchored" data-anchor-id="talking-to-the-weights">Talking to the Weights</h2>
<section id="objective-1" class="level3">
<h3 class="anchored" data-anchor-id="objective-1">Objective</h3>
<p>Once the derivatives with respect to the pre-activation values are computed, the next step is to calculate the derivatives with respect to the weights connecting the neurons. This step enables us to understand how changes in the weights influence the loss function.</p>
</section>
<section id="chain-rule-application-1" class="level3">
<h3 class="anchored" data-anchor-id="chain-rule-application-1">Chain Rule Application</h3>
<p>Similar to the computation at the hidden layers, we apply the chain rule to compute the derivatives of the loss function with respect to the weights. This involves breaking down the computation into two parts:</p>
<ol type="1">
<li>Derivative of the loss function with respect to the output activations.</li>
<li>Derivative of the output activations with respect to the pre-activation values.</li>
</ol>
</section>
<section id="derivative-calculation-3" class="level3">
<h3 class="anchored" data-anchor-id="derivative-calculation-3">Derivative Calculation</h3>
<p>The derivative of the pre-activation values with respect to the weights connecting the neurons can be straightforwardly calculated using the input vector, output activations, and the derivative of the activation function.</p>
</section>
<section id="weight-update" class="level3">
<h3 class="anchored" data-anchor-id="weight-update">Weight Update</h3>
<p>Once the derivatives with respect to the weights are computed, they are used to update the weights through optimization algorithms like gradient descent. By iteratively updating the weights based on the computed gradients, the network learns to minimize the loss function and improve its performance on the given task.</p>
</section>
</section>
</section>
<section id="computing-gradients-with-respect-to-hidden-units" class="level1">
<h1>Computing Gradients with Respect to Hidden Units</h1>
<section id="introduction-to-hidden-units" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-hidden-units">Introduction to Hidden Units</h2>
<p>Hidden units, also known as hidden layers, are intermediary layers in neural networks responsible for capturing complex patterns in the input data. These layers play a crucial role in the networkâ€™s ability to learn and generalize from the training data.</p>
</section>
<section id="chain-rule-for-gradient-computation" class="level2">
<h2 class="anchored" data-anchor-id="chain-rule-for-gradient-computation">Chain Rule for Gradient Computation</h2>
<p>The chain rule of calculus is a fundamental concept used extensively in computing derivatives of composite functions. In the context of neural networks, where the activation of each layer depends on the activations of the previous layers, the chain rule becomes essential for gradient computation.</p>
<p>Mathematically, let <span class="math inline">\(P(Z)\)</span> be a function dependent on intermediate functions <span class="math inline">\(Q_1(Z), Q_2(Z),\)</span> etc., and <span class="math inline">\(P\)</span> being a function of <span class="math inline">\(Z\)</span>. The derivative of <span class="math inline">\(P\)</span> with respect to <span class="math inline">\(Z\)</span> is computed as follows:</p>
<p><span class="math display">\[
\frac{dP}{dZ} = \sum_{i=1}^{m} \frac{dP}{dQ_i} \cdot \frac{dQ_i}{dZ}
\]</span></p>
<p>Here, we sum over all paths from <span class="math inline">\(Z\)</span> to <span class="math inline">\(P\)</span>, multiplying the derivatives along each path.</p>
</section>
<section id="deriving-the-formula" class="level2">
<h2 class="anchored" data-anchor-id="deriving-the-formula">Deriving the Formula</h2>
<p>To compute gradients for hidden units, we apply the chain rule to derive a generic formula. Consider a specific hidden unit <span class="math inline">\(H_{ij}\)</span>, where <span class="math inline">\(i\)</span> denotes the layer number and <span class="math inline">\(j\)</span> represents the neuron number within that layer.</p>
<p>We aim to compute the derivative of the loss function with respect to <span class="math inline">\(H_{ij}\)</span>. This involves summing over all paths from <span class="math inline">\(H_{ij}\)</span> to the loss function, considering each pathâ€™s contribution via the chain rule.</p>
</section>
<section id="computing-gradients-for-hidden-units" class="level2">
<h2 class="anchored" data-anchor-id="computing-gradients-for-hidden-units">Computing Gradients for Hidden Units</h2>
<p>The derivative of the loss function with respect to <span class="math inline">\(H_{ij}\)</span> can be expressed as a dot product between two vectors:</p>
<p><span class="math display">\[
\frac{d\mathcal{L}}{dH_{ij}} = \mathbf{W}^{(i+1)}_j \cdot \frac{d\mathcal{L}}{d\mathbf{a}^{(i+1)}}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{W}^{(i+1)}_j\)</span> represents the <span class="math inline">\(j\)</span>-th column of the weight matrix connecting the <span class="math inline">\((i+1)\)</span>-th and <span class="math inline">\(i\)</span>-th layers, and <span class="math inline">\(\frac{d\mathcal{L}}{d\mathbf{a}^{(i+1)}}\)</span> denotes the gradient of the loss function with respect to the activations in the next layer.</p>
<p>This computation involves the element-wise multiplication of the weight vector and the gradient vector.</p>
</section>
<section id="generalizing-the-formula" class="level2">
<h2 class="anchored" data-anchor-id="generalizing-the-formula">Generalizing the Formula</h2>
<p>We generalize the formula to compute gradients for any hidden layer <span class="math inline">\(H_i\)</span> with multiple units. The derivative of the loss function with respect to <span class="math inline">\(H_i\)</span> is given by:</p>
<p><span class="math display">\[
\frac{d\mathcal{L}}{d\mathbf{H}_i} = \mathbf{W}^{(i+1)T} \cdot \frac{d\mathcal{L}}{d\mathbf{a}^{(i+1)}}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{W}^{(i+1)T}\)</span> denotes the transpose of the weight matrix connecting the <span class="math inline">\((i+1)\)</span>-th and <span class="math inline">\(i\)</span>-th layers, and <span class="math inline">\(\frac{d\mathcal{L}}{d\mathbf{a}^{(i+1)}}\)</span> represents the gradient of the loss function with respect to the activations in the next layer.</p>
<p>This formulation enables efficient computation of gradients for hidden units across all layers of the neural network.</p>
</section>
</section>
<section id="derivatives-with-respect-to-parameters" class="level1">
<h1>Derivatives with Respect to Parameters</h1>
<section id="computing-derivatives-of-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="computing-derivatives-of-loss-function">Computing Derivatives of Loss Function</h2>
<section id="iterative-approach" class="level3">
<h3 class="anchored" data-anchor-id="iterative-approach">Iterative Approach</h3>
<p>Rather than computing the derivatives of the loss function with respect to all parameters simultaneously, we adopt an iterative approach. This involves focusing on one parameter at a time, specifically one element of the weight matrix or one element of the bias vector.</p>
</section>
<section id="derivative-with-respect-to-weight-matrix-element" class="level3">
<h3 class="anchored" data-anchor-id="derivative-with-respect-to-weight-matrix-element">Derivative with Respect to Weight Matrix Element</h3>
<p>Consider the derivative of the loss function with respect to one element of the weight matrix, <span class="math inline">\(w_{ij}^{(k)}\)</span>, connecting the <span class="math inline">\((k-1)\)</span>-th and <span class="math inline">\(k\)</span>-th layers. This derivative is obtained iteratively.</p>
<section id="derivative-with-respect-to-activation" class="level4">
<h4 class="anchored" data-anchor-id="derivative-with-respect-to-activation">Derivative with Respect to Activation</h4>
<p>First, compute the derivative of the loss function with respect to the corresponding activation, <span class="math inline">\(a_{i}^{(k)}\)</span>, using chain rule.</p>
</section>
<section id="derivative-of-activation-with-respect-to-weight" class="level4">
<h4 class="anchored" data-anchor-id="derivative-of-activation-with-respect-to-weight">Derivative of Activation with Respect to Weight</h4>
<p>Next, compute the derivative of the activation with respect to <span class="math inline">\(w_{ij}^{(k)}\)</span>, denoted as <span class="math inline">\(\frac{\partial a_{i}^{(k)}}{\partial w_{ij}^{(k)}}\)</span>.</p>
<section id="mathematical-formulation" class="level5">
<h5 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h5>
<p>Mathematically, this derivative equals the activation of the preceding layer at index <span class="math inline">\(i\)</span>, denoted as <span class="math inline">\(h_{ij}^{(k-1)}\)</span>.</p>
<p><span class="math display">\[\frac{\partial a_{i}^{(k)}}{\partial w_{ij}^{(k)}} = h_{ij}^{(k-1)}\]</span></p>
</section>
</section>
</section>
<section id="outer-product-representation" class="level3">
<h3 class="anchored" data-anchor-id="outer-product-representation">Outer Product Representation</h3>
<p>The derivative of the loss function with respect to <span class="math inline">\(w_{ij}^{(k)}\)</span> can be expressed as the outer product of two vectors: the derivative of the loss function with respect to the activations (<span class="math inline">\(\mathbf{a}^{(k)}\)</span>) and the activations of the preceding layer (<span class="math inline">\(\mathbf{h}^{(k-1)}\)</span>).</p>
<section id="mathematical-representation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-representation">Mathematical Representation</h4>
<p><span class="math display">\[\frac{\partial \mathcal{L}(\theta_t)}{\partial w_{ij}^{(k)}} = \frac{\partial \mathcal{L}(\theta_t)}{\partial \mathbf{a}^{(k)}} \otimes \mathbf{h}^{(k-1)}\]</span></p>
<p>where <span class="math inline">\(\otimes\)</span> represents the outer product operation.</p>
</section>
</section>
<section id="efficient-computation" class="level3">
<h3 class="anchored" data-anchor-id="efficient-computation">Efficient Computation</h3>
<p>Both the quantities involved in the derivative computation can be efficiently computed during the forward pass of the neural network, requiring no additional computations during the backward pass.</p>
</section>
</section>
<section id="derivative-with-respect-to-bias" class="level2">
<h2 class="anchored" data-anchor-id="derivative-with-respect-to-bias">Derivative with Respect to Bias</h2>
<p>Similar to the approach for weight matrices, the derivative of the loss function with respect to the bias vector (<span class="math inline">\(\mathbf{b}^{(k)}\)</span>) is computed iteratively.</p>
<section id="splitting-into-two-parts" class="level3">
<h3 class="anchored" data-anchor-id="splitting-into-two-parts">Splitting into Two Parts</h3>
<section id="derivative-with-respect-to-activation-1" class="level4">
<h4 class="anchored" data-anchor-id="derivative-with-respect-to-activation-1">Derivative with Respect to Activation</h4>
<p>First, compute the derivative of the loss function with respect to the activations (<span class="math inline">\(\mathbf{a}^{(k)}\)</span>) using chain rule.</p>
</section>
<section id="derivative-of-activation-with-respect-to-bias" class="level4">
<h4 class="anchored" data-anchor-id="derivative-of-activation-with-respect-to-bias">Derivative of Activation with Respect to Bias</h4>
<p>Next, compute the derivative of the activation with respect to the bias vector, denoted as <span class="math inline">\(\frac{\partial a_{i}^{(k)}}{\partial b_{i}^{(k)}}\)</span>.</p>
<section id="mathematical-formulation-1" class="level5">
<h5 class="anchored" data-anchor-id="mathematical-formulation-1">Mathematical Formulation</h5>
<p>Mathematically, this derivative is simply 1, as the bias term directly contributes to the activation.</p>
<p><span class="math display">\[\frac{\partial a_{i}^{(k)}}{\partial b_{i}^{(k)}} = 1\]</span></p>
</section>
</section>
</section>
<section id="gradient-vector-1" class="level3">
<h3 class="anchored" data-anchor-id="gradient-vector-1">Gradient Vector</h3>
<p>The derivative of the loss function with respect to the bias vector (<span class="math inline">\(\mathbf{b}^{(k)}\)</span>) is obtained by collecting all the partial derivatives, representing the gradient of the loss function with respect to the activations.</p>
<section id="mathematical-representation-1" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-representation-1">Mathematical Representation</h4>
<p><span class="math display">\[\frac{\partial \mathcal{L}(\theta_t)}{\partial \mathbf{b}^{(k)}} = \frac{\partial \mathcal{L}(\theta_t)}{\partial \mathbf{a}^{(k)}}\]</span></p>
</section>
</section>
</section>
</section>
<section id="backpropagation-algorithm" class="level1">
<h1>Backpropagation Algorithm</h1>
<section id="forward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="forward-propagation">Forward Propagation</h2>
<section id="overview" class="level3">
<h3 class="anchored" data-anchor-id="overview">Overview</h3>
<p>Forward propagation refers to the process of computing the networkâ€™s output given an input. It involves passing the input data through the network layers, computing pre-activations and activations, and finally obtaining the networkâ€™s predictions.</p>
</section>
<section id="computation-of-pre-activations-and-activations" class="level3">
<h3 class="anchored" data-anchor-id="computation-of-pre-activations-and-activations">Computation of Pre-activations and Activations</h3>
<p>For each layer <span class="math inline">\(i\)</span> in the network, forward propagation involves the following steps:</p>
<ol type="1">
<li><p><strong>Pre-activation</strong>: Compute the pre-activation vector <span class="math inline">\(\mathbf{z}^{(i)}\)</span> using the formula: <span class="math display">\[\mathbf{z}^{(i)} = \mathbf{W}^{(i)} \mathbf{a}^{(i-1)} + \mathbf{b}^{(i)}\]</span> Here, <span class="math inline">\(\mathbf{W}^{(i)}\)</span> is the weight matrix connecting the <span class="math inline">\((i-1)\)</span>-th and <span class="math inline">\(i\)</span>-th layers, <span class="math inline">\(\mathbf{a}^{(i-1)}\)</span> is the activation vector from the previous layer, and <span class="math inline">\(\mathbf{b}^{(i)}\)</span> is the bias vector for the <span class="math inline">\(i\)</span>-th layer.</p></li>
<li><p><strong>Activation</strong>: Apply the activation function <span class="math inline">\(\mathbf{g}^{(i)}(\cdot)\)</span> element-wise to the pre-activation vector <span class="math inline">\(\mathbf{z}^{(i)}\)</span> to obtain the activation vector <span class="math inline">\(\mathbf{a}^{(i)}\)</span>: <span class="math display">\[\mathbf{a}^{(i)} = \mathbf{g}^{(i)}(\mathbf{z}^{(i)})\]</span></p></li>
<li><p><strong>Output Activation</strong>: For the output layer, apply a specific output activation function <span class="math inline">\(\mathbf{f}(\cdot)\)</span> to obtain the final output <span class="math inline">\(\hat{\mathbf{y}}\)</span>: <span class="math display">\[\hat{\mathbf{y}} = \mathbf{f}(\mathbf{z}^{(L)})\]</span></p></li>
</ol>
</section>
</section>
<section id="loss-computation" class="level2">
<h2 class="anchored" data-anchor-id="loss-computation">Loss Computation</h2>
<p>After forward propagation, the next step is to compute the loss function, which measures the difference between the predicted output <span class="math inline">\(\hat{\mathbf{y}}\)</span> and the true output <span class="math inline">\(\mathbf{y}\)</span>.</p>
<section id="loss-function-2" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-2">Loss Function</h3>
<p>The loss function <span class="math inline">\(\mathcal{L}(\theta_t)\)</span> is a measure of the error between the predicted and true outputs. It depends on the specific task and can be chosen based on the problem domain. Common loss functions include mean squared error (MSE), cross-entropy loss, and hinge loss.</p>
</section>
<section id="loss-computation-1" class="level3">
<h3 class="anchored" data-anchor-id="loss-computation-1">Loss Computation</h3>
<p>Given the predicted output <span class="math inline">\(\hat{\mathbf{y}}\)</span> and the true output <span class="math inline">\(\mathbf{y}\)</span>, the loss function is computed using the following formula: <span class="math display">\[\mathcal{L}(\theta_t) = \text{Loss}(\hat{\mathbf{y}}, \mathbf{y})\]</span></p>
</section>
</section>
<section id="backward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="backward-propagation">Backward Propagation</h2>
<section id="overview-1" class="level3">
<h3 class="anchored" data-anchor-id="overview-1">Overview</h3>
<p>Backward propagation, also known as backpropagation, is the process of computing gradients of the loss function with respect to the network parameters. These gradients are then used to update the parameters in order to minimize the loss.</p>
</section>
<section id="gradient-computation" class="level3">
<h3 class="anchored" data-anchor-id="gradient-computation">Gradient Computation</h3>
<p>For each layer <span class="math inline">\(i\)</span> in the network, backward propagation involves the following steps:</p>
<ol type="1">
<li><p><strong>Gradient of Loss Function with Respect to Output Layer</strong>: Compute the gradient of the loss function with respect to the output layer activations <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial \mathbf{a}^{(L)}}\)</span>.</p></li>
<li><p><strong>Gradient of Loss Function with Respect to Weights</strong>: Use the chain rule to compute the gradient of the loss function with respect to the weights <span class="math inline">\(\mathbf{W}^{(i)}\)</span> for each layer <span class="math inline">\(i\)</span>: <span class="math display">\[\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(i)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(i)}} \cdot \frac{\partial \mathbf{z}^{(i)}}{\partial \mathbf{W}^{(i)}}\]</span></p></li>
<li><p><strong>Gradient of Loss Function with Respect to Biases</strong>: Similarly, compute the gradient of the loss function with respect to the biases <span class="math inline">\(\mathbf{b}^{(i)}\)</span> for each layer <span class="math inline">\(i\)</span>: <span class="math display">\[\frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(i)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{z}^{(i)}} \cdot \frac{\partial \mathbf{z}^{(i)}}{\partial \mathbf{b}^{(i)}}\]</span></p></li>
</ol>
</section>
<section id="chain-rule" class="level3">
<h3 class="anchored" data-anchor-id="chain-rule">Chain Rule</h3>
<p>The chain rule is used to compute the gradients of the loss function with respect to the weights and biases. It allows us to decompose the overall gradient into smaller gradients that can be computed efficiently.</p>
</section>
<section id="update-rule-1" class="level3">
<h3 class="anchored" data-anchor-id="update-rule-1">Update Rule</h3>
<p>Once the gradients have been computed, they are used to update the network parameters using an optimization algorithm such as gradient descent. The update rule for the weights is given by: <span class="math display">\[\mathbf{W}^{(i)}_{\text{new}} = \mathbf{W}^{(i)}_{\text{old}} - \alpha \frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(i)}}\]</span> Where <span class="math inline">\(\alpha\)</span> is the learning rate, controlling the size of the updates.</p>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In conclusion, understanding the backpropagation algorithm is crucial for grasping the fundamentals of training neural networks. Backpropagation allows us to efficiently compute gradients of the loss function with respect to the network parameters, enabling iterative updates that minimize the loss and improve the modelâ€™s performance. By decomposing the gradient computation using the chain rule and updating the parameters using optimization algorithms like gradient descent, we can effectively train complex neural networks to solve a wide range of tasks.</p>
<section id="points-to-remember" class="level2">
<h2 class="anchored" data-anchor-id="points-to-remember">Points to Remember</h2>
<ol type="1">
<li><strong>Forward Propagation:</strong>
<ul>
<li>Forward propagation computes the networkâ€™s output given an input by passing it through the layers and applying activation functions.</li>
<li>Pre-activations are computed using weight matrices, activation vectors, and biases, followed by activation function application.</li>
<li>Output activation function transforms the final pre-activation into the networkâ€™s prediction.</li>
</ul></li>
<li><strong>Loss Computation:</strong>
<ul>
<li>Loss function measures the error between predicted and true outputs and guides the training process.</li>
<li>Common loss functions include mean squared error, cross-entropy loss, and hinge loss.</li>
<li>Loss computation involves comparing predicted and true outputs using the chosen loss function.</li>
</ul></li>
<li><strong>Backward Propagation:</strong>
<ul>
<li>Backward propagation computes gradients of the loss function with respect to network parameters.</li>
<li>Gradient computation involves the chain rule to decompose gradients efficiently.</li>
<li>Gradients are used to update weights and biases, facilitating model improvement over iterations.</li>
</ul></li>
<li><strong>Chain Rule:</strong>
<ul>
<li>The chain rule allows the decomposition of complex gradients, simplifying the computation of gradients with respect to weights and biases.</li>
</ul></li>
<li><strong>Update Rule:</strong>
<ul>
<li>Update rule adjusts network parameters using gradients and a learning rate.</li>
<li>Learning rate controls the size of parameter updates, influencing the convergence and stability of the training process.</li>
</ul></li>
<li><strong>Optimization Algorithms:</strong>
<ul>
<li>Gradient descent is a common optimization algorithm used in conjunction with backpropagation for training neural networks.</li>
<li>Other optimization algorithms like Adam, RMSprop, and SGD with momentum offer variations for improved convergence and performance.</li>
</ul></li>
<li><strong>Training Process:</strong>
<ul>
<li>Training neural networks involves iterative forward and backward passes, adjusting parameters to minimize the loss function.</li>
<li>Effective training requires careful selection of hyperparameters, regularization techniques, and monitoring of model performance.</li>
</ul></li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>