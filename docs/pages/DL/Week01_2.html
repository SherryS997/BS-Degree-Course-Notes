<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>BS Degree Notes – week01_2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/DL/Week01_1.html">Deep Learning</a></li><li class="breadcrumb-item"><a href="../../pages/DL/Week01_2.html">Week 1.2</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation-from-biological-neuron" id="toc-motivation-from-biological-neuron" class="nav-link active" data-scroll-target="#motivation-from-biological-neuron">Motivation from Biological Neuron</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#biological-neurons" id="toc-biological-neurons" class="nav-link" data-scroll-target="#biological-neurons">Biological Neurons</a>
  <ul class="collapse">
  <li><a href="#components" id="toc-components" class="nav-link" data-scroll-target="#components">Components</a></li>
  <li><a href="#illustration" id="toc-illustration" class="nav-link" data-scroll-target="#illustration">Illustration</a></li>
  </ul></li>
  <li><a href="#neural-network-architecture" id="toc-neural-network-architecture" class="nav-link" data-scroll-target="#neural-network-architecture">Neural Network Architecture</a></li>
  <li><a href="#multi-layer-perceptrons-mlps" id="toc-multi-layer-perceptrons-mlps" class="nav-link" data-scroll-target="#multi-layer-perceptrons-mlps">Multi-Layer Perceptrons (MLPs)</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition">Definition</a></li>
  <li><a href="#information-processing" id="toc-information-processing" class="nav-link" data-scroll-target="#information-processing">Information Processing</a></li>
  <li><a href="#demonstration" id="toc-demonstration" class="nav-link" data-scroll-target="#demonstration">Demonstration</a></li>
  <li><a href="#layer-functions" id="toc-layer-functions" class="nav-link" data-scroll-target="#layer-functions">Layer Functions</a></li>
  <li><a href="#abstraction" id="toc-abstraction" class="nav-link" data-scroll-target="#abstraction">Abstraction</a></li>
  <li><a href="#information-flow" id="toc-information-flow" class="nav-link" data-scroll-target="#information-flow">Information Flow</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#mcculloch-pitts-neuron-and-thresholding-logic" id="toc-mcculloch-pitts-neuron-and-thresholding-logic" class="nav-link" data-scroll-target="#mcculloch-pitts-neuron-and-thresholding-logic">McCulloch-Pitts Neuron and Thresholding Logic</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1">Introduction</a></li>
  <li><a href="#neuron-structure" id="toc-neuron-structure" class="nav-link" data-scroll-target="#neuron-structure">Neuron Structure</a></li>
  <li><a href="#functionality" id="toc-functionality" class="nav-link" data-scroll-target="#functionality">Functionality</a></li>
  <li><a href="#boolean-function-implementation" id="toc-boolean-function-implementation" class="nav-link" data-scroll-target="#boolean-function-implementation">Boolean Function Implementation</a></li>
  <li><a href="#geometric-interpretation" id="toc-geometric-interpretation" class="nav-link" data-scroll-target="#geometric-interpretation">Geometric Interpretation</a></li>
  <li><a href="#linear-separability" id="toc-linear-separability" class="nav-link" data-scroll-target="#linear-separability">Linear Separability</a></li>
  </ul></li>
  <li><a href="#perceptrons-and-boolean-functions" id="toc-perceptrons-and-boolean-functions" class="nav-link" data-scroll-target="#perceptrons-and-boolean-functions">Perceptrons and Boolean Functions</a>
  <ul class="collapse">
  <li><a href="#introduction-2" id="toc-introduction-2" class="nav-link" data-scroll-target="#introduction-2">Introduction</a></li>
  <li><a href="#perceptron-model" id="toc-perceptron-model" class="nav-link" data-scroll-target="#perceptron-model">Perceptron Model</a>
  <ul class="collapse">
  <li><a href="#mathematical-representation" id="toc-mathematical-representation" class="nav-link" data-scroll-target="#mathematical-representation">Mathematical Representation</a></li>
  <li><a href="#neater-formulation" id="toc-neater-formulation" class="nav-link" data-scroll-target="#neater-formulation">Neater Formulation</a></li>
  </ul></li>
  <li><a href="#motivation-for-boolean-functions" id="toc-motivation-for-boolean-functions" class="nav-link" data-scroll-target="#motivation-for-boolean-functions">Motivation for Boolean Functions</a></li>
  <li><a href="#importance-of-weights" id="toc-importance-of-weights" class="nav-link" data-scroll-target="#importance-of-weights">Importance of Weights</a></li>
  <li><a href="#bias-w_0" id="toc-bias-w_0" class="nav-link" data-scroll-target="#bias-w_0">Bias (<span class="math inline">\(w_0\)</span>)</a></li>
  <li><a href="#implementing-boolean-functions" id="toc-implementing-boolean-functions" class="nav-link" data-scroll-target="#implementing-boolean-functions">Implementing Boolean Functions</a></li>
  <li><a href="#errors-and-adjustments" id="toc-errors-and-adjustments" class="nav-link" data-scroll-target="#errors-and-adjustments">Errors and Adjustments</a></li>
  </ul></li>
  <li><a href="#errors-and-error-surfaces" id="toc-errors-and-error-surfaces" class="nav-link" data-scroll-target="#errors-and-error-surfaces">Errors and Error Surfaces</a>
  <ul class="collapse">
  <li><a href="#introduction-3" id="toc-introduction-3" class="nav-link" data-scroll-target="#introduction-3">Introduction</a></li>
  <li><a href="#perceptron-for-and-function" id="toc-perceptron-for-and-function" class="nav-link" data-scroll-target="#perceptron-for-and-function">Perceptron for AND Function</a></li>
  <li><a href="#errors-and-decision-boundaries" id="toc-errors-and-decision-boundaries" class="nav-link" data-scroll-target="#errors-and-decision-boundaries">Errors and Decision Boundaries</a></li>
  <li><a href="#error-function" id="toc-error-function" class="nav-link" data-scroll-target="#error-function">Error Function</a></li>
  <li><a href="#visualizing-the-error-surface" id="toc-visualizing-the-error-surface" class="nav-link" data-scroll-target="#visualizing-the-error-surface">Visualizing the Error Surface</a></li>
  <li><a href="#perceptron-learning-algorithm" id="toc-perceptron-learning-algorithm" class="nav-link" data-scroll-target="#perceptron-learning-algorithm">Perceptron Learning Algorithm</a></li>
  </ul></li>
  <li><a href="#perceptron-learning-algorithm-1" id="toc-perceptron-learning-algorithm-1" class="nav-link" data-scroll-target="#perceptron-learning-algorithm-1">Perceptron Learning Algorithm</a>
  <ul class="collapse">
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#motivation" id="toc-motivation" class="nav-link" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm">Algorithm</a>
  <ul class="collapse">
  <li><a href="#notations" id="toc-notations" class="nav-link" data-scroll-target="#notations">Notations</a></li>
  <li><a href="#convergence" id="toc-convergence" class="nav-link" data-scroll-target="#convergence">Convergence</a></li>
  <li><a href="#steps" id="toc-steps" class="nav-link" data-scroll-target="#steps">Steps</a></li>
  </ul></li>
  <li><a href="#geometric-interpretation-1" id="toc-geometric-interpretation-1" class="nav-link" data-scroll-target="#geometric-interpretation-1">Geometric Interpretation</a></li>
  </ul></li>
  <li><a href="#perceptron-convergence-proof" id="toc-perceptron-convergence-proof" class="nav-link" data-scroll-target="#perceptron-convergence-proof">Perceptron Convergence Proof</a>
  <ul class="collapse">
  <li><a href="#introduction-4" id="toc-introduction-4" class="nav-link" data-scroll-target="#introduction-4">Introduction</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions">Definitions</a></li>
  <li><a href="#proof" id="toc-proof" class="nav-link" data-scroll-target="#proof">Proof</a>
  <ul class="collapse">
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#assumptions-and-definitions" id="toc-assumptions-and-definitions" class="nav-link" data-scroll-target="#assumptions-and-definitions">Assumptions and Definitions</a></li>
  <li><a href="#perceptron-learning-algorithm-2" id="toc-perceptron-learning-algorithm-2" class="nav-link" data-scroll-target="#perceptron-learning-algorithm-2">Perceptron Learning Algorithm</a></li>
  <li><a href="#normalization-and-definitions" id="toc-normalization-and-definitions" class="nav-link" data-scroll-target="#normalization-and-definitions">Normalization and Definitions</a></li>
  <li><a href="#numerator-analysis" id="toc-numerator-analysis" class="nav-link" data-scroll-target="#numerator-analysis">Numerator Analysis</a></li>
  <li><a href="#denominator-analysis" id="toc-denominator-analysis" class="nav-link" data-scroll-target="#denominator-analysis">Denominator Analysis</a></li>
  <li><a href="#combining-numerator-and-denominator" id="toc-combining-numerator-and-denominator" class="nav-link" data-scroll-target="#combining-numerator-and-denominator">Combining Numerator and Denominator</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#points-to-remember" id="toc-points-to-remember" class="nav-link" data-scroll-target="#points-to-remember">Points to Remember</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="motivation-from-biological-neuron" class="level1">
<h1>Motivation from Biological Neuron</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Artificial neurons, the foundational units in artificial neural networks, find their roots in biological neurons, a term coined in the 1890s to describe the brain’s processing units.</p>
</section>
<section id="biological-neurons" class="level2">
<h2 class="anchored" data-anchor-id="biological-neurons">Biological Neurons</h2>
<section id="components" class="level3">
<h3 class="anchored" data-anchor-id="components">Components</h3>
<ul>
<li><strong>Dendrite:</strong> Functions as a signal receiver from other neurons.</li>
<li><strong>Synapse:</strong> The connection point between neurons.</li>
<li><strong>Soma:</strong> The central processing unit for information.</li>
<li><strong>Axon:</strong> Transmits processed information to other neurons.</li>
</ul>
</section>
<section id="illustration" class="level3">
<h3 class="anchored" data-anchor-id="illustration">Illustration</h3>
<p>In a simplified depiction, sense organs interact with the external environment, and neurons process this information, potentially resulting in physical responses, such as laughter.</p>
</section>
</section>
<section id="neural-network-architecture" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-architecture">Neural Network Architecture</h2>
<ul>
<li><strong>Layered Structure:</strong> Neurons are organized into layers.</li>
<li><strong>Interconnected Network:</strong> The human brain comprises approximately 100 billion neurons.</li>
<li><strong>Division of Work:</strong> Neurons may specialize in processing specific information types.</li>
<li><strong>Example:</strong> Neurons responding to visual, auditory, or textual stimuli.</li>
</ul>
</section>
<section id="multi-layer-perceptrons-mlps" class="level2">
<h2 class="anchored" data-anchor-id="multi-layer-perceptrons-mlps">Multi-Layer Perceptrons (MLPs)</h2>
<section id="definition" class="level3">
<h3 class="anchored" data-anchor-id="definition">Definition</h3>
<p>Neural networks with multiple layers.</p>
</section>
<section id="information-processing" class="level3">
<h3 class="anchored" data-anchor-id="information-processing">Information Processing</h3>
<p>Initial neurons interact with sensory organs, and subsequent layers perform increasingly intricate processing.</p>
</section>
<section id="demonstration" class="level3">
<h3 class="anchored" data-anchor-id="demonstration">Demonstration</h3>
<p>Using a cartoon illustration: Neurons in the visual cortex detect edges, form features, and recognize objects.</p>
</section>
<section id="layer-functions" class="level3">
<h3 class="anchored" data-anchor-id="layer-functions">Layer Functions</h3>
<ol type="1">
<li><strong>Layer 1:</strong> Detects edges and corners.</li>
<li><strong>Subsequent Layers:</strong> Organize information into features and recognize complex objects.</li>
</ol>
</section>
<section id="abstraction" class="level3">
<h3 class="anchored" data-anchor-id="abstraction">Abstraction</h3>
<p>Each layer processes more abstract representations of the input.</p>
</section>
<section id="information-flow" class="level3">
<h3 class="anchored" data-anchor-id="information-flow">Information Flow</h3>
<p>Input traverses through layers, resulting in a physical response.</p>
</section>
</section>
</section>
<section id="mcculloch-pitts-neuron-and-thresholding-logic" class="level1">
<h1>McCulloch-Pitts Neuron and Thresholding Logic</h1>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">Introduction</h2>
<ul>
<li><strong>Objective:</strong> Comprehend the McCulloch-Pitts neuron, a simplified computational model inspired by biological neurons.</li>
<li><strong>Historical Context:</strong> Proposed in 1943 by McCulloch (neuroscientist) and Pitts (logician).</li>
<li><strong>Purpose:</strong> Emulate the brain’s complex processing for decision-making.</li>
</ul>
</section>
<section id="neuron-structure" class="level2">
<h2 class="anchored" data-anchor-id="neuron-structure">Neuron Structure</h2>
<ul>
<li><strong>Components:</strong> Divided into two parts - <strong>g</strong> and <strong>f</strong>.</li>
<li><strong>g (Aggregation):</strong> Aggregates binary inputs via a simple summation process.</li>
<li><strong>f (Decision):</strong> Makes a binary decision based on the aggregation.</li>
<li><strong>Excitatory and Inhibitory Inputs:</strong> Inputs can be either excitatory (positive) or inhibitory (negative).</li>
</ul>
</section>
<section id="functionality" class="level2">
<h2 class="anchored" data-anchor-id="functionality">Functionality</h2>
<ul>
<li><strong>Aggregation Function g(x):</strong>
<ul>
<li>Represents the sum of all inputs using the formula <span class="math inline">\(g(x) = \sum_{i=1}^{n} x_i\)</span>, where <span class="math inline">\(x_i\)</span> is a binary input (0 or 1).</li>
</ul></li>
<li><strong>Decision Function f(g(x)):</strong>
<ul>
<li>Utilizes a threshold parameter <span class="math inline">\(\theta\)</span> to determine firing.</li>
<li>Decision is <span class="math inline">\(f(g(x)) = \begin{cases} 1 &amp; \text{if } g(x) \geq \theta \\ 0 &amp; \text{otherwise} \end{cases}\)</span>. </li>
</ul></li>
</ul>
</section>
<section id="boolean-function-implementation" class="level2">
<h2 class="anchored" data-anchor-id="boolean-function-implementation">Boolean Function Implementation</h2>
<ul>
<li><strong>Examples:</strong>
<ul>
<li>Implemented using McCulloch-Pitts neuron for boolean functions like AND, OR, NOR, and NOT.</li>
<li>Excitatory and inhibitory inputs utilized based on boolean function logic.</li>
</ul></li>
</ul>
</section>
<section id="geometric-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation">Geometric Interpretation</h2>
<ul>
<li><strong>In 2D:</strong>
<ul>
<li>Draws a line to separate input space into two halves.</li>
</ul></li>
<li><strong>In 3D:</strong>
<ul>
<li>Uses a plane for separation.</li>
</ul></li>
<li><strong>For n Inputs:</strong>
<ul>
<li>Utilizes a hyperplane for linear separation.</li>
</ul></li>
</ul>
</section>
<section id="linear-separability" class="level2">
<h2 class="anchored" data-anchor-id="linear-separability">Linear Separability</h2>
<ul>
<li><strong>Definition:</strong> Boolean functions representable by a single McCulloch-Pitts neuron are linearly separable.</li>
<li><strong>Implication:</strong> Implies the existence of a plane (or hyperplane) separating points with output 0 and 1.</li>
</ul>
</section>
</section>
<section id="perceptrons-and-boolean-functions" class="level1">
<h1>Perceptrons and Boolean Functions</h1>
<section id="introduction-2" class="level2">
<h2 class="anchored" data-anchor-id="introduction-2">Introduction</h2>
<p>Perceptrons, introduced by Frank Rosenblatt circa 1958, extend the concept of McCulloch-Pitts neurons with non-Boolean inputs, input weights, and a learning algorithm for weight adjustment.</p>
</section>
<section id="perceptron-model" class="level2">
<h2 class="anchored" data-anchor-id="perceptron-model">Perceptron Model</h2>
<section id="mathematical-representation" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-representation">Mathematical Representation</h3>
<p>The perceptron is represented as <span class="math inline">\(y = 1\)</span> if <span class="math inline">\(\sum_{i=1}^{n} w_i x_i \geq \text{threshold}\)</span>; otherwise, <span class="math inline">\(y = 0\)</span>.</p>
<section id="notable-differences" class="level4">
<h4 class="anchored" data-anchor-id="notable-differences">Notable Differences</h4>
<ol type="1">
<li>Inputs can be real, not just Boolean.</li>
<li>Introduction of weights, denoted by <span class="math inline">\(w_i\)</span>, indicating input importance.</li>
<li>Learning algorithm to adapt weights based on data.</li>
</ol>
</section>
</section>
<section id="neater-formulation" class="level3">
<h3 class="anchored" data-anchor-id="neater-formulation">Neater Formulation</h3>
<p>The equation is rearranged for simplicity: <span class="math inline">\(\sum_{i=0}^{n} w_i x_i \geq 0\)</span>, where <span class="math inline">\(x_0 = 1\)</span> and <span class="math inline">\(w_0 = -\text{threshold}\)</span>.</p>
</section>
</section>
<section id="motivation-for-boolean-functions" class="level2">
<h2 class="anchored" data-anchor-id="motivation-for-boolean-functions">Motivation for Boolean Functions</h2>
<p>Boolean functions provide a foundation for understanding perceptrons. For instance, predicting movie preferences using Boolean inputs such as actor, director, and genre.</p>
</section>
<section id="importance-of-weights" class="level2">
<h2 class="anchored" data-anchor-id="importance-of-weights">Importance of Weights</h2>
<p>Weights signify the importance of specific inputs in decision-making. Learning from data helps adjust weights, reflecting user preferences. For example, assigning a high weight to the director may heavily influence the decision to watch a movie.</p>
</section>
<section id="bias-w_0" class="level2">
<h2 class="anchored" data-anchor-id="bias-w_0">Bias (<span class="math inline">\(w_0\)</span>)</h2>
<p><span class="math inline">\(w_0\)</span> acts as a bias or prior, influencing decision-making. It represents the initial bias or prejudice in decision-making. Adjusting <span class="math inline">\(w_0\)</span> alters the decision threshold, accommodating user preferences.</p>
</section>
<section id="implementing-boolean-functions" class="level2">
<h2 class="anchored" data-anchor-id="implementing-boolean-functions">Implementing Boolean Functions</h2>
<p>Perceptrons can implement Boolean functions with linear decision boundaries. For instance, implementing the OR function with a perceptron involves a geometric interpretation where a line separates positive and negative regions based on inputs.</p>
</section>
<section id="errors-and-adjustments" class="level2">
<h2 class="anchored" data-anchor-id="errors-and-adjustments">Errors and Adjustments</h2>
<p>Errors arise when the decision boundary misclassifies inputs. The learning algorithm adjusts weights iteratively to minimize errors and enhance accuracy. It’s an iterative process where weights are modified until the desired decision boundary is achieved.</p>
</section>
</section>
<section id="errors-and-error-surfaces" class="level1">
<h1>Errors and Error Surfaces</h1>
<section id="introduction-3" class="level2">
<h2 class="anchored" data-anchor-id="introduction-3">Introduction</h2>
<p>This section delves into errors within the context of perceptrons and introduces error surfaces as a recurring theme in the course, with a focus on understanding errors related to linear separability.</p>
</section>
<section id="perceptron-for-and-function" class="level2">
<h2 class="anchored" data-anchor-id="perceptron-for-and-function">Perceptron for AND Function</h2>
<p>Consideration of the AND function showcases an output of 1 for a specific input (green) and 0 for others (red). The decision is based on <span class="math inline">\(w_0 + w_1x_1 + w_2x_2 \geq 0\)</span>, with <span class="math inline">\(w_0\)</span> fixed at -1. Exploration of the impact of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> on the decision boundary is undertaken.</p>
</section>
<section id="errors-and-decision-boundaries" class="level2">
<h2 class="anchored" data-anchor-id="errors-and-decision-boundaries">Errors and Decision Boundaries</h2>
<p>Demonstration of errors occurs with specific <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> values, showcasing misclassified points due to incorrect decision boundaries. Variability in errors is noted based on different weight values.</p>
</section>
<section id="error-function" class="level2">
<h2 class="anchored" data-anchor-id="error-function">Error Function</h2>
<p>Viewing error as a function of <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> is introduced. The concept of error surfaces is brought in, where error is plotted against <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> values, each region on the surface corresponding to a distinct error level.</p>
</section>
<section id="visualizing-the-error-surface" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-the-error-surface">Visualizing the Error Surface</h2>
<p>The error surface is plotted for <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> values in the range -4 to +4. Each region on the surface corresponds to a distinct error level, highlighting the utility of visualizations in comprehending perceptron behavior.</p>
</section>
<section id="perceptron-learning-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="perceptron-learning-algorithm">Perceptron Learning Algorithm</h2>
<p>Exploration of the necessity for an algorithmic approach to finding optimal <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> values is undertaken. Limitations in visual inspection, especially in higher dimensions, are acknowledged. A teaser for the upcoming module on the perceptron learning algorithm is provided as a solution for finding suitable weight values algorithmically.</p>
</section>
</section>
<section id="perceptron-learning-algorithm-1" class="level1">
<h1>Perceptron Learning Algorithm</h1>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This module focuses on the Perceptron Learning Algorithm, building upon the perceptron’s concept and introducing a method to iteratively adjust weights for accurate binary classification.</p>
</section>
<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>The perceptron, initially designed for boolean functions, finds practical application in real-world scenarios. Consider a movie recommendation system based on past preferences, where features include both boolean and real-valued inputs. The goal is to learn weights that enable accurate predictions for new inputs.</p>
</section>
<section id="algorithm" class="level2">
<h2 class="anchored" data-anchor-id="algorithm">Algorithm</h2>
<section id="notations" class="level3">
<h3 class="anchored" data-anchor-id="notations">Notations</h3>
<ul>
<li><span class="math inline">\(p\)</span>: Inputs with label 1 (positive points)</li>
<li><span class="math inline">\(n\)</span>: Inputs with label 0 (negative points)</li>
</ul>
</section>
<section id="convergence" class="level3">
<h3 class="anchored" data-anchor-id="convergence">Convergence</h3>
<p>Convergence is achieved when all positive points satisfy <span class="math inline">\(\sum w_i x_i &gt; 0\)</span> and all negative points satisfy <span class="math inline">\(\sum w_i x_i &lt; 0\)</span>.</p>
</section>
<section id="steps" class="level3">
<h3 class="anchored" data-anchor-id="steps">Steps</h3>
<ol type="1">
<li><strong>Initialization</strong>: Randomly initialize weights <span class="math inline">\(w\)</span>.</li>
<li><strong>Iterative Update</strong>:
<ul>
<li>While not converged:
<ul>
<li>Pick a random point <span class="math inline">\(x\)</span> from <span class="math inline">\(p \cup n\)</span>.</li>
<li>If <span class="math inline">\(x\)</span> is in <span class="math inline">\(p\)</span> and <span class="math inline">\(w^T x &lt; 0\)</span>, update <span class="math inline">\(w = w + x\)</span>.</li>
<li>If <span class="math inline">\(x\)</span> is in <span class="math inline">\(n\)</span> and <span class="math inline">\(w^T x \geq 0\)</span>, update <span class="math inline">\(w = w - x\)</span>.</li>
</ul></li>
</ul></li>
</ol>
</section>
</section>
<section id="geometric-interpretation-1" class="level2">
<h2 class="anchored" data-anchor-id="geometric-interpretation-1">Geometric Interpretation</h2>
<p>Understanding the geometric relationship involves recognizing that the angle between <span class="math inline">\(w\)</span> and a point on the decision boundary is 90 degrees. Positive points’ angles should be acute (&lt; 90 degrees), and negative points’ angles should be obtuse (&gt; 90 degrees). Iteratively adjusting <span class="math inline">\(w\)</span> aligns it better with correctly classified points.</p>
</section>
</section>
<section id="perceptron-convergence-proof" class="level1">
<h1>Perceptron Convergence Proof</h1>
<section id="introduction-4" class="level2">
<h2 class="anchored" data-anchor-id="introduction-4">Introduction</h2>
<p>The objective of this lecture is to present a formal proof establishing the convergence of the perceptron learning algorithm. The primary focus is to rigorously determine whether the algorithm exhibits convergence or continues weight updates indefinitely. </p>
</section>
<section id="definitions" class="level2">
<h2 class="anchored" data-anchor-id="definitions">Definitions</h2>
<ol type="1">
<li><strong>Absolutely Linearly Separable Sets</strong>
<ul>
<li>Consider two sets, <span class="math inline">\(P\)</span> and <span class="math inline">\(N\)</span>, in an <span class="math inline">\(n\)</span>-dimensional space. They are deemed absolutely linearly separable if there exist <span class="math inline">\(n + 1\)</span> real numbers <span class="math inline">\(w_0\)</span> to <span class="math inline">\(w_n\)</span> such that the following conditions hold: <span class="math display">\[
w_0x_0 + w_1x_1 + \ldots + w_nx_n \geq 0 \quad \text{for every } \mathbf{x} \in P
\]</span> <span class="math display">\[
w_0x_0 + w_1x_1 + \ldots + w_nx_n &lt; 0 \quad \text{for every } \mathbf{x} \in N
\]</span></li>
</ul></li>
<li><strong>Perceptron Learning Algorithm Convergence Theorem</strong>
<ul>
<li>If sets <span class="math inline">\(P\)</span> and <span class="math inline">\(N\)</span> are finite and linearly separable, the perceptron learning algorithm will update the weight vector a finite number of times. This implies that after a finite number of steps, the algorithm will find a weight vector <span class="math inline">\(\mathbf{w}\)</span> capable of separating sets <span class="math inline">\(P\)</span> and <span class="math inline">\(N\)</span>.</li>
</ul></li>
</ol>
</section>
<section id="proof" class="level2">
<h2 class="anchored" data-anchor-id="proof">Proof</h2>
<section id="setup" class="level3">
<h3 class="anchored" data-anchor-id="setup">Setup</h3>
<p>Define <span class="math inline">\(P'\)</span> as the union of <span class="math inline">\(P\)</span> and the negation of <span class="math inline">\(N\)</span>. Normalize all inputs for convenience.</p>
</section>
<section id="assumptions-and-definitions" class="level3">
<h3 class="anchored" data-anchor-id="assumptions-and-definitions">Assumptions and Definitions</h3>
<p>Assume the existence of a normalized solution vector <span class="math inline">\(\mathbf{w^*}\)</span>. Define the minimum dot product, <span class="math inline">\(\delta\)</span>, as the minimum value obtained by dot products between <span class="math inline">\(\mathbf{w^*}\)</span> and points in <span class="math inline">\(P'\)</span>.</p>
</section>
<section id="perceptron-learning-algorithm-2" class="level3">
<h3 class="anchored" data-anchor-id="perceptron-learning-algorithm-2">Perceptron Learning Algorithm</h3>
<p>The perceptron learning algorithm can be expressed as follows:</p>
<ol type="1">
<li><strong>Initialization:</strong>
<ul>
<li>Initialize weight vector <span class="math inline">\(\mathbf{w}\)</span> randomly.</li>
</ul></li>
<li><strong>Iteration:</strong>
<ul>
<li>At each iteration, randomly select a point <span class="math inline">\(\mathbf{p}\)</span> from <span class="math inline">\(P'\)</span>.</li>
<li>If the condition <span class="math inline">\(\mathbf{w}^T\mathbf{p} \geq 0\)</span> is not satisfied, update <span class="math inline">\(\mathbf{w}\)</span> by <span class="math inline">\(\mathbf{w} = \mathbf{w} + \mathbf{p}\)</span>.</li>
</ul></li>
</ol>
</section>
<section id="normalization-and-definitions" class="level3">
<h3 class="anchored" data-anchor-id="normalization-and-definitions">Normalization and Definitions</h3>
<p>Normalize all inputs, ensuring the norm of <span class="math inline">\(\mathbf{p}\)</span> is 1. Define the numerator of <span class="math inline">\(\cos \beta\)</span> as the dot product between <span class="math inline">\(\mathbf{w^*}\)</span> and the updated weight vector at each iteration.</p>
</section>
<section id="numerator-analysis" class="level3">
<h3 class="anchored" data-anchor-id="numerator-analysis">Numerator Analysis</h3>
<p>Show that the numerator is greater than or equal to <span class="math inline">\(\delta\)</span> for each iteration.</p>
<p>For a randomly selected <span class="math inline">\(\mathbf{p}\)</span>, if <span class="math inline">\(\mathbf{w}^T\mathbf{p} &lt; 0\)</span> and an update is performed, the numerator is:</p>
<p><span class="math display">\[
\mathbf{w^*} \cdot (\mathbf{w} + \mathbf{p}) \geq \delta
\]</span></p>
</section>
<section id="denominator-analysis" class="level3">
<h3 class="anchored" data-anchor-id="denominator-analysis">Denominator Analysis</h3>
<p>Expand the denominator, the square of the norm of the updated weight vector:</p>
<p><span class="math display">\[
\|\mathbf{w} + \mathbf{p}\|^2 = \|\mathbf{w}\|^2 + 2\mathbf{w}^T\mathbf{p} + \|\mathbf{p}\|^2
\]</span></p>
<p>Show that the denominator is less than or equal to a value involving <span class="math inline">\(k\)</span>, the number of updates made:</p>
<p><span class="math display">\[
\|\mathbf{w} + \mathbf{p}\|^2 \leq \|\mathbf{w^*}\|^2 + k
\]</span></p>
</section>
<section id="combining-numerator-and-denominator" class="level3">
<h3 class="anchored" data-anchor-id="combining-numerator-and-denominator">Combining Numerator and Denominator</h3>
<p>Use the definition of <span class="math inline">\(\cos \beta\)</span> to conclude that <span class="math inline">\(\cos \beta\)</span> is greater than or equal to a certain quantity involving the square root of <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\cos \beta \geq \frac{\delta}{\sqrt{k}}
\]</span></p>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In this week’s lectures on deep learning, we explored fundamental concepts related to artificial neural networks, focusing on the motivation drawn from biological neurons, the structure of McCulloch-Pitts neurons, and the evolution into perceptrons. We delved into the mathematical representations, functionalities, and the learning algorithm associated with perceptrons. Additionally, we discussed errors and error surfaces, highlighting their significance in understanding the behavior of perceptrons.</p>
<p>The exploration continued with an in-depth examination of the Perceptron Learning Algorithm, emphasizing its application in real-world scenarios. The algorithm’s convergence was rigorously presented, providing a formal proof for its finite number of weight updates when dealing with linearly separable data.</p>
</section>
<section id="points-to-remember" class="level1">
<h1>Points to Remember</h1>
<ol type="1">
<li><p><strong>Biological Neurons:</strong> Understanding the components of biological neurons, including dendrites, synapses, soma, and axons, served as the foundation for artificial neural networks.</p></li>
<li><p><strong>McCulloch-Pitts Neuron:</strong> The simplified computational model, inspired by biological neurons, introduced the aggregation and decision functions, showcasing its application in implementing boolean functions.</p></li>
<li><p><strong>Perceptrons:</strong> Extending the McCulloch-Pitts model, perceptrons introduced real-valued inputs, weights, and a learning algorithm for weight adjustments, emphasizing their significance in decision-making.</p></li>
<li><p><strong>Perceptron Learning Algorithm:</strong> The iterative algorithm for adjusting weights in perceptrons was presented, highlighting its application in scenarios with boolean and real-valued inputs.</p></li>
<li><p><strong>Convergence Proof:</strong> A formal proof established the convergence of the Perceptron Learning Algorithm for linearly separable data, emphasizing its practical applicability in real-world scenarios.</p></li>
<li><p><strong>Geometric Interpretation:</strong> Recognizing the geometric relationship between the weight vector and decision boundaries provided insights into how the algorithm aligns with correctly classified points.</p></li>
<li><p><strong>Error Surfaces:</strong> Visualizing error surfaces proved essential in comprehending perceptron behavior, showcasing the impact of weight adjustments on error levels.</p></li>
<li><p><strong>Practical Applications:</strong> Motivation for the algorithm was drawn from real-world applications, such as movie recommendation systems, emphasizing the practicality and relevance of the discussed concepts.</p></li>
<li><p><strong>Necessity of Learning Algorithm:</strong> The iterative nature of weight adjustments was emphasized, recognizing the limitations of visual inspection, especially in higher dimensions.</p></li>
<li><p><strong>Implications of Convergence Proof:</strong> The convergence proof provided assurance that, when dealing with linearly separable data, the perceptron learning algorithm will converge after a finite number of updates.</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>