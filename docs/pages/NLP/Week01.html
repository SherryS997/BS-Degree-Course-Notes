<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to NLP – BS Degree Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-sans.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">NLP</a></li><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">Week 1</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 12</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">NLP</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week01.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">LLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#natural-language-processing-tasks" id="toc-natural-language-processing-tasks" class="nav-link active" data-scroll-target="#natural-language-processing-tasks">Natural Language Processing Tasks</a>
  <ul class="collapse">
  <li><a href="#core-technologies" id="toc-core-technologies" class="nav-link" data-scroll-target="#core-technologies">Core Technologies</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a></li>
  </ul></li>
  <li><a href="#why-nlp-is-hard" id="toc-why-nlp-is-hard" class="nav-link" data-scroll-target="#why-nlp-is-hard">Why NLP is hard?</a>
  <ul class="collapse">
  <li><a href="#ambiguity" id="toc-ambiguity" class="nav-link" data-scroll-target="#ambiguity">Ambiguity</a></li>
  <li><a href="#linguistic-diversity" id="toc-linguistic-diversity" class="nav-link" data-scroll-target="#linguistic-diversity">Linguistic Diversity</a></li>
  <li><a href="#data-sparsity" id="toc-data-sparsity" class="nav-link" data-scroll-target="#data-sparsity">Data Sparsity</a></li>
  <li><a href="#variability-and-change" id="toc-variability-and-change" class="nav-link" data-scroll-target="#variability-and-change">Variability and Change</a></li>
  <li><a href="#computational-complexity" id="toc-computational-complexity" class="nav-link" data-scroll-target="#computational-complexity">Computational Complexity</a></li>
  </ul></li>
  <li><a href="#ambiguity-in-natural-language" id="toc-ambiguity-in-natural-language" class="nav-link" data-scroll-target="#ambiguity-in-natural-language">Ambiguity in Natural Language</a>
  <ul class="collapse">
  <li><a href="#types-of-ambiguity" id="toc-types-of-ambiguity" class="nav-link" data-scroll-target="#types-of-ambiguity">Types of Ambiguity</a></li>
  </ul></li>
  <li><a href="#challenges-in-multilingual-nlp" id="toc-challenges-in-multilingual-nlp" class="nav-link" data-scroll-target="#challenges-in-multilingual-nlp">Challenges in Multilingual NLP</a></li>
  <li><a href="#indian-language-data-and-resources" id="toc-indian-language-data-and-resources" class="nav-link" data-scroll-target="#indian-language-data-and-resources">Indian Language Data and Resources</a></li>
  <li><a href="#language-variability-across-speakers" id="toc-language-variability-across-speakers" class="nav-link" data-scroll-target="#language-variability-across-speakers">Language Variability Across Speakers</a>
  <ul class="collapse">
  <li><a href="#dialects" id="toc-dialects" class="nav-link" data-scroll-target="#dialects">Dialects</a></li>
  <li><a href="#sociolects" id="toc-sociolects" class="nav-link" data-scroll-target="#sociolects">Sociolects</a></li>
  <li><a href="#idiolects" id="toc-idiolects" class="nav-link" data-scroll-target="#idiolects">Idiolects</a></li>
  </ul></li>
  <li><a href="#levels-of-language-processing-in-nlp" id="toc-levels-of-language-processing-in-nlp" class="nav-link" data-scroll-target="#levels-of-language-processing-in-nlp">Levels of Language Processing in NLP</a>
  <ul class="collapse">
  <li><a href="#phonological-level" id="toc-phonological-level" class="nav-link" data-scroll-target="#phonological-level">1. Phonological Level</a></li>
  <li><a href="#morphological-level" id="toc-morphological-level" class="nav-link" data-scroll-target="#morphological-level">2. Morphological Level</a></li>
  <li><a href="#lexical-level" id="toc-lexical-level" class="nav-link" data-scroll-target="#lexical-level">3. Lexical Level</a></li>
  <li><a href="#syntactic-level" id="toc-syntactic-level" class="nav-link" data-scroll-target="#syntactic-level">4. Syntactic Level</a></li>
  <li><a href="#semantic-level" id="toc-semantic-level" class="nav-link" data-scroll-target="#semantic-level">5. Semantic Level</a></li>
  <li><a href="#discourse-level" id="toc-discourse-level" class="nav-link" data-scroll-target="#discourse-level">6. Discourse Level</a></li>
  <li><a href="#pragmatic-level" id="toc-pragmatic-level" class="nav-link" data-scroll-target="#pragmatic-level">7. Pragmatic Level</a></li>
  </ul></li>
  <li><a href="#levels-and-applications-of-nlp" id="toc-levels-and-applications-of-nlp" class="nav-link" data-scroll-target="#levels-and-applications-of-nlp">Levels and Applications of NLP</a>
  <ul class="collapse">
  <li><a href="#character-string-level" id="toc-character-string-level" class="nav-link" data-scroll-target="#character-string-level">1. Character &amp; String Level</a></li>
  <li><a href="#word-token-level" id="toc-word-token-level" class="nav-link" data-scroll-target="#word-token-level">2. Word Token Level</a></li>
  <li><a href="#sentence-level" id="toc-sentence-level" class="nav-link" data-scroll-target="#sentence-level">3. Sentence Level</a></li>
  <li><a href="#sentence-window-level" id="toc-sentence-window-level" class="nav-link" data-scroll-target="#sentence-window-level">4. Sentence Window Level</a></li>
  <li><a href="#paragraph-passage-level" id="toc-paragraph-passage-level" class="nav-link" data-scroll-target="#paragraph-passage-level">5. Paragraph &amp; Passage Level</a></li>
  <li><a href="#whole-document-level" id="toc-whole-document-level" class="nav-link" data-scroll-target="#whole-document-level">6. Whole Document Level</a></li>
  <li><a href="#multi-document-collection-level" id="toc-multi-document-collection-level" class="nav-link" data-scroll-target="#multi-document-collection-level">7. Multi-Document Collection Level</a></li>
  </ul></li>
  <li><a href="#early-beginnings-of-nlp-1950s-1970s" id="toc-early-beginnings-of-nlp-1950s-1970s" class="nav-link" data-scroll-target="#early-beginnings-of-nlp-1950s-1970s">Early Beginnings of NLP (1950s-1970s)</a></li>
  <li><a href="#nlp-in-the-1980s" id="toc-nlp-in-the-1980s" class="nav-link" data-scroll-target="#nlp-in-the-1980s">NLP in the 1980s</a></li>
  <li><a href="#nlp-late-1980s---2000s" id="toc-nlp-late-1980s---2000s" class="nav-link" data-scroll-target="#nlp-late-1980s---2000s">NLP (Late 1980s - 2000s)</a></li>
  <li><a href="#nlp-2010s---present" id="toc-nlp-2010s---present" class="nav-link" data-scroll-target="#nlp-2010s---present">NLP (2010s - Present)</a></li>
  <li><a href="#key-differences-between-human-and-machine-nlp" id="toc-key-differences-between-human-and-machine-nlp" class="nav-link" data-scroll-target="#key-differences-between-human-and-machine-nlp">Key Differences Between Human and Machine NLP</a>
  <ul class="collapse">
  <li><a href="#human" id="toc-human" class="nav-link" data-scroll-target="#human">Human</a></li>
  <li><a href="#machine" id="toc-machine" class="nav-link" data-scroll-target="#machine">Machine</a></li>
  </ul></li>
  <li><a href="#review-questions" id="toc-review-questions" class="nav-link" data-scroll-target="#review-questions">Review Questions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">NLP</a></li><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">Week 1</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Introduction to NLP</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="natural-language-processing-tasks" class="level1">
<h1>Natural Language Processing Tasks</h1>
<section id="core-technologies" class="level2">
<h2 class="anchored" data-anchor-id="core-technologies">Core Technologies</h2>
<ul>
<li><p><strong>Language Modeling:</strong> Predicting the probability of a sequence of words. Used in speech recognition, machine translation, and text generation. A language model assigns a probability <span class="math inline">\(P(w_1, w_2, ..., w_n)\)</span> to a sequence of <span class="math inline">\(n\)</span> words. N-gram models estimate this probability using the frequencies of word sequences in a corpus. Neural language models use neural networks to learn more complex relationships between words.</p></li>
<li><p><strong>Part-of-Speech (POS) Tagging:</strong> Assigning grammatical tags (e.g., noun, verb, adjective) to each word in a sentence. Essential for syntactic parsing and other downstream tasks. Accuracy is measured by comparing the predicted tags to a manually tagged corpus.</p></li>
<li><p><strong>Syntactic Parsing:</strong> Analyzing the grammatical structure of a sentence to determine its syntactic relationships, often represented as parse trees. Two main types: constituency parsing (grouping words into phrases) and dependency parsing (identifying relationships between individual words).</p></li>
<li><p><strong>Named Entity Recognition (NER):</strong> Identifying and classifying named entities (e.g., person, organization, location, date) in text. Crucial for information extraction and knowledge graph construction. Evaluation metrics include precision, recall, and F1-score.</p></li>
<li><p><strong>Coreference Resolution:</strong> Determining which mentions in a text refer to the same entity. For example, identifying that “he” and “John” refer to the same person. Improves text understanding and facilitates tasks like summarization and question answering.</p></li>
<li><p><strong>Word Sense Disambiguation (WSD):</strong> Identifying the correct meaning of a word based on its context. For example, determining whether “bank” refers to a financial institution or a river bank. A challenging task due to the prevalence of polysemy in language.</p></li>
<li><p><strong>Semantic Role Labeling (SRL):</strong> Identifying the semantic roles of words in a sentence, such as agent, patient, instrument, and location. This provides a deeper understanding of the meaning of a sentence beyond its syntactic structure.</p></li>
</ul>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<ul>
<li><p><strong>Machine Translation (MT):</strong> Automatically translating text from one language to another. Statistical MT uses parallel corpora to learn translation probabilities. Neural MT uses neural networks to learn complex mappings between languages.</p></li>
<li><p><strong>Information Retrieval (IR):</strong> Retrieving relevant documents from a collection based on a user’s query. Techniques include keyword search, boolean retrieval, and vector space models. Evaluation metrics include precision and recall.</p></li>
<li><p><strong>Question Answering (QA):</strong> Answering questions posed in natural language. Requires understanding the question, finding relevant information, and generating an answer.</p></li>
<li><p><strong>Dialogue Systems:</strong> Building systems that can engage in conversations with humans. Challenges include understanding user intent, managing dialogue flow, and generating appropriate responses.</p></li>
<li><p><strong>Information Extraction (IE):</strong> Extracting structured information from unstructured text. Techniques include named entity recognition, relation extraction, and event extraction.</p></li>
<li><p><strong>Summarization:</strong> Creating concise summaries of longer texts. Approaches include extractive summarization (selecting important sentences) and abstractive summarization (generating new sentences).</p></li>
<li><p><strong>Sentiment Analysis:</strong> Determining the emotional tone of a text, such as positive, negative, or neutral. Used in market research, social media monitoring, and customer service.</p></li>
</ul>
</section>
</section>
<section id="why-nlp-is-hard" class="level1">
<h1>Why NLP is hard?</h1>
<section id="ambiguity" class="level2">
<h2 class="anchored" data-anchor-id="ambiguity">Ambiguity</h2>
<p>Natural language is inherently ambiguous, meaning that words, phrases, and sentences can have multiple interpretations. This makes it difficult for computers to determine the correct meaning. Different types of ambiguity compound this challenge:</p>
<ul>
<li><p><strong>Lexical Ambiguity:</strong> Words can have multiple meanings (homonymy) or multiple senses (polysemy). For example, “bank” can refer to a financial institution or a river bank. WSD (Word Sense Disambiguation) methods aim to resolve these by considering the surrounding context. A simplified probabilistic approach could involve calculating <span class="math inline">\(P(sense_i|context)\)</span>, where <span class="math inline">\(sense_i\)</span> is a particular meaning of the word.</p></li>
<li><p><strong>Syntactic Ambiguity:</strong> The grammatical structure of a sentence can lead to different interpretations. For instance, “I saw the man with the telescope” could mean the man possessed the telescope or the speaker used the telescope to see the man. Parsing algorithms try to create the most likely parse tree, often employing probabilistic context-free grammars (PCFGs). A PCFG assigns probabilities to different parse tree structures: <span class="math inline">\(P(tree|sentence)\)</span>.</p></li>
<li><p><strong>Semantic Ambiguity:</strong> Even with a clear syntactic structure, sentences can have multiple meanings. “Every child loves some movie” can mean each child loves a different movie or there’s one movie loved by all. Formal semantic representations, like lambda calculus, can help disambiguate, but mapping natural language to these representations is challenging.</p></li>
<li><p><strong>Pragmatic Ambiguity:</strong> The intended meaning of a sentence can depend heavily on the context of the conversation or the speaker’s intentions. “Can you pass the salt?” is typically a request, not a question about ability. Modeling pragmatics often requires understanding world knowledge and social cues, which are difficult to encode computationally. For example, sarcasm detection might use sentiment analysis in conjunction with contextual cues, but there’s no foolproof formula.</p></li>
<li><p><strong>Referential Ambiguity:</strong> Pronouns and other referring expressions can be ambiguous, especially in longer texts. “He went to the store after he finished his homework.” Who went to the store? Coreference resolution aims to link these expressions to their intended referents.</p></li>
</ul>
</section>
<section id="linguistic-diversity" class="level2">
<h2 class="anchored" data-anchor-id="linguistic-diversity">Linguistic Diversity</h2>
<p>The vast number of languages, each with unique grammatical rules and structures, presents a major hurdle. Directly porting NLP models from one language to another is rarely effective.</p>
<ul>
<li><p><strong>Morphological Variation:</strong> Languages differ in how words are formed. Agglutinative languages (e.g., Turkish) combine multiple morphemes into single words, requiring complex morphological analysis. This complexity makes tasks like stemming and lemmatization more challenging. Computational morphology uses finite-state transducers (FSTs) to model these complex word formation processes.</p></li>
<li><p><strong>Syntactic Variation:</strong> Word order and grammatical relations vary significantly across languages. Subject-Verb-Object (SVO) is common in English, while Subject-Object-Verb (SOV) is common in others. Parsing and machine translation need to adapt to these variations. Treebanks, annotated with syntactic structure, are crucial for training parsers for different languages.</p></li>
<li><p><strong>Semantic Variation:</strong> Languages can conceptualize and express the same meaning in different ways. Color terms, spatial relations, and even basic concepts can have subtle variations. Cross-lingual word embeddings try to capture these semantic relationships across languages, but perfect alignment is difficult. One approach uses bilingual dictionaries or parallel corpora to align embedding spaces.</p></li>
</ul>
</section>
<section id="data-sparsity" class="level2">
<h2 class="anchored" data-anchor-id="data-sparsity">Data Sparsity</h2>
<p>Building robust statistical NLP models requires large amounts of annotated data. Many languages lack these resources, making it difficult to develop high-performing models. This “low-resource” scenario forces researchers to explore techniques like:</p>
<ul>
<li><p><strong>Cross-lingual Transfer Learning:</strong> Leveraging resources from high-resource languages to improve performance on low-resource ones. Multilingual embeddings or transferring model parameters are common strategies. Success depends on the relatedness of the languages and the specific task.</p></li>
<li><p><strong>Unsupervised and Semi-supervised Learning:</strong> Making the most of limited labeled data by incorporating unlabeled data. Techniques like self-training or using large language models can be beneficial.</p></li>
</ul>
</section>
<section id="variability-and-change" class="level2">
<h2 class="anchored" data-anchor-id="variability-and-change">Variability and Change</h2>
<p>Language is constantly evolving, with new words, slang, and expressions emerging continuously. This dynamic nature makes it difficult for NLP systems to stay up-to-date.</p>
<ul>
<li><p><strong>Dialectal Variation:</strong> Even within a single language, there are regional and social dialects with different pronunciation, vocabulary, and grammar. NLP models need to be robust enough to handle these variations. Adaptation techniques might involve fine-tuning on dialect-specific data.</p></li>
<li><p><strong>Informal Language:</strong> Social media and online communication introduce informal language, abbreviations, and emojis, posing new challenges for NLP systems. Handling this requires models trained on informal text and specialized lexicons.</p></li>
</ul>
</section>
<section id="computational-complexity" class="level2">
<h2 class="anchored" data-anchor-id="computational-complexity">Computational Complexity</h2>
<p>Many NLP tasks are computationally intensive, particularly those involving deep learning models like transformers. Training and deploying these models requires significant computational resources. Optimizations and efficient hardware are necessary for practical applications. For example, model compression techniques can reduce the size and computational requirements of large models.</p>
</section>
</section>
<section id="ambiguity-in-natural-language" class="level1">
<h1>Ambiguity in Natural Language</h1>
<section id="types-of-ambiguity" class="level2">
<h2 class="anchored" data-anchor-id="types-of-ambiguity">Types of Ambiguity</h2>
<ul>
<li><strong>Lexical Ambiguity:</strong> Arises from the multiple meanings of individual words.
<ul>
<li><strong>Homonymy:</strong> Words with identical spelling and pronunciation but distinct unrelated meanings (e.g., “bat” - a nocturnal flying mammal vs.&nbsp;a piece of equipment used in baseball). Consider the sentence: “I saw a bat flying in the cave.” Without further context, the meaning of “bat” is ambiguous.</li>
<li><strong>Polysemy:</strong> Words with multiple related meanings (e.g., “bright” - shining with light vs.&nbsp;intelligent). The sentence “The student has a bright future” demonstrates polysemy; “bright” refers to intelligence and promise, not literal light emission.</li>
<li><strong>Homographs:</strong> Words with the same spelling but different pronunciations and meanings (e.g., “lead” - to guide/ /liːd/ vs.&nbsp;a metal /lɛd/). The sentence, “The lead singer had a heavy lead apron for the x-ray” illustrates homographs with different pronunciations influencing meaning.</li>
<li><strong>Homophones:</strong> Words with the same pronunciation but different spellings and meanings (e.g., “to,” “too,” and “two”). “They went to the store to buy two apples” uses homophones, identifiable only through distinct spellings.</li>
</ul></li>
<li><strong>Syntactic Ambiguity:</strong> Stems from the different ways words can be grammatically arranged in a sentence.
<ul>
<li><strong>Prepositional Phrase Attachment:</strong> Uncertainty in associating a prepositional phrase with a noun phrase or verb phrase (e.g., “I saw the man with the telescope”). Mathematically, two parse trees, <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span>, could exist where in <span class="math inline">\(T_1\)</span>, the prepositional phrase modifies the verb (saw with the telescope), and in <span class="math inline">\(T_2\)</span> it modifies the noun (the man with the telescope).</li>
<li><strong>Coordination Ambiguity:</strong> Ambiguity introduced by coordinating conjunctions like “and” and “or” (e.g., “old men and women”). Does this refer to both old men and old women, or old men and all women? Boolean logic could represent the interpretations: <span class="math inline">\(Old(men) \wedge Old(women)\)</span> vs.&nbsp;<span class="math inline">\(Old(men) \wedge Women\)</span>.</li>
</ul></li>
<li><strong>Semantic Ambiguity:</strong> Concerns multiple possible meanings derived from word or phrase interpretations, even with a clear syntactic structure.
<ul>
<li><strong>Quantifier Scope Ambiguity:</strong> Uncertainty regarding the scope of quantifiers like “all,” “some,” or “every” (e.g., “Every student reads some books”). Does this mean there exists a set of books read by all students, <span class="math inline">\(\exists B (\forall S \in Students, Reads(S,B))\)</span>, or that each student reads a potentially different set of books, <span class="math inline">\(\forall S \in Students, \exists B (Reads(S,B))\)</span>?</li>
<li><strong>Anaphoric Ambiguity:</strong> Difficulty determining the referent of pronouns or other anaphoric expressions (e.g., “John told Peter he was happy”). Does “he” refer to John or Peter?</li>
</ul></li>
<li><strong>Pragmatic Ambiguity:</strong> Deals with meaning reliant on context, speaker intent, and world knowledge.
<ul>
<li><strong>Deictic Ambiguity:</strong> Uncertainty in interpreting words dependent on the speaker’s context, such as “here,” “there,” “now,” or “you” (e.g., “Meet me here tomorrow”). The meaning requires specific spatial and temporal information.</li>
<li><strong>Speech Act Ambiguity:</strong> Difficulty discerning the intent behind an utterance (e.g., “Can you close the window?”). This could be a question about ability or a polite request.</li>
<li><strong>Irony and Sarcasm:</strong> Intended meaning differs from literal meaning, requiring understanding of tone and context (e.g., “Oh great, another meeting!”).</li>
</ul></li>
</ul>
</section>
</section>
<section id="challenges-in-multilingual-nlp" class="level1">
<h1>Challenges in Multilingual NLP</h1>
<ul>
<li><p><strong>Data Sparsity:</strong> Many languages lack large, annotated datasets necessary for training robust NLP models. This leads to difficulties in tasks like machine translation, part-of-speech tagging, and named entity recognition. The distribution of available data often follows a power law, with a few high-resource languages dominating and a long tail of low-resource languages. This can be represented as <span class="math inline">\(P(n) \propto n^{-\gamma}\)</span>, where <span class="math inline">\(P(n)\)</span> is the probability of observing a language with <span class="math inline">\(n\)</span> data points, and <span class="math inline">\(\gamma\)</span> is the power law exponent.</p></li>
<li><p><strong>Morphological Complexity:</strong> Languages exhibit varying levels of morphological complexity. Agglutinative languages (e.g., Turkish, Finnish) combine multiple morphemes into single words, creating a large vocabulary and making it difficult to model word formation and inflection. Consider a word with <span class="math inline">\(m\)</span> possible morphemes, each with an average frequency <span class="math inline">\(f\)</span>. The number of potential word forms can explode to <span class="math inline">\(f^m\)</span>, posing challenges for statistical models.</p></li>
<li><p><strong>Syntactic Divergence:</strong> Languages differ significantly in their syntactic structures. Word order, for example, can vary considerably (e.g., Subject-Verb-Object in English vs.&nbsp;Subject-Object-Verb in Hindi). Creating models that generalize across diverse syntactic structures is a significant challenge. Cross-lingual parsing, where a parser trained on one language is applied to another, often suffers from reduced performance due to these differences. Let <span class="math inline">\(A\)</span> be the accuracy of a parser on language <span class="math inline">\(L_1\)</span>, and let <span class="math inline">\(\delta\)</span> be the syntactic divergence between <span class="math inline">\(L_1\)</span> and <span class="math inline">\(L_2\)</span>. The expected accuracy on <span class="math inline">\(L_2\)</span> might be <span class="math inline">\(A - k\delta\)</span>, where <span class="math inline">\(k\)</span> is a constant representing the impact of divergence on performance.</p></li>
<li><p><strong>Semantic Variation:</strong> Even when languages share similar concepts, their semantic representations can differ. Word sense disambiguation becomes more complex in a multilingual setting, as a single word can have different meanings or nuances across languages. Cross-lingual word embeddings aim to capture semantic similarity across languages, but aligning these embeddings effectively remains a challenge.</p></li>
<li><p><strong>Lack of Linguistic Resources:</strong> For many languages, essential linguistic resources like annotated corpora, dictionaries, and grammatical rules are scarce or non-existent. This hinders the development of basic NLP tools and techniques, further exacerbating the data sparsity problem. Building these resources manually is time-consuming and expensive, requiring expertise in linguistics and computational methods.</p></li>
<li><p><strong>Cross-lingual Transfer Learning:</strong> While transfer learning, where knowledge learned from one language is transferred to another, has shown promise, it faces challenges in effectively adapting to languages with different characteristics. Negative transfer, where transferring knowledge actually hurts performance, can occur when the source and target languages are too dissimilar. Finding optimal strategies for cross-lingual transfer learning remains an active area of research.</p></li>
</ul>
</section>
<section id="indian-language-data-and-resources" class="level1">
<h1>Indian Language Data and Resources</h1>
<ul>
<li>Significant disparity in data availability between Indian languages and resource-rich languages like English.</li>
<li>Data scarcity hinders development of robust NLP tools for many Indian languages.</li>
<li>Availability of resources like parallel corpora, annotated datasets, and linguistic tools varies greatly across languages.</li>
<li>Initiatives to create and share resources for low-resource Indian languages are underway, but challenges remain.</li>
<li>Code-switching (mixing languages in conversation) is common in India and presents a unique challenge for NLP models. This requires specialized datasets and techniques.</li>
<li>Different scripts used for writing various Indian languages require specific processing methods for tasks like tokenization and transliteration.</li>
<li>Dialectal variations within Indian languages add complexity to data collection and model training. Models might need to be trained on data from multiple dialects or adapted for specific dialects.</li>
<li>Oral languages with limited written resources require different approaches for NLP tasks. Speech recognition and text-to-speech systems are particularly important for these languages.</li>
<li>Development of language-specific tools and resources, such as morphological analyzers and part-of-speech taggers, is crucial for advancing NLP in Indian languages.</li>
<li>Measuring data sparsity using metrics like type-token ratio (TTR) can help assess the challenges posed by different languages. TTR = <span class="math inline">\(\frac{\text{Number of unique words}}{\text{Total number of words}}\)</span>. A higher TTR might indicate more data is needed for effective language modeling.</li>
<li>Consider the conditional probability of a word <span class="math inline">\(w\)</span> given its context <span class="math inline">\(c\)</span>, <span class="math inline">\(P(w|c)\)</span>. In resource-rich languages, large datasets enable reliable estimation of this probability. However, in low-resource scenarios, <span class="math inline">\(P(w|c)\)</span> becomes difficult to estimate accurately, hindering tasks like language modeling and machine translation.</li>
<li>Resource availability also impacts the performance of downstream tasks. For example, if the accuracy of a Part-of-Speech tagger is low due to limited training data, the performance of a subsequent task like dependency parsing will also be negatively impacted.</li>
<li>Cross-lingual transfer learning, where knowledge from a high-resource language is transferred to a low-resource language, is a promising technique for mitigating data scarcity.</li>
</ul>
</section>
<section id="language-variability-across-speakers" class="level1">
<h1>Language Variability Across Speakers</h1>
<p>Besides formal language, variations exist across speakers due to geographical, social, and individual factors. These variations are captured in dialects, sociolects, and idiolects and add to the complexity of NLP.</p>
<section id="dialects" class="level2">
<h2 class="anchored" data-anchor-id="dialects">Dialects</h2>
<p>Dialects are regional variations of a language. Differences can appear in several linguistic levels:</p>
<ul>
<li><strong>Phonetics and Phonology:</strong> Variations in pronunciation, intonation, and the sets of sounds used (phonemes). For example, the same phoneme /r/ can be realized differently in different dialects.</li>
<li><strong>Morphology:</strong> Different morphemes (smallest meaningful units) or different rules for combining them may exist. For example, past tense formation can vary across dialects (e.g., “climbed” vs.&nbsp;“clumb”).</li>
<li><strong>Syntax:</strong> Word order and grammatical structures might differ. A sentence grammatically correct in one dialect might not be in another.</li>
<li><strong>Lexicon:</strong> Different words or meanings for the same word can exist (e.g., “soda” vs.&nbsp;“pop” vs.&nbsp;“soft drink”). This impacts vocabulary size and requires dialect-specific lexicons.</li>
</ul>
<p>Dialects form a continuum, and variations exist not only geographically, with <span class="math inline">\(d\)</span> representing the geographical distance and <span class="math inline">\(v\)</span> a measure of language variation possibly correlating with <span class="math inline">\(d\)</span>, but also based on other sociolinguistic parameters (age, gender, etc.) with say <span class="math inline">\(v_a\)</span>, <span class="math inline">\(v_g\)</span> variations due to age, gender respectively. <span class="math display">\[
v = f(d, other\_parameters)
\]</span></p>
</section>
<section id="sociolects" class="level2">
<h2 class="anchored" data-anchor-id="sociolects">Sociolects</h2>
<p>Sociolects are variations based on social groups (age, gender, ethnicity, social class, etc.). Factors influencing sociolects include:</p>
<ul>
<li><strong>Social Class:</strong> Different social classes may use distinct vocabulary, pronunciation, and grammatical structures. Certain linguistic features can become associated with prestige or lack thereof.</li>
<li><strong>Age:</strong> Language use evolves across generations, leading to differences in slang, vocabulary, and even grammar. Younger generations often introduce new terms and expressions.</li>
<li><strong>Ethnicity:</strong> Ethnic groups may retain linguistic features from their heritage languages, influencing their use of the dominant language. This can create distinct ethnolects.</li>
<li><strong>Gender:</strong> Studies have identified differences in language use between genders, although these are often subtle and complex. These may include variations in intonation, vocabulary choice, and conversational styles.</li>
<li><strong>Occupation/Jargon:</strong> Professional groups often develop specialized jargon or technical language related to their field. This allows for precise communication within the group but can create barriers for outsiders.</li>
</ul>
</section>
<section id="idiolects" class="level2">
<h2 class="anchored" data-anchor-id="idiolects">Idiolects</h2>
<p>An idiolect is the unique language variety of an individual. It’s a combination of influences from all other linguistic variations plus individual characteristics:</p>
<ul>
<li><strong>Personal Experiences:</strong> An individual’s experiences shape their vocabulary and language use. Frequent exposure to certain domains or topics leads to specialized vocabulary.</li>
<li><strong>Personality:</strong> Personality traits can influence language style. Extroverted individuals might use more elaborate language compared to introverted ones.</li>
<li><strong>Speech Habits:</strong> Individuals develop unique speech patterns, including voice quality, intonation, and use of filler words (e.g., “um,” “like”).</li>
<li><strong>Physical/Cognitive Factors:</strong> Physical and cognitive differences can impact speech production and comprehension, leading to variations in pronunciation and articulation.</li>
</ul>
<p>These variations (dialects, sociolects, idiolects) represent significant challenges for NLP systems, particularly in tasks like speech recognition, natural language understanding, and information retrieval. Adapting to this speaker variability requires robust models and diverse training data that encompass a wide range of language variations.</p>
</section>
</section>
<section id="levels-of-language-processing-in-nlp" class="level1">
<h1>Levels of Language Processing in NLP</h1>
<section id="phonological-level" class="level2">
<h2 class="anchored" data-anchor-id="phonological-level">1. Phonological Level</h2>
<ul>
<li><strong>Focus:</strong> The sound structure of language. Deals with phonemes (smallest units of sound), phonetics (physical production and perception of speech sounds), and phonotactics (rules governing sound combinations).</li>
<li><strong>NLP Tasks:</strong> Speech recognition, text-to-speech synthesis, pronunciation modeling, and accent detection.</li>
<li><strong>Example:</strong> Distinguishing between /kæt/ (cat) and /bæt/ (bat) relies on recognizing the distinct phonemes /k/ and /b/. Prosody (rhythm, stress, intonation) also plays a role: “You’re going?” (question) vs.&nbsp;“You’re going.” (statement).</li>
</ul>
</section>
<section id="morphological-level" class="level2">
<h2 class="anchored" data-anchor-id="morphological-level">2. Morphological Level</h2>
<ul>
<li><strong>Focus:</strong> The internal structure of words. Analyzes morphemes (smallest meaningful units), including roots, prefixes, suffixes, and inflections.</li>
<li><strong>NLP Tasks:</strong> Morphological analysis (breaking words into morphemes), stemming (reducing words to root forms), lemmatization (finding dictionary forms), part-of-speech tagging.</li>
<li><strong>Example:</strong> “Unbreakable” comprises three morphemes: “un-” (prefix), “break” (root), and “-able” (suffix). Lemmatizing “running” yields “run.”</li>
</ul>
</section>
<section id="lexical-level" class="level2">
<h2 class="anchored" data-anchor-id="lexical-level">3. Lexical Level</h2>
<ul>
<li><strong>Focus:</strong> Individual words and their meanings. Considers lexicon (vocabulary of a language) and lexical semantics (meaning relations between words).</li>
<li><strong>NLP Tasks:</strong> Tokenization (splitting text into words), lexical analysis (identifying word boundaries and types), word sense disambiguation (determining correct meaning of polysemous words), synonym and antonym detection.</li>
<li><strong>Example:</strong> Resolving the ambiguity of “bank” (financial institution vs.&nbsp;river bank) based on surrounding words.</li>
</ul>
</section>
<section id="syntactic-level" class="level2">
<h2 class="anchored" data-anchor-id="syntactic-level">4. Syntactic Level</h2>
<ul>
<li><strong>Focus:</strong> How words combine to form phrases and sentences. Deals with syntax (grammatical rules governing sentence structure) and parsing (analyzing sentence structure).</li>
<li><strong>NLP Tasks:</strong> Parsing (creating parse trees to represent sentence structure), constituency parsing (grouping words into phrases), dependency parsing (identifying relationships between words), grammatical error detection.</li>
<li><strong>Example:</strong> Analyzing “The cat sat on the mat” to determine the subject (“cat”), verb (“sat”), and prepositional phrase (“on the mat”). Dependency parsing would show “sat” as the root, with “cat” as the subject and “mat” as the object of the preposition “on.”</li>
</ul>
</section>
<section id="semantic-level" class="level2">
<h2 class="anchored" data-anchor-id="semantic-level">5. Semantic Level</h2>
<ul>
<li><strong>Focus:</strong> The meaning of phrases and sentences. Deals with semantic roles (roles words play in a sentence) and logical representations of sentence meaning.</li>
<li><strong>NLP Tasks:</strong> Semantic role labeling, named entity recognition, semantic parsing (converting sentences into formal logical representations), relationship extraction.</li>
<li><strong>Example:</strong> Identifying “John” as the agent and “Mary” as the recipient in “John gave Mary a book.”</li>
</ul>
</section>
<section id="discourse-level" class="level2">
<h2 class="anchored" data-anchor-id="discourse-level">6. Discourse Level</h2>
<ul>
<li><strong>Focus:</strong> How sentences connect to form larger units of text (e.g., paragraphs, documents). Considers discourse structure, coherence, and cohesion.</li>
<li><strong>NLP Tasks:</strong> Anaphora resolution (pronoun resolution), text summarization, discourse parsing (analyzing discourse structure), coherence and cohesion analysis.</li>
<li><strong>Example:</strong> Resolving “he” to “John” in “John went to the store. He bought some milk.”</li>
</ul>
</section>
<section id="pragmatic-level" class="level2">
<h2 class="anchored" data-anchor-id="pragmatic-level">7. Pragmatic Level</h2>
<ul>
<li><strong>Focus:</strong> How language is used in context. Considers speaker intent, world knowledge, and the effects of utterances.</li>
<li><strong>NLP Tasks:</strong> Speech act recognition (identifying the intent behind an utterance), sarcasm and irony detection, dialogue management.</li>
<li><strong>Example:</strong> Recognizing “Can you pass the salt?” as a request, not a question about ability. Interpreting “Great weather, isn’t it?” as sarcastic if said during a downpour.</li>
</ul>
</section>
</section>
<section id="levels-and-applications-of-nlp" class="level1">
<h1>Levels and Applications of NLP</h1>
<p>This section details the various levels at which Natural Language Processing (NLP) operates and the corresponding tasks and applications at each level. The levels range from the smallest units of characters to large collections of documents.</p>
<section id="character-string-level" class="level2">
<h2 class="anchored" data-anchor-id="character-string-level">1. Character &amp; String Level</h2>
<p>This level deals with individual characters and strings of characters. It forms the foundation for higher-level NLP tasks.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Word Tokenization:</strong> Segmenting text into individual words (tokens). Example: “This is a sentence.” becomes [“This”, “is”, “a”, “sentence”, “.”]. Tokenization can be complex due to ambiguities like hyphenated words or special characters.</li>
<li><strong>Sentence Boundary Detection:</strong> Identifying the end of sentences, crucial for parsing and understanding. Challenges include abbreviations (e.g., “Dr.”) and sentence fragments.</li>
<li><strong>Gene Symbol Recognition:</strong> In bioinformatics, identifying specific gene symbols within text. This requires specialized knowledge of gene nomenclature.</li>
<li><strong>Text Pattern Extraction:</strong> Identifying and extracting specific patterns within text, like email addresses, phone numbers, or other structured information using regular expressions.</li>
</ul></li>
</ul>
</section>
<section id="word-token-level" class="level2">
<h2 class="anchored" data-anchor-id="word-token-level">2. Word Token Level</h2>
<p>This level focuses on individual words (tokens) and their properties.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Part-of-Speech (POS) Tagging:</strong> Assigning grammatical tags (e.g., noun, verb, adjective) to each word. Example: “The quick brown fox jumps.” becomes [“The/DET”, “quick/ADJ”, “brown/ADJ”, “fox/NOUN”, “jumps/VERB”]. Ambiguity can arise (e.g., “run” can be a noun or verb).</li>
<li><strong>Parsing:</strong> Analyzing the grammatical structure of a sentence, including identifying phrases and their relationships. Different parsing techniques exist like constituency parsing and dependency parsing.</li>
<li><strong>Chunking:</strong> Grouping words into meaningful phrases (chunks), often used as a pre-processing step for other tasks.</li>
<li><strong>Term Extraction:</strong> Identifying important terms or keywords within text, useful for indexing and information retrieval.</li>
<li><strong>Gene Mention Recognition:</strong> Similar to gene symbol recognition, but focusing on mentions of genes, which may be described in various ways.</li>
</ul></li>
</ul>
</section>
<section id="sentence-level" class="level2">
<h2 class="anchored" data-anchor-id="sentence-level">3. Sentence Level</h2>
<p>This level deals with individual sentences as complete units of meaning.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Sentence Classification:</strong> Categorizing sentences based on their meaning or intent (e.g., sentiment analysis, spam detection).</li>
<li><strong>Sentence Retrieval:</strong> Finding relevant sentences from a larger corpus based on a query.</li>
<li><strong>Sentence Ranking:</strong> Ordering sentences based on relevance, importance, or other criteria.</li>
<li><strong>Question Answering:</strong> Answering questions posed in natural language, requiring understanding of both the question and the relevant text.</li>
<li><strong>Automatic Summarization:</strong> Generating concise summaries of individual sentences or longer texts.</li>
</ul></li>
</ul>
</section>
<section id="sentence-window-level" class="level2">
<h2 class="anchored" data-anchor-id="sentence-window-level">4. Sentence Window Level</h2>
<p>This level examines relationships between adjacent sentences within a text, forming a “window” of context.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Anaphora Resolution:</strong> Resolving pronoun references to their correct antecedents in preceding sentences. Example: “John went to the store. He bought milk.” “He” refers to “John.”</li>
</ul></li>
</ul>
</section>
<section id="paragraph-passage-level" class="level2">
<h2 class="anchored" data-anchor-id="paragraph-passage-level">5. Paragraph &amp; Passage Level</h2>
<p>This level considers larger units of text, like paragraphs and passages, focusing on their internal structure and organization.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Detection of Rhetorical Zones:</strong> Identifying different sections within a text based on their rhetorical purpose (e.g., introduction, argument, conclusion).</li>
</ul></li>
</ul>
</section>
<section id="whole-document-level" class="level2">
<h2 class="anchored" data-anchor-id="whole-document-level">6. Whole Document Level</h2>
<p>This level analyzes entire documents as single units.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Document Similarity Calculation:</strong> Determining how similar two documents are based on their content, useful for plagiarism detection or information retrieval. Various similarity measures exist, such as cosine similarity using word embeddings. If documents <span class="math inline">\(D_1\)</span> and <span class="math inline">\(D_2\)</span> have word embedding vectors <span class="math inline">\(V_1\)</span> and <span class="math inline">\(V_2\)</span> respectively, the cosine similarity is calculated as:</li>
</ul></li>
</ul>
<p><span class="math display">\[
\text{Similarity}(D_1, D_2) = \frac{V_1 \cdot V_2}{||V_1|| \times ||V_2||}
\]</span></p>
</section>
<section id="multi-document-collection-level" class="level2">
<h2 class="anchored" data-anchor-id="multi-document-collection-level">7. Multi-Document Collection Level</h2>
<p>This level deals with collections of documents, often large corpora.</p>
<ul>
<li><strong>Tasks/Applications:</strong>
<ul>
<li><strong>Document Clustering:</strong> Grouping similar documents together based on their content.</li>
<li><strong>Multi-Document Summarization:</strong> Generating a summary that captures the key information from multiple documents on a related topic.</li>
</ul></li>
</ul>
<p>This hierarchical structure emphasizes the building-block nature of NLP, with each level contributing to more complex understanding and capabilities.</p>
</section>
</section>
<section id="early-beginnings-of-nlp-1950s-1970s" class="level1">
<h1>Early Beginnings of NLP (1950s-1970s)</h1>
<ul>
<li><p><strong>Early Machine Translation (1950s):</strong> Driven by the Cold War’s need for Russian translation, initial MT efforts were primarily rule-based, relying on dictionaries and basic syntactic transformations. These systems struggled with the complexities of language, often producing literal and inaccurate translations. The Georgetown-IBM experiment (1954) demonstrated a rudimentary system translating a small set of Russian sentences into English.</p></li>
<li><p><strong>Warren Weaver’s Memorandum (1949):</strong> Weaver’s memorandum, inspired by wartime code-breaking and information theory, proposed using statistical and cryptographic techniques for machine translation. This laid some of the groundwork for later statistical approaches. He suggested that translation could be viewed as a decryption problem, mapping one language onto another using probabilities and contextual analysis.</p></li>
<li><p><strong>Chomsky’s Influence (1957):</strong> Noam Chomsky’s work on generative grammar, particularly his book <em>Syntactic Structures</em>, significantly impacted linguistics and, subsequently, NLP. His theories on the hierarchical structure of language and the existence of universal grammar provided a new framework for understanding syntax. While not directly applicable to early computational systems due to computational limitations at the time, it laid the theoretical foundation for later rule-based parsing approaches.</p></li>
<li><p><strong>ELIZA and SHRDLU (1960s-1970s):</strong> ELIZA, a program simulating a Rogerian psychotherapist, demonstrated the possibility of creating human-computer dialogue, although its understanding was superficial, based on pattern matching and keyword identification. SHRDLU, developed by Terry Winograd, operated within a limited “blocks world” and could understand and execute commands related to moving virtual blocks, showcasing more advanced natural language understanding in a constrained environment.</p></li>
<li><p><strong>The ALPAC Report (1966) and the First AI Winter:</strong> The ALPAC report, commissioned by the US government, assessed the progress of MT research and found that it fell short of expectations. This led to a significant decrease in funding for NLP research, marking the beginning of the first AI winter. The report criticized the focus on fully automatic high-quality MT, advocating instead for investment in computational linguistics research and human-aided translation tools.</p></li>
<li><p><strong>Conceptual Dependency Theory (1970s):</strong> Developed by Roger Schank, Conceptual Dependency Theory aimed to represent the meaning of sentences in a canonical form, focusing on the actions and relationships between concepts rather than surface-level syntax. This approach aimed to enable deeper semantic understanding and inference, influencing early work on natural language understanding. It represented sentences as a series of primitive actions, enabling a degree of generalization across different sentence structures expressing similar meanings.</p></li>
<li><p><strong>Limitations of Early Systems:</strong> Early NLP systems faced significant limitations due to limited computing power, lack of large datasets, and the inherent complexity of natural language. Rule-based systems were brittle and difficult to scale, requiring extensive manual effort to encode linguistic rules. The lack of robust statistical methods and data hampered the development of more data-driven approaches.</p></li>
</ul>
</section>
<section id="nlp-in-the-1980s" class="level1">
<h1>NLP in the 1980s</h1>
<ul>
<li><p><strong>Dominance of Symbolic Approaches:</strong> The 1980s saw the rise of symbolic AI, where knowledge was represented through symbols and manipulated using logic and rules. This heavily influenced NLP, leading to the development of <strong>expert systems</strong>.</p></li>
<li><p><strong>Expert Systems:</strong> These systems aimed to emulate the decision-making of human experts in specific domains. They relied on:</p>
<ul>
<li><strong>Knowledge Bases:</strong> Structured repositories of facts, rules, and relationships within a domain, often represented using formal logic (e.g., predicate logic). These knowledge bases were carefully crafted by domain experts.</li>
<li><strong>Inference Engines:</strong> Algorithms that used the knowledge base to deduce new information or answer queries. Common inference methods included forward chaining and backward chaining.</li>
<li><strong>Natural Language Interfaces:</strong> While not the primary focus, some expert systems incorporated basic natural language interfaces to allow users to interact with them using more natural language-like input, although these were often limited in their capabilities.</li>
</ul></li>
<li><p><strong>Focus on Rule-Based Systems:</strong> NLP systems in the 1980s predominantly employed hand-crafted rules for various tasks like parsing, part-of-speech tagging, and semantic analysis. These rules were based on linguistic theories and encoded expert knowledge about language.</p></li>
<li><p><strong>Limitations of Expert Systems:</strong> Although initially promising, expert systems faced several limitations:</p>
<ul>
<li><strong>Knowledge Acquisition Bottleneck:</strong> Building and maintaining knowledge bases was a labor-intensive and time-consuming process, requiring significant effort from domain experts. This limited their scalability to broader domains or languages.</li>
<li><strong>Brittleness:</strong> Expert systems struggled with ambiguity and unexpected input. They could only handle situations explicitly covered in their knowledge bases and lacked the flexibility of human language understanding.</li>
<li><strong>Lack of Robustness:</strong> The hand-crafted rules were often domain-specific and brittle, failing to generalize well to different contexts or variations in language use.</li>
</ul></li>
<li><p><strong>Early Machine Learning Influence:</strong> Although statistically-driven methods did not come to the forefront until the late 1980s and 1990s, certain applications in the 1980s began to incorporate techniques like decision trees and probabilistic models to address some of these limitations by allowing systems to learn from data.</p></li>
</ul>
</section>
<section id="nlp-late-1980s---2000s" class="level1">
<h1>NLP (Late 1980s - 2000s)</h1>
<ul>
<li><p><strong>Transition to Statistical Methods (Late 1980s - Early 1990s):</strong> A significant shift occurred from rule-based systems to statistical models. This was driven by the limitations of hand-crafted rules and the increasing availability of large text corpora. Probabilistic models like Hidden Markov Models (HMMs) were applied to tasks such as part-of-speech tagging. <span class="math inline">\(P(t_i | t_{i-1})\)</span> became a key concept, representing the probability of a tag given the previous tag.</p></li>
<li><p><strong>Rise of Machine Learning:</strong> Machine learning algorithms, particularly statistical learning methods, became central to NLP. These methods allowed models to learn patterns and rules from data, rather than relying on explicit programming. For example, in probabilistic parsing, <span class="math inline">\(P(tree | sentence)\)</span> is estimated from a treebank, a corpus of parsed sentences.</p></li>
<li><p><strong>Early Neural Networks:</strong> While computationally limited at the time, research explored the use of neural networks, especially recurrent neural networks (RNNs), for language modeling. Simple feedforward networks were also used for tasks like text classification. However, training these early networks was challenging due to limited computational resources and the vanishing gradient problem in RNNs.</p></li>
<li><p><strong>Word Embeddings and Distributional Semantics:</strong> The foundation for modern word embeddings was laid during this period. Methods like Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) were developed to capture semantic relationships between words by analyzing their co-occurrence patterns in large text corpora. These methods represented words as vectors in a high-dimensional space, where semantically similar words were closer to each other.</p></li>
<li><p><strong>Statistical Machine Translation (SMT):</strong> SMT systems, based on statistical models trained on large parallel corpora (texts translated between two languages), became increasingly sophisticated. These models used concepts like alignment models to map words and phrases between source and target languages, maximizing the probability of the target sentence given the source sentence: <span class="math inline">\(argmax_{t} P(t|s)\)</span>. Phrase-based and hierarchical phrase-based models improved translation quality significantly.</p></li>
<li><p><strong>Growth of Annotated Corpora:</strong> The development of large annotated corpora, such as the Penn Treebank for syntactic parsing and PropBank for semantic role labeling, played a crucial role in advancing statistical NLP. These resources provided training data for supervised machine learning algorithms.</p></li>
<li><p><strong>Emergence of Open-Source NLP Tools:</strong> The late 1990s and 2000s saw the emergence of open-source NLP tools and libraries, making NLP research and development more accessible.</p></li>
</ul>
</section>
<section id="nlp-2010s---present" class="level1">
<h1>NLP (2010s - Present)</h1>
<ul>
<li><p><strong>Sequence-to-Sequence Models</strong>: The encoder-decoder model, a general framework for sequence-to-sequence tasks like machine translation, gained prominence. The encoder maps an input sequence to a fixed-length vector representation, and the decoder generates an output sequence based on this representation.</p></li>
<li><p><strong>Attention Mechanism</strong>: The introduction of the attention mechanism revolutionized NLP. Attention allows the model to focus on different parts of the input sequence when generating each element of the output sequence. For example, in machine translation, when translating the word “chat,” the model might attend more to the French word “chat” (cat) than other words in the input sentence. The attention weight <span class="math inline">\(a_{ij}\)</span> quantifies the relationship between the <span class="math inline">\(i\)</span>-th output element and the <span class="math inline">\(j\)</span>-th input element.</p></li>
<li><p><strong>Transformer Models</strong>: The transformer architecture, based solely on attention mechanisms, eliminated the need for recurrent or convolutional layers. Transformers use self-attention to relate different positions within a single sequence to compute a representation of that sequence. This is particularly effective for capturing long-range dependencies. The attention function can be described as:</p></li>
</ul>
<p><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]</span></p>
<p>where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> are matrices representing queries, keys, and values respectively, and <span class="math inline">\(d_k\)</span> is the dimension of the keys.</p>
<ul>
<li><p><strong>Pre-trained Language Models</strong>: Large language models (LLMs) like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) are trained on massive text datasets using self-supervised learning objectives, such as masked language modeling (predicting masked words) or next sentence prediction. These pre-trained models can then be fine-tuned for specific downstream tasks with relatively small amounts of task-specific data, achieving state-of-the-art results across a wide range of NLP tasks.</p></li>
<li><p><strong>Transfer Learning</strong>: The paradigm of transfer learning, where knowledge learned from one task is transferred to another, became central to NLP. Pre-training on large datasets allows models to learn general language representations that are beneficial for a variety of downstream tasks.</p></li>
<li><p><strong>Few-Shot and Zero-Shot Learning</strong>: With the advent of powerful LLMs, few-shot learning (adapting to new tasks with limited examples) and zero-shot learning (performing tasks without any examples) have become increasingly viable research directions. This allows for application of NLP to scenarios with limited labeled data.</p></li>
<li><p><strong>Multilingual and Cross-lingual Models</strong>: Multilingual models, trained on data from multiple languages, enable cross-lingual transfer learning, where knowledge learned from high-resource languages can be applied to low-resource languages.</p></li>
</ul>
</section>
<section id="key-differences-between-human-and-machine-nlp" class="level1">
<h1>Key Differences Between Human and Machine NLP</h1>
<section id="human" class="level2">
<h2 class="anchored" data-anchor-id="human">Human</h2>
<ul>
<li><strong>Nuance and Implied Meaning:</strong> Humans excel at understanding subtle nuances like sarcasm, humor, and metaphors, which often rely on complex contextual understanding and shared cultural knowledge. Machines struggle with these aspects, often interpreting language literally.</li>
<li><strong>Creativity and Originality:</strong> Humans can generate novel and creative text formats, styles, and content, whereas machines primarily rely on existing data patterns and struggle with true originality. Think of stylistic elements like alliteration or assonance, where subtle phonetic patterns create an aesthetic effect.</li>
<li><strong>Adaptability and Generalization:</strong> Humans readily adapt to new linguistic contexts, dialects, and even entirely new languages with relatively limited exposure. Machines require substantial retraining and data for each new context. Consider the ease with which a human can understand code-switching compared to a machine.</li>
<li><strong>Emotional Range and Empathy:</strong> Human language understanding is deeply intertwined with emotion and empathy. We can perceive emotional undertones and respond appropriately. Machines lack this emotional depth, hindering their ability to engage in truly empathetic communication.</li>
<li><strong>World Knowledge and Reasoning:</strong> Humans possess extensive world knowledge and reasoning abilities, enabling them to understand complex cause-and-effect relationships, draw inferences, and resolve ambiguities effectively. Machine reasoning is often limited by the data they are trained on and struggles with scenarios requiring real-world knowledge. For example, understanding a sentence like “The politician’s shady dealings led to his downfall” requires understanding the concept of consequences and societal norms.</li>
<li><strong>Intuitive Grasp of Grammar:</strong> Humans develop an intuitive understanding of grammar, even without formal training. This allows us to generate and interpret grammatically complex and sometimes even incorrect sentences, recognizing intent despite errors. Machines often struggle with deviations from standard grammar, as their knowledge is based on pre-defined rules.</li>
</ul>
</section>
<section id="machine" class="level2">
<h2 class="anchored" data-anchor-id="machine">Machine</h2>
<ul>
<li><strong>Computational Speed and Scale:</strong> Machines can process vast amounts of textual data orders of magnitude faster than humans. This enables them to perform tasks like large-scale document analysis, information retrieval, and statistical language modeling efficiently. Consider analyzing millions of tweets for sentiment analysis—a task infeasible for humans.</li>
<li><strong>Pattern Recognition and Statistical Analysis:</strong> Machines excel at identifying complex statistical patterns in language data, allowing them to perform tasks like predicting the next word in a sequence, identifying topic clusters, or detecting plagiarism. Human pattern recognition abilities are comparatively limited in scale and speed. Consider topic modeling, where machines can discover latent themes across a large collection of documents.</li>
<li><strong>Consistency and Objectivity:</strong> Machines offer consistent performance, unaffected by factors like fatigue or bias (assuming the training data is unbiased). This is crucial for tasks requiring objective analysis, like automated essay grading or legal document review. Humans are more susceptible to subjective biases and inconsistencies.</li>
<li><strong>Automation and Scalability:</strong> Machines can automate tedious and repetitive NLP tasks, like translating documents, generating summaries, or extracting key information from text. This scalability is essential for handling large volumes of data in real-world applications. Consider automating customer support through chatbots.</li>
<li><strong>Multilingual Capabilities:</strong> Machines can be trained to handle multiple languages, facilitating tasks like cross-lingual information retrieval and machine translation. While humans can also learn multiple languages, machines can operate across a wider range of languages more efficiently. Consider real-time translation services supporting dozens of languages.</li>
<li><strong>Precise Mathematical Representation:</strong> Machines operate on precise mathematical representations of language, like word embeddings and distributional semantics. This allows for quantifiable analysis and comparison of linguistic elements, enabling tasks like semantic similarity calculations. For example, measuring the cosine similarity between word vectors can determine the relatedness of words like “king” and “queen”. Human semantic understanding is more intuitive and difficult to quantify mathematically. A simple representation would be: <span class="math inline">\(similarity(king, queen) = cos(\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is the angle between the vectors representing “king” and “queen”.</li>
</ul>
</section>
</section>
<section id="review-questions" class="level1">
<h1>Review Questions</h1>
<p><strong>Core Technologies &amp; Applications:</strong></p>
<ol type="1">
<li>What is Language Modeling and how is it applied in NLP tasks like machine translation? Explain the concept of n-gram models and how they estimate probability. How do neural language models differ?</li>
<li>Why is Part-of-Speech tagging important for downstream NLP tasks? How is tagging accuracy typically measured?</li>
<li>Explain the difference between constituency parsing and dependency parsing. Provide examples.</li>
<li>Define Named Entity Recognition (NER) and its significance in Information Extraction. What are some common evaluation metrics used for NER?</li>
<li>How does Coreference Resolution improve text understanding and what role does it play in tasks like summarization?</li>
<li>What challenges does Word Sense Disambiguation (WSD) address? Describe a simplified probabilistic approach for WSD.</li>
<li>Explain Semantic Role Labeling (SRL) and provide an example illustrating its usage.</li>
<li>What are the key differences between Statistical Machine Translation (SMT) and Neural Machine Translation (NMT)?</li>
<li>How is Information Retrieval (IR) evaluated? Briefly describe different IR techniques.</li>
<li>What are the main challenges in building effective Dialogue Systems?</li>
<li>Describe how sentiment analysis is used in practical applications.</li>
</ol>
<p><strong>Why NLP is Hard:</strong></p>
<ol type="1">
<li>Explain the different types of ambiguity in natural language, providing clear examples for each type. How do these ambiguities pose challenges for NLP systems?</li>
<li>Discuss the impact of linguistic diversity on NLP. How do morphological variations, syntactic variations, and semantic variations across languages affect NLP model development?</li>
<li>Explain the challenge of data sparsity in NLP. How does it hinder the development of robust models, and what techniques can be used to address this issue? What is cross-lingual transfer learning and how can it be applied to data sparsity problems?</li>
<li>How does the dynamic nature of language, with its constant evolution and variability, pose challenges for NLP? Explain the concepts of dialectal variation and how it can be addressed in NLP.</li>
<li>Discuss the computational complexity challenges faced by many NLP tasks, particularly those employing deep learning models. What strategies can be used to mitigate these challenges?</li>
</ol>
<p><strong>Ambiguity in Natural Language:</strong></p>
<ol type="1">
<li>Differentiate between homonymy, polysemy, homographs, and homophones with examples. Why are they important to consider in NLP tasks?</li>
<li>Explain Prepositional Phrase Attachment and Coordination Ambiguity with examples. How can these ambiguities be represented mathematically or logically?</li>
<li>How does quantifier scope ambiguity affect the meaning of sentences? Provide examples and explain how formal semantic representations can be used to address this type of ambiguity.</li>
<li>Describe the challenges posed by deictic expressions and speech act ambiguity in NLP. Provide examples. Why is context crucial for resolving pragmatic ambiguity?</li>
</ol>
<p><strong>Challenges in Multilingual NLP:</strong></p>
<ol type="1">
<li>Explain how data sparsity is quantified and modeled in the context of multilingual NLP. Discuss the implication of the power law distribution of language data.</li>
<li>How does morphological complexity affect NLP tasks? Provide examples of agglutinative languages and discuss the computational challenges they present. How can finite-state transducers (FSTs) be helpful?</li>
<li>Describe the impact of syntactic divergence on NLP tasks like cross-lingual parsing. Provide examples of word order differences across languages. How can treebanks mitigate these issues?</li>
<li>Explain the challenges of semantic variation in multilingual word sense disambiguation and cross-lingual word embeddings. How can bilingual dictionaries or parallel corpora help align embedding spaces?</li>
<li>Why is the lack of linguistic resources a major challenge in multilingual NLP? Discuss the difficulties in creating resources manually and the impact on NLP tool development.</li>
<li>What is negative transfer in cross-lingual transfer learning and under what circumstances can it occur?</li>
</ol>
<p><strong>Indian Language Data and Resources:</strong></p>
<ol type="1">
<li>Explain the unique challenges posed by code-switching in Indian languages for NLP tasks. What specialized datasets or techniques are needed to address these?</li>
<li>Discuss the impact of diverse scripts used for Indian languages on NLP preprocessing steps like tokenization and transliteration.</li>
<li>Why is dialectal variation within Indian languages a challenge for NLP model development? Discuss the need for multi-dialect training or dialect adaptation techniques.</li>
<li>What are the specific NLP challenges presented by oral languages with limited written resources? Why are speech technologies important in these scenarios?</li>
<li>How can the development of language-specific NLP tools (like morphological analyzers) contribute to the advancement of NLP for Indian languages?</li>
<li>How can the type-token ratio (TTR) be used to assess data sparsity in Indian languages?</li>
</ol>
<p><strong>Language Variability Across Speakers:</strong></p>
<ol type="1">
<li>How do dialects vary at different linguistic levels (phonetics, morphology, syntax, lexicon)? Provide examples.</li>
<li>What social factors influence the development of sociolects? Provide examples of how social class, age, ethnicity, gender, and occupation can affect language use.</li>
<li>How do idiolects arise from personal experiences, personality traits, speech habits, and physical/cognitive factors?</li>
<li>Why do dialects, sociolects, and idiolects present challenges for NLP tasks like speech recognition and NLU? How can these be addressed?</li>
</ol>
<p><strong>Levels of Language Processing in NLP:</strong></p>
<ol type="1">
<li>Describe the focus of the phonological level and its relevance to specific NLP tasks. How do phonemes and prosody play a role?</li>
<li>Explain the focus of the morphological level and its importance for tasks like stemming and lemmatization. Provide an example of morphological analysis.</li>
<li>What is the lexical level concerned with? How is word sense disambiguation (WSD) relevant at this level?</li>
<li>Describe the syntactic level and its role in parsing. Explain the difference between constituency and dependency parsing in the context of an example sentence.</li>
<li>What is the focus of the semantic level? Explain how semantic role labeling (SRL) and named entity recognition (NER) operate at this level.</li>
<li>Explain the discourse level and its relevance to tasks like anaphora resolution and text summarization. Provide an example.</li>
<li>What does the pragmatic level deal with in NLP? How is it relevant for understanding speech acts and detecting sarcasm or irony? Give examples.</li>
</ol>
<p><strong>Levels and Applications of NLP:</strong></p>
<ol type="1">
<li>Explain the tasks and applications relevant to the Character &amp; String level of NLP. Discuss the challenges of tokenization and sentence boundary detection.</li>
<li>Describe the tasks and applications at the Word Token level, focusing on POS tagging and parsing. Why is ambiguity a challenge at this level?</li>
<li>What are the key applications of NLP at the Sentence Level? Provide examples for sentence classification, retrieval, ranking, question answering, and summarization.</li>
<li>Explain the importance of the Sentence Window level for tasks like anaphora resolution. Provide a clear example.</li>
<li>What is the focus of the Paragraph &amp; Passage level? How is it used for detecting rhetorical zones?</li>
<li>Describe how document similarity is calculated at the Whole Document level. Explain the cosine similarity formula in the context of document vectors.</li>
<li>What tasks and applications are addressed at the Multi-Document Collection level? Provide examples for document clustering and multi-document summarization.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>