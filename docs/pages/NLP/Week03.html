<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>POS, NER, and Sequence Modelling – BS Degree Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-sans.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">NLP</a></li><li class="breadcrumb-item"><a href="../../pages/NLP/Week03.html">Week 3</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 12</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">NLP</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week03.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">LLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformation-based-tagging-tbl" id="toc-transformation-based-tagging-tbl" class="nav-link active" data-scroll-target="#transformation-based-tagging-tbl">Transformation-based Tagging (TBL)</a>
  <ul class="collapse">
  <li><a href="#tbl-analogy-painting" id="toc-tbl-analogy-painting" class="nav-link" data-scroll-target="#tbl-analogy-painting">TBL Analogy: Painting</a></li>
  <li><a href="#transformation-rules" id="toc-transformation-rules" class="nav-link" data-scroll-target="#transformation-rules">Transformation Rules</a></li>
  <li><a href="#key-advantages-of-tbl" id="toc-key-advantages-of-tbl" class="nav-link" data-scroll-target="#key-advantages-of-tbl">Key Advantages of TBL</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  <li><a href="#brill-tagging" id="toc-brill-tagging" class="nav-link" data-scroll-target="#brill-tagging">Brill Tagging</a>
  <ul class="collapse">
  <li><a href="#key-features" id="toc-key-features" class="nav-link" data-scroll-target="#key-features">Key Features:</a></li>
  <li><a href="#input" id="toc-input" class="nav-link" data-scroll-target="#input">Input:</a></li>
  <li><a href="#rule-templates" id="toc-rule-templates" class="nav-link" data-scroll-target="#rule-templates">Rule Templates:</a></li>
  <li><a href="#rule-scoring-and-selection" id="toc-rule-scoring-and-selection" class="nav-link" data-scroll-target="#rule-scoring-and-selection">Rule Scoring and Selection:</a></li>
  <li><a href="#iteration-and-termination" id="toc-iteration-and-termination" class="nav-link" data-scroll-target="#iteration-and-termination">Iteration and Termination:</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output">Output:</a></li>
  <li><a href="#advantages" id="toc-advantages" class="nav-link" data-scroll-target="#advantages">Advantages:</a></li>
  <li><a href="#disadvantages" id="toc-disadvantages" class="nav-link" data-scroll-target="#disadvantages">Disadvantages:</a></li>
  <li><a href="#example-rule" id="toc-example-rule" class="nav-link" data-scroll-target="#example-rule">Example Rule:</a></li>
  </ul></li>
  <li><a href="#stochastic-tagging" id="toc-stochastic-tagging" class="nav-link" data-scroll-target="#stochastic-tagging">Stochastic Tagging</a>
  <ul class="collapse">
  <li><a href="#probability-types" id="toc-probability-types" class="nav-link" data-scroll-target="#probability-types">Probability Types</a></li>
  <li><a href="#simplifying-tag-sequence-probability-calculation" id="toc-simplifying-tag-sequence-probability-calculation" class="nav-link" data-scroll-target="#simplifying-tag-sequence-probability-calculation">Simplifying Tag Sequence Probability Calculation</a></li>
  <li><a href="#applying-stochastic-tagging-for-a-new-sentence" id="toc-applying-stochastic-tagging-for-a-new-sentence" class="nav-link" data-scroll-target="#applying-stochastic-tagging-for-a-new-sentence">Applying Stochastic Tagging for a New Sentence</a></li>
  <li><a href="#common-stochastic-tagging-models" id="toc-common-stochastic-tagging-models" class="nav-link" data-scroll-target="#common-stochastic-tagging-models">Common Stochastic Tagging Models</a></li>
  </ul></li>
  <li><a href="#named-entity-recognition-ner" id="toc-named-entity-recognition-ner" class="nav-link" data-scroll-target="#named-entity-recognition-ner">Named Entity Recognition (NER)</a>
  <ul class="collapse">
  <li><a href="#named-entity-types" id="toc-named-entity-types" class="nav-link" data-scroll-target="#named-entity-types">Named Entity Types</a></li>
  <li><a href="#ner-tagging-schemes" id="toc-ner-tagging-schemes" class="nav-link" data-scroll-target="#ner-tagging-schemes">NER Tagging Schemes</a></li>
  <li><a href="#features-for-ner" id="toc-features-for-ner" class="nav-link" data-scroll-target="#features-for-ner">Features for NER</a></li>
  <li><a href="#ner-models-and-algorithms" id="toc-ner-models-and-algorithms" class="nav-link" data-scroll-target="#ner-models-and-algorithms">NER Models and Algorithms</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics">Evaluation Metrics</a></li>
  <li><a href="#challenges-in-ner" id="toc-challenges-in-ner" class="nav-link" data-scroll-target="#challenges-in-ner">Challenges in NER</a></li>
  <li><a href="#applications-of-ner" id="toc-applications-of-ner" class="nav-link" data-scroll-target="#applications-of-ner">Applications of NER</a></li>
  </ul></li>
  <li><a href="#sequence-labeling-and-algorithms" id="toc-sequence-labeling-and-algorithms" class="nav-link" data-scroll-target="#sequence-labeling-and-algorithms">Sequence Labeling and Algorithms</a>
  <ul class="collapse">
  <li><a href="#hidden-markov-models-hmms" id="toc-hidden-markov-models-hmms" class="nav-link" data-scroll-target="#hidden-markov-models-hmms">Hidden Markov Models (HMMs)</a></li>
  <li><a href="#conditional-random-fields-crfs-maximum-entropy-markov-models-memms" id="toc-conditional-random-fields-crfs-maximum-entropy-markov-models-memms" class="nav-link" data-scroll-target="#conditional-random-fields-crfs-maximum-entropy-markov-models-memms">Conditional Random Fields (CRFs) / Maximum Entropy Markov Models (MEMMs)</a></li>
  <li><a href="#supervised-machine-learning" id="toc-supervised-machine-learning" class="nav-link" data-scroll-target="#supervised-machine-learning">Supervised Machine Learning</a></li>
  <li><a href="#neural-sequence-models" id="toc-neural-sequence-models" class="nav-link" data-scroll-target="#neural-sequence-models">Neural Sequence Models</a></li>
  <li><a href="#large-language-models-llms" id="toc-large-language-models-llms" class="nav-link" data-scroll-target="#large-language-models-llms">Large Language Models (LLMs)</a></li>
  </ul></li>
  <li><a href="#challenges-in-ner-tagging" id="toc-challenges-in-ner-tagging" class="nav-link" data-scroll-target="#challenges-in-ner-tagging">Challenges in NER Tagging</a>
  <ul class="collapse">
  <li><a href="#challenges-in-indian-language-ner-detailed" id="toc-challenges-in-indian-language-ner-detailed" class="nav-link" data-scroll-target="#challenges-in-indian-language-ner-detailed">Challenges in Indian Language NER (Detailed)</a></li>
  </ul></li>
  <li><a href="#evaluation-of-ner-systems" id="toc-evaluation-of-ner-systems" class="nav-link" data-scroll-target="#evaluation-of-ner-systems">Evaluation of NER Systems</a></li>
  <li><a href="#sequence-modeling" id="toc-sequence-modeling" class="nav-link" data-scroll-target="#sequence-modeling">Sequence Modeling</a>
  <ul class="collapse">
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts</a></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges">Challenges</a></li>
  <li><a href="#common-approaches" id="toc-common-approaches" class="nav-link" data-scroll-target="#common-approaches">Common Approaches</a></li>
  </ul></li>
  <li><a href="#hidden-markov-models-hmm" id="toc-hidden-markov-models-hmm" class="nav-link" data-scroll-target="#hidden-markov-models-hmm">Hidden Markov Models (HMM)</a>
  <ul class="collapse">
  <li><a href="#components" id="toc-components" class="nav-link" data-scroll-target="#components">Components</a></li>
  <li><a href="#hmm-notation" id="toc-hmm-notation" class="nav-link" data-scroll-target="#hmm-notation">HMM Notation</a></li>
  <li><a href="#how-hmm-works" id="toc-how-hmm-works" class="nav-link" data-scroll-target="#how-hmm-works">How HMM Works</a></li>
  <li><a href="#using-hmm-for-pos-tagging" id="toc-using-hmm-for-pos-tagging" class="nav-link" data-scroll-target="#using-hmm-for-pos-tagging">Using HMM for POS Tagging</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  <li><a href="#advantages-of-hmms-for-pos-tagging" id="toc-advantages-of-hmms-for-pos-tagging" class="nav-link" data-scroll-target="#advantages-of-hmms-for-pos-tagging">Advantages of HMMs for POS Tagging</a></li>
  <li><a href="#limitations-of-hmms-for-pos-tagging" id="toc-limitations-of-hmms-for-pos-tagging" class="nav-link" data-scroll-target="#limitations-of-hmms-for-pos-tagging">Limitations of HMMs for POS Tagging</a></li>
  </ul></li>
  <li><a href="#maximum-entropy-markov-model-memm" id="toc-maximum-entropy-markov-model-memm" class="nav-link" data-scroll-target="#maximum-entropy-markov-model-memm">Maximum Entropy Markov Model (MEMM)</a>
  <ul class="collapse">
  <li><a href="#key-features-1" id="toc-key-features-1" class="nav-link" data-scroll-target="#key-features-1">Key Features</a></li>
  <li><a href="#formulation" id="toc-formulation" class="nav-link" data-scroll-target="#formulation">Formulation</a></li>
  <li><a href="#advantages-1" id="toc-advantages-1" class="nav-link" data-scroll-target="#advantages-1">Advantages</a></li>
  <li><a href="#disadvantage" id="toc-disadvantage" class="nav-link" data-scroll-target="#disadvantage">Disadvantage</a></li>
  </ul></li>
  <li><a href="#conditional-random-fields-crf" id="toc-conditional-random-fields-crf" class="nav-link" data-scroll-target="#conditional-random-fields-crf">Conditional Random Fields (CRF)</a></li>
  <li><a href="#conclusion-hmm-memm-and-crf" id="toc-conclusion-hmm-memm-and-crf" class="nav-link" data-scroll-target="#conclusion-hmm-memm-and-crf">Conclusion: HMM, MEMM, and CRF</a></li>
  <li><a href="#importance-of-classification-models" id="toc-importance-of-classification-models" class="nav-link" data-scroll-target="#importance-of-classification-models">Importance of Classification Models</a></li>
  <li><a href="#classification-models-for-pos-and-ner-tasks" id="toc-classification-models-for-pos-and-ner-tasks" class="nav-link" data-scroll-target="#classification-models-for-pos-and-ner-tasks">Classification Models for POS and NER Tasks</a>
  <ul class="collapse">
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes">1. Naive Bayes</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">2. Logistic Regression</a></li>
  <li><a href="#clustering" id="toc-clustering" class="nav-link" data-scroll-target="#clustering">3. Clustering</a></li>
  </ul></li>
  <li><a href="#evaluation-metrics-1" id="toc-evaluation-metrics-1" class="nav-link" data-scroll-target="#evaluation-metrics-1">Evaluation Metrics</a></li>
  <li><a href="#classification-models-in-other-nlp-tasks" id="toc-classification-models-in-other-nlp-tasks" class="nav-link" data-scroll-target="#classification-models-in-other-nlp-tasks">Classification Models in Other NLP Tasks</a>
  <ul class="collapse">
  <li><a href="#text-classification" id="toc-text-classification" class="nav-link" data-scroll-target="#text-classification">Text Classification</a></li>
  <li><a href="#speech-recognition" id="toc-speech-recognition" class="nav-link" data-scroll-target="#speech-recognition">Speech Recognition</a></li>
  <li><a href="#machine-translation" id="toc-machine-translation" class="nav-link" data-scroll-target="#machine-translation">Machine Translation</a></li>
  <li><a href="#information-retrieval" id="toc-information-retrieval" class="nav-link" data-scroll-target="#information-retrieval">Information Retrieval</a></li>
  </ul></li>
  <li><a href="#review-questions" id="toc-review-questions" class="nav-link" data-scroll-target="#review-questions">Review Questions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">NLP</a></li><li class="breadcrumb-item"><a href="../../pages/NLP/Week03.html">Week 3</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">POS, NER, and Sequence Modelling</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="transformation-based-tagging-tbl" class="level2">
<h2 class="anchored" data-anchor-id="transformation-based-tagging-tbl">Transformation-based Tagging (TBL)</h2>
<ul>
<li>Developed by Eric Brill (1992).</li>
<li>Iteratively refines an initial tagging using transformation rules.</li>
<li><strong>Initial State:</strong> Assigns a basic tag (e.g., most frequent tag) to each word. This acts as a baseline tagging.</li>
<li><strong>Transformations:</strong> Hand-written rules designed to correct errors in the current tagging based on the context. These rules have the following form:
<ul>
<li><em>Change tag ‘a’ to ‘b’ when: some condition(s) is/are met.</em></li>
</ul></li>
<li><strong>Learning:</strong> TBL learns a sequence of rules from the data.
<ol type="1">
<li>The algorithm identifies the most common tagging errors.</li>
<li>It creates a transformation rule that, when applied to the corpus, corrects the most errors.</li>
<li>This rule is then added to the ordered rule set.</li>
<li>This process is repeated until a stopping criterion is reached (e.g., desired accuracy is achieved).</li>
</ol></li>
<li><strong>Output:</strong> An ordered list of transformation rules. These rules can then be applied to new, unseen text to predict the POS tags.</li>
</ul>
<section id="tbl-analogy-painting" class="level3">
<h3 class="anchored" data-anchor-id="tbl-analogy-painting">TBL Analogy: Painting</h3>
<p>TBL can be compared to painting where: - <strong>Initial tagging</strong> is like applying a base coat of paint to a canvas. It’s a starting point, but may not be perfect. - <strong>Transformation rules</strong> are like applying additional layers of paint to refine the image. Each rule corrects specific errors, gradually improving the overall picture.</p>
</section>
<section id="transformation-rules" class="level3">
<h3 class="anchored" data-anchor-id="transformation-rules">Transformation Rules</h3>
<ul>
<li>Rules are generally based on two types of evidence:
<ul>
<li><strong>Internal evidence:</strong> This refers to the morphological features of a word itself (e.g., prefixes, suffixes, capitalization). Example: <em>Change the tag to NN if the word ends in “-tion”</em></li>
<li><strong>Contextual evidence:</strong> This takes into account the surrounding words and their tags. Examples:
<ul>
<li><em>Change tag ‘a’ to ‘b’ if the previous word is tagged ‘c’.</em></li>
<li><em>Change tag ‘a’ to ‘b’ if the next word is “the”.</em></li>
<li><em>Change tag ‘a’ to ‘b’ if the previous word is a verb.</em></li>
</ul></li>
</ul></li>
<li><strong>Rule order is crucial.</strong> Applying rules in a different order can lead to different results, as one rule might “undo” the correction made by another rule.</li>
<li><strong>Cascading effects:</strong> A rule might correct one error but inadvertently introduce another. This necessitates further rules to rectify these new mistakes.</li>
</ul>
<section id="examples" class="level4">
<h4 class="anchored" data-anchor-id="examples">Examples</h4>
<ul>
<li><em>Change NN to NNS if the word has a suffix of length 1 and the suffix is ‘s’.</em> (Internal Evidence)</li>
<li><em>Change any tag to RB if the word has a suffix of length 2 and the suffix is ‘ly’.</em> (Internal Evidence)</li>
<li><em>Change VBN to VBD if the previous tag is NN.</em> (Contextual Evidence)</li>
<li><em>Change VBD to VBN if the next word is ‘by’.</em> (Contextual Evidence)</li>
<li><em>Change tag from TO to IN if the next word is a noun.</em> (Contextual Evidence)</li>
</ul>
</section>
</section>
<section id="key-advantages-of-tbl" class="level3">
<h3 class="anchored" data-anchor-id="key-advantages-of-tbl">Key Advantages of TBL</h3>
<ul>
<li><strong>Transparency:</strong> The rules are human-readable and easily understandable.</li>
<li><strong>Flexibility:</strong> TBL can accommodate a wide range of linguistic phenomena by crafting appropriate rules.</li>
<li><strong>Domain Specificity:</strong> Rules can be tailored to specific domains or genres of text.</li>
</ul>
</section>
<section id="limitations" class="level3">
<h3 class="anchored" data-anchor-id="limitations">Limitations</h3>
<ul>
<li><strong>Labor Intensive:</strong> Creating and maintaining a comprehensive rule set can be a manual and time-consuming task.</li>
<li><strong>Limited Generalization:</strong> Rules might overfit to the training data and perform poorly on unseen text with different linguistic patterns.</li>
<li><strong>Challenging for Complex Phenomena:</strong> Some complex linguistic dependencies may be difficult to capture accurately using simple transformation rules.</li>
</ul>
</section>
</section>
<section id="brill-tagging" class="level2">
<h2 class="anchored" data-anchor-id="brill-tagging">Brill Tagging</h2>
<p>Brill tagging, a hybrid approach, combines rule-based and stochastic methods for POS tagging. It leverages the power of rules to specify tagging behavior in certain contexts while using a data-driven approach to learn the most effective rules from a tagged corpus.</p>
<section id="key-features" class="level3">
<h3 class="anchored" data-anchor-id="key-features">Key Features:</h3>
<ul>
<li><strong>Data-Driven Rule Learning:</strong> Unlike purely rule-based taggers where rules are manually crafted, Brill tagging automatically learns rules from a training corpus.</li>
<li><strong>Ordered Rule Application:</strong> The learned rules are applied in a specific order, with each rule potentially correcting errors introduced by previous rules. This ordered application allows for capturing complex linguistic phenomena.</li>
<li><strong>Focus on Error Correction:</strong> Brill tagging focuses on iteratively improving an initial tagging by identifying and correcting the most frequent errors.</li>
</ul>
</section>
<section id="input" class="level3">
<h3 class="anchored" data-anchor-id="input">Input:</h3>
<ul>
<li><strong>Tagged Corpus:</strong> A corpus of text where each word is already assigned its correct POS tag. This serves as the training data from which the Brill tagger learns the rules.</li>
<li><strong>Dictionary:</strong> A dictionary that lists the most frequent POS tags associated with each word. This dictionary is used to provide an initial tagging to the corpus.</li>
</ul>
</section>
<section id="rule-templates" class="level3">
<h3 class="anchored" data-anchor-id="rule-templates">Rule Templates:</h3>
<p>The Brill tagger uses a set of predefined rule templates to generate potential transformation rules. These templates define the general structure of the rules, allowing for flexibility in capturing various linguistic patterns. Common rule templates include:</p>
<ul>
<li><strong>Lexical Rules:</strong> Change the tag of a word based on its own lexical features (e.g., “Change NN to VB if the word is ‘race’”).</li>
<li><strong>Contextual Rules:</strong> Change the tag of a word based on the tags or words surrounding it (e.g., “Change VBN to VBD if the previous tag is NN”).</li>
<li><strong>Morphological Rules:</strong> Change the tag of a word based on its morphology (e.g., “Change NN to NNS if the word ends in ‘s’”).</li>
</ul>
</section>
<section id="rule-scoring-and-selection" class="level3">
<h3 class="anchored" data-anchor-id="rule-scoring-and-selection">Rule Scoring and Selection:</h3>
<ul>
<li>For each rule template, the Brill tagger generates a set of candidate rules by instantiating the template with specific words or tags from the training corpus.</li>
<li>Each candidate rule is then scored based on its ability to improve the accuracy of the initial tagging on the training corpus.</li>
<li>The rule with the highest score (i.e., the rule that corrects the most errors) is selected and added to the ordered rule set.</li>
</ul>
</section>
<section id="iteration-and-termination" class="level3">
<h3 class="anchored" data-anchor-id="iteration-and-termination">Iteration and Termination:</h3>
<ul>
<li>The process of rule generation, scoring, and selection is repeated iteratively.</li>
<li>In each iteration, the corpus is re-tagged using the current rule set, and new candidate rules are generated based on the remaining errors.</li>
<li>The algorithm terminates when a stopping criterion is met. This could be:
<ul>
<li>A predefined number of iterations.</li>
<li>A threshold on the tagging accuracy achieved on the training corpus.</li>
<li>No further significant improvement in accuracy.</li>
</ul></li>
</ul>
</section>
<section id="output" class="level3">
<h3 class="anchored" data-anchor-id="output">Output:</h3>
<p>The output of the Brill tagging algorithm is an ordered list of transformation rules. These rules can be applied to new, unseen text to assign POS tags, starting from an initial tagging based on the dictionary. The ordered nature of the rules ensures that corrections made by earlier rules are taken into account by subsequent rules.</p>
</section>
<section id="advantages" class="level3">
<h3 class="anchored" data-anchor-id="advantages">Advantages:</h3>
<ul>
<li><strong>High Accuracy:</strong> Brill tagging often achieves high accuracy, comparable to or even exceeding purely statistical methods.</li>
<li><strong>Transparency and Interpretability:</strong> The learned rules are human-readable, making it possible to understand the linguistic patterns captured by the tagger.</li>
<li><strong>Flexibility:</strong> The rule templates allow for capturing diverse linguistic phenomena, making Brill tagging adaptable to different languages and domains.</li>
</ul>
</section>
<section id="disadvantages" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages">Disadvantages:</h3>
<ul>
<li><strong>Computational Cost:</strong> Rule learning can be computationally expensive, especially for large corpora and complex rule templates.</li>
<li><strong>Rule Ordering Sensitivity:</strong> The performance of the tagger can be sensitive to the order in which the rules are applied. Finding the optimal rule order can be challenging.</li>
</ul>
</section>
<section id="example-rule" class="level3">
<h3 class="anchored" data-anchor-id="example-rule">Example Rule:</h3>
<p>“Change NN to VB if the previous tag is TO”.</p>
<p>This rule illustrates how Brill tagging captures contextual information to improve tagging accuracy. The rule states that if a word is currently tagged as a noun (NN) and the preceding word is tagged as a “to” (infinitive marker), then the word’s tag should be changed to a verb (VB).</p>
</section>
</section>
<section id="stochastic-tagging" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-tagging">Stochastic Tagging</h2>
<ul>
<li>Uses probabilities to predict tags based on statistical properties learned from data.</li>
<li>Relies on a tagged corpus for training and probability estimation.</li>
</ul>
<section id="probability-types" class="level3">
<h3 class="anchored" data-anchor-id="probability-types">Probability Types</h3>
<section id="individual-word-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="individual-word-probabilities">Individual Word Probabilities</h4>
<ul>
<li>Represents the probability of a given tag <span class="math inline">\(t\)</span> being appropriate for a given word <span class="math inline">\(w\)</span>.</li>
<li>Calculated using the formula:</li>
</ul>
<p><span class="math display">\[
P(t|w) = \frac{f(t,w)}{f(w)}
\]</span></p>
<p>Where: - <span class="math inline">\(f(t, w)\)</span>: Frequency of word <span class="math inline">\(w\)</span> occurring with tag <span class="math inline">\(t\)</span> in the corpus. - <span class="math inline">\(f(w)\)</span>: Total frequency of word <span class="math inline">\(w\)</span> in the corpus.</p>
</section>
<section id="tag-sequence-probabilities" class="level4">
<h4 class="anchored" data-anchor-id="tag-sequence-probabilities">Tag Sequence Probabilities</h4>
<ul>
<li>Represents the probability of a specific sequence of tags <span class="math inline">\(t_1, t_2, ..., t_n\)</span> being suitable for a given word sequence <span class="math inline">\(w_1, w_2, ..., w_n\)</span>.</li>
<li>Can be expressed as:</li>
</ul>
<p><span class="math display">\[
P(t_1, t_2, ..., t_n | w_1, w_2, ..., w_n)
\]</span></p>
<ul>
<li>Direct computation of this probability for long sequences is computationally expensive.</li>
</ul>
</section>
</section>
<section id="simplifying-tag-sequence-probability-calculation" class="level3">
<h3 class="anchored" data-anchor-id="simplifying-tag-sequence-probability-calculation">Simplifying Tag Sequence Probability Calculation</h3>
<ul>
<li>Instead of calculating the full sequence probability directly, we utilize simplified models using shorter subsequences of tags.</li>
<li>This approach makes computation more tractable.</li>
<li>Common subsequence models include:</li>
</ul>
<section id="bigram-model" class="level4">
<h4 class="anchored" data-anchor-id="bigram-model">Bigram Model</h4>
<ul>
<li>Considers a sequence of two tags.</li>
<li>Probability of a tag sequence <span class="math inline">\(t_1, t_2\)</span> is:</li>
</ul>
<p><span class="math display">\[
P(t_1, t_2) = P(t_2 | t_1)
\]</span></p>
</section>
<section id="trigram-model" class="level4">
<h4 class="anchored" data-anchor-id="trigram-model">Trigram Model</h4>
<ul>
<li>Considers a sequence of three tags.</li>
<li>Probability of a tag sequence <span class="math inline">\(t_1, t_2, t_3\)</span> is:</li>
</ul>
<p><span class="math display">\[
P(t_1, t_2, t_3) = P(t_2 | t_1) \times P(t_3 | t_2)
\]</span></p>
</section>
<section id="n-gram-model" class="level4">
<h4 class="anchored" data-anchor-id="n-gram-model">N-gram Model</h4>
<ul>
<li>Generalization to sequences of <span class="math inline">\(N\)</span> tags.</li>
<li>Probability of a tag sequence <span class="math inline">\(t_1, t_2, ..., t_N\)</span> is:</li>
</ul>
<p><span class="math display">\[
P(t_1, t_2, ..., t_N) = \prod_{i=2}^N P(t_i | t_{i-1}, ..., t_{i-N+1})
\]</span></p>
</section>
</section>
<section id="applying-stochastic-tagging-for-a-new-sentence" class="level3">
<h3 class="anchored" data-anchor-id="applying-stochastic-tagging-for-a-new-sentence">Applying Stochastic Tagging for a New Sentence</h3>
<ul>
<li>Given a new sentence, the goal is to find the most likely tag sequence.</li>
<li>Achieved by considering the probabilities of different possible tag sequences and choosing the sequence with the highest probability.</li>
<li>The probabilities are calculated using the learned individual word probabilities and tag sequence probabilities (obtained from the n-gram models).</li>
</ul>
</section>
<section id="common-stochastic-tagging-models" class="level3">
<h3 class="anchored" data-anchor-id="common-stochastic-tagging-models">Common Stochastic Tagging Models</h3>
<ul>
<li><strong>Hidden Markov Model (HMM):</strong>
<ul>
<li>A generative probabilistic model that models tag sequences as hidden states and words as observable emissions.</li>
<li>Uses transition probabilities (between tags) and emission probabilities (word given a tag) to calculate the most likely tag sequence.</li>
</ul></li>
<li><strong>Maximum Entropy Markov Model (MEMM):</strong>
<ul>
<li>A discriminative probabilistic model that directly models the conditional probability of a tag sequence given a word sequence.</li>
<li>Uses features from the input data and the previous tag to predict the current tag.</li>
</ul></li>
<li><strong>Conditional Random Fields (CRF):</strong>
<ul>
<li>A discriminative probabilistic model that models the conditional probability of a tag sequence given a word sequence, considering the entire sequence globally.</li>
<li>Avoids the label bias problem of MEMM and allows for the inclusion of various features.</li>
</ul></li>
</ul>
</section>
</section>
<section id="named-entity-recognition-ner" class="level2">
<h2 class="anchored" data-anchor-id="named-entity-recognition-ner">Named Entity Recognition (NER)</h2>
<ul>
<li><strong>Task:</strong> Identifying and classifying named entities (e.g., person, organization, location) in text.</li>
</ul>
<section id="named-entity-types" class="level3">
<h3 class="anchored" data-anchor-id="named-entity-types">Named Entity Types</h3>
<ul>
<li><strong>Generic Types:</strong> Person (PER), Organization (ORG), Location (LOC), Geo-Political Entity (GPE).</li>
<li><strong>Domain-Specific Types:</strong> Can be more granular based on the application (e.g., product names, medical terms).</li>
<li><strong>Nested Entities:</strong> Entities can be nested within each other (e.g., “University of California, Berkeley” contains the entities “University of California” and “Berkeley”).</li>
</ul>
</section>
<section id="ner-tagging-schemes" class="level3">
<h3 class="anchored" data-anchor-id="ner-tagging-schemes">NER Tagging Schemes</h3>
<ul>
<li><strong>BIO Tagging:</strong>
<ul>
<li><strong>B:</strong> Beginning of entity.</li>
<li><strong>I:</strong> Inside entity.</li>
<li><strong>O:</strong> Outside any entity.</li>
</ul></li>
<li><strong>IO Tagging:</strong> Only I and O tags.</li>
<li><strong>BIOES Tagging:</strong>
<ul>
<li><strong>B:</strong> Beginning of entity.</li>
<li><strong>I:</strong> Inside entity.</li>
<li><strong>O:</strong> Outside any entity.</li>
<li><strong>E:</strong> End of entity.</li>
<li><strong>S:</strong> Single-token entity.</li>
</ul></li>
</ul>
</section>
<section id="features-for-ner" class="level3">
<h3 class="anchored" data-anchor-id="features-for-ner">Features for NER</h3>
<ul>
<li><strong>Lexical Features:</strong>
<ul>
<li>Words themselves.</li>
<li>Capitalization patterns.</li>
<li>Prefixes and suffixes.</li>
<li>Word shape (e.g., “1999” is a number, “iPhone” is camel case).</li>
</ul></li>
<li><strong>Contextual Features:</strong>
<ul>
<li>Part-of-speech tags of surrounding words.</li>
<li>Syntactic dependencies.</li>
<li>Words in a window around the target word.</li>
</ul></li>
<li><strong>External Knowledge:</strong>
<ul>
<li>Gazetteers (lists of known entities).</li>
<li>Word embeddings (capture semantic relationships).</li>
</ul></li>
</ul>
</section>
<section id="ner-models-and-algorithms" class="level3">
<h3 class="anchored" data-anchor-id="ner-models-and-algorithms">NER Models and Algorithms</h3>
<ul>
<li><strong>Rule-Based Systems:</strong> Use hand-crafted rules based on linguistic patterns and domain knowledge.</li>
<li><strong>Machine Learning-Based Systems:</strong>
<ul>
<li><strong>Feature-Based Models:</strong> Extract features and train a classifier (e.g., Naive Bayes, Logistic Regression, Support Vector Machines).</li>
<li><strong>Sequence Labeling Models:</strong>
<ul>
<li>Hidden Markov Models (HMMs)</li>
<li>Conditional Random Fields (CRFs)</li>
<li>Maximum Entropy Markov Models (MEMMs)</li>
</ul></li>
<li><strong>Deep Learning Models:</strong>
<ul>
<li>Recurrent Neural Networks (RNNs)</li>
<li>Long Short-Term Memory (LSTM) networks</li>
<li>Transformers (e.g., BERT)</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="evaluation-metrics" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metrics">Evaluation Metrics</h3>
<ul>
<li><strong>Entity-Level Metrics:</strong> Consider entire entities as units for evaluation.
<ul>
<li><strong>Precision:</strong> <span class="math inline">\(Precision = \frac{Correctly\ Identified\ Entities}{Total\ Identified\ Entities}\)</span></li>
<li><strong>Recall:</strong> <span class="math inline">\(Recall = \frac{Correctly\ Identified\ Entities}{Total\ Actual\ Entities}\)</span></li>
<li><strong>F1-Score:</strong> <span class="math inline">\(F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}\)</span></li>
</ul></li>
<li><strong>Token-Level Metrics:</strong> Evaluate performance at the individual token level. Can be less informative for NER.</li>
</ul>
</section>
<section id="challenges-in-ner" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-ner">Challenges in NER</h3>
<ul>
<li><strong>Ambiguity:</strong>
<ul>
<li>Same word can refer to different entities depending on context (e.g., “Washington”).</li>
<li>Overlapping entities.</li>
</ul></li>
<li><strong>Data Sparsity:</strong> Lack of labeled data, especially for specialized domains.</li>
<li><strong>Out-of-Vocabulary Words:</strong> Handling new or unseen entities.</li>
<li><strong>Entity Boundary Detection:</strong> Accurately identifying the start and end of entities.</li>
</ul>
</section>
<section id="applications-of-ner" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-ner">Applications of NER</h3>
<ul>
<li><strong>Information Extraction:</strong> Extracting structured information from unstructured text (e.g., populating a database).</li>
<li><strong>Question Answering:</strong> Identifying entities in questions and retrieving relevant answers.</li>
<li><strong>Text Summarization:</strong> Generating summaries that focus on key entities.</li>
<li><strong>Machine Translation:</strong> Improving translation quality by correctly identifying and translating named entities.</li>
<li><strong>Sentiment Analysis:</strong> Understanding sentiment towards specific entities.</li>
<li><strong>Knowledge Base Population:</strong> Automatically extracting entities and their relationships to build knowledge graphs.</li>
</ul>
</section>
</section>
<section id="sequence-labeling-and-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="sequence-labeling-and-algorithms">Sequence Labeling and Algorithms</h2>
<p>Sequence labeling in NER involves assigning a tag to each token in a sequence, indicating whether it is part of a named entity and, if so, what type of entity it belongs to. This is achieved using various algorithms, each with its own strengths and weaknesses:</p>
<section id="hidden-markov-models-hmms" class="level3">
<h3 class="anchored" data-anchor-id="hidden-markov-models-hmms">Hidden Markov Models (HMMs)</h3>
<p>HMMs are statistical models that assume a sequence of hidden states generates the observed data. In NER, the hidden states are the entity tags (e.g., PER, LOC, ORG), and the observed data is the sequence of words. HMMs use transition probabilities (probability of moving from one state to another) and emission probabilities (probability of emitting a word given a state) to model the sequence.</p>
</section>
<section id="conditional-random-fields-crfs-maximum-entropy-markov-models-memms" class="level3">
<h3 class="anchored" data-anchor-id="conditional-random-fields-crfs-maximum-entropy-markov-models-memms">Conditional Random Fields (CRFs) / Maximum Entropy Markov Models (MEMMs)</h3>
<p>CRFs and MEMMs are discriminative models that directly model the conditional probability of the tag sequence given the observed word sequence. They overcome some limitations of HMMs, such as the assumption of independence between observations. CRFs model the entire sequence jointly, considering dependencies between tags, while MEMMs focus on local tag predictions.</p>
<section id="mathematical-formulation-of-memm" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-of-memm">Mathematical Formulation of MEMM:</h4>
<p>The probability of a tag sequence <span class="math inline">\(T = t_1, t_2, ..., t_n\)</span> given a word sequence <span class="math inline">\(W = w_1, w_2, ..., w_n\)</span> is modeled as:</p>
<p><span class="math display">\[
P(T|W) = \prod_{i=1}^{n} P(t_i|t_{i-1}, w_i)
\]</span></p>
<p>where <span class="math inline">\(t_0\)</span> is a special start state. Each local probability <span class="math inline">\(P(t_i|t_{i-1}, w_i)\)</span> is modeled using a maximum entropy classifier that considers features of the current word <span class="math inline">\(w_i\)</span> and the previous tag <span class="math inline">\(t_{i-1}\)</span>.</p>
</section>
<section id="mathematical-formulation-of-crf" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation-of-crf">Mathematical Formulation of CRF:</h4>
<p>CRFs model the conditional probability of the entire tag sequence as:</p>
<p><span class="math display">\[
P(T|W) \propto \exp \left( \sum_{i=1}^{n} \sum_{k} w_k f_k(t_i, t_{i-1}, w_i) \right)
\]</span></p>
<p>where <span class="math inline">\(f_k(t_i, t_{i-1}, w_i)\)</span> are feature functions that capture relationships between tags and words, and <span class="math inline">\(w_k\)</span> are learned weights. The normalization factor ensures that the probabilities sum to 1.</p>
</section>
</section>
<section id="supervised-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-machine-learning">Supervised Machine Learning</h3>
<p>Traditional supervised machine learning algorithms, such as Support Vector Machines (SVMs) and decision trees, can also be used for sequence labeling. These algorithms learn a mapping from input features (e.g., word embeddings, part-of-speech tags) to output tags. However, they typically require careful feature engineering and may not capture long-range dependencies as effectively as HMMs, MEMMs, or CRFs.</p>
</section>
<section id="neural-sequence-models" class="level3">
<h3 class="anchored" data-anchor-id="neural-sequence-models">Neural Sequence Models</h3>
<p>Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, are well-suited for sequence labeling tasks. They can learn complex dependencies between words and tags over long sequences. Transformer networks, with their attention mechanism, have also shown remarkable performance in NER.</p>
</section>
<section id="large-language-models-llms" class="level3">
<h3 class="anchored" data-anchor-id="large-language-models-llms">Large Language Models (LLMs)</h3>
<p>LLMs, such as BERT and GPT, are pre-trained on massive text corpora and can be fine-tuned for NER. They leverage their vast knowledge of language and context to achieve state-of-the-art performance. Fine-tuning involves training the model on a smaller, labeled NER dataset to adapt its knowledge to the specific task.</p>
</section>
</section>
<section id="challenges-in-ner-tagging" class="level2">
<h2 class="anchored" data-anchor-id="challenges-in-ner-tagging">Challenges in NER Tagging</h2>
<ul>
<li><p><strong>Segmentation Ambiguity:</strong> Accurately identifying the beginning and end of named entities can be difficult. For instance, “New York City” is a single entity, but a naive system might incorrectly segment it as “New”, “York”, and “City”.</p></li>
<li><p><strong>Determining Entity Boundaries:</strong> Deciding what constitutes a named entity and where its boundaries lie can be subjective and context-dependent. For example, “the White House” might be a location or an organization depending on the context.</p></li>
<li><p><strong>Type Ambiguity:</strong> A single word or phrase can often represent multiple entity types. For example, “Apple” can be a fruit, a company, or a person’s name. Disambiguating these types requires understanding the surrounding context.</p></li>
<li><p><strong>Category Definitions and Metonymy:</strong> Defining clear boundaries between entity categories can be challenging due to overlapping concepts and figurative language use. For example, “Washington” can refer to a person, a city, a state, or even an organization (e.g., the Washington Redskins). Metonymy, using a word to represent a related concept (e.g., “the White House decided” to mean “the President decided”), further complicates categorization.</p></li>
<li><p><strong>Variation in Entity Expressions:</strong> Named entities can be expressed in different ways, including abbreviations, acronyms, and informal variations. For example, “International Business Machines”, “IBM”, and “Big Blue” all refer to the same company.</p></li>
<li><p><strong>Nested Entities:</strong> Entities can be nested within other entities, leading to complexity in tagging. For example, “The Department of Computer Science at Stanford University” contains nested entities: “Department of Computer Science” (organization) within “Stanford University” (organization).</p></li>
<li><p><strong>Data Sparsity:</strong> Limited training data for specific entity types, especially in specialized domains or for less-resourced languages, can lead to poor performance in recognizing those entities.</p></li>
<li><p><strong>Noisy Data:</strong> Real-world text often contains errors, inconsistencies, and informal language, making it difficult for NER systems to accurately identify and classify entities.</p></li>
</ul>
<section id="challenges-in-indian-language-ner-detailed" class="level3">
<h3 class="anchored" data-anchor-id="challenges-in-indian-language-ner-detailed">Challenges in Indian Language NER (Detailed)</h3>
<ul>
<li><p><strong>Sandhi:</strong> The phenomenon of word boundary changes due to phonetic fusion poses challenges for tokenization and entity boundary detection. For example, in Hindi, “रामेश” (<span class="math inline">\(rāmeś\)</span>) could be a single name or a combination of two words “राम” (<span class="math inline">\(rām\)</span>) and “ईश” (<span class="math inline">\(īś\)</span>).</p></li>
<li><p><strong>Complex Morphology:</strong> Agglutinative nature, where morphemes (meaningful units) are strung together, leads to high inflection and derivation. A single word can have numerous forms. This impacts feature extraction and requires robust morphological analyzers.</p></li>
<li><p><strong>Code-Mixing:</strong> Frequent use of multiple languages within a single sentence or phrase adds complexity. Identifying entity boundaries and disambiguating entity types becomes more challenging when dealing with mixed language data.</p></li>
<li><p><strong>Named Entity Ambiguity:</strong> Many words have multiple meanings and can function as both common and proper nouns. Resolving this ambiguity requires context analysis and semantic understanding. For instance, “कल” (<span class="math inline">\(kal\)</span>) can mean “yesterday” or “machine” depending on the context.</p></li>
<li><p><strong>Lack of Standardized Resources:</strong> While efforts are increasing, there’s still a comparative lack of:</p>
<ul>
<li><strong>Large, Annotated Corpora:</strong> Training NER models requires substantial labeled data, which is limited for many Indian languages.</li>
<li><strong>High-Quality Linguistic Tools:</strong> Tools for tasks like morphological analysis, POS tagging, and chunking are less developed or have lower accuracy compared to resource-rich languages.</li>
</ul></li>
<li><p><strong>Script Variations:</strong> Several Indian languages are written in multiple scripts. This can create challenges for text processing and requires script-aware models or transliteration techniques.</p></li>
<li><p><strong>Dialectal Variations:</strong> Significant dialectal variations within a language impact vocabulary, pronunciation, and grammatical structures, posing challenges for model generalization.</p></li>
<li><p><strong>Informal Language Use:</strong> Social media and informal texts often contain colloquialisms, slang, and non-standard spellings, making entity recognition more difficult.</p></li>
</ul>
</section>
</section>
<section id="evaluation-of-ner-systems" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-ner-systems">Evaluation of NER Systems</h2>
<ul>
<li><strong>Metrics:</strong>
<ul>
<li><p><strong>Precision:</strong> Measures the accuracy of the system’s positive predictions. It is calculated as the number of correctly identified entities divided by the total number of entities identified by the system. <span class="math display">\[ \text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}} \]</span></p></li>
<li><p><strong>Recall:</strong> Measures the system’s ability to identify all relevant entities. It is calculated as the number of correctly identified entities divided by the total number of actual entities in the data. <span class="math display">\[ \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}} \]</span></p></li>
<li><p><strong>F1-score:</strong> Provides a balanced measure of precision and recall, calculated as the harmonic mean of the two. <span class="math display">\[F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\]</span></p></li>
</ul></li>
<li><strong>Modern Metrics:</strong>
<ul>
<li><strong>Exact Match Ratio:</strong> Evaluates the strict correctness of entity recognition. It measures the proportion of entities that are identified with both correct boundaries and correct type labels.</li>
<li><strong>Entity-Level F1-score:</strong> Computes precision, recall, and F1-score at the entity level. This means that an entity is considered correctly identified only if all its tokens are correctly tagged with the appropriate entity type.</li>
</ul></li>
<li><strong>Challenges in Evaluation:</strong>
<ul>
<li><strong>Consistent Annotation Guidelines:</strong> Variations in annotation guidelines across different datasets can lead to inconsistencies in evaluation results.</li>
<li><strong>Partial Matches:</strong> Deciding how to score partial matches of entities (e.g., recognizing “Barack Obama” when the gold standard is “President Barack Obama”) is a challenge.</li>
<li><strong>Cross-domain Evaluation:</strong> Performance can vary significantly across different domains (e.g., news articles vs.&nbsp;social media posts) due to variations in language use and entity types.</li>
<li><strong>Cross-lingual Evaluation:</strong> Evaluating NER systems across different languages presents challenges due to linguistic differences and the availability of annotated data in various languages.</li>
</ul></li>
</ul>
</section>
<section id="sequence-modeling" class="level2">
<h2 class="anchored" data-anchor-id="sequence-modeling">Sequence Modeling</h2>
<ul>
<li>Involves predicting or labeling sequences of data, such as words in a sentence or characters in a word.</li>
<li><strong>Goal:</strong> Given an input sequence, assign a label or category to each element in the sequence.</li>
<li><strong>Applications:</strong>
<ul>
<li>Part-of-Speech (POS) tagging: Assigning grammatical categories to words.</li>
<li>Named Entity Recognition (NER): Identifying and classifying entities like people, organizations, and locations.</li>
<li>Speech recognition: Converting spoken audio into text.</li>
<li>Machine translation: Translating text from one language to another.</li>
<li>Protein structure prediction: Determining the three-dimensional structure of proteins from amino acid sequences.</li>
</ul></li>
</ul>
<section id="key-concepts" class="level3">
<h3 class="anchored" data-anchor-id="key-concepts">Key Concepts</h3>
<ul>
<li><strong>Input Sequence:</strong> A series of data points, such as words, characters, or phonemes.</li>
<li><strong>Output Sequence:</strong> A series of labels or categories corresponding to each element in the input sequence.</li>
<li><strong>Contextual Information:</strong> The relationships and dependencies between elements in the sequence play a crucial role in accurate prediction.</li>
<li><strong>Probabilistic Models:</strong> Often used to estimate the likelihood of different output sequences given an input sequence.</li>
</ul>
</section>
<section id="challenges" class="level3">
<h3 class="anchored" data-anchor-id="challenges">Challenges</h3>
<ul>
<li><strong>Variable Sequence Lengths:</strong> Sequences can have varying lengths, making it challenging to model them consistently.</li>
<li><strong>Long-Range Dependencies:</strong> Elements in a sequence can depend on others that are far apart, requiring models to capture these long-range relationships.</li>
<li><strong>Ambiguity:</strong> Multiple possible output sequences might fit a given input sequence.</li>
<li><strong>Data Sparsity:</strong> Limited training data can make it difficult to estimate probabilities accurately, especially for rare sequences.</li>
</ul>
</section>
<section id="common-approaches" class="level3">
<h3 class="anchored" data-anchor-id="common-approaches">Common Approaches</h3>
<ul>
<li><strong>Markov Models:</strong> Model sequences based on the assumption that the current element depends only on a limited history of previous elements (e.g., Hidden Markov Models).</li>
<li><strong>Recurrent Neural Networks (RNNs):</strong> Use internal memory to process sequences, capturing dependencies over variable lengths.</li>
<li><strong>Conditional Random Fields (CRFs):</strong> Probabilistic models that define a conditional probability distribution over label sequences given an observation sequence.</li>
<li><strong>Transformers:</strong> Attention-based models that excel at capturing long-range dependencies and have achieved state-of-the-art results on various sequence modeling tasks.</li>
</ul>
</section>
</section>
<section id="hidden-markov-models-hmm" class="level2">
<h2 class="anchored" data-anchor-id="hidden-markov-models-hmm">Hidden Markov Models (HMM)</h2>
<ul>
<li>Models systems with observable outputs and hidden states (e.g., words and POS tags).</li>
<li><strong>Goal:</strong> To determine the hidden state sequence that best explains the observed sequence.</li>
<li><strong>Assumption:</strong> The Markov Property - the probability of transitioning to a new state depends only on the current state, not the entire history of states.</li>
</ul>
<section id="components" class="level3">
<h3 class="anchored" data-anchor-id="components">Components</h3>
<ul>
<li><p><strong>States (<span class="math inline">\(S\)</span>):</strong> Hidden variables that influence the observed outcomes (e.g., POS tags like Noun (NN), Verb (VB), etc.).</p></li>
<li><p><strong>Observations (<span class="math inline">\(O\)</span>):</strong> The visible outcomes influenced by the hidden states (e.g., Words in a sentence like “dog”, “runs”, etc.).</p></li>
<li><p><strong>Transition Probabilities (<span class="math inline">\(A\)</span>):</strong> Probability of transitioning from one state to another. Represented by a matrix <span class="math inline">\(A = \{a_{ij}\}\)</span> where:</p>
<ul>
<li><span class="math inline">\(a_{ij} = P(s_j \text{ at } t+1 | s_i \text{ at } t)\)</span> is the probability of transitioning from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span>.</li>
</ul></li>
<li><p><strong>Emission Probabilities (<span class="math inline">\(B\)</span>):</strong> Probability of an observation being generated from a state. Represented by a matrix <span class="math inline">\(B = \{b_{ik}\}\)</span> where:</p>
<ul>
<li><span class="math inline">\(b_{ik} = P(o_k \text{ at } t | s_i \text{ at } t)\)</span> is the probability of observing <span class="math inline">\(o_k\)</span> given that the current state is <span class="math inline">\(s_i\)</span>.</li>
</ul></li>
<li><p><strong>Initial State Distribution (<span class="math inline">\(\pi\)</span>):</strong> Probability distribution over the initial states, represented by a vector <span class="math inline">\(\pi = \{\pi_i\}\)</span> where:</p>
<ul>
<li><span class="math inline">\(\pi_i = P(s_i \text{ at } t = 1)\)</span> is the probability that the initial state is <span class="math inline">\(s_i\)</span>.</li>
</ul></li>
</ul>
</section>
<section id="hmm-notation" class="level3">
<h3 class="anchored" data-anchor-id="hmm-notation">HMM Notation</h3>
<p>An HMM is often represented as a tuple: <span class="math display">\[\lambda = (A, B, \pi)\]</span></p>
</section>
<section id="how-hmm-works" class="level3">
<h3 class="anchored" data-anchor-id="how-hmm-works">How HMM Works</h3>
<ol type="1">
<li><p><strong>Initialization:</strong> The model starts with an initial state distribution.</p></li>
<li><p><strong>Transition:</strong> The model transitions from one state to another based on transition probabilities.</p></li>
<li><p><strong>Emission:</strong> In each state, the model emits an observation based on emission probabilities.</p></li>
<li><p><strong>Sequence Generation:</strong> This process repeats to generate a sequence of observations.</p></li>
</ol>
</section>
<section id="using-hmm-for-pos-tagging" class="level3">
<h3 class="anchored" data-anchor-id="using-hmm-for-pos-tagging">Using HMM for POS Tagging</h3>
<ol type="1">
<li><p><strong>Training:</strong> The HMM is trained on a corpus of text that is already tagged with POS tags. This training process estimates the transition, emission, and initial state probabilities based on the observed frequencies in the training data.</p></li>
<li><p><strong>Decoding:</strong> Given a new sentence, the HMM uses these probabilities to find the most likely sequence of POS tags for that sentence. This is typically done using the Viterbi algorithm, a dynamic programming algorithm that efficiently finds the most probable path through the HMM.</p></li>
</ol>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>Consider the sentence “John can see Will.” We might want to assign the following POS tags: Noun, Modal, Verb, Noun. The HMM would calculate:</p>
<ul>
<li><strong>Transition probabilities:</strong> How likely is a Noun followed by a Modal, a Modal by a Verb, and a Verb by a Noun?</li>
<li><strong>Emission probabilities:</strong> How likely is “John” to be a Noun, “can” to be a Modal, “see” to be a Verb, and “Will” to be a Noun?</li>
</ul>
<p>The HMM aims to find the sequence of tags that maximizes the product of these probabilities.</p>
</section>
<section id="advantages-of-hmms-for-pos-tagging" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-hmms-for-pos-tagging">Advantages of HMMs for POS Tagging</h3>
<ul>
<li><strong>Simplicity:</strong> HMMs are relatively simple to understand and implement.</li>
<li><strong>Efficiency:</strong> The Viterbi algorithm allows for efficient decoding of tag sequences.</li>
<li><strong>Probabilistic Framework:</strong> HMMs provide a principled way of handling uncertainty in language.</li>
</ul>
</section>
<section id="limitations-of-hmms-for-pos-tagging" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-hmms-for-pos-tagging">Limitations of HMMs for POS Tagging</h3>
<ul>
<li><strong>The Markov Assumption:</strong> The assumption that the current state only depends on the previous state is often too restrictive for natural language.</li>
<li><strong>Limited Feature Representation:</strong> Basic HMMs only consider the current word and tag, limiting their ability to capture more complex linguistic phenomena.</li>
</ul>
</section>
</section>
<section id="maximum-entropy-markov-model-memm" class="level2">
<h2 class="anchored" data-anchor-id="maximum-entropy-markov-model-memm">Maximum Entropy Markov Model (MEMM)</h2>
<ul>
<li>A discriminative model specifically designed for sequence labeling tasks, such as POS tagging and NER.</li>
<li>Combines the power of Maximum Entropy models with the sequential nature of Markov models.</li>
</ul>
<section id="key-features-1" class="level3">
<h3 class="anchored" data-anchor-id="key-features-1">Key Features</h3>
<ol type="1">
<li><strong>Conditional Probability:</strong>
<ul>
<li>Unlike HMMs that model the joint probability <span class="math inline">\(P(W,T)\)</span>, MEMMs directly model the conditional probability <span class="math inline">\(P(T|W)\)</span> of a tag sequence <span class="math inline">\(T\)</span> given an observed word sequence <span class="math inline">\(W\)</span>.</li>
<li>This focus on the conditional probability makes MEMMs more suitable for discriminative tasks, where the goal is to predict the most likely label sequence given the input.</li>
</ul></li>
<li><strong>Feature Richness:</strong>
<ul>
<li>MEMMs leverage a wide range of features to capture linguistic and contextual information. These features can include:
<ul>
<li>Word-level features: Current word, previous word, next word, word prefixes and suffixes, capitalization, word shape (e.g., all-caps, capitalized).</li>
<li>Tag-level features: Previous tag, previous two tags (for higher-order MEMMs).</li>
<li>Combined features: Interactions between word and tag features.</li>
</ul></li>
<li>This flexibility in feature representation enables MEMMs to capture complex dependencies and patterns in the data.</li>
</ul></li>
<li><strong>Maximum Entropy Principle:</strong>
<ul>
<li>MEMMs employ the Maximum Entropy principle to estimate the conditional probabilities.</li>
<li>This principle states that among all probability distributions that satisfy the constraints imposed by the observed features, the distribution with the maximum entropy (i.e., the most uniform distribution) is preferred.</li>
<li>The maximum entropy distribution represents the least biased model that is consistent with the data.</li>
</ul></li>
</ol>
</section>
<section id="formulation" class="level3">
<h3 class="anchored" data-anchor-id="formulation">Formulation</h3>
<p><span class="math display">\[
P(T|W) = \prod_{i=1}^{L} P(t_i|t_{i-1}, w_i) = \prod_{i=1}^{L} \frac{\exp(\sum_j \beta_j f_j(t_{i-1}, w_i))}{Z(t_{i-1}, w_i)}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(T = t_1, t_2, ..., t_L\)</span> is the tag sequence.</li>
<li><span class="math inline">\(W = w_1, w_2, ..., w_L\)</span> is the word sequence.</li>
<li><span class="math inline">\(f_j(t_{i-1}, w_i)\)</span> represents the j-th feature function, which extracts a feature from the current word <span class="math inline">\(w_i\)</span> and previous tag <span class="math inline">\(t_{i-1}\)</span>.</li>
<li><span class="math inline">\(\beta_j\)</span> is the weight associated with the j-th feature function, learned during training.</li>
<li><span class="math inline">\(Z(t_{i-1}, w_i)\)</span> is a normalization factor, ensuring that the probabilities sum to 1 for all possible tags at position <span class="math inline">\(i\)</span>.</li>
</ul>
</section>
<section id="advantages-1" class="level3">
<h3 class="anchored" data-anchor-id="advantages-1">Advantages</h3>
<ul>
<li><strong>Discriminative Power:</strong> MEMMs directly model the conditional probability, making them more suitable for discriminative tasks.</li>
<li><strong>Feature Flexibility:</strong> Can incorporate a wide range of features to capture linguistic and contextual information.</li>
<li><strong>Maximum Entropy Principle:</strong> Provides a principled approach to estimate probabilities, ensuring minimal bias.</li>
</ul>
</section>
<section id="disadvantage" class="level3">
<h3 class="anchored" data-anchor-id="disadvantage">Disadvantage</h3>
<p>Despite its advantages, MEMMs suffer from a limitation known as the <strong>label bias problem</strong>, which we will discuss in the next section. This issue motivates the use of Conditional Random Fields (CRFs) for sequence labeling.</p>
</section>
</section>
<section id="conditional-random-fields-crf" class="level2">
<h2 class="anchored" data-anchor-id="conditional-random-fields-crf">Conditional Random Fields (CRF)</h2>
<ul>
<li><p><strong>Definition:</strong> A CRF is a discriminative probabilistic model used for labeling and segmenting sequences. Unlike HMMs which are generative, CRFs directly model the conditional probability of the label sequence given the observation sequence: <span class="math inline">\(P(T|W)\)</span>. This makes them particularly suitable for tasks like POS tagging and NER where we want to predict labels based on observed words.</p></li>
<li><p><strong>Global Optimization:</strong> CRFs address the limitations of MEMMs by considering the entire sequence globally during training and inference. This means that the model doesn’t just make local decisions at each word but optimizes for the most likely tag sequence across the entire sentence.</p></li>
<li><p><strong>Feature Engineering:</strong> A key strength of CRFs lies in their ability to incorporate a wide range of features, both for individual words and for dependencies between adjacent tags. These features can be broadly categorized as:</p>
<ul>
<li><strong>Word-Level Features:</strong>
<ul>
<li>Word identity: The word itself (e.g., “run”).</li>
<li>Prefixes and suffixes: Morphological information (e.g., “-ing”, “re-”).</li>
<li>Capitalization: Whether the word is capitalized.</li>
<li>Word shape: Abstract representation of the word’s form (e.g., “XxXx” for “John”).</li>
<li>Part-of-speech: If available from a separate POS tagger.</li>
</ul></li>
<li><strong>Sequence-Level Features:</strong>
<ul>
<li>Tag transitions: The likelihood of one tag following another (e.g., a noun following a determiner).</li>
<li>Tag-word combinations: Joint features of a word and its predicted tag.</li>
</ul></li>
</ul></li>
<li><p><strong>Mathematical Formulation:</strong></p></li>
</ul>
<p>A linear-chain CRF defines the conditional probability of a tag sequence <span class="math inline">\(t = (t_1, t_2,...,t_n)\)</span> given an input sequence <span class="math inline">\(x = (x_1, x_2,...,x_n)\)</span> as:</p>
<p><span class="math display">\[
P(t|x) = \frac{1}{Z(x)} exp(\sum_{i=1}^{n} \sum_{k} \lambda_k f_k(t_i, t_{i-1}, x, i))
\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(Z(x)\)</span> is a normalization factor to ensure a valid probability distribution.</p></li>
<li><p><span class="math inline">\(\lambda_k\)</span> are the model parameters (weights) learned during training.</p></li>
<li><p><span class="math inline">\(f_k(t_i, t_{i-1}, x, i)\)</span> are feature functions that capture various aspects of the input and tag sequences, as described above.</p></li>
<li><p><strong>Training:</strong> CRFs are trained using maximum likelihood estimation. This involves adjusting the model parameters (<span class="math inline">\(\lambda_k\)</span>) to maximize the probability of the observed tag sequences in the training data.</p></li>
<li><p><strong>Inference:</strong> Given a new sentence, the Viterbi algorithm is commonly used to find the most likely tag sequence according to the trained CRF model.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li><strong>No Label Bias:</strong> The global normalization in CRFs eliminates the label bias problem inherent in MEMMs.</li>
<li><strong>Rich Feature Representation:</strong> CRFs allow for a flexible and powerful representation of the input data through diverse feature functions.</li>
<li><strong>State-of-the-art Performance:</strong> CRFs have consistently achieved high accuracy in POS tagging, NER, and other sequence labeling tasks.</li>
</ul></li>
</ul>
</section>
<section id="conclusion-hmm-memm-and-crf" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-hmm-memm-and-crf">Conclusion: HMM, MEMM, and CRF</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Type</th>
<th>Probability</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>HMM</td>
<td>Generative</td>
<td>Joint (P(W, T))</td>
<td>Simple, interpretable</td>
<td>Limited by independence assumptions</td>
</tr>
<tr class="even">
<td>MEMM</td>
<td>Discriminative</td>
<td>Conditional (P(T W))</td>
<td>Rich features, flexible</td>
<td>Label bias problem</td>
</tr>
<tr class="odd">
<td>CRF</td>
<td>Discriminative</td>
<td>Global conditional (P(T W))</td>
<td>Overcomes label bias, global optimization</td>
<td>Computationally intensive</td>
</tr>
</tbody>
</table>
</section>
<section id="importance-of-classification-models" class="level2">
<h2 class="anchored" data-anchor-id="importance-of-classification-models">Importance of Classification Models</h2>
<p>Classification models play a crucial role in Natural Language Processing (NLP) tasks, particularly in Part-of-Speech (POS) tagging and Named Entity Recognition (NER). These models provide a structured and efficient way to predict categorical labels for words or sequences of words within a text, enabling a deeper understanding of the linguistic structure and meaning.</p>
<p>Here’s why classification models are important for POS and NER:</p>
<ul>
<li><p><strong>Categorical Prediction:</strong> Classification models excel at predicting categorical labels, making them ideal for assigning POS tags (e.g., noun, verb, adjective) to individual words or identifying named entities (e.g., person, organization, location) within a sentence.</p></li>
<li><p><strong>Feature Handling:</strong> These models can handle a wide range of features, both linguistic and contextual. For POS tagging, features might include the word itself, its prefixes and suffixes, surrounding words, and their POS tags. For NER, features could include capitalization, word shape, part-of-speech tags, and the context of surrounding words.</p></li>
<li><p><strong>Probabilistic Outputs:</strong> Classification models often provide probabilistic outputs, giving a measure of confidence in the predicted label. This is valuable in NLP, where ambiguity is common, and multiple interpretations might be possible.</p></li>
<li><p><strong>Model Interpretability:</strong> Some classification models, such as logistic regression, offer interpretability, allowing us to understand the relationship between features and predicted labels. This can be useful for analyzing the model’s decision-making process and gaining insights into the linguistic factors that influence tagging.</p></li>
<li><p><strong>Flexibility and Adaptability:</strong> Classification models can be adapted to various NLP tasks and domains. They can be trained on different datasets, tailored with specific features, and used in both supervised and semi-supervised learning settings, making them highly versatile tools for NLP applications.</p></li>
<li><p><strong>Foundation for Downstream Tasks:</strong> Accurate POS tagging and NER, driven by effective classification models, serve as a strong foundation for various downstream NLP tasks. For instance, they can be used to improve the performance of sentiment analysis, machine translation, information extraction, and question answering systems.</p></li>
</ul>
</section>
<section id="classification-models-for-pos-and-ner-tasks" class="level2">
<h2 class="anchored" data-anchor-id="classification-models-for-pos-and-ner-tasks">Classification Models for POS and NER Tasks</h2>
<section id="naive-bayes" class="level3">
<h3 class="anchored" data-anchor-id="naive-bayes">1. Naive Bayes</h3>
<ul>
<li>A probabilistic classifier based on Bayes’ Theorem.</li>
<li>Assumes that features are conditionally independent given the class. This is known as the <em>naive assumption</em>.</li>
<li>Simple to implement and computationally efficient, making it suitable for large datasets and real-time applications.</li>
<li>Works well even with limited training data.</li>
</ul>
<section id="formula" class="level4">
<h4 class="anchored" data-anchor-id="formula">Formula:</h4>
<p>Bayes’ Theorem states: <span class="math display">\[ P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)} \]</span> where: - <span class="math inline">\(Y\)</span> is the class label (e.g., a POS tag or NER category). - <span class="math inline">\(X\)</span> represents the observed features (e.g., words, context).</p>
<ul>
<li>Due to the naive assumption, this can be simplified for multiple features: <span class="math display">\[ P(Y|X_1, X_2, ... , X_n) = \frac{P(Y) \prod_{i=1}^n P(X_i|Y)}{P(X_1, X_2, ..., X_n)}\]</span></li>
</ul>
</section>
<section id="types" class="level4">
<h4 class="anchored" data-anchor-id="types">Types:</h4>
<pre><code>- **Multinomial Naive Bayes**: Well-suited for text data where features represent word counts or frequencies.</code></pre>
</section>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">2. Logistic Regression</h3>
<ul>
<li>A discriminative model that directly learns the relationship between features and the probability of a class.</li>
<li>Uses the sigmoid function (logistic function) to output a probability between 0 and 1.</li>
<li>Can handle both binary (two classes) and multi-class classification problems.</li>
</ul>
<section id="formula-1" class="level4">
<h4 class="anchored" data-anchor-id="formula-1">Formula:</h4>
<p><span class="math display">\[ P(Y = 1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_n X_n)}}\]</span> where: - <span class="math inline">\(Y\)</span> is the target class. - <span class="math inline">\(X_i\)</span> are the features. - <span class="math inline">\(\beta_i\)</span> are the weights learned by the model.</p>
</section>
<section id="advantages-2" class="level4">
<h4 class="anchored" data-anchor-id="advantages-2">Advantages:</h4>
<pre><code>- Can model complex relationships between features and classes.
- Provides probabilities for each class, allowing for ranking and thresholding.</code></pre>
</section>
<section id="techniques" class="level4">
<h4 class="anchored" data-anchor-id="techniques">Techniques:</h4>
<pre><code>- **Regularization** (L1 or L2) to prevent overfitting.</code></pre>
</section>
</section>
<section id="clustering" class="level3">
<h3 class="anchored" data-anchor-id="clustering">3. Clustering</h3>
<ul>
<li>An unsupervised learning technique that groups similar data points into clusters.</li>
<li>Useful for discovering patterns and structure in unlabeled data.</li>
</ul>
<section id="types-1" class="level4">
<h4 class="anchored" data-anchor-id="types-1">Types:</h4>
<pre><code>- **K-Means Clustering**: A popular algorithm that partitions data points into *K* clusters based on distance to cluster centroids.</code></pre>
</section>
<section id="applications-for-pos-and-ner" class="level4">
<h4 class="anchored" data-anchor-id="applications-for-pos-and-ner">Applications for POS and NER:</h4>
<ul>
<li>Clustering can be used to group words with similar syntactic behavior (for POS tagging) or entities with similar characteristics (for NER).</li>
<li>It can assist in identifying new POS tags or NER categories in unsupervised or semi-supervised settings.</li>
</ul>
</section>
</section>
</section>
<section id="evaluation-metrics-1" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-metrics-1">Evaluation Metrics</h2>
<ul>
<li><strong>Accuracy:</strong> Proportion of correct predictions.</li>
<li><strong>Precision:</strong> Proportion of true positives out of all positive predictions.</li>
<li><strong>Recall:</strong> Proportion of true positives out of all actual positives.</li>
<li><strong>F1-Score:</strong> Harmonic mean of Precision and Recall.</li>
</ul>
</section>
<section id="classification-models-in-other-nlp-tasks" class="level2">
<h2 class="anchored" data-anchor-id="classification-models-in-other-nlp-tasks">Classification Models in Other NLP Tasks</h2>
<p>While we’ve primarily focused on POS tagging and NER, classification models extend to various other NLP tasks:</p>
<section id="text-classification" class="level3">
<h3 class="anchored" data-anchor-id="text-classification">Text Classification</h3>
<ul>
<li><strong>Sentiment Analysis:</strong> Classifying text based on emotional tone (positive, negative, neutral). Uses features like word choices, punctuation, and emoticons.</li>
<li><strong>Topic Categorization:</strong> Assigning documents or articles to predefined topics (e.g., sports, politics, technology). Relies on identifying keywords and thematic patterns.</li>
<li><strong>Spam Detection:</strong> Identifying unsolicited or unwanted emails. Utilizes features like sender information, email content, and presence of suspicious links.</li>
</ul>
</section>
<section id="speech-recognition" class="level3">
<h3 class="anchored" data-anchor-id="speech-recognition">Speech Recognition</h3>
<ul>
<li><strong>Phoneme Prediction:</strong> Classifying segments of speech into distinct phonetic units (phonemes), which are then combined to recognize words. Models use acoustic features extracted from the speech signal.</li>
<li><strong>Mapping Speech to Text:</strong> Converting spoken audio into written text. This involves acoustic modeling (phoneme prediction), language modeling (predicting word sequences), and decoding to find the most likely text sequence.</li>
</ul>
</section>
<section id="machine-translation" class="level3">
<h3 class="anchored" data-anchor-id="machine-translation">Machine Translation</h3>
<ul>
<li><strong>Language Model Predictions:</strong> Predicting the probability of word sequences in a target language. These probabilities are used to guide the translation process towards more fluent and grammatical output.</li>
<li><strong>Word Alignment:</strong> Determining which words in the source language correspond to which words in the target language. This is a classification problem where each word pair is classified as aligned or not aligned.</li>
</ul>
</section>
<section id="information-retrieval" class="level3">
<h3 class="anchored" data-anchor-id="information-retrieval">Information Retrieval</h3>
<ul>
<li><strong>Document Ranking:</strong> Sorting search results based on relevance to a user’s query. Classification models can be used to classify documents as relevant or irrelevant to a given query.</li>
<li><strong>Query Classification:</strong> Determining the intent behind a search query (e.g., informational, navigational, transactional). This helps in tailoring search results to the user’s specific needs.</li>
</ul>
</section>
</section>
<section id="review-questions" class="level2">
<h2 class="anchored" data-anchor-id="review-questions">Review Questions</h2>
<p><strong>Transformation-based Tagging (TBL)</strong></p>
<ol type="1">
<li>Explain the basic idea behind Transformation-based Tagging (TBL) and how it differs from purely rule-based tagging.</li>
<li>What are the key components of a TBL system?</li>
<li>Describe the process of rule learning in TBL. How are rules scored and selected?</li>
<li>What are the advantages and disadvantages of TBL?</li>
<li>Give examples of transformation rules based on internal and contextual evidence.</li>
<li>Explain the analogy of TBL to painting.</li>
</ol>
<p><strong>Brill Tagging</strong></p>
<ol type="1">
<li>How does Brill Tagging combine rule-based and stochastic approaches?</li>
<li>What are the inputs to a Brill Tagger?</li>
<li>Describe the process of rule learning and application in Brill Tagging.</li>
<li>What are the advantages and disadvantages of Brill Tagging compared to purely rule-based or statistical taggers?</li>
<li>Give an example of a rule template used in Brill Tagging and explain how it would be applied.</li>
</ol>
<p><strong>Stochastic Tagging</strong></p>
<ol type="1">
<li>Explain the concept of Stochastic Tagging and how it uses probabilities for prediction.</li>
<li>What is the difference between individual word probabilities and tag sequence probabilities?</li>
<li>Describe the Bigram, Trigram, and N-gram models for simplifying tag sequence probability calculation.</li>
<li>What are the common stochastic tagging models? Briefly explain each.</li>
<li>How is a stochastic tagger trained and how is it used to predict POS tags for a new sentence?</li>
</ol>
<p><strong>Named Entity Recognition (NER)</strong></p>
<ol type="1">
<li>What is Named Entity Recognition (NER) and what are its key applications?</li>
<li>What are the common types of named entities? Give examples.</li>
<li>Explain the different NER tagging schemes (BIO, IO, BIOES) and their purpose.</li>
<li>What are the types of features used in NER? Provide examples of each.</li>
<li>Describe the various models and algorithms used for NER, from rule-based systems to deep learning approaches.</li>
<li>Explain the challenges in NER, both generally and specifically for Indian languages.</li>
<li>How are NER systems evaluated? What are the commonly used metrics?</li>
</ol>
<p><strong>Sequence Modeling</strong></p>
<ol type="1">
<li>What is sequence modeling? Give examples of NLP tasks that involve sequence modeling.</li>
<li>Explain the key concepts of sequence modeling, including input/output sequences, contextual information, and probabilistic models.</li>
<li>What are the challenges in sequence modeling?</li>
<li>Briefly describe the common approaches to sequence modeling (Markov models, RNNs, CRFs, Transformers).</li>
</ol>
<p><strong>Hidden Markov Models (HMMs)</strong></p>
<ol type="1">
<li>Explain the concept of a Hidden Markov Model (HMM) and its application to sequence labeling.</li>
<li>Describe the key components of an HMM and their roles.</li>
<li>How is an HMM used for POS tagging? Explain the training and decoding processes.</li>
<li>What are the advantages and limitations of HMMs for POS tagging?</li>
</ol>
<p><strong>Maximum Entropy Markov Model (MEMM)</strong></p>
<ol type="1">
<li>Explain how a Maximum Entropy Markov Model (MEMM) differs from an HMM.</li>
<li>What is the label bias problem in MEMMs, and how does it arise?</li>
<li>Describe the key advantages of using MEMMs for sequence labeling.</li>
</ol>
<p><strong>Conditional Random Fields (CRFs)</strong></p>
<ol type="1">
<li>Explain how a CRF addresses the label bias problem found in MEMMs.</li>
<li>What are the advantages of using CRFs for sequence labeling compared to HMMs and MEMMs?</li>
<li>Describe the types of features that can be used in CRFs for NER. Provide examples.</li>
</ol>
<p><strong>General</strong></p>
<ol type="1">
<li>Compare and contrast the three models (HMM, MEMM, CRF) in terms of their type, probability modeling, advantages, and disadvantages.</li>
<li>What are the key considerations when choosing a classification model for a particular NLP task?</li>
<li>How can POS tagging and NER contribute to the effectiveness of other NLP applications like sentiment analysis or machine translation?</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>