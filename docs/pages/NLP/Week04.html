<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Syntax, Dependency Parsing, and SLR – BS Degree Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-roman.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-sans.css">
<link rel="stylesheet" href="https://lalten.github.io/lmweb/style/latinmodern-mono.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">NLP</a></li><li class="breadcrumb-item"><a href="../../pages/NLP/Week04.html">Week 4</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">BS Degree Notes</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Deep Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/DL/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">AI: Search Methods</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/AI/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week05.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 5</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week06.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 6</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week07.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 7</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week08.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 8</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/ST/Week12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 12</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Software Engineering</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week09.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 9</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 10</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/SE/Week11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 11</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Reinforcement Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week01_2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1.2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/RL/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">NLP</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/NLP/Week04.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">LLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/LLM/Week04.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 4</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#phrasal-categories" id="toc-phrasal-categories" class="nav-link active" data-scroll-target="#phrasal-categories">Phrasal Categories</a>
  <ul class="collapse">
  <li><a href="#phrase-structure-grammar-psg" id="toc-phrase-structure-grammar-psg" class="nav-link" data-scroll-target="#phrase-structure-grammar-psg">Phrase Structure Grammar (PSG)</a></li>
  </ul></li>
  <li><a href="#sentence-structure-in-syntax" id="toc-sentence-structure-in-syntax" class="nav-link" data-scroll-target="#sentence-structure-in-syntax">Sentence Structure in Syntax</a></li>
  <li><a href="#types-of-clauses-and-sentences" id="toc-types-of-clauses-and-sentences" class="nav-link" data-scroll-target="#types-of-clauses-and-sentences">Types of Clauses and Sentences</a></li>
  <li><a href="#complexities-in-syntax-ambiguities-garden-path-recursiveness-ellipsis" id="toc-complexities-in-syntax-ambiguities-garden-path-recursiveness-ellipsis" class="nav-link" data-scroll-target="#complexities-in-syntax-ambiguities-garden-path-recursiveness-ellipsis">Complexities in Syntax: Ambiguities, Garden-Path, Recursiveness, Ellipsis</a></li>
  <li><a href="#introduction-to-context-free-grammar-cfg" id="toc-introduction-to-context-free-grammar-cfg" class="nav-link" data-scroll-target="#introduction-to-context-free-grammar-cfg">Introduction to Context-Free Grammar (CFG)</a></li>
  <li><a href="#cfg-rules-and-example" id="toc-cfg-rules-and-example" class="nav-link" data-scroll-target="#cfg-rules-and-example">CFG Rules and Example</a></li>
  <li><a href="#parsing-with-cfg-and-parse-trees" id="toc-parsing-with-cfg-and-parse-trees" class="nav-link" data-scroll-target="#parsing-with-cfg-and-parse-trees">Parsing with CFG and Parse Trees</a></li>
  <li><a href="#l₀-and-the-lexicon-for-l₀" id="toc-l₀-and-the-lexicon-for-l₀" class="nav-link" data-scroll-target="#l₀-and-the-lexicon-for-l₀">L₀ and the lexicon for L₀</a>
  <ul class="collapse">
  <li><a href="#lexicon" id="toc-lexicon" class="nav-link" data-scroll-target="#lexicon">Lexicon</a></li>
  <li><a href="#grammar-rules" id="toc-grammar-rules" class="nav-link" data-scroll-target="#grammar-rules">Grammar Rules</a></li>
  </ul></li>
  <li><a href="#parse-tree" id="toc-parse-tree" class="nav-link" data-scroll-target="#parse-tree">Parse Tree</a></li>
  <li><a href="#treebanks" id="toc-treebanks" class="nav-link" data-scroll-target="#treebanks">Treebanks</a></li>
  <li><a href="#penn-treebank" id="toc-penn-treebank" class="nav-link" data-scroll-target="#penn-treebank">Penn Treebank</a>
  <ul class="collapse">
  <li><a href="#detailed-overview" id="toc-detailed-overview" class="nav-link" data-scroll-target="#detailed-overview">Detailed Overview</a>
  <ul class="collapse">
  <li><a href="#key-features" id="toc-key-features" class="nav-link" data-scroll-target="#key-features">Key Features</a></li>
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a></li>
  <li><a href="#example-annotation" id="toc-example-annotation" class="nav-link" data-scroll-target="#example-annotation">Example Annotation</a></li>
  <li><a href="#significance" id="toc-significance" class="nav-link" data-scroll-target="#significance">Significance</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#penn-treebank-sentences" id="toc-penn-treebank-sentences" class="nav-link" data-scroll-target="#penn-treebank-sentences">Penn Treebank Sentences</a>
  <ul class="collapse">
  <li><a href="#example-a" id="toc-example-a" class="nav-link" data-scroll-target="#example-a">Example (a)</a></li>
  <li><a href="#example-b" id="toc-example-b" class="nav-link" data-scroll-target="#example-b">Example (b)</a></li>
  </ul></li>
  <li><a href="#cfg-rules-from-penn-treebank" id="toc-cfg-rules-from-penn-treebank" class="nav-link" data-scroll-target="#cfg-rules-from-penn-treebank">CFG Rules from Penn Treebank</a>
  <ul class="collapse">
  <li><a href="#flattening-and-binarization" id="toc-flattening-and-binarization" class="nav-link" data-scroll-target="#flattening-and-binarization">Flattening and Binarization</a>
  <ul class="collapse">
  <li><a href="#flattening" id="toc-flattening" class="nav-link" data-scroll-target="#flattening">Flattening</a></li>
  <li><a href="#binarization" id="toc-binarization" class="nav-link" data-scroll-target="#binarization">Binarization</a></li>
  </ul></li>
  <li><a href="#example-vp-expansion-rules" id="toc-example-vp-expansion-rules" class="nav-link" data-scroll-target="#example-vp-expansion-rules">Example VP Expansion Rules</a></li>
  </ul></li>
  <li><a href="#grammar-equivalence-and-normal-form" id="toc-grammar-equivalence-and-normal-form" class="nav-link" data-scroll-target="#grammar-equivalence-and-normal-form">Grammar Equivalence and Normal Form</a></li>
  <li><a href="#ambiguities-in-context-free-grammar-cfg" id="toc-ambiguities-in-context-free-grammar-cfg" class="nav-link" data-scroll-target="#ambiguities-in-context-free-grammar-cfg">Ambiguities in Context-Free Grammar (CFG)</a>
  <ul class="collapse">
  <li><a href="#types-of-ambiguities" id="toc-types-of-ambiguities" class="nav-link" data-scroll-target="#types-of-ambiguities">Types of Ambiguities:</a></li>
  <li><a href="#handling-ambiguity" id="toc-handling-ambiguity" class="nav-link" data-scroll-target="#handling-ambiguity">Handling Ambiguity</a></li>
  </ul></li>
  <li><a href="#introduction-to-constituency-parsing" id="toc-introduction-to-constituency-parsing" class="nav-link" data-scroll-target="#introduction-to-constituency-parsing">Introduction to Constituency Parsing</a></li>
  <li><a href="#constituents-in-natural-language" id="toc-constituents-in-natural-language" class="nav-link" data-scroll-target="#constituents-in-natural-language">Constituents in Natural Language</a></li>
  <li><a href="#introduction-to-cky-parsing" id="toc-introduction-to-cky-parsing" class="nav-link" data-scroll-target="#introduction-to-cky-parsing">Introduction to CKY Parsing</a></li>
  <li><a href="#requirements-for-cky-parsing" id="toc-requirements-for-cky-parsing" class="nav-link" data-scroll-target="#requirements-for-cky-parsing">Requirements for CKY Parsing</a></li>
  <li><a href="#cky-parsing-algorithm" id="toc-cky-parsing-algorithm" class="nav-link" data-scroll-target="#cky-parsing-algorithm">CKY Parsing Algorithm</a>
  <ul class="collapse">
  <li><a href="#algorithm-steps" id="toc-algorithm-steps" class="nav-link" data-scroll-target="#algorithm-steps">Algorithm Steps:</a></li>
  <li><a href="#illustration" id="toc-illustration" class="nav-link" data-scroll-target="#illustration">Illustration:</a></li>
  <li><a href="#complexity" id="toc-complexity" class="nav-link" data-scroll-target="#complexity">Complexity:</a></li>
  <li><a href="#key-properties" id="toc-key-properties" class="nav-link" data-scroll-target="#key-properties">Key Properties:</a></li>
  </ul></li>
  <li><a href="#cky-algorithm-workflow" id="toc-cky-algorithm-workflow" class="nav-link" data-scroll-target="#cky-algorithm-workflow">CKY Algorithm Workflow</a>
  <ul class="collapse">
  <li><a href="#step-by-step-process" id="toc-step-by-step-process" class="nav-link" data-scroll-target="#step-by-step-process">Step-by-Step Process:</a></li>
  <li><a href="#key-advantages-of-cky-parsing" id="toc-key-advantages-of-cky-parsing" class="nav-link" data-scroll-target="#key-advantages-of-cky-parsing">Key Advantages of CKY Parsing:</a></li>
  </ul></li>
  <li><a href="#practical-considerations-of-cky-parsing" id="toc-practical-considerations-of-cky-parsing" class="nav-link" data-scroll-target="#practical-considerations-of-cky-parsing">Practical Considerations of CKY Parsing</a></li>
  <li><a href="#dependency-parsing" id="toc-dependency-parsing" class="nav-link" data-scroll-target="#dependency-parsing">Dependency Parsing</a>
  <ul class="collapse">
  <li><a href="#key-concepts" id="toc-key-concepts" class="nav-link" data-scroll-target="#key-concepts">Key Concepts:</a></li>
  <li><a href="#dependency-trees" id="toc-dependency-trees" class="nav-link" data-scroll-target="#dependency-trees">Dependency Trees:</a></li>
  <li><a href="#types-of-dependency-relations" id="toc-types-of-dependency-relations" class="nav-link" data-scroll-target="#types-of-dependency-relations">Types of Dependency Relations:</a></li>
  <li><a href="#projectivity" id="toc-projectivity" class="nav-link" data-scroll-target="#projectivity">Projectivity:</a></li>
  <li><a href="#dependency-parsing-algorithms" id="toc-dependency-parsing-algorithms" class="nav-link" data-scroll-target="#dependency-parsing-algorithms">Dependency Parsing Algorithms:</a></li>
  <li><a href="#advantages-of-dependency-parsing" id="toc-advantages-of-dependency-parsing" class="nav-link" data-scroll-target="#advantages-of-dependency-parsing">Advantages of Dependency Parsing:</a></li>
  </ul></li>
  <li><a href="#dependency-relations" id="toc-dependency-relations" class="nav-link" data-scroll-target="#dependency-relations">Dependency Relations</a>
  <ul class="collapse">
  <li><a href="#head-dependent-relationships-the-foundation" id="toc-head-dependent-relationships-the-foundation" class="nav-link" data-scroll-target="#head-dependent-relationships-the-foundation">Head-Dependent Relationships: The Foundation</a></li>
  <li><a href="#categorizing-dependency-relations-universal-and-language-specific" id="toc-categorizing-dependency-relations-universal-and-language-specific" class="nav-link" data-scroll-target="#categorizing-dependency-relations-universal-and-language-specific">Categorizing Dependency Relations: Universal and Language-Specific</a></li>
  <li><a href="#clausal-and-nominal-relations-linking-to-verbs-and-nouns" id="toc-clausal-and-nominal-relations-linking-to-verbs-and-nouns" class="nav-link" data-scroll-target="#clausal-and-nominal-relations-linking-to-verbs-and-nouns">Clausal and Nominal Relations: Linking to Verbs and Nouns</a></li>
  <li><a href="#handling-word-order-variation-going-beyond-linearity" id="toc-handling-word-order-variation-going-beyond-linearity" class="nav-link" data-scroll-target="#handling-word-order-variation-going-beyond-linearity">Handling Word Order Variation: Going Beyond Linearity</a></li>
  <li><a href="#formal-representation-capturing-the-structure" id="toc-formal-representation-capturing-the-structure" class="nav-link" data-scroll-target="#formal-representation-capturing-the-structure">Formal Representation: Capturing the Structure</a></li>
  </ul></li>
  <li><a href="#paninian-dependency-and-tags" id="toc-paninian-dependency-and-tags" class="nav-link" data-scroll-target="#paninian-dependency-and-tags">Paninian Dependency and Tags</a></li>
  <li><a href="#dependency-formalisms" id="toc-dependency-formalisms" class="nav-link" data-scroll-target="#dependency-formalisms">Dependency Formalisms</a></li>
  <li><a href="#dependency-treebanks" id="toc-dependency-treebanks" class="nav-link" data-scroll-target="#dependency-treebanks">Dependency Treebanks</a></li>
  <li><a href="#transition-based-dependency-parsing" id="toc-transition-based-dependency-parsing" class="nav-link" data-scroll-target="#transition-based-dependency-parsing">Transition-Based Dependency Parsing</a></li>
  <li><a href="#graph-based-syntactic-parsing-in-depth" id="toc-graph-based-syntactic-parsing-in-depth" class="nav-link" data-scroll-target="#graph-based-syntactic-parsing-in-depth">Graph-based Syntactic Parsing: In-depth</a>
  <ul class="collapse">
  <li><a href="#scoring-function" id="toc-scoring-function" class="nav-link" data-scroll-target="#scoring-function">Scoring Function</a></li>
  <li><a href="#maximum-spanning-tree-algorithms" id="toc-maximum-spanning-tree-algorithms" class="nav-link" data-scroll-target="#maximum-spanning-tree-algorithms">Maximum Spanning Tree Algorithms</a></li>
  <li><a href="#handling-non-projectivity" id="toc-handling-non-projectivity" class="nav-link" data-scroll-target="#handling-non-projectivity">Handling Non-Projectivity</a></li>
  <li><a href="#advantages-of-graph-based-parsing" id="toc-advantages-of-graph-based-parsing" class="nav-link" data-scroll-target="#advantages-of-graph-based-parsing">Advantages of Graph-based Parsing</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations">Limitations</a></li>
  </ul></li>
  <li><a href="#introduction-to-meaning-representation" id="toc-introduction-to-meaning-representation" class="nav-link" data-scroll-target="#introduction-to-meaning-representation">Introduction to Meaning Representation</a></li>
  <li><a href="#challenges-in-meaning-representation" id="toc-challenges-in-meaning-representation" class="nav-link" data-scroll-target="#challenges-in-meaning-representation">Challenges in Meaning Representation</a></li>
  <li><a href="#logical-semantics-overview" id="toc-logical-semantics-overview" class="nav-link" data-scroll-target="#logical-semantics-overview">Logical Semantics Overview</a></li>
  <li><a href="#components-of-first-order-logic-fol" id="toc-components-of-first-order-logic-fol" class="nav-link" data-scroll-target="#components-of-first-order-logic-fol">Components of First-Order Logic (FOL)</a></li>
  <li><a href="#example-of-first-order-logic-fol" id="toc-example-of-first-order-logic-fol" class="nav-link" data-scroll-target="#example-of-first-order-logic-fol">Example of First-Order Logic (FOL)</a></li>
  <li><a href="#quantifiers-in-first-order-logic-fol" id="toc-quantifiers-in-first-order-logic-fol" class="nav-link" data-scroll-target="#quantifiers-in-first-order-logic-fol">Quantifiers in First-Order Logic (FOL)</a></li>
  <li><a href="#logical-semantics-and-truth-conditions" id="toc-logical-semantics-and-truth-conditions" class="nav-link" data-scroll-target="#logical-semantics-and-truth-conditions">Logical Semantics and Truth Conditions</a></li>
  <li><a href="#what-is-semantic-role-labeling-srl" id="toc-what-is-semantic-role-labeling-srl" class="nav-link" data-scroll-target="#what-is-semantic-role-labeling-srl">What is Semantic Role Labeling (SRL)?</a></li>
  <li><a href="#importance-of-semantic-role-labeling-srl" id="toc-importance-of-semantic-role-labeling-srl" class="nav-link" data-scroll-target="#importance-of-semantic-role-labeling-srl">Importance of Semantic Role Labeling (SRL)</a></li>
  <li><a href="#key-concepts-in-srl" id="toc-key-concepts-in-srl" class="nav-link" data-scroll-target="#key-concepts-in-srl">Key Concepts in SRL</a></li>
  <li><a href="#semantic-role-sets" id="toc-semantic-role-sets" class="nav-link" data-scroll-target="#semantic-role-sets">Semantic Role Sets</a></li>
  <li><a href="#srl-and-machine-learning" id="toc-srl-and-machine-learning" class="nav-link" data-scroll-target="#srl-and-machine-learning">SRL and Machine Learning</a></li>
  <li><a href="#evaluation-metrics-for-srl" id="toc-evaluation-metrics-for-srl" class="nav-link" data-scroll-target="#evaluation-metrics-for-srl">Evaluation Metrics for SRL</a></li>
  <li><a href="#logical-semantics-vs.-srl" id="toc-logical-semantics-vs.-srl" class="nav-link" data-scroll-target="#logical-semantics-vs.-srl">Logical Semantics vs.&nbsp;SRL</a></li>
  <li><a href="#applications-of-logical-semantics-and-srl" id="toc-applications-of-logical-semantics-and-srl" class="nav-link" data-scroll-target="#applications-of-logical-semantics-and-srl">Applications of Logical Semantics and SRL</a></li>
  <li><a href="#review-questions" id="toc-review-questions" class="nav-link" data-scroll-target="#review-questions">Review Questions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/NLP/Week01.html">NLP</a></li><li class="breadcrumb-item"><a href="../../pages/NLP/Week04.html">Week 4</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Syntax, Dependency Parsing, and SLR</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="phrasal-categories" class="level1">
<h1>Phrasal Categories</h1>
<ul>
<li><p><strong>Phrasal categories</strong> group words into units that function as single elements in a sentence’s structure. These are also sometimes called syntactic categories. They can be lexical or functional.</p></li>
<li><p><strong>Noun Phrase (NP):</strong> Acts as the subject or object of a verb, or the object of a preposition. Can be simple (a single noun) or complex. Includes determiners, adjectives, and other modifiers associated with the noun. A key characteristic of NPs is that they can often be replaced by pronouns. For example:</p>
<ul>
<li><em>The big red ball</em> bounced. (NP: <em>The big red ball</em>)</li>
<li>She threw <em>the old, worn-out toy</em>. (NP: <em>the old, worn-out toy</em>)</li>
</ul></li>
<li><p><strong>Verb Phrase (VP):</strong> Expresses an action or state of being. Includes the main verb and any auxiliaries, adverbs, or other elements that modify or complete the verb’s meaning. For example:</p>
<ul>
<li>She <em>is running quickly</em>. (VP: <em>is running quickly</em>)</li>
<li>They <em>have been playing the game for hours</em>. (VP: <em>have been playing the game for hours</em>)</li>
</ul></li>
<li><p><strong>Prepositional Phrase (PP):</strong> Begins with a preposition and typically includes a noun phrase as its object. Modifies a verb, noun, or adjective, indicating location, time, manner, or other relationships. For example:</p>
<ul>
<li>The cat sat <em>on the mat</em>. (PP: <em>on the mat</em>)</li>
<li>He arrived <em>in the morning</em>. (PP: <em>in the morning</em>)</li>
<li>The book <em>with the torn cover</em> is mine. (PP: <em>with the torn cover</em>)</li>
</ul></li>
<li><p><strong>Adjective Phrase (AP):</strong> Modifies a noun and provides more information about its qualities. It’s headed by an adjective and may include adverbs that modify the adjective. For example:</p>
<ul>
<li>She wore a <em>brightly colored</em> dress. (AP: <em>brightly colored</em>)</li>
<li>The cake was <em>too sweet</em>. (AP: <em>too sweet</em>)</li>
</ul></li>
<li><p><strong>Adverb Phrase (AdvP):</strong> Modifies a verb, adjective, or another adverb. It’s headed by an adverb and can include other modifying adverbs. For example:</p>
<ul>
<li>He ran <em>very quickly</em>. (AdvP: <em>very quickly</em>)</li>
<li>She sang <em>incredibly beautifully</em>. (AdvP: <em>incredibly beautifully</em>)</li>
</ul></li>
</ul>
<section id="phrase-structure-grammar-psg" class="level2">
<h2 class="anchored" data-anchor-id="phrase-structure-grammar-psg">Phrase Structure Grammar (PSG)</h2>
<ul>
<li><p><strong>Phrase Structure Grammar (PSG)</strong> is a formal system that describes the hierarchical syntactic structure of sentences. It uses a set of <strong>phrase structure rules</strong> to define how smaller linguistic units can be combined to form larger units.</p></li>
<li><p><strong>Key Components:</strong></p>
<ul>
<li><strong>Lexicon</strong>: A list of words in the language (terminal symbols) and their corresponding syntactic categories (non-terminal symbols).</li>
<li><strong>Phrase Structure Rules</strong>: Rules that specify how syntactic categories can be combined to form phrases. These rules are typically written in the form <span class="math inline">\(A \rightarrow B \ C\)</span>, where <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> are syntactic categories. The symbol on the left-hand side (<span class="math inline">\(A\)</span>) is the parent category, while the symbols on the right-hand side (<span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>) are the child categories.</li>
<li><strong>Start Symbol</strong>: A designated non-terminal symbol (usually <code>S</code> for sentence) that represents the top-level structure of the sentence.</li>
</ul></li>
<li><p><strong>Example:</strong></p>
<ul>
<li><strong>Lexicon:</strong>
<ul>
<li>the: Det</li>
<li>cat: N</li>
<li>sat: V</li>
<li>on: P</li>
<li>mat: N</li>
</ul></li>
<li><strong>Phrase Structure Rules:</strong>
<ul>
<li>S → NP VP</li>
<li>NP → Det N</li>
<li>VP → V PP</li>
<li>PP → P NP</li>
</ul></li>
<li><strong>Start Symbol</strong>: S</li>
</ul></li>
<li><p><strong>Generating Sentences with PSG</strong>: By applying the phrase structure rules recursively, starting from the start symbol, we can generate a tree structure that represents the syntactic structure of a sentence. This tree is called a <strong>parse tree</strong>.</p></li>
<li><p><strong>Example Parse Tree</strong>: The sentence “The cat sat on the mat” can be represented by the following parse tree:</p></li>
</ul>
<pre><code>      S
     / \
    NP   VP
   / \   / \
  Det N  V  PP
  |  |  |  / \
  the cat sat P  NP
           |  / \
           on Det N
              |  |
              the mat</code></pre>
<ul>
<li>The parse tree shows the hierarchical relationships between the words in the sentence. For example, the verb phrase “sat on the mat” consists of the verb “sat” and the prepositional phrase “on the mat”, which in turn consists of the preposition “on” and the noun phrase “the mat”.</li>
</ul>
</section>
</section>
<section id="sentence-structure-in-syntax" class="level1">
<h1>Sentence Structure in Syntax</h1>
<ul>
<li><p>A sentence in any language is expected to follow a certain structure. For instance, in English, it must contain both a Noun Phrase (NP) and a Verb Phrase (VP).</p></li>
<li><p>The most basic structure of a well-formed English sentence can be represented by the following phrase structure rule:</p>
<ul>
<li><p><span class="math inline">\(S \rightarrow NP\ VP\)</span></p></li>
<li><p>This rule indicates that a sentence (S) consists of a noun phrase (NP) followed by a verb phrase (VP).</p></li>
</ul></li>
<li><p><strong>Examples:</strong></p>
<ul>
<li><em>The dog barked.</em>
<ul>
<li>NP = The dog</li>
<li>VP = barked</li>
</ul></li>
<li><em>The cat sat on the mat.</em>
<ul>
<li>NP = The cat</li>
<li>VP = sat on the mat</li>
</ul></li>
</ul></li>
<li><p>Any sentence that deviates from this basic structure is considered <strong>ill-formed</strong> in English.</p></li>
<li><p><strong>Examples of Ill-formed Sentences:</strong></p>
<ul>
<li><em>The dog.</em> (Missing VP)</li>
<li><em>Barked.</em> (Missing NP)</li>
</ul></li>
<li><p>While this rule captures the fundamental structure, it’s important to note that sentences can be more complex, involving various types of clauses, phrases, and other grammatical elements. However, the core principle of a sentence requiring both a subject (usually represented by NP) and a predicate (usually represented by VP) remains consistent.</p></li>
</ul>
</section>
<section id="types-of-clauses-and-sentences" class="level1">
<h1>Types of Clauses and Sentences</h1>
<ul>
<li><strong>Clauses:</strong>
<ul>
<li><strong>Independent Clause:</strong> A clause that can stand alone as a complete sentence. It expresses a complete thought and has a subject and a predicate.
<ul>
<li>Example: <em>I went to the store.</em></li>
</ul></li>
<li><strong>Dependent Clause:</strong> A clause that cannot stand alone as a sentence. It depends on an independent clause to complete its meaning and often starts with a subordinating conjunction (because, if, when, although, etc.).
<ul>
<li>Example: <em>If I go out</em> (This clause needs an independent clause to make sense, e.g., <em>If I go out, I will buy some milk.</em>)</li>
</ul></li>
</ul></li>
<li><strong>Types of Sentences:</strong>
<ul>
<li><strong>Simple Sentence:</strong> Contains only one independent clause.
<ul>
<li>Example: <em>I like pizza.</em></li>
</ul></li>
<li><strong>Compound Sentence:</strong> Contains two or more independent clauses joined by a coordinating conjunction (and, but, or, nor, for, so, yet) or a semicolon (;).
<ul>
<li>Example: <em>I like pizza, and he likes pasta.</em></li>
</ul></li>
<li><strong>Complex Sentence:</strong> Contains one independent clause and at least one dependent clause.
<ul>
<li>Example: <em>I laughed when he fell.</em> (The dependent clause <em>when he fell</em> modifies the verb <em>laughed</em> in the independent clause).</li>
</ul></li>
<li><strong>Compound-Complex Sentence:</strong> Contains at least two independent clauses and one or more dependent clauses.
<ul>
<li>Example: <em>I laughed when he fell, but he was fine.</em> (Two independent clauses: <em>I laughed</em> and <em>he was fine</em>, and one dependent clause: <em>when he fell</em>).</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="complexities-in-syntax-ambiguities-garden-path-recursiveness-ellipsis" class="level1">
<h1>Complexities in Syntax: Ambiguities, Garden-Path, Recursiveness, Ellipsis</h1>
<ul>
<li><strong>Ambiguities:</strong> These occur when a sentence can be interpreted in multiple ways.
<ul>
<li><strong>Structural Ambiguity:</strong> Arises from the different possible ways to group words and phrases together, leading to multiple valid parse trees. For example, “I saw the man with the telescope” can mean either “I used the telescope to see the man” or “I saw a man who possessed a telescope.”</li>
<li><strong>Coordination Ambiguity:</strong> Occurs due to the uncertain scope of conjunctions like “and.” For instance, in “old men and women,” it’s unclear whether “old” modifies only “men” or both “men and women.”</li>
</ul></li>
<li><strong>Garden-Path Sentences:</strong> These sentences initially lead the reader towards an incorrect interpretation due to a temporary ambiguity. The reader is “led down the garden path” only to realize later that a different parsing is required. A classic example is “The horse raced past the barn fell.” The initial parse might lead you to believe “raced” is the main verb, but the correct parse has “fell” as the main verb, modifying “horse” – the horse that was raced past the barn fell.</li>
<li><strong>Recursiveness:</strong> A key feature of natural language is the ability to embed structures within structures. This recursiveness means that phrases can contain other phrases of the same type, theoretically allowing for infinitely long sentences. A simple example is “This is the cat that ate the rat that ate the cheese.” The relative clause “that ate the rat” itself contains another relative clause “that ate the cheese,” demonstrating recursion.</li>
<li><strong>Ellipsis:</strong> This refers to the omission of words or phrases that are understood from the context. While this makes language concise, it can add to the complexity of parsing. For example, in the sentences “I went to the store, and he did too,” the second sentence omits “went to the store,” relying on the context of the first sentence.</li>
</ul>
</section>
<section id="introduction-to-context-free-grammar-cfg" class="level1">
<h1>Introduction to Context-Free Grammar (CFG)</h1>
<ul>
<li><p><strong>Context-Free Grammar (CFG)</strong> is a formal system used to describe the syntax of natural languages. It provides a set of rules that define how words and phrases can be combined to form grammatically correct sentences.</p></li>
<li><p><strong>Formal Definition:</strong> A CFG, often denoted as <span class="math inline">\(G\)</span>, is a 4-tuple: <span class="math display">\[G = (V, \Sigma, R, S)\]</span> where:</p>
<ul>
<li><span class="math inline">\(V\)</span>: A finite set of <em>non-terminal symbols</em> or <em>variables</em>. These represent syntactic categories like Noun Phrase (NP), Verb Phrase (VP), etc.</li>
<li><span class="math inline">\(\Sigma\)</span>: A finite set of <em>terminal symbols</em>, which are the actual words of the language.</li>
<li><span class="math inline">\(R\)</span>: A finite set of <em>production rules</em> or <em>rewrite rules</em> of the form <span class="math inline">\(A \rightarrow \alpha\)</span>, where <span class="math inline">\(A \in V\)</span> (a non-terminal) and <span class="math inline">\(\alpha \in (V \cup \Sigma)^*\)</span> (a string of terminals and/or non-terminals). These rules specify how non-terminals can be rewritten as sequences of terminals and other non-terminals.</li>
<li><span class="math inline">\(S\)</span>: The <em>start symbol</em>, a special non-terminal that represents the whole sentence (often denoted as ‘S’).</li>
</ul></li>
<li><p><strong>Generative Process:</strong> CFGs are generative grammars. Starting from the start symbol, production rules are applied repeatedly to derive a string of terminal symbols, which represents a sentence. This process can be visualized as a tree, called a <em>parse tree</em> or <em>derivation tree</em>.</p></li>
<li><p><strong>Example:</strong> Consider the rule <span class="math inline">\(NP \rightarrow Det\ Noun\)</span>. This rule states that a Noun Phrase (NP) can be rewritten as a Determiner (Det) followed by a Noun. Applying this rule to the non-terminal ‘NP’ could generate the string “the cat,” where ‘the’ is a terminal symbol belonging to the category Det and ‘cat’ is a terminal symbol of category Noun.</p></li>
</ul>
</section>
<section id="cfg-rules-and-example" class="level1">
<h1>CFG Rules and Example</h1>
<ul>
<li><p><strong>Context-Free Grammar (CFG)</strong> rules are used to define how symbols can be rewritten as other symbols. They provide a way to generate grammatically correct structures in a language.</p></li>
<li><p>CFG rules follow a specific format:</p>
<ul>
<li><p><strong>Left-hand Side (LHS):</strong> A single non-terminal symbol (e.g., NP, VP). This represents a syntactic category.</p></li>
<li><p><strong>Right-hand Side (RHS):</strong> A sequence of one or more terminals or non-terminals. This shows how the LHS symbol can be expanded.</p></li>
<li><p><strong>Arrow (→):</strong> Indicates “can be rewritten as.”</p></li>
</ul></li>
<li><p><strong>Example Rules:</strong></p>
<ul>
<li><code>NP → Det Nominal</code></li>
<li><code>VP → Verb NP</code></li>
<li><code>Nominal → Noun | Nominal Noun</code></li>
</ul></li>
<li><p><strong>Explanation:</strong></p>
<ul>
<li>The first rule states that a Noun Phrase (NP) can be rewritten as a Determiner (Det) followed by a Nominal.</li>
<li>The second rule indicates that a Verb Phrase (VP) can be a Verb followed by a Noun Phrase.</li>
<li>The third rule uses the or-symbol <code>|</code> to show that a Nominal can be either a single Noun or a Nominal followed by another Noun (allowing for noun compounds).</li>
</ul></li>
<li><p><strong>Terminals vs.&nbsp;Non-terminals:</strong></p>
<ul>
<li><p><strong>Terminals:</strong> Actual words in the language (e.g., “the,” “flight”).</p></li>
<li><p><strong>Non-terminals:</strong> Syntactic categories that can be further expanded (e.g., NP, VP).</p></li>
</ul></li>
<li><p><strong>Derivation:</strong> The process of applying CFG rules to generate a sequence of terminals (a sentence) is called a derivation.</p></li>
<li><p><strong>Example Derivation:</strong> Let’s derive the noun phrase “the flight” using the rules above:</p>
<ol type="1">
<li><p><strong>Start with the non-terminal <code>NP</code>.</strong></p></li>
<li><p><strong>Apply the rule <code>NP → Det Nominal</code>.</strong> This gives us: <code>Det Nominal</code></p></li>
<li><p><strong>Apply the rule <code>Det → the</code>.</strong> Now we have: <code>the Nominal</code></p></li>
<li><p><strong>Apply the rule <code>Nominal → Noun</code>.</strong> We get: <code>the Noun</code></p></li>
<li><p><strong>Finally, apply the rule <code>Noun → flight</code>.</strong> This results in: <code>the flight</code></p></li>
</ol></li>
<li><p>This derivation demonstrates how CFG rules can be used to generate a valid noun phrase. The process can be extended to generate entire sentences by applying rules to the start symbol (usually ‘S’ for sentence) and recursively expanding non-terminals until only terminals remain.</p></li>
</ul>
</section>
<section id="parsing-with-cfg-and-parse-trees" class="level1">
<h1>Parsing with CFG and Parse Trees</h1>
<ul>
<li><p><strong>Parsing</strong> is the process of analyzing a sentence to determine its grammatical structure according to a given Context-Free Grammar (CFG). It involves assigning a syntactic structure, typically represented as a tree, to the sentence.</p></li>
<li><p><strong>Parse Trees</strong>, also known as syntax trees or derivation trees, visually represent the syntactic structure derived from the parsing process. They depict the hierarchical relationships between words and phrases in a sentence, based on the rules of the CFG.</p></li>
<li><p><strong>Components of a Parse Tree:</strong></p>
<ul>
<li><strong>Root Node</strong>: Represents the start symbol of the grammar, usually ‘S’ for sentence.</li>
<li><strong>Internal Nodes</strong>: Represent non-terminal symbols from the CFG (e.g., NP, VP).</li>
<li><strong>Leaf Nodes</strong>: Represent terminal symbols, which are the words of the sentence.</li>
<li><strong>Branches</strong>: Connect nodes, indicating how constituents are combined based on grammar rules.</li>
</ul></li>
<li><p><strong>Derivation</strong>: The process of building a parse tree is called derivation. It starts from the start symbol and applies production rules of the CFG to progressively rewrite non-terminal symbols until only terminal symbols (words) remain.</p></li>
<li><p><strong>Example:</strong> Consider the sentence “The cat sat on the mat” and a simple CFG with the following rules:</p>
<ol type="1">
<li><span class="math inline">\(S \rightarrow NP\ VP\)</span></li>
<li><span class="math inline">\(NP \rightarrow Det\ N\)</span></li>
<li><span class="math inline">\(VP \rightarrow V\ PP\)</span></li>
<li><span class="math inline">\(PP \rightarrow P\ NP\)</span></li>
</ol></li>
<li><p>The parse tree for this sentence would look like this:</p></li>
</ul>
<pre><code>     S
    / \
   NP   VP
   |    | \
  Det  N  PP
   |   |  | \ 
  The cat V  NP
         |  | \
        sat P  Det N
            |  |   |
           on the mat </code></pre>
<ul>
<li><p>This tree shows that the sentence consists of a noun phrase (NP) “The cat” and a verb phrase (VP) “sat on the mat.” The VP further breaks down into a verb (V) “sat” and a prepositional phrase (PP) “on the mat.” Each branch corresponds to the application of a grammar rule.</p></li>
<li><p><strong>Ambiguity</strong>: A sentence can have multiple valid parse trees, leading to ambiguity. This is a common challenge in parsing.</p></li>
<li><p><strong>Applications:</strong> Parse trees are crucial for various NLP tasks, including:</p>
<ul>
<li><strong>Understanding Sentence Structure</strong>: Analyzing the grammatical relationships between words.</li>
<li><strong>Machine Translation</strong>: Understanding the source language structure for accurate translation.</li>
<li><strong>Information Extraction</strong>: Identifying key elements and their roles in a sentence.</li>
<li><strong>Question Answering</strong>: Interpreting the question and finding relevant information in text.</li>
</ul></li>
</ul>
</section>
<section id="l₀-and-the-lexicon-for-l₀" class="level1">
<h1>L₀ and the lexicon for L₀</h1>
<p>This section introduces a miniature English grammar, denoted as <span class="math inline">\(\mathcal{L}_0\)</span>, along with its corresponding lexicon. The grammar rules for <span class="math inline">\(\mathcal{L}_0\)</span> are presented in a simplified format, demonstrating how various parts of speech can be combined to create meaningful phrases and sentences.</p>
<section id="lexicon" class="level2">
<h2 class="anchored" data-anchor-id="lexicon">Lexicon</h2>
<p>The lexicon for <span class="math inline">\(\mathcal{L}_0\)</span> comprises a set of words categorized into different parts of speech. This lexicon serves as the vocabulary for the grammar rules, providing the terminal symbols that can be used to construct sentences.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Words</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Noun</td>
<td>flights, flight, breeze, trip, morning</td>
</tr>
<tr class="even">
<td>Verb</td>
<td>is, prefer, like, need, want, fly, do</td>
</tr>
<tr class="odd">
<td>Adjective</td>
<td>cheapest, non-stop, first, latest, other, direct</td>
</tr>
<tr class="even">
<td>Pronoun</td>
<td>me, I, you, it</td>
</tr>
<tr class="odd">
<td>Proper-Noun</td>
<td>Alaska, Baltimore, Los Angeles, Chicago, United, American</td>
</tr>
<tr class="even">
<td>Determiner</td>
<td>the, a, an, this, these, that</td>
</tr>
<tr class="odd">
<td>Preposition</td>
<td>from, to, on, near, in</td>
</tr>
<tr class="even">
<td>Conjunction</td>
<td>and, or, but</td>
</tr>
</tbody>
</table>
</section>
<section id="grammar-rules" class="level2">
<h2 class="anchored" data-anchor-id="grammar-rules">Grammar Rules</h2>
<p>The grammar rules for <span class="math inline">\(\mathcal{L}_0\)</span> are defined using a simple notation. Each rule consists of a left-hand side (LHS) and a right-hand side (RHS), separated by an arrow. The LHS represents a non-terminal symbol, which can be further expanded, while the RHS specifies the possible expansions, consisting of terminal symbols (words from the lexicon) and/or other non-terminal symbols.</p>
<p>Here are the grammar rules for <span class="math inline">\(\mathcal{L}_0\)</span>, accompanied by illustrative examples for each rule:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Rule</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S → NP VP</td>
<td>A sentence consists of a noun phrase followed by a verb phrase.</td>
<td>I + want a morning flight</td>
</tr>
<tr class="even">
<td>NP → Pronoun</td>
<td>A noun phrase can be a pronoun.</td>
<td>I</td>
</tr>
<tr class="odd">
<td>NP → Proper-Noun</td>
<td>A noun phrase can be a proper noun.</td>
<td>Los Angeles</td>
</tr>
<tr class="even">
<td>NP → Det Nominal</td>
<td>A noun phrase can be a determiner followed by a nominal.</td>
<td>a + flight</td>
</tr>
<tr class="odd">
<td>NP → Nominal Noun</td>
<td>A noun phrase can be a nominal followed by a noun.</td>
<td>morning + flight</td>
</tr>
<tr class="even">
<td>NP → Noun</td>
<td>A noun phrase can be a single noun.</td>
<td>flights</td>
</tr>
<tr class="odd">
<td>VP → Verb</td>
<td>A verb phrase can be a single verb.</td>
<td>do</td>
</tr>
<tr class="even">
<td>VP → Verb NP</td>
<td>A verb phrase can be a verb followed by a noun phrase.</td>
<td>want + a flight</td>
</tr>
<tr class="odd">
<td>VP → Verb NP PP</td>
<td>A verb phrase can be a verb followed by a noun phrase and a prepositional phrase.</td>
<td>leave + Boston + in the morning</td>
</tr>
<tr class="even">
<td>VP → Verb PP</td>
<td>A verb phrase can be a verb followed by a prepositional phrase.</td>
<td>leaving + on Thursday</td>
</tr>
<tr class="odd">
<td>PP → Preposition NP</td>
<td>A prepositional phrase consists of a preposition followed by a noun phrase.</td>
<td>from + Los Angeles</td>
</tr>
</tbody>
</table>
<p>These rules, in conjunction with the lexicon, define the permissible sentence structures and word combinations within the miniature English grammar <span class="math inline">\(\mathcal{L}_0\)</span>.</p>
</section>
</section>
<section id="parse-tree" class="level1">
<h1>Parse Tree</h1>
<ul>
<li><p>A parse tree visually represents the syntactic structure of a sentence derived from a grammar. It is a hierarchical structure where:</p>
<ul>
<li>The <strong>root node</strong> represents the start symbol of the grammar (usually ‘S’ for sentence).</li>
<li><strong>Internal nodes</strong> represent non-terminal symbols (e.g., NP, VP, PP).</li>
<li><strong>Leaf nodes</strong> represent terminal symbols (words in the sentence).</li>
</ul></li>
<li><p>Each node shows how the corresponding symbol is rewritten according to the grammar rules.</p></li>
<li><p>Example: For the sentence “I prefer a morning flight,” and a simplified grammar, the parse tree might look like:</p></li>
</ul>
<pre><code>       S
      / \
    NP    VP
    |     /  \
    I   V    NP
        |    / \
     prefer Det  Nom
             |   / \
             a  Adj Noun
                |    |
             morning flight</code></pre>
<ul>
<li><p>This tree shows that:</p>
<ul>
<li>The sentence ‘S’ consists of a Noun Phrase (NP) and a Verb Phrase (VP).</li>
<li>The NP “I” is a simple pronoun.</li>
<li>The VP “prefer a morning flight” consists of the verb ‘prefer’ and another NP.</li>
<li>This nested NP further breaks down into a determiner (‘a’), an adjective (‘morning’), and a noun (‘flight’).</li>
</ul></li>
<li><p>Parse trees provide a clear and unambiguous representation of the syntactic relationships between words in a sentence, revealing the hierarchical grouping of words into phrases.</p></li>
</ul>
</section>
<section id="treebanks" class="level1">
<h1>Treebanks</h1>
<ul>
<li><p><strong>Definition:</strong> A treebank is a corpus in which each sentence has been annotated with its syntactic structure. This structure is typically represented as a parse tree, either in constituency or dependency format.</p></li>
<li><p><strong>Purpose:</strong> Treebanks are essential resources for developing and evaluating natural language processing (NLP) systems, particularly those focused on syntactic parsing. They serve as training data for machine learning models and as gold standards for measuring the accuracy of parsers.</p></li>
<li><p><strong>Construction:</strong></p>
<ul>
<li><strong>Manual Annotation:</strong> Linguists carefully analyze sentences and manually assign syntactic structures, following specific linguistic guidelines. This process is time-consuming and requires expert knowledge but yields high-quality annotations.</li>
<li><strong>Automatic Parsing + Human Correction:</strong> Parsers automatically generate initial parse trees, which are then reviewed and corrected by human annotators. This approach is faster but can be less accurate than purely manual annotation.</li>
<li><strong>Conversion from Other Formalisms:</strong> Some treebanks are created by converting annotations from different syntactic formalisms, such as phrase-structure trees to dependency trees. This allows for leveraging existing resources and facilitating cross-formalism comparisons.</li>
</ul></li>
<li><p><strong>Representations:</strong></p>
<ul>
<li><strong>Bracketed Notation:</strong> Trees are represented using nested parentheses, commonly in LISP-style format. For example, a simple sentence like “The cat sat on the mat” could be represented as <code>(S (NP (DT The) (NN cat)) (VP (VBD sat) (PP (IN on) (NP (DT the) (NN mat)))))</code>.</li>
<li><strong>Node-and-Line Trees:</strong> Trees are depicted graphically with nodes representing syntactic categories and lines connecting them to show parent-child relationships.</li>
</ul></li>
<li><p><strong>Linguistic Insights:</strong> Analyzing treebanks allows for investigating various linguistic phenomena, such as the frequency of different grammatical constructions, the distribution of dependency relations, and the prevalence of non-projective dependencies in specific languages.</p></li>
<li><p><strong>Grammar Induction:</strong> Treebanks can be used to automatically extract grammar rules for a language. By analyzing the patterns of syntactic structures in the annotated sentences, statistical models can learn probabilistic context-free grammars (PCFGs) that capture the syntactic regularities of the language.</p></li>
<li><p><strong>Examples:</strong></p>
<ul>
<li><strong>Penn Treebank:</strong> One of the earliest and most influential treebanks, containing annotated English text from the Wall Street Journal. It has been extended to include other languages like Arabic and Chinese.</li>
<li><strong>Universal Dependencies (UD):</strong> A large-scale project aiming to create cross-linguistically consistent treebanks for over 100 languages, using a standardized set of dependency relations and annotation guidelines.</li>
</ul></li>
<li><p><strong>Impact on NLP:</strong> Treebanks have significantly advanced the field of NLP by providing training data for parsing models, enabling the development of more accurate and robust parsers. They have also contributed to research on syntactic phenomena, cross-linguistic analysis, and grammar induction.</p></li>
</ul>
</section>
<section id="penn-treebank" class="level1">
<h1>Penn Treebank</h1>
<section id="detailed-overview" class="level2">
<h2 class="anchored" data-anchor-id="detailed-overview">Detailed Overview</h2>
<p>The Penn Treebank is a widely used resource in natural language processing (NLP) that provides a large corpus of English text annotated with syntactic structure. It serves as a valuable training and evaluation dataset for various NLP tasks, particularly those related to parsing and syntactic analysis.</p>
<section id="key-features" class="level3">
<h3 class="anchored" data-anchor-id="key-features">Key Features</h3>
<ul>
<li><p><strong>Annotated Sentences</strong>: The core of the Penn Treebank consists of a vast collection of sentences from different sources, including the Wall Street Journal, Brown Corpus, and Switchboard corpus. Each sentence is meticulously annotated with its syntactic structure using a hierarchical tree representation.</p></li>
<li><p><strong>Constituency-Based Representation</strong>: The annotation scheme of the Penn Treebank is based on constituency parsing, which breaks down sentences into their constituent parts, such as noun phrases (NP), verb phrases (VP), and prepositional phrases (PP).</p></li>
<li><p><strong>Hierarchical Tree Structure</strong>: The syntactic structure of each sentence is represented as a tree, where each node corresponds to a constituent. The root node represents the entire sentence (S), and its children represent the major constituents, which are further broken down into smaller constituents.</p></li>
<li><p><strong>Part-of-Speech (POS) Tags</strong>: Each word in the corpus is also tagged with its part of speech, such as noun, verb, adjective, adverb, etc. These POS tags provide additional information about the grammatical role of each word.</p></li>
<li><p><strong>Standardized Notation</strong>: The Penn Treebank uses a standardized notation for representing syntactic structure, which has been widely adopted in the NLP community. This notation consists of parentheses and labels that indicate the type of constituent and its relationships to other constituents.</p></li>
</ul>
</section>
<section id="applications" class="level3">
<h3 class="anchored" data-anchor-id="applications">Applications</h3>
<p>The Penn Treebank has been instrumental in advancing research and development in several NLP areas, including:</p>
<ul>
<li><p><strong>Parsing</strong>: Training and evaluating statistical parsers that automatically assign syntactic structure to sentences.</p></li>
<li><p><strong>Grammar Induction</strong>: Extracting grammar rules from the annotated data to build formal grammars for English.</p></li>
<li><p><strong>Syntactic Analysis</strong>: Studying linguistic phenomena related to sentence structure, such as phrase structure, dependency relations, and constituent types.</p></li>
<li><p><strong>Language Modeling</strong>: Incorporating syntactic information into language models to improve their accuracy and fluency.</p></li>
<li><p><strong>Machine Translation</strong>: Enhancing the quality of machine translation systems by leveraging syntactic knowledge to better align and translate sentences.</p></li>
</ul>
</section>
<section id="example-annotation" class="level3">
<h3 class="anchored" data-anchor-id="example-annotation">Example Annotation</h3>
<p>A simplified example of a Penn Treebank annotation for the sentence “The cat sat on the mat” is as follows:</p>
<pre><code>(S
  (NP (DT The) (NN cat))
  (VP (VBD sat)
    (PP (IN on)
      (NP (DT the) (NN mat))))
)</code></pre>
<p>In this example:</p>
<ul>
<li><strong>S</strong>: Represents the sentence.</li>
<li><strong>NP</strong>: Represents a Noun Phrase.</li>
<li><strong>VP</strong>: Represents a Verb Phrase.</li>
<li><strong>PP</strong>: Represents a Prepositional Phrase.</li>
<li><strong>DT</strong>: Represents a Determiner.</li>
<li><strong>NN</strong>: Represents a Noun.</li>
<li><strong>VBD</strong>: Represents a Verb in the Past Tense.</li>
<li><strong>IN</strong>: Represents a Preposition.</li>
</ul>
</section>
<section id="significance" class="level3">
<h3 class="anchored" data-anchor-id="significance">Significance</h3>
<p>The Penn Treebank remains a cornerstone resource for NLP research and applications, providing a rich and valuable dataset for developing and evaluating systems that process and understand natural language. Its standardized annotation scheme, comprehensive coverage, and widespread use have made it an invaluable tool for advancing our understanding of syntax and its role in language processing.</p>
</section>
</section>
</section>
<section id="penn-treebank-sentences" class="level1">
<h1>Penn Treebank Sentences</h1>
<p>This section showcases examples of sentences from the Penn Treebank, a corpus where each sentence is annotated with a parse tree. The examples illustrate how the treebank represents the syntactic structure of sentences using labeled brackets.</p>
<section id="example-a" class="level2">
<h2 class="anchored" data-anchor-id="example-a">Example (a)</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>((S</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  (NP-SBJ (DT That)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    (JJ cold) (, , .)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    (JJ empty) (NN sky) )</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  (VP (VBD was)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    (ADJP-PRD (JJ full)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>      (PP (IN of)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        (NP (NN fire)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>          (CC and)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>          (NN light) ))))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  (, . .) ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This example demonstrates a complex sentence structure with nested constituents. Here’s a breakdown:</p>
<ul>
<li><strong>S</strong>: The top-level node, representing the entire sentence.</li>
<li><strong>NP-SBJ</strong>: Noun Phrase functioning as the subject of the sentence (“That cold, empty sky”).
<ul>
<li><strong>DT</strong>: Determiner (“That”).</li>
<li><strong>JJ</strong>: Adjectives (“cold”, “empty”).</li>
<li><strong>NN</strong>: Noun (“sky”).</li>
</ul></li>
<li><strong>VP</strong>: Verb Phrase (“was full of fire and light”).
<ul>
<li><strong>VBD</strong>: Verb, past tense (“was”).</li>
<li><strong>ADJP-PRD</strong>: Adjective Phrase functioning as a predicate (“full of fire and light”).
<ul>
<li><strong>JJ</strong>: Adjective (“full”).</li>
<li><strong>PP</strong>: Prepositional Phrase (“of fire and light”).
<ul>
<li><strong>IN</strong>: Preposition (“of”).</li>
<li><strong>NP</strong>: Noun Phrase (“fire and light”).
<ul>
<li><strong>NN</strong>: Nouns (“fire”, “light”).</li>
<li><strong>CC</strong>: Coordinating Conjunction (“and”).</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="example-b" class="level2">
<h2 class="anchored" data-anchor-id="example-b">Example (b)</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>((S</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  (NP-SBJ The/DT flight/NN )</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  (VP should/MD</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    (VP arrive/VB</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>      (PP-TMP at/IN</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        (NP eleven/CD a.m/RB ))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>      (NP-TMP tomorrow/NN ))))</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This example showcases a sentence with temporal modifiers.</p>
<ul>
<li><strong>S</strong>: Sentence.</li>
<li><strong>NP-SBJ</strong>: Noun Phrase, subject (“The flight”).</li>
<li><strong>VP</strong>: Verb Phrase (“should arrive at eleven a.m tomorrow”).
<ul>
<li><strong>MD</strong>: Modal verb (“should”).</li>
<li><strong>VP</strong>: Verb Phrase (“arrive at eleven a.m tomorrow”).
<ul>
<li><strong>VB</strong>: Verb (“arrive”).</li>
<li><strong>PP-TMP</strong>: Prepositional Phrase, temporal modifier (“at eleven a.m”).
<ul>
<li><strong>IN</strong>: Preposition (“at”).</li>
<li><strong>NP</strong>: Noun Phrase (“eleven a.m.”).
<ul>
<li><strong>CD</strong>: Cardinal Number (“eleven”).</li>
<li><strong>RB</strong>: Adverb (“a.m.”).</li>
</ul></li>
</ul></li>
<li><strong>NP-TMP</strong>: Noun Phrase, temporal modifier (“tomorrow”).</li>
</ul></li>
</ul></li>
</ul>
<p>These examples highlight how the Penn Treebank uses labeled brackets to represent the hierarchical relationships between different constituents in a sentence, providing a rich resource for studying syntax.</p>
</section>
</section>
<section id="cfg-rules-from-penn-treebank" class="level1">
<h1>CFG Rules from Penn Treebank</h1>
<ul>
<li>The Penn Treebank uses a <strong>context-free grammar (CFG)</strong> to represent the syntactic structure of sentences.</li>
<li>The grammar is extracted from the annotated trees in the treebank.</li>
<li>It is a <strong>very flat grammar</strong>, meaning that it has a large number of rules and that many of the rules are very specific.</li>
<li>For example, there are over 4,500 different rules for expanding verb phrases (VPs).</li>
</ul>
<section id="flattening-and-binarization" class="level2">
<h2 class="anchored" data-anchor-id="flattening-and-binarization">Flattening and Binarization</h2>
<ul>
<li>This flatness is due in part to the way that the treebank was created. The original annotations were done using a <strong>phrase-structure grammar</strong>, which is a more hierarchical type of grammar. However, the annotations were then <strong>flattened</strong> and <strong>binarized</strong> to make them easier to process by computers.</li>
</ul>
<section id="flattening" class="level3">
<h3 class="anchored" data-anchor-id="flattening">Flattening</h3>
<ul>
<li>Flattening involves removing intermediate nodes in the parse tree. For example, consider the following phrase-structure tree:</li>
</ul>
<pre><code>    S
   / \
  NP  VP
  |   |
  John slept</code></pre>
<ul>
<li>This tree could be flattened by removing the NP node:</li>
</ul>
<pre><code>  S
 / \
John slept</code></pre>
<ul>
<li>Flattening makes the grammar less hierarchical, but it also makes it more ambiguous.</li>
</ul>
</section>
<section id="binarization" class="level3">
<h3 class="anchored" data-anchor-id="binarization">Binarization</h3>
<ul>
<li>Binarization involves rewriting rules with more than two children as a sequence of binary rules. For example, the rule:</li>
</ul>
<pre><code>VP → V NP PP</code></pre>
<p>could be rewritten as two binary rules:</p>
<pre><code>VP → V XP
XP → NP PP</code></pre>
<ul>
<li>Binarization is necessary for some parsing algorithms, such as the <strong>CKY algorithm</strong>, which can only handle binary rules.</li>
</ul>
</section>
</section>
<section id="example-vp-expansion-rules" class="level2">
<h2 class="anchored" data-anchor-id="example-vp-expansion-rules">Example VP Expansion Rules</h2>
<ul>
<li>The result of flattening and binarization is a large number of very specific rules. Here are a few examples of the rules for expanding VPs in the Penn Treebank:</li>
</ul>
<pre><code>VP → VBD PP
VP → VBD PP PP
VP → VBD PP PP PP
VP → VBD PP PP PP PP
VP → VB ADVP PP
VP → VB PP ADVP
VP → ADVP VB PP</code></pre>
<ul>
<li><p>These rules cover different combinations of verb arguments and modifiers, resulting in fine-grained distinctions in VP structures.</p></li>
<li><p>Despite its flatness, the Penn Treebank grammar is a valuable resource for NLP research. It has been used to train a wide range of parsers, and it has also been used to study the syntactic structure of English.</p></li>
</ul>
</section>
</section>
<section id="grammar-equivalence-and-normal-form" class="level1">
<h1>Grammar Equivalence and Normal Form</h1>
<ul>
<li><strong>Grammar Equivalence:</strong> Determines if two grammars are essentially the same in terms of the languages they generate and the structures they assign.
<ul>
<li><strong>Strong Equivalence:</strong> Two grammars are strongly equivalent if they generate the exact same set of strings and assign the same phrase structure to each sentence. This allows for renaming of non-terminal symbols, meaning that the labels of the internal nodes in the parse trees can be different, but the overall structure must be identical.</li>
<li><strong>Weak Equivalence:</strong> Two grammars are weakly equivalent if they generate the same set of strings, but they may differ in the phrase structure they assign to those strings. In other words, they can produce different parse trees for the same sentence.</li>
</ul></li>
<li><strong>Normal Form:</strong> Specific forms for context-free grammars (CFGs) that simplify their structure while maintaining their generative capacity. These forms are beneficial for theoretical analysis and for developing efficient parsing algorithms.
<ul>
<li><strong>Chomsky Normal Form (CNF):</strong> A CFG is in CNF if all its production rules adhere to one of the following two forms:
<ul>
<li><span class="math inline">\(A \rightarrow BC\)</span>: A non-terminal symbol <span class="math inline">\(A\)</span> is rewritten as two non-terminal symbols <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>.</li>
<li><span class="math inline">\(A \rightarrow a\)</span>: A non-terminal symbol <span class="math inline">\(A\)</span> is rewritten as a single terminal symbol <span class="math inline">\(a\)</span>. CNF specifically excludes empty productions (rules of the form <span class="math inline">\(A \rightarrow \epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> represents the empty string).</li>
</ul></li>
<li><strong>Binary Branching:</strong> CNF enforces binary branching in parse trees. Each non-terminal node in a parse tree generated by a CNF grammar will have at most two children.</li>
<li><strong>Conversion:</strong> Any context-free grammar can be systematically converted into an equivalent grammar in CNF. This conversion process often involves introducing new non-terminal symbols and breaking down complex rules into simpler ones. For example, a rule like <span class="math inline">\(A \rightarrow BCD\)</span> can be transformed into two CNF rules:
<ul>
<li><span class="math inline">\(A \rightarrow BX\)</span></li>
<li><span class="math inline">\(X \rightarrow CD\)</span> Where <span class="math inline">\(X\)</span> is a new non-terminal symbol.</li>
</ul></li>
</ul></li>
<li><strong>Advantages of Normal Forms:</strong>
<ul>
<li><strong>Simplified Parsing:</strong> Normal forms like CNF make it easier to develop and implement parsing algorithms because the rules have a predictable, restricted form.</li>
<li><strong>Theoretical Analysis:</strong> Normal forms provide a standardized representation for CFGs, simplifying the analysis of their properties, such as ambiguity and the types of languages they can generate.</li>
<li><strong>Grammar Size Reduction:</strong> In some cases, converting a grammar to CNF can reduce the number of rules, although this is not always guaranteed.</li>
</ul></li>
</ul>
</section>
<section id="ambiguities-in-context-free-grammar-cfg" class="level1">
<h1>Ambiguities in Context-Free Grammar (CFG)</h1>
<ul>
<li>Ambiguity arises when a sentence can have multiple valid interpretations based on its structure or the meaning of its words. CFGs, while powerful for representing syntax, can sometimes fail to capture these nuances, leading to multiple parse trees for a single sentence.</li>
</ul>
<section id="types-of-ambiguities" class="level2">
<h2 class="anchored" data-anchor-id="types-of-ambiguities">Types of Ambiguities:</h2>
<ul>
<li><p><strong>Structural Ambiguity</strong>: This occurs when the grammatical structure of a sentence allows for multiple valid parse trees. The same sequence of words can be grouped differently, resulting in different interpretations.</p>
<p><strong>Example:</strong> <em>I saw the man with the telescope.</em></p>
<p>This sentence can be parsed in two ways:</p>
<ol type="1">
<li><strong>[S [NP I] [VP [V saw] [NP [NP the man] [PP with the telescope]]]]</strong>
<ul>
<li>Here, “with the telescope” modifies “the man,” suggesting the man possesses the telescope.</li>
</ul></li>
<li><strong>[S [NP I] [VP [VP [V saw] [NP the man]] [PP with the telescope]]]]</strong>
<ul>
<li>In this parse, “with the telescope” modifies the verb “saw,” implying the telescope was used for seeing.</li>
</ul></li>
</ol></li>
<li><p><strong>Lexical Ambiguity</strong>: This type of ambiguity stems from words having multiple meanings (polysemy). When a word with multiple senses appears in a sentence, the CFG might not be able to disambiguate which sense is intended.</p>
<p><strong>Example:</strong> <em>I went to the bank.</em></p>
<p>The word “bank” could refer to a financial institution or the edge of a river. The CFG would likely have rules that allow “bank” to be a noun in both contexts, leading to ambiguity:</p>
<ol type="1">
<li><p><strong>[S [NP I] [VP [V went] [PP to [NP the [N bank (financial)]]]]]</strong></p></li>
<li><p><strong>[S [NP I] [VP [V went] [PP to [NP the [N bank (river)]]]]]</strong></p></li>
</ol></li>
</ul>
</section>
<section id="handling-ambiguity" class="level2">
<h2 class="anchored" data-anchor-id="handling-ambiguity">Handling Ambiguity</h2>
<ul>
<li><p>Resolving ambiguity in CFGs often requires incorporating additional information, such as:</p>
<ul>
<li><strong>Semantic constraints</strong>: Using knowledge about word meanings and relationships to rule out implausible interpretations.</li>
<li><strong>Probabilistic parsing</strong>: Assigning probabilities to different parse trees based on statistical models trained on large corpora.</li>
<li><strong>Contextual information</strong>: Considering the surrounding text or the broader discourse to determine the intended meaning.</li>
</ul></li>
<li><p>It’s important to note that ambiguity is not always a problem. In some cases, multiple interpretations might be valid, and preserving this ambiguity might be desired. However, for tasks like machine translation or question answering, resolving ambiguity is crucial for accurate and meaningful results.</p></li>
</ul>
</section>
</section>
<section id="introduction-to-constituency-parsing" class="level1">
<h1>Introduction to Constituency Parsing</h1>
<p>Constituency Parsing is a fundamental task in Natural Language Processing (NLP) that aims to analyze the syntactic structure of a sentence by identifying its constituent phrases. A constituent is a group of words that function as a single unit within the sentence. These units can be nested within each other, forming a hierarchical structure.</p>
<p>The core idea behind constituency parsing is that sentences are not just linear sequences of words but are composed of meaningful groups of words, which play specific roles in conveying the sentence’s meaning. By identifying these constituents, we gain a deeper understanding of how the sentence is structured and how its meaning is derived.</p>
<p>Consider the sentence: “The cat sat on the mat.”</p>
<p>Constituency parsing would break this sentence down into the following constituents:</p>
<ul>
<li><strong>Noun Phrase (NP)</strong>: “The cat”</li>
<li><strong>Verb Phrase (VP)</strong>: “sat on the mat”
<ul>
<li><strong>Verb (V)</strong>: “sat”</li>
<li><strong>Prepositional Phrase (PP)</strong>: “on the mat”
<ul>
<li><strong>Preposition (P)</strong>: “on”</li>
<li><strong>Noun Phrase (NP)</strong>: “the mat”</li>
</ul></li>
</ul></li>
</ul>
<p>This hierarchical structure can be visually represented as a parse tree, where each node represents a constituent and branches indicate the relationships between them. The parse tree for the example sentence would look like this:</p>
<pre><code>      S
     / \
    NP   VP
    |    / \
    |   V   PP
    |   |   / \
    |   |  P  NP
    |   |  |   |
   The cat sat on the mat </code></pre>
<p>Constituency parsing is essential for various NLP applications, including:</p>
<ul>
<li><strong>Grammar Checking</strong>: Determining the grammatical correctness of a sentence.</li>
<li><strong>Machine Translation</strong>: Accurately translating sentences while preserving meaning and structure.</li>
<li><strong>Question Answering</strong>: Identifying relevant parts of a sentence to answer questions.</li>
<li><strong>Information Extraction</strong>: Extracting key information from text by understanding relationships between constituents.</li>
</ul>
<p>Various methods are used for constituency parsing, including rule-based approaches, statistical models, and deep learning techniques.</p>
</section>
<section id="constituents-in-natural-language" class="level1">
<h1>Constituents in Natural Language</h1>
<ul>
<li><p><strong>Constituents</strong> are groups of words that function as a single unit within a sentence’s syntactic structure. These units can be nested within each other, forming a hierarchical structure that reflects the sentence’s grammatical organization.</p></li>
<li><p>There are several <strong>tests</strong> that linguists use to identify whether a group of words forms a constituent:</p>
<ul>
<li><p><strong>Substitution:</strong> If a group of words can be replaced by a single word (like a pronoun) without changing the grammaticality of the sentence, it is likely a constituent. For example, in “The big dog barked,” the phrase “The big dog” can be replaced with “It” (“It barked”), suggesting that it is a constituent.</p></li>
<li><p><strong>Movement:</strong> If a group of words can be moved to a different position in the sentence while maintaining grammaticality, it’s evidence for constituency. Consider “The cat sat on the mat.” We can move “on the mat” to the beginning: “On the mat, the cat sat.” This ability to move as a unit points to constituency.</p></li>
<li><p><strong>Coordination:</strong> Conjunctions like “and” and “or” can link constituents of the same type. If a group of words can be coordinated with another group, it’s likely a constituent. For instance: “The cat sat on the mat and the dog slept under the table.” Here, “on the mat” and “under the table” are both prepositional phrases (PP), coordinated by “and,” indicating they are constituents of the same type.</p></li>
</ul></li>
<li><p><strong>Example:</strong> Consider the sentence: <em>The quick brown fox jumps over the lazy dog.</em></p>
<ul>
<li>“The quick brown fox” is a Noun Phrase (NP) constituent. It can be substituted with “It,” moved (“Over the lazy dog, the quick brown fox jumps”), and coordinated (“The quick brown fox and the playful cat…”).</li>
<li>“jumps over the lazy dog” is a Verb Phrase (VP) constituent.</li>
<li>“the lazy dog” is another NP constituent.</li>
<li>“over the lazy dog” is a Prepositional Phrase (PP) constituent.</li>
</ul></li>
<li><p>These tests help determine the constituent structure of sentences, which is crucial for understanding the grammatical relationships between words and for building accurate parsers.</p></li>
</ul>
</section>
<section id="introduction-to-cky-parsing" class="level1">
<h1>Introduction to CKY Parsing</h1>
<p>The Cocke-Kasami-Younger (CKY) algorithm is a dynamic programming approach used for parsing sentences with context-free grammars (CFGs). Its primary strength lies in its efficiency, achieved by systematically building a parse table that stores parsing results for all possible substrings of the input sentence. This bottom-up approach eliminates redundant computations, making it suitable for handling complex sentence structures.</p>
<p>The CKY algorithm’s name reflects its independent discovery by three individuals: John Cocke, Tadao Kasami, and Daniel Younger. It’s also known as CYK (Cocke-Younger-Kasami) parsing. The algorithm operates on grammars in Chomsky Normal Form (CNF), where all production rules are in the form:</p>
<p><span class="math display">\[A \rightarrow BC\]</span></p>
<p>or</p>
<p><span class="math display">\[A \rightarrow a \]</span></p>
<p>where <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> represent non-terminal symbols, and <span class="math inline">\(a\)</span> represents a terminal symbol (a word in the vocabulary).</p>
<p>One of the key benefits of CKY parsing is its ability to identify <em>all</em> possible parse trees for a given sentence. This is particularly valuable for dealing with ambiguous sentences, where multiple grammatical interpretations exist. By providing a complete set of valid parse trees, the CKY algorithm lays the groundwork for further analysis and disambiguation.</p>
</section>
<section id="requirements-for-cky-parsing" class="level1">
<h1>Requirements for CKY Parsing</h1>
<ul>
<li><p><strong>Chomsky Normal Form (CNF):</strong> The grammar used for CKY parsing <em>must</em> be in Chomsky Normal Form. This is a specific form of context-free grammar where each rule adheres to one of two structures:</p>
<ol type="1">
<li><strong>Binary Branching Rules:</strong> A non-terminal symbol is rewritten as two other non-terminal symbols. <span class="math display">\[A \rightarrow BC\]</span> Where:
<ul>
<li><span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span> represent non-terminal symbols.</li>
</ul></li>
<li><strong>Lexical Rules:</strong> A non-terminal symbol is rewritten as a single terminal symbol (a word). <span class="math display">\[A \rightarrow a\]</span> Where:
<ul>
<li><span class="math inline">\(A\)</span> represents a non-terminal symbol.</li>
<li><span class="math inline">\(a\)</span> represents a terminal symbol.</li>
</ul></li>
</ol></li>
<li><p><strong>No Empty Productions:</strong> CNF does not allow empty productions (rules that rewrite to nothing, often denoted as <span class="math inline">\(\epsilon\)</span>).</p></li>
<li><p><strong>Why CNF?</strong> The binary branching structure enforced by CNF is crucial for the efficient bottom-up operation of the CKY algorithm. Each cell in the CKY parse table corresponds to a specific substring of the input, and the algorithm relies on combining two smaller constituents from adjacent cells to build larger constituents.</p></li>
</ul>
</section>
<section id="cky-parsing-algorithm" class="level1">
<h1>CKY Parsing Algorithm</h1>
<p>The Cocke-Kasami-Younger (CKY) algorithm is a dynamic programming approach to parsing sentences based on a context-free grammar (CFG) in Chomsky Normal Form (CNF). It efficiently determines whether a string belongs to the language generated by the grammar and, if so, constructs all possible parse trees.</p>
<p>The algorithm operates by filling a triangular table, often referred to as the <strong>parse table</strong> or <strong>CKY chart</strong>. The table entries <span class="math inline">\(T[i, j]\)</span> represent the set of non-terminal symbols that can generate the substring of the input sentence spanning from word <span class="math inline">\(i\)</span> to word <span class="math inline">\(j\)</span>.</p>
<section id="algorithm-steps" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-steps">Algorithm Steps:</h2>
<ol type="1">
<li><strong>Initialization</strong>:
<ul>
<li>For each word <span class="math inline">\(w_i\)</span> in the input sentence, set <span class="math inline">\(T[i, i]\)</span> to the set of non-terminals <span class="math inline">\(A\)</span> where the grammar contains the rule <span class="math inline">\(A \rightarrow w_i\)</span>. This step establishes the base case for single words.</li>
</ul></li>
<li><strong>Recursive Filling</strong>:
<ul>
<li>For each span length <span class="math inline">\(l\)</span> from 2 to the length of the sentence:
<ul>
<li>For each starting position <span class="math inline">\(i\)</span> from 1 to the length of the sentence minus <span class="math inline">\(l + 1\)</span>:
<ul>
<li>Set <span class="math inline">\(j = i + l - 1\)</span> (defining the end of the span).</li>
<li>For each split point <span class="math inline">\(k\)</span> between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>:
<ul>
<li>Examine all pairs of non-terminals <span class="math inline">\((B, C)\)</span> where <span class="math inline">\(B \in T[i, k]\)</span> and <span class="math inline">\(C \in T[k+1, j]\)</span>.</li>
<li>For each such pair, if the grammar contains a rule <span class="math inline">\(A \rightarrow BC\)</span>, add <span class="math inline">\(A\)</span> to <span class="math inline">\(T[i, j]\)</span>.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>Completion</strong>:
<ul>
<li>If the start symbol <span class="math inline">\(S\)</span> is in the cell <span class="math inline">\(T[1, n]\)</span>, where <span class="math inline">\(n\)</span> is the length of the sentence, then the sentence is grammatically valid according to the grammar. The set of all parse trees for the sentence can be obtained by backtracking through the table.</li>
</ul></li>
</ol>
</section>
<section id="illustration" class="level2">
<h2 class="anchored" data-anchor-id="illustration">Illustration:</h2>
<p>Consider the sentence “The cat sat on the mat” and a simplified CNF grammar. The CKY algorithm would fill the parse table as follows:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>1: The</th>
<th>2: cat</th>
<th>3: sat</th>
<th>4: on</th>
<th>5: the</th>
<th>6: mat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>{Det}</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td></td>
<td>{Noun}</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td></td>
<td></td>
<td>{Verb}</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td></td>
<td></td>
<td></td>
<td>{Prep}</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>{Det}</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>{Noun}</td>
</tr>
</tbody>
</table>
<p>As the algorithm progresses, it would fill the upper-right portion of the table by combining entries according to the grammar rules. For instance, if the grammar contains rules like <code>NP → Det Noun</code> and <code>VP → Verb PP</code>, the algorithm would add <code>NP</code> to cell <span class="math inline">\(T[1, 2]\)</span> and <code>VP</code> to <span class="math inline">\(T[3, 6]\)</span>, and eventually <code>S</code> to <span class="math inline">\(T[1, 6]\)</span>.</p>
</section>
<section id="complexity" class="level2">
<h2 class="anchored" data-anchor-id="complexity">Complexity:</h2>
<p>The CKY algorithm has a time complexity of <span class="math inline">\(O(n^3 \cdot |G|)\)</span>, where <span class="math inline">\(n\)</span> is the sentence length and <span class="math inline">\(|G|\)</span> is the size of the grammar. The cubic complexity arises from iterating over all possible spans and split points.</p>
</section>
<section id="key-properties" class="level2">
<h2 class="anchored" data-anchor-id="key-properties">Key Properties:</h2>
<ul>
<li><strong>Completeness</strong>: The CKY algorithm guarantees finding all possible parses for a sentence based on the provided CNF grammar.</li>
<li><strong>Efficiency</strong>: Dynamic programming avoids redundant computations, making the algorithm relatively efficient for parsing.</li>
<li><strong>CNF Requirement</strong>: The grammar must be in CNF for the algorithm to function correctly.</li>
</ul>
</section>
</section>
<section id="cky-algorithm-workflow" class="level1">
<h1>CKY Algorithm Workflow</h1>
<section id="step-by-step-process" class="level2">
<h2 class="anchored" data-anchor-id="step-by-step-process">Step-by-Step Process:</h2>
<ol type="1">
<li><p><strong>Initialization</strong>: For a sentence with <span class="math inline">\(n\)</span> words, create an <span class="math inline">\((n+1) \times (n+1)\)</span> upper-triangular matrix (the CKY table). Fill the main diagonal (cells with indices <span class="math inline">\((i, i+1)\)</span>) with the possible parts of speech for each word <span class="math inline">\(w_i\)</span> based on the lexical rules of the grammar.</p></li>
<li><p><strong>Filling the Table</strong>: Proceed row by row, starting from the second row.</p>
<ul>
<li>For each cell <span class="math inline">\((i, j)\)</span> in the table, consider all possible split points <span class="math inline">\(k\)</span> such that <span class="math inline">\(i &lt; k &lt; j\)</span>. This represents dividing the substring <span class="math inline">\(w_i ... w_{j-1}\)</span> into two sub-strings: <span class="math inline">\(w_i ... w_{k-1}\)</span> and <span class="math inline">\(w_k ... w_{j-1}\)</span>.</li>
<li>For each split point <span class="math inline">\(k\)</span>, examine all pairs of non-terminals <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> where <span class="math inline">\(A\)</span> is in cell <span class="math inline">\((i, k)\)</span> and <span class="math inline">\(B\)</span> is in cell <span class="math inline">\((k, j)\)</span>.</li>
<li>If the grammar contains a rule of the form <span class="math inline">\(C \rightarrow AB\)</span>, add the non-terminal <span class="math inline">\(C\)</span> to cell <span class="math inline">\((i, j)\)</span>.</li>
</ul></li>
<li><p><strong>Final Step</strong>: After filling the entire table, examine the top-right cell <span class="math inline">\((1, n+1)\)</span>. If this cell contains the start symbol ‘S’, the sentence is considered grammatically valid according to the grammar.</p></li>
</ol>
</section>
<section id="key-advantages-of-cky-parsing" class="level2">
<h2 class="anchored" data-anchor-id="key-advantages-of-cky-parsing">Key Advantages of CKY Parsing:</h2>
<ul>
<li>Handles ambiguous sentences by finding all possible parses.</li>
<li>Efficient dynamic programming approach, as it avoids redundant computations by storing and reusing results for sub-strings.</li>
<li>Suitable for sentences parsed using CNF grammars.</li>
</ul>
</section>
</section>
<section id="practical-considerations-of-cky-parsing" class="level1">
<h1>Practical Considerations of CKY Parsing</h1>
<ul>
<li><p><strong>Efficiency:</strong> CKY parsing is generally considered efficient due to its use of dynamic programming. It avoids redundant computations by storing and reusing the results of parsing subproblems in the parse table. The time complexity of CKY parsing is <span class="math inline">\(O(n^3 \cdot |G|)\)</span>, where <span class="math inline">\(n\)</span> is the length of the sentence and <span class="math inline">\(|G|\)</span> is the size of the grammar. This makes it suitable for parsing moderately long sentences with reasonable grammar sizes.</p></li>
<li><p><strong>Ambiguity Handling:</strong> One of the significant advantages of CKY parsing is its ability to handle ambiguous sentences. Since it systematically explores all possible parse trees, it can identify and represent multiple valid parses for a given sentence. This is crucial for natural language processing tasks where ambiguity is common.</p></li>
<li><p><strong>Limitations:</strong></p>
<ul>
<li><strong>CNF Conversion:</strong> The requirement for the grammar to be in Chomsky Normal Form (CNF) can be a limitation. Converting a grammar to CNF can sometimes lead to an increase in the grammar’s size and complexity. This can impact parsing speed and memory usage, especially for grammars with a large number of rules.</li>
<li><strong>Data Sparsity:</strong> CKY parsing relies on lexical rules to initiate the parsing process. However, for words with limited occurrences in the training data, the parser may struggle to find appropriate lexical rules. This data sparsity issue can lead to parsing errors, particularly for sentences containing rare or out-of-vocabulary words.</li>
<li><strong>Limited Context Sensitivity:</strong> As a context-free parsing algorithm, CKY parsing cannot handle long-range dependencies or contextual information that goes beyond the immediate syntactic structure. This limits its ability to capture more nuanced linguistic phenomena that require broader context for disambiguation.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li><strong>Syntactic Analysis:</strong> CKY parsing is widely used for syntactic analysis tasks, such as determining the grammatical structure of sentences, identifying phrase boundaries, and generating parse trees.</li>
<li><strong>Foundation for More Advanced Parsers:</strong> While CKY parsing itself may have limitations in handling complex linguistic phenomena, it often serves as a foundation for building more sophisticated parsing models. For example, probabilistic CKY parsing incorporates probabilities into the grammar rules, allowing for a more nuanced ranking of possible parse trees.</li>
<li><strong>Grammar Induction:</strong> CKY parsing can also be used for grammar induction, where the goal is to learn a grammar from a corpus of text data. By analyzing the parse trees generated by CKY parsing, patterns in sentence structure can be identified and used to build a grammar that captures the observed syntactic regularities.</li>
</ul></li>
</ul>
</section>
<section id="dependency-parsing" class="level1">
<h1>Dependency Parsing</h1>
<ul>
<li><strong>Dependency parsing</strong> is a grammatical analysis technique that focuses on the relationships between individual words in a sentence. Unlike constituency parsing, which builds hierarchical structures of phrases, dependency parsing represents sentence structure as a directed graph, where:
<ul>
<li><strong>Nodes</strong>: Represent words.</li>
<li><strong>Edges</strong>: Represent directed, labeled arcs indicating the grammatical relationship between two words.</li>
</ul></li>
</ul>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key Concepts:</h2>
<ul>
<li><strong>Head</strong>: The word that governs or modifies another word.</li>
<li><strong>Dependent</strong>: The word that is governed or modified by the head.</li>
<li><strong>Dependency Relation</strong>: The specific grammatical relationship between a head and its dependent, often represented by a label (e.g., ‘nsubj’, ‘obj’, ‘amod’).</li>
</ul>
</section>
<section id="dependency-trees" class="level2">
<h2 class="anchored" data-anchor-id="dependency-trees">Dependency Trees:</h2>
<ul>
<li>A dependency parse of a sentence is typically visualized as a tree, where the root of the tree is the main verb or the head of the sentence.</li>
<li>Each word in the sentence (except the root) has exactly one head.</li>
<li>Dependencies form a hierarchical structure showing how words modify each other.</li>
</ul>
</section>
<section id="types-of-dependency-relations" class="level2">
<h2 class="anchored" data-anchor-id="types-of-dependency-relations">Types of Dependency Relations:</h2>
<ul>
<li>Dependency relations are often categorized based on the grammatical function they represent. Some common categories include:
<ul>
<li><strong>Subject (nsubj)</strong>: The noun phrase that performs the action of the verb.</li>
<li><strong>Object (obj)</strong>: The noun phrase that receives the action of the verb.</li>
<li><strong>Indirect Object (iobj)</strong>: The noun phrase that indirectly benefits from or is affected by the action.</li>
<li><strong>Modifier (various types)</strong>: Words or phrases that provide additional information about other words, such as adjectives (‘amod’), adverbs (‘advmod’), and prepositional phrases (‘nmod’).</li>
<li><strong>Complements</strong>: Clauses or phrases that complete the meaning of a verb or other head (e.g., ‘ccomp’).</li>
</ul></li>
</ul>
</section>
<section id="projectivity" class="level2">
<h2 class="anchored" data-anchor-id="projectivity">Projectivity:</h2>
<ul>
<li>A dependency tree is <strong>projective</strong> if all arcs can be drawn without crossing other arcs when the words are arranged in linear order. This means that for any head word, all its dependents form a contiguous span in the sentence.</li>
<li><strong>Non-projective</strong> trees have crossing arcs, which are more common in languages with flexible word order.</li>
</ul>
</section>
<section id="dependency-parsing-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="dependency-parsing-algorithms">Dependency Parsing Algorithms:</h2>
<ul>
<li>There are various algorithms for performing dependency parsing. Two broad categories are:
<ul>
<li><strong>Transition-based Parsing</strong>: Uses a sequence of actions (shift, reduce, left-arc, right-arc) to incrementally build the dependency tree.</li>
<li><strong>Graph-based Parsing</strong>: Treats parsing as finding the highest-scoring tree in a graph, where edges represent potential dependencies.</li>
</ul></li>
</ul>
</section>
<section id="advantages-of-dependency-parsing" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-dependency-parsing">Advantages of Dependency Parsing:</h2>
<ul>
<li><strong>Captures word-level relationships</strong>: Provides a finer-grained analysis compared to constituency parsing.</li>
<li><strong>Suitable for free word-order languages</strong>: Works well for languages where word order is flexible.</li>
<li><strong>Relatively efficient</strong>: Many parsing algorithms have polynomial time complexity.</li>
<li><strong>Useful for semantic analysis</strong>: Dependency relations provide a useful foundation for semantic role labeling and other meaning-related tasks.</li>
</ul>
</section>
</section>
<section id="dependency-relations" class="level1">
<h1>Dependency Relations</h1>
<p>Dependency relations are the backbone of dependency parsing, defining the specific connections between words in a sentence. These relations go beyond simple word order, capturing the <em>grammatical function</em> of each word in relation to its head. Here’s a deeper look at what makes dependency relations so important:</p>
<section id="head-dependent-relationships-the-foundation" class="level2">
<h2 class="anchored" data-anchor-id="head-dependent-relationships-the-foundation">Head-Dependent Relationships: The Foundation</h2>
<p>Every dependency relation involves two key components:</p>
<ul>
<li><strong>Head</strong>: The word that governs or modifies another word.</li>
<li><strong>Dependent</strong>: The word that is being governed or modified.</li>
</ul>
<p>Think of it like a parent-child relationship. The parent (head) guides and influences the child (dependent). For example, in the phrase “red car,” “car” is the head and “red” is the dependent, as “red” describes the “car.”</p>
</section>
<section id="categorizing-dependency-relations-universal-and-language-specific" class="level2">
<h2 class="anchored" data-anchor-id="categorizing-dependency-relations-universal-and-language-specific">Categorizing Dependency Relations: Universal and Language-Specific</h2>
<p>There are two main ways to categorize dependency relations:</p>
<ul>
<li><p><strong>Universal Dependencies (UD)</strong>: A standardized set of relations designed to be applicable across a wide range of languages. This makes it easier to compare syntactic structures across different languages and build multilingual NLP systems. UD consists of about 37 core relations, covering common grammatical functions. Examples include <code>nsubj</code> (nominal subject), <code>obj</code> (direct object), <code>amod</code> (adjectival modifier), and <code>nmod</code> (nominal modifier).</p></li>
<li><p><strong>Language-Specific Relations</strong>: Some relations are unique to specific languages, reflecting particular grammatical features. For instance, the relation <code>k1</code> in the Paninian model, derived from Sanskrit grammar, represents the agent or doer of an action, reflecting a concept central to that grammatical system.</p></li>
</ul>
</section>
<section id="clausal-and-nominal-relations-linking-to-verbs-and-nouns" class="level2">
<h2 class="anchored" data-anchor-id="clausal-and-nominal-relations-linking-to-verbs-and-nouns">Clausal and Nominal Relations: Linking to Verbs and Nouns</h2>
<p>Dependency relations can also be classified based on whether they modify a verb or a noun:</p>
<ul>
<li><strong>Clausal Relations</strong>: These relations connect directly to a verb and define its core arguments. They include roles like:
<ul>
<li><code>nsubj</code>: The nominal subject (the doer of the action).</li>
<li><code>obj</code>: The direct object (the thing acted upon).</li>
<li><code>iobj</code>: The indirect object (the recipient of the action).</li>
<li><code>ccomp</code>: The clausal complement (a subordinate clause that completes the verb’s meaning).</li>
</ul></li>
<li><strong>Nominal Relations</strong>: These relations modify a noun, adding further description or context. Examples include:
<ul>
<li><code>nmod</code>: A general nominal modifier (can express various relationships, like possession or location).</li>
<li><code>amod</code>: An adjectival modifier (describes a quality of the noun).</li>
<li><code>det</code>: A determiner (specifies the noun, e.g., “the,” “a,” “this”).</li>
</ul></li>
</ul>
</section>
<section id="handling-word-order-variation-going-beyond-linearity" class="level2">
<h2 class="anchored" data-anchor-id="handling-word-order-variation-going-beyond-linearity">Handling Word Order Variation: Going Beyond Linearity</h2>
<p>Dependency relations are especially crucial for languages with <em>flexible word order</em>. Unlike English, which largely relies on a fixed subject-verb-object structure, many languages allow words to move around freely in a sentence.</p>
<p>For example, consider the sentence:</p>
<ul>
<li><strong>English:</strong> The dog chased the cat.</li>
<li><strong>Hindi:</strong> Kutte ne billi ka peecha kiya. (Literally: Dog by cat of chase did)</li>
</ul>
<p>In Hindi, the grammatical relations are conveyed through case markings (like “ne” and “ka”) rather than a fixed word order. Dependency parsing can correctly capture these relations despite the variations in word order.</p>
</section>
<section id="formal-representation-capturing-the-structure" class="level2">
<h2 class="anchored" data-anchor-id="formal-representation-capturing-the-structure">Formal Representation: Capturing the Structure</h2>
<p>Dependency relations are typically represented visually in <em>dependency trees</em>. These trees show the head-dependent relationships as arcs connecting the words. The root of the tree is usually the main verb or a special ROOT node.</p>
<p>Dependency relations can also be expressed formally using mathematical notation. For example, if <span class="math inline">\(h\)</span> represents the head word and <span class="math inline">\(d\)</span> represents the dependent word, the relation <code>nsubj</code> can be represented as <span class="math inline">\(nsubj(h,d)\)</span>. This notation emphasizes the binary nature of dependency relations, always connecting two words in a specific grammatical function.</p>
</section>
</section>
<section id="paninian-dependency-and-tags" class="level1">
<h1>Paninian Dependency and Tags</h1>
<ul>
<li><p><strong>Origin and Relevance:</strong> The Paninian Dependency Model stems from the ancient grammatical framework developed by the Indian grammarian Pāṇini, primarily for Sanskrit. It has found renewed relevance in modern computational linguistics, especially for analyzing languages with relatively free word order, such as Hindi, Sanskrit, and other South Asian languages.</p></li>
<li><p><strong>Core Idea:</strong> Instead of relying solely on word order, the Paninian model uses <strong>Kāraka relations</strong> to express the syntactic and semantic relationships between a verb and its arguments. Kāraka relations essentially represent the semantic roles played by words in a sentence.</p></li>
<li><p><strong>Key Kāraka Relations:</strong></p>
<ul>
<li><strong>Karta (Agent):</strong> The agent is the doer of the action denoted by the verb. It is often the subject of the sentence in active voice constructions.</li>
<li><strong>Karma (Object):</strong> The karma is the entity that is directly affected by the action. It usually corresponds to the object of the verb.</li>
<li><strong>Karana (Instrument):</strong> The karana denotes the instrument or means by which the action is carried out.</li>
<li><strong>Sampradāna (Recipient):</strong> The sampradāna represents the recipient of the action, typically the indirect object of the verb.</li>
<li><strong>Apādāna (Source):</strong> This refers to the point of separation or origin, often used with verbs of motion or removal.</li>
<li><strong>Adhikarana (Location):</strong> Adhikarana indicates the location or place where the action occurs.</li>
<li><strong>Sambandha (Relation):</strong> Expresses a possessive or genitive relationship.</li>
</ul></li>
<li><p><strong>Mapping to Modern Dependency Tags:</strong> While the Paninian model uses Kāraka tags, these can be mapped to modern dependency tags commonly used in computational linguistics. For instance:</p>
<ul>
<li><strong>Kartā</strong> maps to the subject relation (<code>nsubj</code>).</li>
<li><strong>Karma</strong> maps to the object relation (<code>obj</code>).</li>
<li><strong>Karana</strong> can be represented as an oblique nominal modifier (<code>obl</code>) or an instrumental modifier (<code>nmod:instr</code>).</li>
</ul></li>
<li><p><strong>Advantages for Free Word Order Languages:</strong> The strength of the Paninian framework lies in its ability to capture the semantic relationships between words even when word order is flexible. This makes it particularly suitable for languages where grammatical roles are more important than linear word order in determining sentence meaning.</p></li>
</ul>
</section>
<section id="dependency-formalisms" class="level1">
<h1>Dependency Formalisms</h1>
<ul>
<li><p><strong>Graph Representation</strong>: Dependency structures are represented as directed graphs. This formalism helps visually illustrate syntactic dependencies.</p>
<ul>
<li><strong>Vertices</strong>: Each word in a sentence corresponds to a vertex in the graph.</li>
<li><strong>Arcs</strong>: Directed arcs connect the vertices, representing the head-dependent relations between words. The direction of the arc points from the head word to the dependent word.</li>
<li><strong>Labels</strong>: Each arc is typically labeled with the specific grammatical function it represents (e.g., ‘nsubj’ for nominal subject, ‘obj’ for direct object).</li>
</ul></li>
<li><p><strong>Formal Definition</strong>: A dependency graph for a sentence <span class="math inline">\(S\)</span> consisting of <span class="math inline">\(n\)</span> words can be formally defined as a tuple <span class="math inline">\(G = (V, A)\)</span>, where:</p>
<ul>
<li><span class="math inline">\(V = \{w_1, w_2, ..., w_n\}\)</span> is the set of vertices, representing the words in the sentence.</li>
<li><span class="math inline">\(A \subseteq V \times V \times L\)</span> is the set of arcs, where each arc is a triple <span class="math inline">\((w_i, w_j, l)\)</span>:
<ul>
<li><span class="math inline">\(w_i\)</span> is the head word (source of the arc).</li>
<li><span class="math inline">\(w_j\)</span> is the dependent word (target of the arc).</li>
<li><span class="math inline">\(l \in L\)</span> is a label from a set of grammatical function labels <span class="math inline">\(L\)</span>, specifying the type of relation between <span class="math inline">\(w_i\)</span> and <span class="math inline">\(w_j\)</span>.</li>
</ul></li>
</ul></li>
<li><p>This representation allows for a compact and computationally tractable way to model the syntactic structure of a sentence, capturing the relationships between individual words.</p></li>
</ul>
</section>
<section id="dependency-treebanks" class="level1">
<h1>Dependency Treebanks</h1>
<ul>
<li><p><strong>Purpose:</strong> Serve as gold-standard datasets for training and evaluating dependency parsers. Provide annotated sentences with their corresponding dependency structures.</p></li>
<li><p><strong>Creation:</strong></p>
<ul>
<li><strong>Manual Annotation:</strong> Linguists meticulously analyze sentences and annotate the dependency relations between words. This method is time-consuming and expensive but yields highly accurate treebanks.</li>
<li><strong>Automatic Parsing + Human Correction:</strong> Parsers automatically generate dependency structures, which are then reviewed and corrected by human annotators. This approach is faster and more cost-effective but may have lower accuracy.</li>
</ul></li>
<li><p><strong>Formats:</strong></p>
<ul>
<li><strong>CONLL-U Format:</strong> A standardized format for representing dependency treebanks, widely adopted by the NLP community. Each word in a sentence is represented on a separate line with multiple columns containing information like word form, POS tag, dependency head, and dependency relation.</li>
<li><strong>Other Formats:</strong> Some treebanks use specific formats or annotations depending on the language or project. However, the CONLL-U format is increasingly becoming the standard.</li>
</ul></li>
<li><p><strong>Features and Annotations:</strong></p>
<ul>
<li><strong>Basic Dependencies:</strong> Treebanks typically annotate core dependency relations, including subject (nsubj), object (obj), modifier (nmod), and others.</li>
<li><strong>Enhanced Dependencies:</strong> Some treebanks include additional layers of annotation, such as semantic roles, named entities, or coreference information.</li>
<li><strong>Language-Specific Considerations:</strong> Treebanks often incorporate language-specific features or relations to accommodate the unique grammatical properties of different languages.</li>
</ul></li>
<li><p><strong>Importance for Parser Development:</strong></p>
<ul>
<li><strong>Training Data:</strong> Treebanks provide the necessary data for training supervised dependency parsers, allowing them to learn from human-annotated examples.</li>
<li><strong>Evaluation Benchmark:</strong> Treebanks serve as evaluation benchmarks to assess the performance of dependency parsers. Parsers are evaluated based on their ability to accurately predict the dependency structure of sentences from the treebank.</li>
</ul></li>
<li><p><strong>Availability:</strong></p>
<ul>
<li><strong>Universal Dependencies (UD):</strong> The largest multilingual collection of dependency treebanks, covering over 100 languages.</li>
<li><strong>Language-Specific Treebanks:</strong> Numerous treebanks exist for individual languages, often developed for specific research purposes or domains.</li>
</ul></li>
<li><p><strong>Challenges:</strong></p>
<ul>
<li><strong>Annotation Consistency:</strong> Ensuring consistent annotation across different annotators and languages is a major challenge.</li>
<li><strong>Data Sparsity:</strong> Creating large-scale treebanks for under-resourced languages is challenging due to limited annotated data.</li>
<li><strong>Domain Adaptation:</strong> Parsers trained on one domain may perform poorly on another. Domain-specific treebanks are needed for various applications.</li>
</ul></li>
</ul>
</section>
<section id="transition-based-dependency-parsing" class="level1">
<h1>Transition-Based Dependency Parsing</h1>
<ul>
<li><p><strong>Overview:</strong> Transition-based dependency parsing is a data-driven approach that uses a state machine to incrementally build a dependency tree for a sentence. It is inspired by shift-reduce parsers used in compiler design.</p></li>
<li><p><strong>Core Components:</strong></p>
<ul>
<li><strong>Stack:</strong> Holds partially processed words.</li>
<li><strong>Buffer:</strong> Holds the remaining input words.</li>
<li><strong>Configuration:</strong> Represents the current state of the parser, consisting of the stack, buffer, and the set of dependencies built so far.</li>
<li><strong>Transitions:</strong> Operations that modify the parser’s configuration.</li>
<li><strong>Oracle:</strong> A function that, given a configuration, predicts the best transition to apply.</li>
</ul></li>
<li><p><strong>Transitions:</strong> Common transitions in arc-standard dependency parsing:</p>
<ul>
<li><strong>SHIFT:</strong> Moves the first word from the buffer onto the stack.</li>
<li><strong>LEFTARC(<span class="math inline">\(l\)</span>):</strong> Adds a dependency arc with label <span class="math inline">\(l\)</span> from the top word on the stack (dependent) to the second word (head), and removes the dependent from the stack.</li>
<li><strong>RIGHTARC(<span class="math inline">\(l\)</span>):</strong> Adds a dependency arc with label <span class="math inline">\(l\)</span> from the second word on the stack (dependent) to the top word (head), and removes the dependent from the stack.</li>
</ul></li>
<li><p><strong>Parsing Process:</strong></p>
<ol type="1">
<li>Initialize: The stack contains only the ROOT node, the buffer contains the input sentence, and the dependency set is empty.</li>
<li>Iterate: Until the buffer is empty and the stack contains only ROOT:
<ul>
<li>Predict: The oracle predicts the next transition based on the current configuration.</li>
<li>Apply: The predicted transition is applied, updating the stack, buffer, and dependencies.</li>
</ul></li>
<li>Output: The set of dependencies built represents the dependency parse tree.</li>
</ol></li>
<li><p><strong>Oracle Design:</strong></p>
<ul>
<li>The oracle is typically a machine learning model trained on a labeled dependency treebank.</li>
<li>Features used for prediction often include word forms, POS tags, and dependency labels of words near the top of the stack and the beginning of the buffer.</li>
</ul></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li><strong>Efficiency:</strong> Linear time complexity, making a single pass through the sentence.</li>
<li><strong>Data-Driven:</strong> Learns parsing strategies from data, adapting to specific language characteristics.</li>
</ul></li>
<li><p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Greedy Decisions:</strong> Each transition is chosen locally, potentially leading to globally suboptimal parses.</li>
<li><strong>Error Propagation:</strong> Errors in early transitions can cascade, impacting subsequent decisions.</li>
</ul></li>
<li><p><strong>Variations:</strong></p>
<ul>
<li>Different sets of transitions can be used, leading to variations like arc-eager parsing.</li>
<li>Beam search can be employed to explore multiple parsing paths, mitigating the greediness issue.</li>
</ul></li>
<li><p><strong>Applications:</strong></p>
<ul>
<li>A fundamental technique in dependency parsing, serving as the basis for many state-of-the-art parsers.</li>
<li>Used in various NLP applications, including information extraction, machine translation, and question answering.</li>
</ul></li>
</ul>
</section>
<section id="graph-based-syntactic-parsing-in-depth" class="level1">
<h1>Graph-based Syntactic Parsing: In-depth</h1>
<section id="scoring-function" class="level2">
<h2 class="anchored" data-anchor-id="scoring-function">Scoring Function</h2>
<ul>
<li>At the heart of graph-based parsing is the scoring function, which assigns a weight or score to each potential dependency edge in the graph.<br>
</li>
<li>This function is crucial as it determines the likelihood of each dependency relationship.</li>
<li>Typically, the scoring function is learned from a labeled dependency treebank.<br>
</li>
<li>Features used in the scoring function can include:
<ul>
<li>Part-of-speech tags of the head and dependent words</li>
<li>Distance between the head and dependent words</li>
<li>Lexical information (the actual words themselves)</li>
<li>Combinations of these features</li>
</ul></li>
</ul>
</section>
<section id="maximum-spanning-tree-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="maximum-spanning-tree-algorithms">Maximum Spanning Tree Algorithms</h2>
<ul>
<li><p>Once the scoring function is defined, a maximum spanning tree (MST) algorithm is used to find the tree with the highest total score, which represents the most likely parse.</p></li>
<li><p>Common algorithms for this task include:</p>
<ul>
<li><p><strong>Chu-Liu/Edmonds’ Algorithm:</strong> This classic algorithm is specifically designed for finding the MST in directed graphs. It guarantees finding the optimal tree in polynomial time.</p></li>
<li><p><strong>Kruskal’s Algorithm:</strong> While primarily designed for undirected graphs, variations can be applied to directed graphs for MST finding.</p></li>
</ul></li>
</ul>
</section>
<section id="handling-non-projectivity" class="level2">
<h2 class="anchored" data-anchor-id="handling-non-projectivity">Handling Non-Projectivity</h2>
<ul>
<li>One key advantage of graph-based parsing is its ability to handle non-projective dependencies.</li>
<li>Non-projective dependencies arise in languages with flexible word order, where the head and dependent may be separated by words that are not part of their dependency relation.</li>
<li>To handle non-projectivity, the graph representation allows for crossing edges, and the MST algorithm can still find the optimal tree even when dependencies are non-projective.</li>
</ul>
</section>
<section id="advantages-of-graph-based-parsing" class="level2">
<h2 class="anchored" data-anchor-id="advantages-of-graph-based-parsing">Advantages of Graph-based Parsing</h2>
<ul>
<li><strong>Global Optimization:</strong> By finding the MST, graph-based parsing optimizes the entire dependency structure of the sentence, rather than making local, greedy decisions.</li>
<li><strong>Non-Projectivity Handling:</strong> Effectively parses sentences with non-projective dependencies.</li>
<li><strong>Feature Richness</strong>: Allows for a wide variety of features to be incorporated into the scoring function, leading to potentially more accurate parses.</li>
</ul>
</section>
<section id="limitations" class="level2">
<h2 class="anchored" data-anchor-id="limitations">Limitations</h2>
<ul>
<li><strong>Computational Complexity</strong>: Finding the MST can be computationally expensive for long sentences, particularly when considering a large set of potential dependencies and features.</li>
<li><strong>Data Requirements</strong>: Requires a substantial amount of labeled data to train the scoring function effectively.</li>
</ul>
</section>
</section>
<section id="introduction-to-meaning-representation" class="level1">
<h1>Introduction to Meaning Representation</h1>
<p>Meaning Representation bridges the gap between human language and machine understanding. It focuses on transforming natural language expressions into formal, structured representations that computers can process and manipulate. This is crucial because while humans easily grasp the meaning behind words, machines require a more explicit and unambiguous format.</p>
<p>Meaning representation is not just about translating words into symbols; it involves capturing the underlying semantics – the relationships between words, the implied information, and the logical structure of sentences. This deeper understanding enables machines to perform more complex tasks like:</p>
<ul>
<li><strong>Inference:</strong> Drawing logical conclusions from given information.</li>
<li><strong>Reasoning:</strong> Solving problems and making decisions based on understood facts.</li>
<li><strong>Question Answering:</strong> Providing accurate answers by comprehending questions and retrieving relevant information.</li>
<li><strong>Summarization:</strong> Condensing textual content while preserving key information and meaning.</li>
<li><strong>Dialogue Systems:</strong> Engaging in meaningful conversations with humans.</li>
</ul>
<p>Various approaches are used for meaning representation, each with its strengths and weaknesses. The choice of approach often depends on the specific task and the desired level of semantic detail. Key aspects of meaning representation include:</p>
<ul>
<li><strong>Formalism:</strong> Choosing a suitable representation language (e.g., First-Order Logic, Semantic Networks, Frames).</li>
<li><strong>Scope:</strong> Determining the level of representation, from word-level meanings to full sentence or discourse-level understanding.</li>
<li><strong>Ambiguity Resolution:</strong> Handling multiple possible interpretations of words or phrases using contextual information.</li>
<li><strong>Knowledge Integration:</strong> Incorporating external knowledge sources (e.g., ontologies, common sense knowledge) to enhance understanding.</li>
</ul>
<p>Meaning Representation is an active area of research, with ongoing efforts to develop more powerful and robust techniques. Advances in machine learning, particularly deep learning, are significantly contributing to progress in this field.</p>
</section>
<section id="challenges-in-meaning-representation" class="level1">
<h1>Challenges in Meaning Representation</h1>
<ul>
<li><strong>Ambiguity:</strong> Natural language is inherently ambiguous. Words can have multiple meanings (lexical ambiguity) and sentences can have multiple possible interpretations (structural ambiguity). Resolving these ambiguities to arrive at the intended meaning is a major challenge.</li>
<li><strong>Context-Dependence:</strong> The meaning of words and sentences can vary significantly depending on the context in which they are used. Consider the word “bank” – it could refer to a financial institution or the edge of a river, depending on the surrounding words and the overall discourse. Capturing this context-dependence requires sophisticated models that can analyze and integrate information from various sources.</li>
<li><strong>Flexibility and Variability:</strong> Natural language allows for a great deal of flexibility in expressing the same meaning. People use different words, sentence structures, and even levels of formality to convey similar ideas. This variability makes it difficult to define rigid rules for mapping language to meaning, demanding robust and adaptive models.</li>
<li><strong>Figurative Language and Idioms:</strong> Expressions like metaphors, similes, and idioms pose significant challenges. Their literal meanings differ from their intended interpretations, requiring cultural knowledge and understanding of non-literal language use.</li>
<li><strong>Implicit Information and Common Sense:</strong> Humans often convey meaning implicitly, relying on shared background knowledge and common sense. For machines to truly understand natural language, they need access to this vast and often unstated world knowledge.</li>
<li><strong>Compositionality</strong>: While meaning is often built up compositionally from the meanings of individual words and phrases, the process is not always straightforward. The way meaning combines can be complex and dependent on subtle linguistic cues, making it difficult to model compositionality effectively.</li>
<li><strong>Logical Reasoning and Inference</strong>: To go beyond surface-level understanding, meaning representations must support logical reasoning and inference. This involves handling quantifiers (e.g., “all”, “some”), negation, and modal verbs (e.g., “could”, “should”) in a way that allows machines to draw conclusions and make predictions.</li>
<li><strong>Scalability and Efficiency</strong>: The sheer volume of natural language data available presents computational challenges. Meaning representation models need to be scalable and efficient to process large amounts of text without excessive resource consumption.</li>
</ul>
</section>
<section id="logical-semantics-overview" class="level1">
<h1>Logical Semantics Overview</h1>
<ul>
<li><p><strong>Logical Semantics</strong> aims to represent the meaning of natural language expressions using the tools and techniques of formal logic, specifically <strong>First-Order Logic (FOL)</strong>. This approach emphasizes the importance of truth conditions and logical relationships in understanding meaning.</p></li>
<li><p><strong>Truth Conditions</strong>: A core concept in Logical Semantics is the idea of truth conditions - the circumstances under which a sentence is considered true. These conditions are defined in terms of the entities and relationships in the world that the sentence describes. For example, the sentence “John loves Mary” is true if and only if there exist entities in the world corresponding to “John” and “Mary” and a relationship of “love” holds between them.</p></li>
<li><p><strong>Compositionality</strong>: Logical Semantics generally adheres to the principle of compositionality. This means that the meaning of a complex expression is a function of the meanings of its parts and the way they are combined. In other words, the meaning of a sentence can be built up systematically from the meanings of its individual words and phrases.</p></li>
<li><p><strong>Model-Theoretic Interpretation</strong>: FOL provides a model-theoretic interpretation of language, where sentences are evaluated against a model - a representation of the world. This allows for formal reasoning about the truth of statements and the validity of inferences.</p></li>
<li><p><strong>Formal Representation</strong>: Logical Semantics utilizes a formal, symbolic language to capture the underlying meaning of natural language. This formal language is unambiguous and lends itself to computational manipulation and reasoning.</p></li>
<li><p><strong>Inference</strong>: By representing the meaning of sentences in a logical form, we can perform logical inference - deriving new conclusions from existing information. For example, if we know that “All men are mortal” and “Socrates is a man,” we can infer that “Socrates is mortal.”</p></li>
<li><p><strong>Limitations</strong>: While Logical Semantics provides a powerful framework for representing meaning, it faces challenges in dealing with the full complexity of natural language, especially aspects like ambiguity, context-dependence, and vagueness.</p></li>
</ul>
</section>
<section id="components-of-first-order-logic-fol" class="level1">
<h1>Components of First-Order Logic (FOL)</h1>
<p>First-order logic (FOL) is a formal system used to represent knowledge and reason about the world. It provides a way to express statements about objects, their properties, and relationships between them. Here’s a detailed look at the components of FOL:</p>
<ul>
<li><p><strong>Constants</strong>: Constants represent specific objects or individuals in the domain. They are denoted by lowercase letters (e.g., <span class="math inline">\(john\)</span>, <span class="math inline">\(mary\)</span>, <span class="math inline">\(book1\)</span>). For instance, <span class="math inline">\(john\)</span> might represent a particular person named John.</p></li>
<li><p><strong>Predicates</strong>: Predicates represent properties of objects or relations between objects. They are denoted by uppercase letters or words (e.g., <span class="math inline">\(Loves\)</span>, <span class="math inline">\(Tall\)</span>, <span class="math inline">\(IsAuthorOf\)</span>).</p>
<ul>
<li>A predicate takes one or more arguments, which are terms representing objects. For example:
<ul>
<li><span class="math inline">\(Tall(john)\)</span> represents the proposition that John is tall.</li>
<li><span class="math inline">\(Loves(john, mary)\)</span> expresses that John loves Mary.</li>
<li><span class="math inline">\(IsAuthorOf(tolkien, hobbit)\)</span> states that Tolkien is the author of the book “Hobbit”.</li>
</ul></li>
</ul></li>
<li><p><strong>Variables</strong>: Variables are symbols that can stand for any object in the domain. They are usually denoted by lowercase letters from the end of the alphabet (e.g., <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, <span class="math inline">\(z\)</span>). Variables are crucial for expressing general statements about objects.</p></li>
<li><p><strong>Quantifiers</strong>: Quantifiers are used to specify the quantity of objects for which a statement holds true. There are two main quantifiers in FOL:</p>
<ul>
<li><strong>Universal Quantifier (∀)</strong>: The universal quantifier, denoted by <span class="math inline">\(\forall\)</span>, means “for all” or “for every”. For example, <span class="math inline">\(\forall x (Bird(x) \rightarrow Flies(x))\)</span> means “For every x, if x is a bird, then x flies”.</li>
<li><strong>Existential Quantifier (∃)</strong>: The existential quantifier, denoted by <span class="math inline">\(\exists\)</span>, means “there exists” or “there is at least one”. For example, <span class="math inline">\(\exists x (Dog(x) \land Brown(x))\)</span> means “There exists an x such that x is a dog and x is brown”.</li>
</ul></li>
<li><p><strong>Connectives</strong>: Connectives are logical operators used to combine or modify propositions. Common connectives include:</p>
<ul>
<li><strong>Negation (¬ or ~)</strong>: Negates a proposition (e.g., ¬<span class="math inline">\(Loves(john, mary)\)</span> means “John does not love Mary”).</li>
<li><strong>Conjunction (∧ or &amp;)</strong>: Represents “and” (e.g., <span class="math inline">\(Tall(john) \land Loves(john, mary)\)</span> means “John is tall and John loves Mary”).</li>
<li><strong>Disjunction (∨)</strong>: Represents “or” (e.g., <span class="math inline">\(Cat(x) \vee Dog(x)\)</span> means “x is a cat or x is a dog”).</li>
<li><strong>Implication (→)</strong>: Represents “if… then” (e.g., <span class="math inline">\(Raining(today) \rightarrow CarryUmbrella(john)\)</span> means “If it is raining today, then John will carry an umbrella”).</li>
<li><strong>Equivalence (↔︎)</strong>: Represents “if and only if” (e.g., <span class="math inline">\(Happy(x) \leftrightarrow HasCake(x)\)</span> means “x is happy if and only if x has cake”).</li>
</ul></li>
<li><p><strong>Functions</strong>: (Optional) Functions map objects to other objects. They are denoted by lowercase letters (e.g., <span class="math inline">\(fatherOf(john)\)</span> might represent John’s father).</p></li>
</ul>
<p>These components allow FOL to express a wide range of statements about the world, from simple facts to complex logical relationships, enabling reasoning and inference.</p>
</section>
<section id="example-of-first-order-logic-fol" class="level1">
<h1>Example of First-Order Logic (FOL)</h1>
<p><strong>Sentence:</strong> John loves Mary.</p>
<ul>
<li><p><strong>Logical Form:</strong> Loves(John, Mary)</p>
<ul>
<li>Here, ‘Loves’ is a predicate that represents the relationship “loves” between two entities.</li>
<li>‘John’ and ‘Mary’ are constants representing specific individuals.</li>
</ul></li>
<li><p><strong>Breakdown:</strong></p>
<ul>
<li>The predicate ‘Loves’ takes two arguments, representing the lover and the loved.</li>
<li>The order of arguments matters: ‘Loves(John, Mary)’ is different from ‘Loves(Mary, John)’.</li>
</ul></li>
<li><p><strong>Interpretation:</strong> This logical form states that the relationship “loves” holds between the entity ‘John’ and the entity ‘Mary’.</p></li>
<li><p><strong>Possible Extensions:</strong> FOL allows for more complex expressions:</p>
<ul>
<li><p>To express “Everyone loves Mary,” we use a quantifier and a variable:<br>
<span class="math display">\[
  \forall x (Person(x) \rightarrow Loves(x, Mary))
  \]</span></p></li>
<li><p>$ x$ is the universal quantifier, meaning “for all x.”</p></li>
<li><p><span class="math inline">\(Person(x)\)</span> is a predicate indicating that <span class="math inline">\(x\)</span> is a person.</p></li>
<li><p>$ $ is the implication symbol, meaning “if…then.”</p></li>
<li><p>To express “John loves someone,” we use the existential quantifier: <span class="math display">\[
\exists x (Person(x) \land Loves(John, x))
\]</span></p>
<ul>
<li><span class="math inline">\(\exists x\)</span> means “there exists an x.”</li>
<li><span class="math inline">\(\land\)</span> is the conjunction symbol, meaning “and.”</li>
</ul></li>
</ul></li>
<li><p><strong>Key Point:</strong> FOL provides a structured way to represent the meaning of sentences, allowing for reasoning and inference beyond simple statements.</p></li>
</ul>
</section>
<section id="quantifiers-in-first-order-logic-fol" class="level1">
<h1>Quantifiers in First-Order Logic (FOL)</h1>
<p>Quantifiers are essential components of FOL, allowing us to express statements about quantities of entities. They play a crucial role in representing the meaning of sentences that involve generalizations or claims about the existence of specific entities.</p>
<ul>
<li><p><strong>Universal Quantifier (<span class="math inline">\(∀\)</span>)</strong>: The universal quantifier, represented by the symbol <span class="math inline">\(∀\)</span>, asserts that a statement holds true for all entities within a particular domain. It signifies “for all” or “for every.”</p>
<ul>
<li><strong>Syntax</strong>: <span class="math inline">\(∀x (P(x))\)</span> , where:
<ul>
<li><span class="math inline">\(x\)</span> is a variable representing an arbitrary entity.</li>
<li><span class="math inline">\(P(x)\)</span> is a predicate that expresses a property or relation involving the variable <span class="math inline">\(x\)</span>.</li>
</ul></li>
<li><strong>Interpretation</strong>: The formula <span class="math inline">\(∀x (P(x))\)</span> is true if and only if the predicate <span class="math inline">\(P(x)\)</span> is true for every possible value of <span class="math inline">\(x\)</span> within the specified domain.</li>
</ul></li>
<li><p><strong>Existential Quantifier (<span class="math inline">\(∃\)</span>)</strong>: The existential quantifier, represented by the symbol <span class="math inline">\(∃\)</span>, asserts that there exists at least one entity within a domain for which a statement holds true. It signifies “there exists” or “for some.”</p>
<ul>
<li><strong>Syntax</strong>: <span class="math inline">\(∃x (P(x))\)</span>, where:
<ul>
<li><span class="math inline">\(x\)</span> is a variable representing an arbitrary entity.</li>
<li><span class="math inline">\(P(x)\)</span> is a predicate that expresses a property or relation involving the variable <span class="math inline">\(x\)</span>.</li>
</ul></li>
<li><strong>Interpretation</strong>: The formula <span class="math inline">\(∃x (P(x))\)</span> is true if and only if there is at least one value of <span class="math inline">\(x\)</span> within the specified domain for which the predicate <span class="math inline">\(P(x)\)</span> is true.</li>
</ul></li>
<li><p><strong>Scope of Quantifiers</strong>: The scope of a quantifier determines the part of the formula to which the quantifier applies. Parentheses are often used to delimit the scope.</p>
<ul>
<li><strong>Example</strong>: <span class="math inline">\(∀x (P(x) → Q(x))\)</span> asserts that for all <span class="math inline">\(x\)</span>, if <span class="math inline">\(P(x)\)</span> is true, then <span class="math inline">\(Q(x)\)</span> is also true.</li>
</ul></li>
<li><p><strong>Interaction of Quantifiers</strong>: The order of quantifiers significantly affects the meaning of a formula.</p>
<ul>
<li><strong>Example</strong>:
<ul>
<li><span class="math inline">\(∀x ∃y (Loves(x,y))\)</span> means “Everyone loves someone.”</li>
<li><span class="math inline">\(∃y ∀x (Loves(x,y))\)</span> means “There is someone who is loved by everyone.”</li>
</ul></li>
</ul></li>
<li><p><strong>Negation and Quantifiers</strong>: Negating a quantified statement involves changing the quantifier and negating the predicate.</p>
<ul>
<li><strong>Example</strong>:
<ul>
<li>The negation of <span class="math inline">\(∀x (P(x))\)</span> is <span class="math inline">\(∃x (¬P(x))\)</span>.</li>
<li>The negation of <span class="math inline">\(∃x (P(x))\)</span> is <span class="math inline">\(∀x (¬P(x))\)</span>.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="logical-semantics-and-truth-conditions" class="level1">
<h1>Logical Semantics and Truth Conditions</h1>
<ul>
<li><p><strong>Truth Conditions:</strong> Logical semantics defines the conditions under which a sentence is considered true. It provides a formal framework for evaluating the truth value of statements based on their logical structure and the interpretation of their components.</p></li>
<li><p><strong>Model-Theoretic Interpretation:</strong> In logical semantics, meaning is interpreted relative to a model, which represents a possible world or a state of affairs. A model consists of:</p>
<ul>
<li><strong>Domain</strong>: A set of entities or objects.</li>
<li><strong>Interpretation Function</strong>: A mapping from symbols in the logical language (constants, predicates) to entities and relations in the domain.</li>
</ul></li>
<li><p><strong>Truth Evaluation:</strong> The truth value of a sentence is determined by evaluating it against a specific model.</p>
<ul>
<li><strong>Atomic Sentences:</strong> For atomic sentences (e.g., ‘Loves(John, Mary)’), the sentence is true if and only if the relation denoted by the predicate (‘Loves’) holds between the entities denoted by the constants (‘John’, ‘Mary’) in the model.</li>
<li><strong>Complex Sentences:</strong> For complex sentences involving logical connectives (AND, OR, NOT) and quantifiers (∀, ∃), the truth value is recursively determined based on the truth values of their sub-sentences and the interpretation of the connectives and quantifiers.</li>
</ul></li>
<li><p><strong>Example</strong>: Consider the sentence “Every student loves some teacher.” In first-order logic, this can be represented as:</p></li>
</ul>
<p><span class="math display">\[
\forall x (Student(x) \rightarrow \exists y (Teacher(y) \land Loves(x, y)))
\]</span></p>
<p>To determine the truth conditions of this sentence, we need to consider a model. Let’s assume a model with the following: - <strong>Domain</strong>: {John, Mary, Physics, Math} - <strong>Interpretation</strong>: - ‘Student’ is true for John and Mary. - ‘Teacher’ is true for Physics and Math. - ‘Loves(x, y)’ is true if student <span class="math inline">\(x\)</span> loves subject <span class="math inline">\(y\)</span>.</p>
<p>We evaluate the sentence in this model. If it holds true for all possible students (John and Mary) in the domain, then the sentence is considered true in this model. If there exists at least one student who doesn’t love any teacher in the domain, the sentence would be false in this model.</p>
<ul>
<li><p><strong>Entailment</strong>: Logical semantics also allows us to define the notion of entailment, where one sentence logically follows from another. Sentence A entails sentence B if, whenever A is true, B must also be true. This is crucial for reasoning and inference.</p></li>
<li><p><strong>Practical Applications</strong>: Truth conditions and entailment are foundational concepts in logical semantics and have applications in various NLP tasks, including:</p>
<ul>
<li>Question answering: Determining if a given answer is consistent with a question and its context.</li>
<li>Textual entailment: Identifying if one text snippet logically entails another.</li>
<li>Reasoning and inference: Drawing logical conclusions from given information.</li>
<li>Knowledge representation: Encoding knowledge in a formal, unambiguous way.</li>
</ul></li>
</ul>
</section>
<section id="what-is-semantic-role-labeling-srl" class="level1">
<h1>What is Semantic Role Labeling (SRL)?</h1>
<ul>
<li><p><strong>Semantic Role Labeling (SRL)</strong>, also known as thematic role labeling, is a task in Natural Language Processing (NLP) that focuses on identifying the semantic roles of words or phrases within a sentence.</p></li>
<li><p>It goes beyond simply identifying parts of speech and delves deeper into understanding the meaning and relationships between words in relation to a specific verb (predicate) in the sentence.</p></li>
<li><p>SRL aims to answer the question: “Who did what to whom, when, where, why, and how?” by assigning semantic labels to words or phrases that represent their roles in the event described by the sentence.</p></li>
<li><p>For instance, consider the sentence: “The chef cooked a delicious meal for the guests in the kitchen.” SRL would identify “chef” as the <em>agent</em> performing the action, “cooked” as the <em>predicate</em>, “meal” as the <em>theme</em> being acted upon, “guests” as the <em>beneficiary</em>, and “kitchen” as the <em>location</em>.</p></li>
<li><p>In essence, SRL seeks to represent the underlying meaning of a sentence by identifying the participants in the event and their roles, providing a more structured and informative representation compared to just syntactic parsing.</p></li>
</ul>
</section>
<section id="importance-of-semantic-role-labeling-srl" class="level1">
<h1>Importance of Semantic Role Labeling (SRL)</h1>
<ul>
<li><p><strong>Deeper Understanding of Sentence Meaning:</strong> SRL goes beyond syntactic structure to identify the semantic roles of words, providing a deeper understanding of the meaning of a sentence. This allows machines to grasp “who did what to whom,” even when word order or grammatical structure is complex.</p></li>
<li><p><strong>Abstraction from Surface Syntax:</strong> SRL focuses on the underlying meaning, abstracting away from the surface form of sentences. This is particularly useful for handling paraphrases, where different syntactic structures convey the same semantic information. For example, “John gave Mary a book” and “A book was given to Mary by John” have different syntax but the same semantic roles.</p></li>
<li><p><strong>Facilitation of Downstream NLP Tasks:</strong> SRL output serves as valuable input for a variety of NLP tasks, such as:</p>
<ul>
<li><strong>Information Extraction:</strong> Extracting key events and their participants from text, enabling the creation of structured knowledge bases.</li>
<li><strong>Question Answering:</strong> Understanding the roles of words in both questions and candidate answers, allowing for more accurate retrieval of relevant information.</li>
<li><strong>Machine Translation:</strong> Preserving the semantic roles of words during translation, ensuring the meaning is accurately conveyed in the target language.</li>
<li><strong>Text Summarization:</strong> Identifying the most important participants and actions in a text, leading to more concise and informative summaries.</li>
<li><strong>Textual Entailment:</strong> Determining whether one sentence logically follows from another by analyzing the roles of their arguments.</li>
</ul></li>
<li><p><strong>Enhanced Textual Representation:</strong> SRL enriches textual representations by adding a layer of semantic information, enabling machines to go beyond word-level analysis and understand the relationships between words in a more meaningful way.</p></li>
<li><p><strong>Cross-Lingual Applicability:</strong> SRL can be applied to a wide range of languages, allowing for the development of cross-lingual applications that rely on understanding the semantic roles of words.</p></li>
<li><p><strong>Domain Adaptability:</strong> SRL can be adapted to specific domains by training models on labeled data from that domain, resulting in more accurate role identification for specialized language use.</p></li>
</ul>
<p>In essence, SRL bridges the gap between syntactic structure and semantic understanding, providing a powerful tool for unlocking the meaning of natural language text.</p>
</section>
<section id="key-concepts-in-srl" class="level1">
<h1>Key Concepts in SRL</h1>
<ul>
<li><p><strong>Predicate:</strong> The central verb or action that the sentence describes. It’s the core of the situation being represented, and the semantic roles relate to it. Predicates can also sometimes be nouns or adjectives, especially when they evoke an event or situation (e.g., “the <em>destruction</em> of the city”).</p></li>
<li><p><strong>Arguments:</strong> These are the phrases or constituents that participate in the action or situation described by the predicate. They fill the various semantic roles. Arguments are typically noun phrases, but can also be prepositional phrases or clauses, depending on the role and the predicate.</p></li>
<li><p><strong>Semantic Roles (Thematic Roles):</strong> These define the role each argument plays with respect to the predicate. They capture the underlying meaning of the relationship, going beyond the surface syntax (e.g., subject, object). Common semantic roles include:</p>
<ul>
<li><strong>Agent:</strong> The entity that intentionally performs the action.</li>
<li><strong>Patient:</strong> The entity undergoing the action or being affected by it. Often, it’s the entity that changes state or location.</li>
<li><strong>Instrument:</strong> The entity used by the agent to perform the action.</li>
<li><strong>Location:</strong> The place where the action takes place.</li>
<li><strong>Beneficiary:</strong> The entity that benefits from the action.</li>
<li><strong>Goal:</strong> The destination or endpoint of a movement or action.</li>
<li><strong>Source:</strong> The origin or starting point of a movement or action.</li>
<li><strong>Time:</strong> When the action takes place.</li>
<li><strong>Manner:</strong> How the action is performed.</li>
<li><strong>Cause:</strong> The event or entity that causes the action to occur.</li>
<li><strong>Purpose:</strong> The reason for which the action is performed. Sometimes called “reason” or “motive”.</li>
<li><strong>Experiencer:</strong> The entity that experiences a feeling, perception, or state. This role is typically associated with verbs like “feel,” “see,” “hear,” “know,” etc. The experiencer doesn’t actively control the action but receives sensory or mental input. For example, in “John saw the movie,” John is the experiencer, and the movie is the stimulus (or theme).</li>
<li><strong>Stimulus (Theme):</strong> The entity that evokes a response or feeling in the experiencer. It’s the object of perception, thought, or emotion.</li>
<li><strong>Recipient:</strong> The entity that receives something (often in verbs like “give”, “send”, etc.). While similar to the Beneficiary, the Recipient is primarily defined by receiving something concrete, whereas the Beneficiary experiences a broader advantage due to the action.</li>
<li>There are many other, more nuanced roles, and different SRL systems may use slightly different sets.</li>
</ul></li>
<li><p><strong>Formal Representations:</strong> Semantic roles can be represented in various ways. One common approach is to use labeled edges in a graph where nodes are the words of the sentence. For example:</p>
<p><span class="math inline">\(John \xrightarrow{Agent} gave \xrightarrow{Recipient} Mary \xrightarrow{Theme} a\ book\)</span></p></li>
<li><p><strong>Ambiguity and Challenges:</strong> Identifying semantic roles can be difficult due to ambiguities in language. The same sentence can sometimes have multiple interpretations with different role assignments. Context and world knowledge often play a crucial role in disambiguation. For example:</p>
<p>“The hammer broke the window.” (Agent: hammer, Patient: window) “John broke the window with the hammer.” (Agent: John, Instrument: hammer, Patient: window)</p></li>
<li><p><strong>SRL Systems:</strong> Various computational methods are used to automatically assign semantic roles, often relying on machine learning and annotated corpora like PropBank and FrameNet (discussed in other sections).</p></li>
</ul>
</section>
<section id="semantic-role-sets" class="level1">
<h1>Semantic Role Sets</h1>
<ul>
<li><p>Different semantic role sets provide standardized ways to label the roles of words or phrases in relation to a predicate (usually a verb). Two prominent examples are PropBank and FrameNet. While both aim to capture semantic roles, they differ in their approach and the granularity of the roles they define.</p></li>
<li><p><strong>PropBank (Proposition Bank):</strong></p>
<ul>
<li>Focuses on verb-specific argument structures.</li>
<li>Uses numbered arguments (Arg0, Arg1, Arg2, etc.) to represent core roles, roughly corresponding to proto-agent, proto-patient, instrument, beneficiary, etc. These are not universal thematic roles but are verb-specific.</li>
<li>Includes adjunct arguments (ArgM) for modifying phrases expressing location, manner, temporal information, etc. These are labeled with more specific tags like LOC, TMP, MNR.</li>
<li>Allows for finer-grained distinctions in roles for different senses of a verb. For instance, the verb <em>break</em> would have different role sets for meanings like “to fracture” and “to violate a law.”</li>
</ul></li>
<li><p><strong>FrameNet:</strong></p>
<ul>
<li>Organizes semantic roles around the concept of “frames.” A frame represents a stereotypical event or scenario, such as <em>Commerce_buy</em>, <em>Judgment_communication</em>, or <em>Medical_treatment</em>.</li>
<li>Defines “frame elements” – semantic roles specific to each frame. These are more descriptive and less verb-specific than PropBank roles. For example, the <em>Commerce_buy</em> frame might include roles like <em>Buyer</em>, <em>Seller</em>, <em>Goods</em>, and <em>Money</em>.</li>
<li>Aims to capture the broader context and semantics of a situation, going beyond the immediate verb arguments.</li>
<li>Provides lexical units (LUs), which are words or phrases that evoke a particular frame. For instance, “buy,” “purchase,” “sell,” and “vendor” could all be LUs associated with the <em>Commerce_buy</em> frame.</li>
</ul></li>
<li><p><strong>Comparison:</strong></p>
<ul>
<li>PropBank is more verb-centric and granular, making it suitable for tasks requiring specific verb-argument analysis.</li>
<li>FrameNet provides a richer, more holistic understanding of events and scenarios, making it useful for tasks requiring contextualized understanding.</li>
</ul></li>
</ul>
</section>
<section id="srl-and-machine-learning" class="level1">
<h1>SRL and Machine Learning</h1>
<ul>
<li><p><strong>Supervised Learning Paradigm:</strong> SRL often employs supervised machine learning, where models are trained on annotated data to predict semantic roles. The training data consists of sentences paired with their corresponding semantic role labels.</p></li>
<li><p><strong>Feature Engineering:</strong> Traditional machine learning models for SRL rely on handcrafted features extracted from the text. These features can include:</p>
<ul>
<li>Words themselves</li>
<li>Part-of-speech tags</li>
<li>Syntactic dependencies (e.g., output from a dependency parser)</li>
<li>Word embeddings (vector representations of words)</li>
<li>Predicate information</li>
</ul></li>
<li><p><strong>Conditional Random Fields (CRFs):</strong> CRFs are a popular choice for SRL due to their ability to model sequential dependencies in the data. They define a conditional probability distribution over possible label sequences given an input sentence and its features. Training involves maximizing the likelihood of the observed label sequences in the training data.</p></li>
<li><p><strong>Recurrent Neural Networks (RNNs):</strong> RNNs, particularly Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), are also effective for SRL. They can capture long-range dependencies in sentences and learn complex representations of words and phrases in context.</p></li>
<li><p><strong>Transformers:</strong> More recently, transformer-based models have achieved state-of-the-art results in SRL. The self-attention mechanism in transformers allows the model to weigh the importance of different words in the sentence when predicting semantic roles for a given word. Pre-trained transformer models like BERT and RoBERTa are often fine-tuned on SRL datasets.</p></li>
<li><p><strong>Training Process:</strong> The training process typically involves minimizing a loss function, such as the negative log-likelihood, using optimization algorithms like stochastic gradient descent.</p></li>
<li><p><strong>Inference:</strong> During inference, the trained model takes an input sentence and predicts the most likely semantic role label for each word based on the learned features and model parameters.</p></li>
<li><p><strong>Deep SRL:</strong> Recent advancements involve using deep learning models with minimal feature engineering. These models learn to extract relevant features automatically from the raw text, often through word and contextualized embeddings.</p></li>
</ul>
</section>
<section id="evaluation-metrics-for-srl" class="level1">
<h1>Evaluation Metrics for SRL</h1>
<p>Several metrics are used to evaluate the performance of Semantic Role Labeling (SRL) systems. These typically assess the accuracy of identifying both the arguments of a predicate and the correct assignment of roles to those arguments. Here’s a breakdown of common metrics:</p>
<ul>
<li><p><strong>Precision:</strong> Measures the accuracy of the identified arguments and their assigned roles. It’s the proportion of correctly identified semantic roles out of all the roles predicted by the system. Formally:</p>
<p><span class="math inline">\(Precision = \frac{\text{Number of correctly identified roles}}{\text{Total number of roles identified by the system}}\)</span></p></li>
<li><p><strong>Recall:</strong> Assesses the ability of the system to find all the true semantic roles in a dataset. It’s the proportion of correctly identified semantic roles out of all the gold-standard roles in the dataset. Formally:</p>
<p><span class="math inline">\(Recall = \frac{\text{Number of correctly identified roles}}{\text{Total number of true roles in the dataset}}\)</span></p></li>
<li><p><strong>F1-Score:</strong> The F1-score is the harmonic mean of precision and recall. It provides a balanced measure that considers both the accuracy of the predictions and the coverage of the true roles. Formally:</p>
<p><span class="math inline">\(F1 = \frac{2 \times Precision \times Recall}{Precision + Recall}\)</span></p></li>
<li><p><strong>Accuracy</strong>: While less commonly used in formal evaluations due to issues with handling partial matches, accuracy can provide a simple measure of overall correctness. This is particularly relevant when each predicate has a single set of arguments to identify, simplifying the evaluation:</p></li>
</ul>
<p><span class="math inline">\(Accuracy = \frac{\text{Number of completely correct predicate-argument structures}}{\text{Total number of predicate-argument structures}}\)</span></p>
<ul>
<li><strong>Label Accuracy Score (LS):</strong> Focuses on the correctness of dependency labels only given that the correct set of arguments are identified for the predicate. Formally:</li>
</ul>
<p><span class="math inline">\(LS = \frac{\text{Number of correctly labeled arguments given correct argument identification}}{\text{Total number of arguments correctly identified}}\)</span></p>
<p>It’s important to note that different SRL systems and datasets might use variations or combinations of these metrics. Careful consideration of the specific evaluation setup is crucial when comparing results across different studies.</p>
</section>
<section id="logical-semantics-vs.-srl" class="level1">
<h1>Logical Semantics vs.&nbsp;SRL</h1>
<ul>
<li><p><strong>Logical Semantics</strong>: Primarily concerned with representing the meaning of sentences in a formal, logical system, often using First-Order Logic (FOL). The focus is on defining the truth conditions of a sentence—specifying what must be true in the world for the sentence to be considered true. It uses logical connectives (<span class="math inline">\(\land\)</span>, <span class="math inline">\(\lor\)</span>, <span class="math inline">\(\neg\)</span>, <span class="math inline">\(\rightarrow\)</span>), quantifiers (<span class="math inline">\(\forall\)</span>, <span class="math inline">\(\exists\)</span>), predicates, constants, and variables to create logical forms that can be used for reasoning and inference. For example, the sentence “Every cat is a mammal” might be represented as <span class="math inline">\(\forall x (\text{Cat}(x) \rightarrow \text{Mammal}(x))\)</span>. Logical semantics doesn’t explicitly label the roles of participants in events. Its power lies in its ability to represent complex relationships between entities and reason about them.</p></li>
<li><p><strong>Semantic Role Labeling (SRL)</strong>: Focuses on identifying the semantic roles played by different words or phrases in a sentence, specifically in relation to a predicate (usually a verb). It aims to answer “who did what to whom, when, where, why, and how?” SRL is concerned with the participants in an event and their relationship to the event itself, rather than the truth conditions of the entire sentence. While SRL systems may implicitly capture some logical relationships, their primary output is a set of labeled arguments for each predicate. For example, in the sentence “John gave Mary a book,” SRL would identify “John” as the Agent, “Mary” as the Recipient, and “book” as the Theme. It doesn’t necessarily create a full logical representation suitable for automated reasoning, but provides a structured representation of the event described in the sentence. The output of SRL can be used as input to other NLP tasks that benefit from understanding the roles of entities within events.</p></li>
</ul>
</section>
<section id="applications-of-logical-semantics-and-srl" class="level1">
<h1>Applications of Logical Semantics and SRL</h1>
<ul>
<li><strong>Logical Semantics:</strong>
<ul>
<li><strong>Formal Reasoning:</strong> Creating systems that can deduce new knowledge from existing facts and rules. For example, given the facts “All men are mortal” and “Socrates is a man,” a system using logical semantics can infer “Socrates is mortal.” This can be represented in FOL as:
<ul>
<li><span class="math inline">\(\forall x (Man(x) \rightarrow Mortal(x))\)</span></li>
<li><span class="math inline">\(Man(Socrates)\)</span></li>
<li>Therefore, <span class="math inline">\(Mortal(Socrates)\)</span></li>
</ul></li>
<li><strong>Query Systems:</strong> Answering questions based on a structured knowledge base. Logical semantics can be used to formulate precise queries and retrieve relevant information. For instance, in a database of facts represented in FOL, a query like “Find all <span class="math inline">\(x\)</span> such that <span class="math inline">\(Loves(x, Mary)\)</span>” retrieves all entities that love Mary.</li>
<li><strong>Ontology Mapping:</strong> Aligning different knowledge bases or ontologies that may use different terminologies but represent similar concepts. Logical semantics can be used to define mappings between concepts and relations, facilitating knowledge sharing and integration. For example, mapping the concept “automobile” in one ontology to “car” in another.</li>
</ul></li>
<li><strong>Semantic Role Labeling:</strong>
<ul>
<li><strong>Information Extraction:</strong> Identifying specific information from text, such as events, entities, and their relationships. SRL can help extract arguments of events, like the perpetrator, victim, and location of a crime described in a news article. Example: “John robbed the bank with a gun” can be extracted as: Rob(Agent: John, Patient: bank, Instrument: gun).</li>
<li><strong>Machine Translation:</strong> Improving the accuracy of machine translation by capturing the semantic roles of words, which can help resolve ambiguities and generate more fluent translations. For example, correctly translating a sentence with a passive voice construction by identifying the agent and patient roles.</li>
<li><strong>Summarization:</strong> Generating concise summaries of text by identifying the most important information, often represented by the core semantic roles. SRL helps determine which entities and events are central to the meaning of the text and should be included in the summary.</li>
<li><strong>Dialogue Systems &amp; Chatbots:</strong> Understanding user intent and extracting relevant information from user utterances. SRL helps to determine the actions, objects, and other elements of user requests, enabling the system to respond appropriately.</li>
<li><strong>Sentiment Analysis &amp; Opinion Mining:</strong> Determining the sentiment expressed towards specific entities or aspects. SRL can identify the target of the sentiment and the holder of the opinion, allowing for more nuanced analysis. Example: “John loves the movie but hates the ending” can be analyzed by identifying “John” as the experiencer and “movie,” “ending” as the themes, with corresponding sentiments.</li>
</ul></li>
</ul>
</section>
<section id="review-questions" class="level1">
<h1>Review Questions</h1>
<ol type="1">
<li><strong>Phrasal Categories:</strong>
<ul>
<li>Define phrasal categories and give examples of different types (NP, VP, PP, AP, AdvP).</li>
<li>How can you identify a noun phrase (NP) in a sentence?</li>
<li>Explain the role of a verb phrase (VP) and its components.</li>
<li>How do prepositional phrases (PPs) contribute to sentence meaning? Give examples.</li>
<li>How do adjective phrases (AP) and adverb phrases (AdvP) modify other elements in a sentence?</li>
</ul></li>
<li><strong>Phrase Structure Grammar (PSG):</strong>
<ul>
<li>What is Phrase Structure Grammar, and what are its key components (lexicon, phrase structure rules, start symbol)?</li>
<li>Explain how PSG can be used to generate sentences and represent their structure.</li>
<li>Draw a parse tree for the sentence “The quick brown fox jumps over the lazy dog” using simple PSG rules.</li>
<li>What are the limitations of using PSG to represent the meaning of sentences?</li>
</ul></li>
<li><strong>Sentence Structure:</strong>
<ul>
<li>What are the fundamental components of a grammatically correct sentence in English?</li>
<li>Give examples of ill-formed sentences and explain why they are considered incorrect.</li>
<li>How does the rule <span class="math inline">\(S \rightarrow NP\ VP\)</span> represent basic sentence structure?</li>
</ul></li>
<li><strong>Types of Clauses and Sentences:</strong>
<ul>
<li>Differentiate between independent and dependent clauses with examples.</li>
<li>Define and provide examples of simple, compound, complex, and compound-complex sentences.</li>
<li>How can you identify the different types of clauses and sentences in a text?</li>
</ul></li>
<li><strong>Syntactic Complexities:</strong>
<ul>
<li>Explain structural ambiguity and coordination ambiguity with examples. Why are they important for NLP?</li>
<li>What are garden-path sentences, and how do they illustrate challenges in sentence processing?</li>
<li>Define recursiveness in language and provide examples. Why is recursiveness a powerful feature of natural language?</li>
<li>What is ellipsis, and how can it create difficulties for NLP tasks?</li>
</ul></li>
<li><strong>Context-Free Grammar (CFG):</strong>
<ul>
<li>What is a CFG, and what are its formal components (<span class="math inline">\(V\)</span>, <span class="math inline">\(\Sigma\)</span>, <span class="math inline">\(R\)</span>, <span class="math inline">\(S\)</span>)?</li>
<li>Explain the generative process of CFGs and how parse trees are derived.</li>
<li>How do terminals and non-terminals differ in a CFG? Give examples.</li>
<li>How can CFGs be used to represent the syntax of a programming language?</li>
</ul></li>
<li><strong>Parsing with CFG:</strong>
<ul>
<li>Define parsing and explain its importance in NLP.</li>
<li>How are parse trees used to represent the grammatical structure of a sentence? Give an example.</li>
<li>Describe how the sentence “She enjoys reading books in the park” can be parsed using a simple CFG.</li>
<li>What are the challenges associated with parsing ambiguous sentences using CFGs?</li>
</ul></li>
<li><strong>Treebanks:</strong>
<ul>
<li>What is a treebank, and why is it important for developing and evaluating NLP systems?</li>
<li>Describe different methods for constructing treebanks (manual, automatic, conversion).</li>
<li>What are the common formats for representing parse trees in treebanks (e.g., bracketed notation)?</li>
<li>How can treebanks be used for grammar induction?</li>
</ul></li>
<li><strong>Penn Treebank:</strong>
<ul>
<li>What is the Penn Treebank, and what are its key features?</li>
<li>Give examples of sentences annotated in the Penn Treebank format. Explain the annotations.</li>
<li>How has the Penn Treebank contributed to advancements in NLP research?</li>
<li>What are some limitations of the Penn Treebank annotation scheme?</li>
</ul></li>
<li><strong>CKY Parsing:</strong>
<ul>
<li>Describe the CKY parsing algorithm. How does it use dynamic programming?</li>
<li>What are the requirements for a grammar to be used with the CKY algorithm? Why?</li>
<li>Explain the steps involved in parsing a sentence using the CKY algorithm. Use an example.</li>
<li>What are the advantages and disadvantages of CKY parsing compared to other parsing methods?</li>
</ul></li>
<li><strong>Dependency Parsing:</strong>
<ul>
<li>Define dependency parsing and explain how it differs from constituency parsing.</li>
<li>What are heads, dependents, and dependency relations? Give examples.</li>
<li>Explain projectivity and non-projectivity in dependency trees. Why is non-projectivity important for some languages?</li>
<li>What are the advantages of dependency parsing, especially for languages with free word order?</li>
</ul></li>
<li><strong>Dependency Relations:</strong>
<ul>
<li>Explain the concept of a head-dependent relationship in dependency grammar.</li>
<li>What are Universal Dependencies (UD), and why are they important for cross-linguistic NLP?</li>
<li>Give examples of clausal and nominal dependency relations.</li>
<li>How do dependency relations help capture the meaning of sentences in languages with flexible word order?</li>
</ul></li>
<li><strong>Paninian Dependency Model:</strong>
<ul>
<li>What is the Paninian Dependency Model, and what are its key Kāraka relations?</li>
<li>How do Kāraka relations relate to modern dependency tags like ‘nsubj’ and ‘obj’?</li>
<li>Why is the Paninian model particularly relevant for languages like Hindi and Sanskrit?</li>
</ul></li>
<li><strong>Transition-Based Dependency Parsing:</strong>
<ul>
<li>Explain the basic concepts of transition-based dependency parsing (stack, buffer, transitions, oracle).</li>
<li>Describe the common transitions (SHIFT, LEFTARC, RIGHTARC) and their effects.</li>
<li>How is the oracle used in transition-based parsing? What kind of model can be used as an oracle?</li>
<li>What are the advantages and limitations of transition-based parsing?</li>
</ul></li>
<li><strong>Graph-Based Dependency Parsing:</strong>
<ul>
<li>Explain how graph-based parsing works, including the role of the scoring function and MST algorithms.</li>
<li>Discuss the advantages of graph-based parsing, particularly for handling non-projective dependencies.</li>
<li>What are the computational challenges associated with graph-based parsing?</li>
</ul></li>
<li><strong>Dependency Treebanks:</strong>
<ul>
<li>What are dependency treebanks, and how are they created?</li>
<li>Describe the CONLL-U format and its importance.</li>
<li>Why are treebanks crucial for developing and evaluating dependency parsers?</li>
</ul></li>
<li><strong>Meaning Representation:</strong>
<ul>
<li>What is the purpose of meaning representation in NLP? Why is it important for machine understanding?</li>
<li>Describe some key applications of meaning representation (e.g., question answering, summarization).</li>
</ul></li>
<li><strong>Challenges in Meaning Representation:</strong>
<ul>
<li>Discuss the major challenges in representing meaning in NLP, including ambiguity, context-dependence, variability, and implicit information.</li>
<li>How do these challenges impact the development of effective NLP systems?</li>
</ul></li>
<li><strong>Logical Semantics and First-Order Logic (FOL):</strong>
<ul>
<li>How does logical semantics use FOL to represent meaning? What is the concept of truth conditions?</li>
<li>Describe the different components of FOL (constants, predicates, variables, quantifiers, connectives).</li>
<li>Translate the following sentence into FOL: “John gave a book to Mary.”</li>
<li>What are the limitations of logical semantics for representing the full complexity of natural language?</li>
</ul></li>
<li><strong>Semantic Role Labeling (SRL):</strong>
<ul>
<li>What is Semantic Role Labeling (SRL), and what are predicates, arguments, and semantic roles?</li>
<li>Provide examples of common semantic roles (Agent, Patient, Instrument, Location, etc.) and explain their significance.</li>
<li>How does SRL contribute to a deeper understanding of sentence meaning?</li>
<li>What are the main differences between PropBank and FrameNet for representing semantic roles?</li>
</ul></li>
<li><strong>SRL and Machine Learning:</strong>
<ul>
<li>How is supervised machine learning used for training SRL models?</li>
<li>Describe some common features used in machine learning-based SRL.</li>
<li>What are the strengths and weaknesses of different machine learning approaches for SRL (CRFs, RNNs, Transformers)?</li>
</ul></li>
<li><strong>Evaluation of SRL:</strong>
<ul>
<li>What metrics are commonly used to evaluate the performance of SRL systems (precision, recall, F1-score)? Explain their calculation.</li>
<li>Why are different metrics necessary for a comprehensive evaluation?</li>
</ul></li>
<li><strong>Comparison of Logical Semantics and SRL:</strong>
<ul>
<li>Compare and contrast logical semantics and SRL. What are their strengths and limitations?</li>
<li>How do these two approaches complement each other in achieving a deeper understanding of natural language?</li>
</ul></li>
<li><strong>Applications of Logical Semantics and SRL:</strong>
<ul>
<li>Describe how logical semantics is used in formal reasoning, query systems, and ontology mapping.</li>
<li>How is SRL applied in information extraction, machine translation, summarization, dialogue systems, and sentiment analysis?</li>
</ul></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>