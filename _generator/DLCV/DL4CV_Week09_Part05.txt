# DL4CV_Week09_Part05.pdf - Page 1

```markdown
# Deep Learning for Computer Vision

# Self-Attention and Transformers

**Vineeth N Balasubramanian**

Department of Computer Science and Engineering
Indian Institute of Technology, Hyderabad

![IIT Hyderabad Logo](https://www.iith.ac.in/img/logo.png)

---

Vineeth N B (IIT-H)

## 9.5 Self-Attention and Transformers

### Slide 1 / 22

---

**Self-Attention and Transformers**

Self-attention mechanisms and transformers have revolutionized the field of computer vision by enabling models to capture long-range dependencies and contextual information more effectively. This section delves into the principles of self-attention and the architecture of transformer models.

#### Self-Attention Mechanism

The self-attention mechanism allows a model to weigh the importance of different parts of the input sequence when generating an output. This is particularly useful in capturing relationships between distant elements in the data.

1. **Input Representation**: Each element in the input sequence is represented as a vector.
2. **Query, Key, and Value**: Each input vector is transformed into three different vectors: query (Q), key (K), and value (V).
3. **Attention Scores**: The attention scores are computed by taking the dot product of the query vector with all key vectors.
4. **Softmax Normalization**: The attention scores are normalized using the softmax function.
5. **Weighted Sum**: The final output is a weighted sum of the value vectors, where the weights are the attention scores.

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
```

#### Transformer Architecture

The transformer architecture leverages the self-attention mechanism to process sequences in parallel, making it highly efficient for tasks such as machine translation, image captioning, and more.

1. **Encoder**: The encoder consists of a stack of identical layers, each containing a multi-head self-attention sub-layer and a position-wise fully connected feed-forward network.
2. **Decoder**: The decoder also consists of a stack of identical layers, which includes a mask multi-head self-attention sub-layer, a multi-head attention sub-layer, and a position-wise fully connected feed-forward network.
3. **Positional Encoding**: Since transformers do not inherently capture the order of the input sequence, positional encodings are added to the input embeddings to incorporate positional information.

```math
\text{PE}_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
```

```math
\text{PE}_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
```

#### Applications in Computer Vision

Self-attention and transformers have been successfully applied to various computer vision tasks, including:

- **Image Classification**: Transformer models can be used to classify images by understanding global and local features.
- **Object Detection**: Transformer-based models can detect objects by attending to relevant parts of the image.
- **Semantic Segmentation**: Transformers can segment images into semantically meaningful regions.

### Slide 2 / 22

---

#### Example Use Cases

1. **Vision Transformers (ViT)**: ViT models split an image into patches and treat them as tokens, applying the transformer architecture to these tokens.
2. **DETR (DEtection TRansformer)**: DETR uses a transformer-based architecture to perform object detection, eliminating the need for handcrafted features and anchor boxes.
3. **Transformers for Image Captioning**: Transformers can generate descriptive captions for images by attending to relevant regions and understanding the context.

### Slide 3 / 22

---

#### Future Directions

The integration of self-attention mechanisms and transformer models in computer vision continues to evolve, with promising research directions including:

- **Improved Attention Mechanisms**: Developing more efficient and effective attention mechanisms.
- **Hybrid Models**: Combining transformers with convolutional neural networks (CNNs) to leverage the strengths of both architectures.
- **Scalability**: Enhancing the scalability of transformer models for larger datasets and higher resolution images.

---

**References**

- Vaswani, A., et al. (2017). "Attention is All You Need." In *Advances in Neural Information Processing Systems (NeurIPS)*.
- Dosovitskiy, A., et al. (2020). "Image Transformers." In *International Conference on Learning Representations (ICLR)*.
- Carion, N., et al. (2020). "End-to-End Object Detection with Transformers." In *NeurIPS*.

---

**Questions & Discussion**

- What are the key advantages of self-attention mechanisms over traditional methods in computer vision?
- How do transformers handle the sequential nature of data in tasks like image captioning?
- What are some of the limitations of current transformer models in computer vision?

---

**Slide 4 / 22**

```

# DL4CV_Week09_Part05.pdf - Page 2

 is required.

```markdown
# Review: Question

Other ways to evaluate Visual Dialog systems?

![NPTEL Logo](image_url)

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

2 / 22
```
```

# DL4CV_Week09_Part05.pdf - Page 3

```markdown
# Review: Question

Other ways to evaluate Visual Dialog systems?

Look to NLP for consensus metrics that measure consensus between answers generated by model and a set of relevant answers; see Massiceti et al, A Revised Generative Evaluation of Visual Dialogue, arXiv 2020

---

Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers

---

![NPTEL](https://placeholder.com/your-placeholder-image-url)

---

2 / 22
```

# DL4CV_Week09_Part05.pdf - Page 4

ideas

---

# Acknowledgements

- Most of this lecture’s slides are based on Jay Alammar’s article on **"The Illustrated Transformer"**
- Unless explicitly specified, assume that content and figures are either directly taken or adapted from above source

---

Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 3 / 22

---

NPTEL

---

```

# DL4CV_Week09_Part05.pdf - Page 5

```markdown
# Motivation for Transformers

- Sequential computation prevents parallelization

![Diagram](image_url)

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

---

## Sequential computation prevents parallelization

![LSTM Diagram](image_url)

### LSTM

#### Linear

#### LSTM/Linear

#### Linear

#### LSTM/Linear
```

# DL4CV_Week09_Part05.pdf - Page 6



```markdown
# Motivation for Transformers

- Sequential computation prevents parallelization

![LSTM Diagram](image_url)

- Despite GRUs and LSTMs, RNNs still need attention mechanism to deal with long-range dependencies – path length for co-dependent computation between states grows with sequence length

*Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers*

*4 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 7

```markdown
# Motivation for Transformers

- Sequential computation prevents parallelization

![LSTM Diagram](image_url_placeholder)

- Despite GRUs and LSTMs, RNNs still need attention mechanism to deal with long-range dependencies – path length for co-dependent computation between states grows with sequence length
- But if attention gives us access to any state, maybe we don’t need the RNN?!

**Credits:** Richard Socher (Stanford CS224n)

Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 4 / 22
```

# DL4CV_Week09_Part05.pdf - Page 8

```markdown
# Transformers

![Transformers Diagram](https://via.placeholder.com/150)

- **The work "Attention is All you Need" (Vaswani et al, NeurIPS 2017) first made it possible to do Seq2Seq modeling without RNNs**

## Diagram Components

- **Input Embedding**
  - Implemented with Positional Encoding

- **Output Embedding**
  - Implemented with Positional Encoding

### Encoder

- **Input Embedding**
  - Implemented with Positional Encoding

- **Positional Encoding**
  - Nx

- **Add & Norm**

- **Feed Forward**

- **Multi-Head Attention**

### Decoder

- **Input Embedding**
  - Implemented with Positional Encoding

- **Positional Encoding**

- **Add & Norm**

- **Feed Forward**

- **Multi-Head Attention**

- **Masked Multi-Head Attention**

- **Add & Norm**

- **Output Probabilities**

- **Softmax**

- **Loss**

## References

1. Vaswani et al, *Attention is All You Need*, NeurIPS 2017
2. Vinneeth N B (*IIT-H*)
3. §9.5 Self-Attention and Transformers

---

Date: 5 / 22
```

# DL4CV_Week09_Part05.pdf - Page 9

```markdown
# Transformers<sup>1</sup>

![Transformer Architecture Diagram](image-placeholder.png)

- **The work** "Attention is All you Need" (Vaswani et al, NeurIPS 2017) first made it possible to do Seq2Seq modeling without RNNs.

- **Proposed transformer model**, entirely built on **self-attention mechanism** without using sequence-aligned recurrent architectures.

<sup>1</sup> Vaswani et al, Attention is All You Need, NeurIPS 2017

**Vineeth N B (IIT-H)**

**§9.5 Self-Attention and Transformers**

5 / 22
```

The provided markdown format ensures the scientific integrity of the content while maintaining proper formatting and structure for headings, lists, images, and footnotes.

# DL4CV_Week09_Part05.pdf - Page 10

 is an important requirement.

```markdown

## Transformers<sup>1</sup>

![Transformer Diagram](https://example.com/transformer-diagram.png)

- **The work “Attention is All you Need” (Vaswani et al, NeurIPS 2017) first made it possible to do Seq2Seq modeling without RNNs**

- **Proposed transformer model, entirely built on self-attention mechanism without using sequence-aligned recurrent architectures**

- **Key components:**
  - **Self-Attention**
  - **Multi-Head Attention**
  - **Positional Encoding**
  - **Encoder-Decoder Architecture**

<sup>1</sup> Vaswani et al, Attention is All You Need, NeurIPS 2017

Vineeth N B (IIT-H)

&9.5 Self-Attention and Transformers

5 / 22
```

# DL4CV_Week09_Part05.pdf - Page 11

```markdown
# Transformers in a Nutshell

![Transformer Diagram](https://example.com/transformer_diagram.png)

**Vineeth N B (IIT-H)**

**Section 9.5: Self-Attention and Transformers**

## Overview

### Input Representation
- **Input Text**: `je -> suis -> étudiant`

### Transformer Model
The Transformer model is a neural network architecture introduced in the paper "Attention is All You Need." It uses mechanisms such as self-attention to process sequential data.

### Output
- **Output Text**: `je -> suis -> un étudiant`

## Diagram Description

- The Transformer architecture uses a mechanism called self-attention to weigh the importance of different words in a sentence relative to each other.
- This allows the model to capture dependencies regardless of their distance within the sentence.

![NPTel Logo](https://example.com/nptel_logo.png)

---

**Note**: For more information, refer to the lecture materials on Self-Attention and Transformers by Vineeth N B from IIT-H.

6 / 22
```

# DL4CV_Week09_Part05.pdf - Page 12

```markdown
# Transformers in a Nutshell

## Vineeth N B (IIT-H) &9.5 Self-Attention and Transformers

### Slide: 6 / 22

![Transformers Diagram](placeholder-for-diagram)

**INPUT**: he's a student

**THE TRANSFORMER**:

- **INPUT**: he's a student
- **OUTPUT**: il est étudiant

**INPUT**: il est étudiant @Github

**SATISFIES**:

1. **SATISFIES**: he's a student
2. **SATISFIES**: il est étudiant

**OUTPUT**: am je student
```

# DL4CV_Week09_Part05.pdf - Page 13

```markdown
# Transformers in a Nutshell

![Transformers Diagram](https://via.placeholder.com/600x400?text=Transformers+Diagram)

## Overview

Vineeth N B (IIT-H)

### Section 9.5: Self-Attention and Transformers

### Transformers Diagram

#### Input
- **Input**: "js - suj - student"

#### Transformer
- **The Transformer** processes the input "js - suj - student" and outputs "js - a student".

#### Post-Processing
- **Output**: "js - a student"

```plaintext
> Input: "js - suj - student"
>
> The Transformer:
> 
> Output: "js - a student"
>
> Processing:
>
> Shuffle:
> 
> Shuffle:
> 
> Output: "js - a student"
```

#### Encoder-Decoder Architecture

```plaintext
Encoder
│
├── Encoder
├── Encoder
├── Encoder
├── Encoder
├── Encoder
└── Encoder

Decoder
│
├── Decoder
├── Decoder
├── Decoder
├── Decoder
├── Decoder
└── Decoder

Input: "js - suj - student"

Output: "js - a - students"
```

```plaintext
Input: "js - suj - student"
        ↓
Encoder -> Decoder
Encoder -> Decoder
Encoder -> Decoder
Encoder -> Decoder
Encoder -> Decoder
Encoder -> Decoder
        ↓
Output: "js - a - students"
```

#### Output
- **Output**: "js - a students"
```

# DL4CV_Week09_Part05.pdf - Page 14

```markdown
# Transformers in a Nutshell

## Vineeth N B (IIT-H) &9.5 Self-Attention and Transformers

### Transformer Architecture

The transformer is an architecture designed for natural language processing tasks. It employs a mechanism known as self-attention to weigh the importance of different words in a sentence.

#### Input and Output

```markdown
INPUT: je suis étudiant

OUTPUT: I am a student
```

```markdown
INPUT: je suis étudiant

OUTPUT: I am a student
```

#### Encoder-Decoder Structure

1. **Encoder**:
    - Takes the input sequence and processes it through multiple layers.
    - Each encoder layer consists of:
        - **Self-Attention Mechanism**: Computes attention scores for each word relative to all other words.
        - **Feed-Forward Neural Network**: Applies a feed-forward network to each position.

2. **Decoder**:
    - Takes the encoded sequence and the target sequence.
    - Each decoder layer consists of:
        - **Self-Attention Mechanism**: Similar to the encoder.
        - **Encoder-Decoder Attention**: Attention mechanism that focuses on the encoder’s output.
        - **Feed-Forward Neural Network**: Applies a feed-forward network to each position.

### Detailed Workflow

1. **Input Sequence**: The input sequence "je suis étudiant" is fed into the transformer.

2. **Encoders**:
    - Multiple encoder layers process the input sequence.
    - Each layer applies self-attention and feed-forward mechanisms.

3. **Decoders**:
    - Multiple decoder layers process the output from the encoder and the target sequence.
    - Each layer applies self-attention, encoder-decoder attention, and feed-forward mechanisms.

4. **Output Sequence**: The final output from the decoder layers is the translated sequence "I am a student".

### Components of Transformer Blocks

- **Self-Attention Mechanism**: 
    - Computes attention scores for each word in the sentence.
    ```markdown
    x1  -> Thinking
    x2  -> Machines
    ```

    - Outputs are computed using the formula:
    ```markdown
    z1 = Feed Forward Neural Network(x1)
    z2 = Feed Forward Neural Network(x2)
    ```

    - Self-attention is used to generate contextualized representations.

### Diagram Representation

```markdown
![]()
```

- **Encoders**: Multiple encoders process the input sequence.
- **Decoders**: Multiple decoders process the encoded sequence and the target sequence.
- **Self-Attention**: Illustrated within each encoder and decoder block.
- **Feed-Forward Networks**: Shown as part of the transformer layers.
```

# DL4CV_Week09_Part05.pdf - Page 15

```markdown
# Self-Attention

- Consider two input sentences we want to translate:

![Diagram Placeholder](image_url)

```markdown
Layer: 5  Attention: Input - Input  ▾

| Input Sentence 1         | Input Sentence 2        |
|-------------------------|------------------------|
| The_                    | The_                   |
| animal_                 | animal_                |
| didn_                   | didn_                   |
| ,                       | ,                      |
| L                       | L                       |
| cross_                  | cross_                 |
| the_                    | the_                   |
| street_                 | street_                |
| because_                | because_               |
| it_                     | it_                    |
| was_                    | was_                   |
| too_                    | too_                   |
| tire_                   | tire_                  |
| d_                      | d_                     |
```

*Vineeth N B (IIIT-H)*

## §9.5 Self-Attention and Transformers

Page 7 / 22
```

# DL4CV_Week09_Part05.pdf - Page 16



```markdown
# Self-Attention

- Consider two input sentences we want to translate:
  - **The animal didn’t** cross the street because it was too tired

## Layer: 5 § Attention: Input - Input

- The_animal_didn_
  - The_animal_didn_
  - The_animal_didn_
  - The_animal_didn_
  - The_animal_didn_

- didn’t_
  - didn’t_
  - didn’t_
  - didn’t_
  - didn’t_
  - ![Attention Weights](image_url)

- cross_the_street_
  - cross_the_street_
  - cross_the_street_
  - cross_the_street_
  - cross_the_street_
  - ![Attention Weights](image_url)

- because_
  - because_
  - because_
  - because_
  - because_
  - ![Attention Weights](image_url)

- it_was_
  - it_was_
  - it_was_
  - it_was_
  - it_was_
  - ![Attention Weights](image_url)

- too_tired_
  - too_tired_
  - too_tired_
  - too_tired_
  - too_tired_
  - ![Attention Weights](image_url)

- d_
  - d_
  - d_
  - d_
  - d_

![NPTEL Logo](image_url)

Vineeth N B (IIT-H)
89.5 Self-Attention and Transformers
7 / 22
```

Note: Placeholder `image_url` should be replaced with the actual image URL if provided in the OCR process.
```

# DL4CV_Week09_Part05.pdf - Page 17



```markdown
# Self-Attention

- Consider two input sentences we want to translate:
  - *The animal didn’t cross the street because it was too tired*
  - *The animal didn’t cross the street because it was too wide*

## Layer: 5 ⌐ Attention: Input - Input

### Sentence 1
```
The_  animal_  didn_  .  L  cross_  the_  street_  because_  it_  was_  too_  tire  d_  .
```
### Sentence 2
```
The_  animal_  didn_  .  L  cross_  the_  street_  because_  it_  was_  too_  wide_  .
```
```

### Visualization
![Self-Attention Mechanism](imageuria.png)

**Vineeth N B (IIT-H)**
**§9.5 Self-Attention and Transformers**
**7 / 22**
```

# DL4CV_Week09_Part05.pdf - Page 18

```markdown
# Self-Attention

**Vineeth N B (IIT-H)**

## §9.5 Self-Attention and Transformers

### Consider two input sentences we want to translate:

- The animal didn't cross the street because it was too tired
- The animal didn't cross the street because it was too wide

- **"it"** refers to **"animal"** in the first case, but to **"street"** in the second case; this is hard for traditional Seq2Seq models to model

![Diagram of Self-Attention Mechanism](image_url)

```plaintext
Layer: 5 ; Attention: Input - Input
---------------------------------------

The_animal_didn_t_cross_the_street_because_it_was_too_tired
---------------------------------------------------------------

The_animal_didn_t_cross_the_street_because_it_was_too_wide
---------------------------------------------------------------
```

- The animal_
- didn_
- t_
- cross_
- the_
- street_
- because_
- it_
- was_
- too_
- tire_
- d_

- The_
- animal_
- didn_
- t_
- cross_
- the_
- street_
- because_
- it_
- was_
- too_
- wide

- The_
- animal_
- didn_
- t_
- cross_
- the_
- street_

- because_
- it_
- was_
- too_
- tire_
- d_
```


# DL4CV_Week09_Part05.pdf - Page 19

```markdown
# Self-Attention

![Self-Attention Diagram](image-placeholder.png)

- **Consider two input sentences we want to translate:**
  - **The animal didn’t cross the street because it was too tired**
  - **The animal didn’t cross the street because it was too wide**

- **"it" refers to "animal" in the first case, but to "street" in the second case; this is hard for traditional Seq2Seq models to model**

- **As the model processes each word, self-attention allows it to look at other positions in the input sequence to help get a better encoding**

**Layer: 5 | Attention: Input - Input**

```plaintext
The_animal_.didn’t_cross_.the_street_.because_.it_.was_.too_.tired_.d_
The_animal_.didn’t_cross_.the_street_.because_.it_.was_.too_.wide_.d_
```

*Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 7 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 20

```markdown
# Self-Attention

- Consider two input sentences we want to translate:
  - *The animal didn’t cross the street because it was too tired*
  - *The animal didn’t cross the street because it was too wide*
    - "it" refers to "animal" in the first case, but to "street" in the second case; this is hard for traditional Seq2Seq models to model.
- As the model processes each word, self-attention allows it to look at other positions in the input sequence to help get a better encoding.
- Recall RNNs: we now no longer need to maintain a hidden state to incorporate representation of previous words/vectors!

![Self-Attention Mechanism Diagram](image_url)

Layer: 5, Attention: Input - Input

| Left    | Right  |
|---------|--------|
| The_    | The_   |
| animal_ | animal_|
| didn_   | didn_  |
| t_     | t_    |
| cross_ | cross_|
| the_   | the_  |
| street_| street_|
| because_|because_|
| it_    | it_   |
| was_   | was_  |
| too_   | too_  |
| tire_  | tire_ |
| d_     | d_    |

**Vineeth N B (IIT-H)**

§9.5 Self-Attention and Transformers

7 / 22
```

# DL4CV_Week09_Part05.pdf - Page 21

 extraction and conversion.

```markdown
# Self-Attention

## Step 1: Create three vectors from encoder's input vector (x_i):

- **Input**:
  - **Thinking**
  - **Machines**

- **Embedding**:
  - x_i (Thinking)
  - x_i (Machines)

- **Queries**:
  - q_i (Thinking)
  - q_i (Machines)

- **Keys**:
  - k_i (Thinking)
  - k_i (Machines)

- **Values**:
  - v_i (Thinking)
  - v_i (Machines)

### Weight Matrices:
- $W^Q$
- $W^K$
- $W^V$

![Diagram](image_url)

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

8 / 22
```

# DL4CV_Week09_Part05.pdf - Page 22

:

```markdown
# Self-Attention

## Step 1: Create three vectors from encoder’s input vector (x_i):

- **Query vector (q_i)**
- **Key vector (k_i)**
- **Value vector (v_i)**

These are created by multiplying input with weight matrices \(W^Q\), \(W^K\), \(W^V\), learned during training.

### Input
- **Thinking**
- **Machines**

### Embedding
```
x_i
```
```
x_i
```

### Queries
```
q_i
```
```
q_i
```
```
W^Q
```

### Keys
```
k_i
```
```
k_i
```
```
W^K
```

### Values
```
v_i
```
```
v_i
```
```
W^V
```

![NPTEL](https://example.com/nptel.png)

**Vineeth N B (IIT-H)**

**§9.5 Self-Attention and Transformers**

**8 / 22**
```

# DL4CV_Week09_Part05.pdf - Page 23



```markdown
# Self-Attention

## Step 1: Create three vectors from encoder's input vector (xi):

- **Query vector (qi)**
- **Key vector (ki)**
- **Value vector (vi)**

These are created by multiplying input with weight matrices $W^Q, W^K, W^V$, learned during training.

### Input
- **Embedding:**
  - **Thinking**
  - **Machines**

### Queries
- **Query (qi)**
- **Query (qi)**

### Keys
- **Key (ki)**
- **Key (ki)**

### Values
- **Value (vi)**
- **Value (vi)**

### Notation:
- In the paper, $q, k, v \in \mathbb{R}^{64}$ and $x \in \mathbb{R}^{512}$

_Image placeholder: ![]()_

*Vineeth N B (IIT-H)*

_§9.5 Self-Attention and Transformers*

_NPTEL_

*8 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 24



```markdown
# Self-Attention

## Step 1: Create three vectors from encoder’s input vector (x_i):

- Input:
  - Thinking
  - Machines

- Embedding:
  - x_i (Thinking)
  - x_i (Machines)

- Queries:
  - q_i (Thinking)
  - q_i (Machines)

- Keys:
  - k_i (Thinking)
  - k_i (Machines)

- Values:
  - v_i (Thinking)
  - v_i (Machines)

### Notes:
- These are created by multiplying input with weight matrices \( W^Q, W^K, W^V \), learned during training.
- In the paper, \( q, k, v \in \mathbb{R}^{64} \) and \( x \in \mathbb{R}^{512} \).
- Do \( q, k, v \) always have to be smaller than \( x \)?

![Diagram](image_url_placeholder)

Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 8 / 22
```

# DL4CV_Week09_Part05.pdf - Page 25

```markdown
# Self-Attention

## Step 1: Create three vectors from encoder’s input vector \( x_i \):

- **Query vector** \( q_i \)
- **Key vector** \( k_i \)
- **Value vector** \( v_i \)

These are created by multiplying input with weight matrices \( W^Q, W^K, W^V \), learned during training.

### Input
- **Thinking**
- **Machines**

### Embedding
\[ x_i \]
\[ x_i \]

### Queries
\[ q_i \]

### Keys
\[ k_i \]

### Values
\[ v_i \]

### Dimensions

- In the paper, \( q, k, v \in \mathbb{R}^{64} \) and \( x \in \mathbb{R}^{512} \)
- Do \( q, k, v \) always have to be smaller than \( x \)? 
  - No, this was done perhaps to make computation of multi-headed attention constant

### Weight Matrices
- What are the dimensions of \( W^Q, W^K, W^V \)?

*Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 8 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 26

```markdown
# Self-Attention

## STEP 2: Calculate self-attention scores - score all words of input sentence against themselves; how?

![NPTEL Logo](https://via.placeholder.com/150)

- **Input**
- **Embedding**
- **Queries**
- **Keys**
- **Values**

![Diagram](https://via.placeholder.com/300)

### Thinking
- **x1**
- **q1**
- **k1**
- **v1**

### Machines
- **x2**
- **q2**
- **k2**
- **v2**

*Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers*

*Page 9 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 27

```markdown
# Self-Attention

## Step 2: Calculate self-attention scores

- Score all words of input sentence against themselves; how?
- By taking dot product of **query vector** with **key vector** of respective words

![NPTEL](https://example.com/nptel_logo.png)

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

9 / 22

---

**Thinking**

- Input
  - \( x_1 \)
- Embedding
  - \( x_1 \)
- Queries
  - \( q_1 \)
- Keys
  - \( k_1 \)
- Values
  - \( v_1 \)

**Machines**

- Input
  - \( x_2 \)
- Embedding
  - \( x_2 \)
- Queries
  - \( q_2 \)
- Keys
  - \( k_2 \)
- Values
  - \( v_2 \)
```

# DL4CV_Week09_Part05.pdf - Page 28

 this markdown file can be rendered correctly by GitHub and other markdown renderers.

```markdown
# Self-Attention

## Step 2: Calculate self-attention scores

- **Score all words of input sentence against themselves; how?**

  - By taking dot product of **query vector** with **key vector** of respective words

    ```markdown
    E.g. for input "Thinking", first score would be `q1 * k1` (with itself);
    second score would be dot product of `q1 * k2` (with "Machines"), and so on
    ```

![Diagram](https://example.com/diagram)

| Category | Thinking                       | Machines                    |
|----------|---------------------------------|-----------------------------|
| Input    | x1                              | x2                          |
| Embedding|                                  |                             |
| Queries  | q1                              | q2                          |
| Keys     | k1                              | k2                          |
| Values   | v1                              | v2                          |
| Score    | q1 * k1 = 112                   | q1 * k2 = 96                |

Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 9 / 22
```

# DL4CV_Week09_Part05.pdf - Page 29

```markdown
# Self-Attention

## STEP 2: Calculate self-attention scores
- Score all words of input sentence against themselves; how?

### By taking dot product of **query vector** with **key vector** of respective words

- E.g. for input "Thinking", first score would be \( q_1 \cdot k_1 \) (with itself); second score would be dot product of \( q_1 \cdot k_2 \) (with "Machines"), and so on

- Scores then divided by \( \sqrt{\text{length}(k)} \)

![Self-Attention Diagram](image_url)

- **Input**
    - **Thinking**
        - \( x_1 \) (green) 
        - \( q_1 \) (purple)
        - \( k_1 \) (orange)
        - \( v_1 \) (blue)

    - **Machines**
        - \( x_2 \) (green)
        - \( q_2 \) (purple)
        - \( k_2 \) (orange)
        - \( v_2 \) (blue)

- **Score Calculation**
    - \( q_1 \cdot k_1 = 112 \)
    - \( q_1 \cdot k_2 = 96 \)
    - Scores: 
        - \( q_1 \cdot k_1 \div 8 (\sqrt{d_k}) = 14 \)
        - \( q_1 \cdot k_2 \div 8 (\sqrt{d_k}) = 12 \)

*Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 9 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 30

```markdown
# Self-Attention

## STEP 2: Calculate self-attention scores

- **Score all words of input sentence against themselves; how?**

  - By taking dot product of **query vector** with **key vector** of respective words

  - E.g. for input "Thinking", first score would be \( q_1 \cdot k_1 \) (with itself); second score would be dot product of \( q_1 \cdot k_2 \) (with "Machines"), and so on

  - Scores then divided by \( \sqrt{\text{length}(k)} \)

  - This is **Scaled Dot-Product Attention**, recall from W9P1; this design choice leads to more stable gradients

![Image placeholder](image-url)

| Thinking     | Machines  |
|--------------|-----------|
| \( x_1 \)    | \( x_2 \)  |
| \( q_1 \)    | \( q_2 \)  |
| \( k_1 \)    | \( k_2 \)  |
| \( v_1 \)    | \( v_2 \)  |
| \( q_1 \cdot k_1 = 112 \)   | \( q_1 \cdot k_2 = 96 \)   |
| Divide by 8 (\(\sqrt{8}\)) | Divide by 8 (\(\sqrt{8}\)) |

**Vineeth N B (IIIT-H)**

§9.5 Self-Attention and Transformers

9 / 22
```

# DL4CV_Week09_Part05.pdf - Page 31

 accurately the content as text, tables, symbols, and diagrams.

```markdown
# Self-Attention

## STEP 3: Softmax used to get normalized probability scores; determines how much each word will be expressed at this position

![Diagram of Self-Attention Mechanism](image_url)

### Input
- **Embedding**
  - **Thinking**
    - \( x_1 \)
    - \( x_2 \)
  - **Machines**
    - \( x_2 \)
    - \( x_2 \)

- **Queries**
  - \( q_1 \)
  - \( q_2 \)
  
- **Keys**
  - \( k_1 \)
  - \( k_2 \)
  
- **Values**
  - \( v_1 \)
  - \( v_2 \)

- **Score**
  - \( q_1 \cdot k_1 = 112 \)
  - \( q_1 \cdot k_2 = 96 \)
  
  - **Divide by 8 ( \( \sqrt{d_k} \) )**
    - 14
    - 12

- **Softmax**
  - 0.88
  - 0.12

**Vineeth N B (IIT-H)**

**§9.5 Self-Attention and Transformers**

10 / 22
```

# DL4CV_Week09_Part05.pdf - Page 32



```markdown
# Self-Attention

- **STEP 3:** Softmax used to get normalized probability scores; determines how much each word will be expressed at this position

  Clearly, word at this position will have highest softmax score, but sometimes it's useful to attend to another word that is relevant

![Diagram](image_url)

## Input
- Embedding
- Queries
- Keys
- Values

## Step-by-Step Breakdown

### Thinking Example
- Input: `x1`
- Queries: `q1`
- Keys: `k1`
- Values: `v1`

### Machines Example
- Input: `x2`
- Queries: `q2`
- Keys: `k2`
- Values: `v2`

### Calculations
- `q1 * k1 = 112`
- `q2 * k2 = 96`

### Score Calculation
- Score: `14`
- Score: `12`

### Normalization
- Divide by `8 (sqrt(d_k))`

### Softmax Output
- `0.88`
- `0.12`

## References
- Vineeth N B (IIT-H)
- §9.5 Self-Attention and Transformers

```

# DL4CV_Week09_Part05.pdf - Page 33



```markdown
# Self-Attention

- **STEP 3**: Softmax used to get normalized probability scores; determines how much each word will be expressed at this position
- Clearly, word at this position will have highest softmax score, but sometimes it's useful to attend to another word that is relevant

- **STEP 4**: Multiply each **value vector** by softmax score; **why**? Keep values of word(s) we want to focus on intact, and drown out irrelevant words

![Diagram of the self-attention process](image_url)

## Thinking
- **x1**: \[green bar\]
- **q1**: \[pink bar\]
- **k1**: \[orange bar\]
- **v1**: \[blue bar\]

## Machines
- **x2**: \[green bar\]
- **q2**: \[pink bar\]
- **k2**: \[orange bar\]
- **v2**: \[blue bar\]

### Calculations
- **q1 * k1 = 112**
  - 14
  - 0.88
- **q1 * k2 = 96**
  - 12
  - 0.12

### Resulting Values
- **v1**: \[blue bar\]
- **v2**: \[very light blue bar\]

**Note**: The specific embeddings and values may vary, but the process of calculating the attention scores and adjusting the value vectors remains consistent.
```

# DL4CV_Week09_Part05.pdf - Page 34

 special care for the mathematical notations and steps.

```markdown
# Self-Attention

- **STEP 3**: Softmax used to get normalized probability scores; determines how much each word will be expressed at this position

  Clearly, word at this position will have highest softmax score, but sometimes it's useful to attend to another word that is relevant

- **STEP 4**: Multiply each *value vector* by softmax score; why? Keep values of word(s) we want to focus on intact, and drown out irrelevant words

- **STEP 5**: Sum up weighted value vectors → produces output of self-attention layer at this position (for first word)

![Diagram](https://via.placeholder.com/150)

| Category   | Thinking                  | Machines              |
|------------|---------------------------|-----------------------|
| Input      | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
| Embedding  | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
| Queries    | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
| Keys       | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
| Values     | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
| Score      | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
|            | `q1 * k1 = 112`          | `q1 * k2 = 96`        |
|            | `14`                     | `12`                  |
|            | `0.88`                   | `0.12`                |
|            | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
|            | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |
|            | ![Placeholder](https://via.placeholder.com/150) | ![Placeholder](https://via.placeholder.com/150) |

**Vineeth N B (IIIT-H)**
§9.5 Self-Attention and Transformers
```

# DL4CV_Week09_Part05.pdf - Page 35

```markdown
# Self-Attention: Illustration

---

## Components of Self-Attention

### Input Matrix (X)
- Represented as a green matrix.

### Weight Matrices
Three weight matrices (WQ, WK, WV):

- **WQ**: Weights for the Query (Q)
  - Formula: \( X \times WQ \)
  - Result: Query matrix (Q)

- **WK**: Weights for the Key (K)
  - Formula: \( X \times WK \)
  - Result: Key matrix (K)

- **WV**: Weights for the Value (V)
  - Formula: \( X \times WV \)
  - Result: Value matrix (V)

### Self-Attention Mechanism
- Uses the softmax function to compute attention scores.
  - Formula: \( \text{softmax} \left( \frac{Q \times K^T}{\sqrt{d_k}} \right) \)

### Output Matrix (Z)
- Resulting from the self-attention mechanism.
  - Formula: \( \text{softmax} \left( \frac{Q \times K^T}{\sqrt{d_k}} \right) \times V \)
  - Result: Output matrix (Z)

---

**Vineeth N B (IIIT-H)**

**§9.5 Self-Attention and Transformers**

---

![Diagram of Self-Attention Mechanism](image_placeholder.png)

```math
\text{softmax} \left( \frac{Q \times K^T}{\sqrt{d_k}} \right) \times V
```
```

# DL4CV_Week09_Part05.pdf - Page 36

:
```markdown
# Multi-Head Attention

## Multi-Head Attention

![Multi-Head Attention Diagram](image-url)

**Improves performance of the attention layer in two ways:**

- **Linear**
- **Concat**
- **Scaled Dot-Product Attention**

![Scaled Dot-Product Attention Diagram](image-url)

- **Linear**
- **Linear**
- **Linear**

**Vineeth N B (IIT-H)**

**Section 9.5 Self-Attention and Transformers**

**NPTEL**

```

# DL4CV_Week09_Part05.pdf - Page 37

 or slide content:

---

## Multi-Head Attention

### Multi-Head Attention

```markdown
# Multi-Head Attention

### Components:
- Linear
- Concat
- Scaled Dot-Product Attention
  - Linear (V)
  - Linear (K)
  - Linear (Q)

### Explanation:
- **Improves performance of the attention layer in two ways:**
  - Expands model’s ability to focus on different positions.
  - For example, \( z_1 \) contains a bit of every other encoding, but dominated by the actual word itself.

### Visual Representation:
![Visual Representation](https://via.placeholder.com/150)

---

*Source: Vineeth N B (IIIT-H)*

*Section: §9.5 Self-Attention and Transformers*

*Slide Number: 12 / 22*
```

---

This markdown format ensures that the content is structured and readable, maintaining the integrity of the scientific notation and visual components.

# DL4CV_Week09_Part05.pdf - Page 38

:

```markdown
# Multi-Head Attention

## Multi-Head Attention

### Diagram
```markdown
![Diagram](image_url)
```

- Improves performance of the attention layer in two ways:
  - Expands model’s ability to focus on different positions. In example above, \(z_1\) contains a bit of every other encoding, but dominated by actual word itself.
  - Gives attention layer multiple “representation subspaces”; we have not one, but multiple sets of Query/Key/Value weight matrices; after training, each set is used to project input embeddings into different representation subspaces.

### Credit
Vaswani et al, *Attention is All You Need*, NeurIPS 2017

Vineeth N B (IIT-H)

**§9.5 Self-Attention and Transformers**

12 / 22
```

Ensure to replace the placeholder `image_url` with the actual URL or filename if available from the OCR process.
```

# DL4CV_Week09_Part05.pdf - Page 39

```markdown
# Multi-Head Attention: Illustration

1) This is our input sentence\*.
2) We embed each word\*.
3) Split into 8 heads. We multiply X or R with weight matrices.
4) Calculate attention using the resulting Q/K/V matrices.
5) Concatenate the resulting Z matrices, then multiply with weight matrix W<sup>O</sup> to produce the output of the layer.

---

## Steps and Diagrams

### 1. Input Sentence
![Input Sentence](data:image/png;base64,...)

### 2. Embedding
![Embedding](data:image/png;base64,...)

### 3. Splitting into Heads
![Splitting into Heads](data:image/png;base64,...)

### 4. Calculate Attention
![Calculate Attention](data:image/png;base64,...)

### 5. Concatenate Z Matrices
![Concatenate Z Matrices](data:image/png;base64,...)

---

\* In all encoders other than #0, we don't need embedding. We start directly with the output of the encoder right below this one.

---

### Diagram Components

- **X**: Embedding of the input sentence.
- **R**: Output from a previous encoder.
- **W<sub>0Q</sub>, W<sub>0K</sub>, W<sub>0V</sub>**: Weight matrices for the first head.
- **Q<sub>0</sub>, K<sub>0</sub>, V<sub>0</sub>**: Resulting query, key, and value matrices for the first head.
- **Z<sub>0</sub>**: Resulting attention matrix for the first head.
- **W<sub>O</sub>**: Output weight matrix.

---

### Equations and Notations

- **Q**: Query matrix
- **K**: Key matrix
- **V**: Value matrix
- **Z**: Concatenated attention matrix

---

### References

- Vineeth N B (IIT-H)
- §9.5 Self-Attention and Transformers
- Page 13 / 22
```

Note: Placeholder images (`data:image/png;base64,...`) are used where actual images could not be captured or identified during OCR. Replace these with the actual images if available.

# DL4CV_Week09_Part05.pdf - Page 40

```markdown
# Positional Encoding

- Unlike RNN and CNN encoders, attention encoder outputs do not depend on order of inputs (Why?)

![NPTEL Logo](https://example.com/logo.png)

Vineeth N B (IIT-H) &9.5 Self-Attention and Transformers

14 / 22
```

# DL4CV_Week09_Part05.pdf - Page 41

```markdown
# Positional Encoding

- Unlike RNN and CNN encoders, attention encoder outputs do not depend on order of inputs (**Why?**)

- But order of sequence conveys important information for machine translation tasks and language modeling

![NPTEL Logo](image_url)

Vineeth N B (IIT-H) & sections 9.5 Self-Attention and Transformers 14 / 22
```

# DL4CV_Week09_Part05.pdf - Page 42



```markdown
# Positional Encoding

- Unlike RNN and CNN encoders, attention encoder outputs do not depend on order of inputs (Why?)

- But order of sequence conveys important information for machine translation tasks and language modeling

- The idea: Add positional information of input token in the sequence into input embedding vectors

  \[
  PE_{pos, 2i} = \sin \left( \frac{pos}{10000^{\frac{2i}{d_{emb}}}} \right)
  \]

  \[
  PE_{pos, 2i+1} = \cos \left( \frac{pos}{10000^{\frac{2i}{d_{emb}}}} \right)
  \]

![Diagram Placeholder](image.png)

*Vineeth N B (IIT-H)*

*§9.5 Self-Attention and Transformers*

*14 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 43



```markdown
# Positional Encoding

- Unlike RNN and CNN encoders, attention encoder outputs do not depend on the order of inputs (**Why?**)

- But order of sequence conveys important information for machine translation tasks and language modeling

- The idea: Add positional information of input token in the sequence into input embedding vectors

  \[
  PE_{pos, 2i} = \sin \left( \frac{pos}{10000^{\frac{2i}{d_{emb}}}} \right)
  \]

  \[
  PE_{pos, 2i+1} = \cos \left( \frac{pos}{10000^{\frac{2i}{d_{emb}}}} \right)
  \]

- Final input embeddings are concatenation of learnable embedding and positional encoding

*Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 14 / 22*
```

# DL4CV_Week09_Part05.pdf - Page 44

 accuracy is critical for this task.

```markdown
# Encoder

![Encoder Diagram](image-url)

- **Stack of N=6 identical layers**

## Diagram Explanation

- **Add & Norm**: Add & Normalization step
- **Feed Forward**: Feed forward neural network layer
- **Multi-Head Attention**: Multiple attention mechanisms working in parallel

## Diagram Components

![Component Diagram](image-url)

- **Add & Norm**: Add & Normalization step
- **Feed Forward**: Feed forward neural network layer
- **Multi-Head Attention**: Multiple attention mechanisms working in parallel

### Detailed Breakdown

- **Input**: Takes input through the LayerNorm(x) + SubLayer(x)
- **Add & Norm**: Normalizes the input
- **Multi-Head Attention**: Applies multiple attention heads
- **Feed Forward**: Applies a feed-forward neural network
- **Output**: Produces the final output through additional normalization

## Credit

- **"Attention? Attention!"** by Lilian Weng
- Vineeth N B (IIT-H)
- §9.5 Self-Attention and Transformers

---

Page 15 / 22

```

Note: Replace `image-url` with the actual URL or placeholder for images if OCR couldn’t capture them directly.

# DL4CV_Week09_Part05.pdf - Page 45

```markdown
# Encoder

![Encoder Diagram](image_url)

- Stack of N=6 identical layers
- Each layer has a **multi-head self-attention layer** and a simple position-wise fully connected **feedforward network**

**Credit:** *"Attention? Attention!"* by Lilian Weng

Vineeth N B (IIIT-H) §9.5 Self-Attention and Transformers 15 / 22
```

# DL4CV_Week09_Part05.pdf - Page 46



```markdown
# Encoder

![Encoder Diagram](image_url)

- Stack of N=6 identical layers
- Each layer has a **multi-head self-attention layer** and a simple position-wise fully connected **feedforward network**
- Each sub-layer has a **residual connection** and **layer-normalization**; all sub-layers output data of same dimension \( d_{model} = 512 \)

**Credit:** *"Attention? Attention!"* by Lilian Weng

_Vineeth N B (IIIT-H) §9.5 Self-Attention and Transformers_

![Transformer Diagram](image_url)

## Diagram Details

![Add & Norm](image_url) - Add & Norm

- **Feed Forward** within the layer
- **Multi-Head Attention**

### Self-Attention Mechanism

![Self-Attention Diagram](image_url)

- **Query, Key, Value** (Q, K, V) mechanism
  - **Threshing**
  - **Softmax**
  - **Multiply**
  - **Concatenate**

### Position-wise Fully Connected Feedforward Network

![Feed Forward](image_url)

- **Add & Norm**
- **Feed Forward**
- **Layer Normalization**

### Residual Connections and Layer Normalization

![Residual Connection](image_url)

- **Residual Connection** within each sub-layer
- **Layer Normalization**

### Model Dimension

\[ d_{model} = 512 \]
```

# DL4CV_Week09_Part05.pdf - Page 47

 the output should be detailed and formatted in markdown.

```markdown
# Decoder

![Decoder Diagram](https://example.com/decoder-diagram.png)

- **Stack of N=6 identical layers**

    ```
    Add & Norm
    ├──── Feed Forward
    ├──── Add & Norm
    └──── Multi-Head Attention
    ├──── Add & Norm
    └──── Masked Multi-Head Attention
    ```

    - **LayerNorm(x) + SubLayer(x)**
    - **Nx = 6**
    - **Masked**: not to use the information in the future.

**Credit**: "Attention? Attention!" by Lilian Weng

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

NPTel

16 / 22
```

# DL4CV_Week09_Part05.pdf - Page 48

 this markdown output can be used directly in a markdown-rendering environment.

```markdown
# Decoder

![Decoder Diagram](image.png)

- **Stack of N=6 identical layers**

  Each layer has two sub-layers of **multi-head attention** mechanisms and one sub-layer of **fully-connected feedforward network**

  - **Add & Norm**
  - **Feed Forward**
  - **Add & Norm**
  - **Multi-Head Attention**
  - **Add & Norm**
  - **Masked Multi-Head Attention**

  **Note**: Masked: not to use the information in the future.

  Credit: *"Attention? Attention!"* by Lilian Weng

  Vineeth N B (IIT-H)

  §9.5 Self-Attention and Transformers

  16 / 22

  NPTEL

```

# DL4CV_Week09_Part05.pdf - Page 49

 your response with a heading for clarity.

# Decoder

```markdown
# Decoder

![Decoder Diagram](path/to/diagram.png)

- Stack of **N=6** identical layers
- Each layer has two sub-layers of **multi-head attention mechanisms** and one sub-layer of **fully-connected feedforward network**
- Similar to encoder, each sub-layer adopts a **residual connection** and a **layer-normalization**

**Credit: "Attention? Attention!" by Lilian Weng**

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

**Note:**
- `Add & Norm` components are present in the layers.
- `Feed Forward` sub-layer is indicated.
- `Masked Multi-Head Attention` is used, which prevents using future information.

## Diagram Details:

- The decoder consists of a stack of 6 identical layers.
- Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feedforward network.
- Similar to the encoder, each sub-layer adopts a residual connection and a layer-normalization.
- `Masked` indicates that future information is not used in the attention mechanism.
```

Ensure that the image file path is replaced with the actual path if available. If not available, you can use a placeholder image.

# DL4CV_Week09_Part05.pdf - Page 50

```markdown
# Decoder

![Decoder Diagram](image-url)

Credit: "Attention? Attention!" by Lilian Weng

Vineeth N B (IIT-H)

§9.5 Self-Attention and Transformers

## Decoder

![Decoder Diagram](image-url)

- **Stack of N=6 identical layers**
  - Each layer has two sub-layers of **multi-head attention mechanisms** and one sub-layer of **fully-connected feedforward network**
  - Similar to encoder, each sub-layer adopts a **residual connection** and a **layer-normalization**
  - First multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into future of target sequence when predicting current position

```markdown
```

# DL4CV_Week09_Part05.pdf - Page 51

```markdown
# Transformers: Full Architecture

## Components

1. **Input Embedding**
    - Positional Encoding
    - Input Embedding

2. **Encoder**
    - Composed of multiple layers (Nx)
        - **Add & Norm**
            - Multi-Head Attention
            - Feed Forward
        - **Add & Norm**
            - Multi-Head Attention
            - Feed Forward

3. **Decoder**
    - Composed of multiple layers (Nx)
        - **Add & Norm**
            - Multi-Head Attention
            - Feed Forward
        - **Add & Norm**
            - Multi-Head Attention
            - Feed Forward

4. **Positional Encoding**
    - Added to both Input Embedding and Output Embedding

5. **Output Embedding**
    - Positional Encoding
    - Output Embedding

6. **Output Layer**
    - Output Probabilities
    - Softmax

## Multi-head Attention

### Scaled Dot-Product Attention

- **Matrix Operations**
    - MatMul: Matrix multiplication
    - SoftMax: Softmax function
    - MatMul: Matrix multiplication again
- **Steps**
    1. Compute **Query (Q)**, **Key (K)**, and **Value (V)**
    2. Perform dot product between **Q** and **K** and scale by \(\sqrt{d_k}\)
    3. Apply **SoftMax** to the scaled dot product
    4. Multiply the result by **V**

### Multi-head Attention

- Combine multiple scaled dot-product attentions
- Apply **Add & Norm** and **Feed Forward** operations

## Credits

- **Source**: "Attention? Attention!" by Lilian Weng
- **Reference**: Vineeth N B (IIT-H)
- **Section**: §9.5 Self-Attention and Transformers

![Diagram](https://via.placeholder.com/600x400)
```

**Note:** The placeholder URL `https://via.placeholder.com/600x400` is used for the image. Replace it with the actual image URL if available.

# DL4CV_Week09_Part05.pdf - Page 52

:

# Transformers in Computer Vision: Object Detection

## Breakdown of Object Detection Process

### Components

#### Backbone
- **CNN (Convolutional Neural Network)**
- **Positional Encoding**: 
  - Adds positional information to image features.

#### Encoder
- **Transformer Encoder**
- **Set of Image Features**: 
  - Output from CNN and positional encoding.

#### Decoder
- **Transformer Decoder**
- **Object Queries**: 
  - Input to decoder, representing potential objects in the image.

#### Prediction Heads
- **Feed Forward Networks (FFN)**: 
  - Output layers for final predictions.

### Workflow
1. **Backbone Stage**:
    - CNN processes image to extract features.
    - Positional encoding is added to the image features.

2. **Encoder Stage**:
    - Transformer encoder processes the combined image features and positional encoding.

3. **Decoder Stage**:
    - Transformer decoder processes object queries.
    - Interacts with the encoder to refine object queries.

4. **Prediction Heads Stage**:
    - FFNs use the refined object queries to predict:
      - **Class**
      - **Bounding Box (Box)**
      - **No Object (No)**

### Example Output
- Represented by an image of birds on the beach.
- Detected objects are highlighted with bounding boxes:
  - **Red Box**: One bird.
  - **Yellow Box**: Another bird.

### References
- Carion et al., "End-to-End Object Detection with Transformers," ECCV 2020.
- [Vineeth N B (IIT-H)](https://github.com/VineethNB)
- Slide from "§9.5 Self-Attention and Transformers"

```

# DL4CV_Week09_Part05.pdf - Page 53

: 
 
**Text to be converted:**

```plaintext
Transformers in Computer Vision: Object Detection

Faster R-CNN
--------------------------------------------------
- Up to 200,000 coarse proposals
- Filter and deduplicate (NMS)
- Detection-specific components

- CNN features
  - Crop (RoIAlign) on coarse proposals
  - Classify and refine proposals
  - Deduplicate proposals (NMS)

Predicted boxes and classes

DETR
--------------------------------------------------
- CNN features
- Standard transformer encoder-decoder

Predicted boxes and classes

Credit: Ram Sagar, Analytics India Mag
Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 19 / 22
```

**Detailed Markdown Conversion:**

```markdown
# Transformers in Computer Vision: Object Detection

## Faster R-CNN

### Detection-Specific Components

- **Up to 200,000 coarse proposals**
- **Filter and deduplicate (NMS)**
- **Detection-specific components**

### CNN Features

- **Crop (RoIAlign) on coarse proposals**
- **Classify and refine proposals**
- **Deduplicate proposals (NMS)**

### Predicted Boxes and Classes

## DETR

### Standard Transformer Encoder-Decoder

### Predicted Boxes and Classes

**Credit:** Ram Sagar, Analytics India Mag
Vineeth N B (IIT-H) §9.5 Self-Attention and Transformers 19 / 22
```

This markdown structure ensures that the information is clearly presented and maintains the scientific integrity of the original text.

# DL4CV_Week09_Part05.pdf - Page 54

 extracted content into a detailed markdown format.

```markdown
# Transformers in Computer Vision: Object Detection

## Results on MS COCO validation set

| Model                      | GFLOPS/FPS | #params | AP   | AP_50 | AP_75 | AP_S | AP_M | AP_L |
|----------------------------|------------|---------|------|-------|-------|------|------|------|
| Faster RCNN-DC5            | 320/16     | 166M    | 39.0 | 60.5  | 42.3  | 21.4 | 43.5 | 52.5 |
| Faster RCNN-FPN             | 180/26     | 42M     | 40.2 | 61.0  | 43.8  | 24.2 | 43.5 | 52.0 |
| Faster RCNN-R101-FPN        | 246/20     | 60M     | 42.0 | 62.5  | 45.9  | 25.2 | 45.6 | 54.6 |
| Faster RCNN-DC5+            | 320/16     | 166M    | 41.1 | 61.4  | 44.3  | 22.9 | 45.9 | 55.0 |
| Faster RCNN-FPN+            | 180/26     | 42M     | 42.0 | 62.1  | 45.5  | 26.6 | 45.4 | 53.4 |
| Faster RCNN-R101-FPN+        | 246/20     | 60M     | 44.0 | 63.9  | 47.8  | 27.2 | 48.1 | 56.0 |
| DETR                        | 86/28      | 41M     | 42.0 | 62.4  | 44.2  | 20.5 | 45.8 | 61.1 |
| DETR-DC5                    | 187/12     | 41M     | 43.3 | 63.1  | 45.9  | 22.5 | 47.3 | 61.1 |
| DETR-R101                   | 152/20     | 60M     | 43.5 | 63.8  | 46.4  | 21.9 | 48.0 | 61.8 |
| DETR-DC5-R101               | 253/10     | 60M     | 44.9 | 64.7  | 47.7  | 23.7 | 49.5 | 62.3 |

*Carion et al., End-to-End Object Detection with Transformers, ECCV 2020*

*Vinesh N B (IIIT-H)*

*§9.5 Self-Attention and Transformers*
```

# DL4CV_Week09_Part05.pdf - Page 55

 the original content in the explanation above.

---

# Transformers in Computer Vision: Image Recognition<sup>4</sup>

![Diagram of Vision Transformer (ViT)](image_url)

**Credit: Nabil Madali, Gitconnected**

## Vision Transformer (ViT)

- **Input (Patch + Position Embedding)**
  - **Extra Learnable Embedding**
  - **Linear Projection of Flattened Patches**

- **Components**
  - Transformer Encoder
  - MLP Head

## Transformer Encoder

### Steps

1. **Image split into fixed-size patches**
   - Each of them linearly embedded
   - Position embeddings added to resulting sequence of vectors
2. **Patches fed to standard Transformer encoder**
   - In order to perform classification, standard approach of adding an extra learnable "classification token" added to sequence

### Detailed Steps

1. Image split into fixed-size patches
2. Each of them linearly embedded
3. Position embeddings added to resulting sequence of vectors
4. Patches fed to standard Transformer encoder
5. In order to perform classification, standard approach of adding an extra learnable "classification token" added to sequence

### Diagram Components

- **Embedded Patches**
- **Norm**
- **Multi-Head Attention**
- **MLP**

### References

<sup>4</sup> Dosovitskiy et al, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, arXiv 2020

Vineeth N B (IIT-M)

§9.5 Self-Attention and Transformers

21 / 22

---

This markdown format maintains the scientific integrity of the content while ensuring proper formatting and accurate representation of the original material.

# DL4CV_Week09_Part05.pdf - Page 56

:

```markdown
# Homework

## Readings

- Watch the Transformers in Action video provided in the week’s lecture materials

- **The Illustrated Transformer** article by Jay Alammar

- A detailed explanation of positional encoding by Amirhossein Kazemnejad

- For more information: *Attention is All You Need* paper by Vaswani, et al. (NeurIPS 2017)

## Questions

- Are transformers faster or slower than LSTMs? What is the reason for your opinion?
```

