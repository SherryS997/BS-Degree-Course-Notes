# DL4CV_Week09_Part04.pdf - Page 1

```markdown
# Deep Learning for Computer Vision

# Other Attention Models in Vision

**Vineeth N Balasubramanian**

Department of Computer Science and Engineering
Indian Institute of Technology, Hyderabad

![IIT Hyderabad Logo](https://www.iith.ac.in/sites/default/files/logo.png)

---

### Vineeth N B (IIT-H)

## §9.4 Other Attention Models

---

```markdown
```

# DL4CV_Week09_Part04.pdf - Page 2

```markdown
# Neural Turing Machines

- **Neural Turing Machines (or NTMs)** are a class of neural networks designed to be analogous to a Turing Machine

![Turing Machine](image_url)

- Just like a Turing Machine (see above), an NTM has a memory and allows read/write operations

- **However, unlike a Turing machine, the memory of an NTM is limited**

---

1. Graves et al., Neural Turing Machines, arXiv 2014

Vineeth N B (IIT-H)

§9.4 Other Attention Models

---

2 / 30
```

# DL4CV_Week09_Part04.pdf - Page 3

```markdown
# Neural Turing Machines

## Components of a Neural Turing Machine

A Neural Turing Machine has two main parts:

1. **Controller**: A neural network which executes operations on memory.
2. **Memory**: A finite `R x C` matrix to store information of `R` locations each having `C` dimensions.

### Diagram

![Neural Turing Machine Diagram](image-link)

- **External Input**
- **External Output**

```
Controller
 
|-- Read Heads
|-- Write Heads

Memory
```
*Vineeth N B. (IIT-H)*
*§9.4 Other Attention Models*
*3 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 4



```markdown
# Neural Turing Machines

![Neural Turing Machine Diagram](image_url)

## A Neural Turing Machine has two main parts:

- **Controller**: A neural network which executes operations on memory
- **Memory**: A finite \(R \times C\) matrix to store information of \(R\) locations each having \(C\) dimensions

## Question:

Like a Turing Machine, if we do read/write operations on NTM by specifying a row and column index, can we train the NTM end-to-end?

Vineeth N B. (IIT-H) §9.4. Other Attention Models 3 / 30
```

Note: Replace `image_url` with the actual URL or placeholder for the image if available. If the image couldn't be processed, you can mention it or leave it out.

# DL4CV_Week09_Part04.pdf - Page 5

```markdown
# Neural Turing Machines

- The answer is **No**; we can't take the gradient of an index, since it is a hard choice (similar to hard attention). What do we do?

![NPTEL Logo](attachment:logo.png)

Vineeth N B. (IIIT-H)

§9.4 Other Attention Models

Page 4 / 30
```

# DL4CV_Week09_Part04.pdf - Page 6

:

# Neural Turing Machines

- **The answer is **No**; we can't take the gradient of an index, since it is a hard choice (similar to hard attention). What do we do?**

- Just like we use softmax as a continuous alternative to max, we use a **"blurry"** attention vector to index the matrix

- This makes the NTM end-to-end trainable using backpropagation and gradient descent

![Diagram of Neural Turing Machine](https://via.placeholder.com/400)

```
- External Input
- External Output
    - Controller
        - Read Heads
        - Write Heads
    - Memory
```

Vineeth N B. (IIIT-H)  
§9.4 Other Attention Models  
4 / 30

# DL4CV_Week09_Part04.pdf - Page 7

: 

```markdown
# Neural Turing Machines: Reading

- Memory matrix at time \( t \), \( M_t \), has \( R \) rows and \( C \) columns
- **Normalized attention vector** \( w_t \) specifies location of memory to read from:
  \[
  0 \leq w_t(i) \leq 1 \text{ and } \sum_{i=1}^{R} w_t(i) = 1
  \]

- **Read head** is a sum of matrix rows, weighted by attention vector:
  \[
  r_t = \sum_{i=1}^{R} w_t(i) M_t(i)
  \]

![Diagram](https://via.placeholder.com/600x400?text=Diagram+Here)

- External Input
- External Output
  - Controller
    - Read Heads
    - Write Heads
  - Memory
```
```

# DL4CV_Week09_Part04.pdf - Page 8



```markdown

# Neural Turing Machines: Writing

- **For writing**, part of memory at time $t-1$ first erased, specified by **erase vector** $e_t$ of length $C$:
  \[
  M_t'(i) = M_{t-1}[1 - w_t(i)e_t]
  \]

- Then, information in an **add vector** $a_t$ is added to memory:
  \[
  M_t(i) = M_t'(i) + w_t(i)a_t
  \]

![](https://via.placeholder.com/150) 

- **External Input** and **External Output** connected to:
  - **Controller**
    - **Read Heads**: Access memory
    - **Write Heads**: Modify memory

  - **Memory**

---

_Image placeholder for diagram_

---

**Vineeth N B (IIT-H)**

**§9.4 Other Attention Models**

**6 / 30**
```

This markdown format ensures that the scientific text is accurately and appropriately formatted for readability and technical precision.

# DL4CV_Week09_Part04.pdf - Page 9

```markdown
# Neural Turing Machines: Attention

- **How to generate attention vector `w_t`?**

![NPTEL Logo](image_url)

Vineeth N B (IIT-H)

## 9.4 Other Attention Models

Page 7 / 30
```

# DL4CV_Week09_Part04.pdf - Page 10

```markdown
# Neural Turing Machines: Attention

- **How to generate attention vector \( w_t \)?**
  - NTMs use a combination of **content-based addressing** and **location-based addressing**. What does this mean?

![Graphical Representation Placeholder](image_url)

Vineeth N B (IIIT-H) 

§ 9.4 Other Attention Models

Page 7 / 30
```

# DL4CV_Week09_Part04.pdf - Page 11



```markdown
# Neural Turing Machines: Attention

- **How to generate attention vector \( w_t \)?**
- NTMs use a combination of **content-based addressing** and **location-based addressing**. What does this mean?
- Each head has a **key vector** \( k_t \) which is compared with each row in matrix \( M_t \).
- **Content weight vector** generated as follows:

  \[
  w_t^c(i) = \frac{\exp(\beta_t \cos (k_t, M_t(i)))}{\sum_j \exp(\beta_t \cos (k_t, M_t(j)))}
  \]

- Here, \( \beta_t \) known as **key strength**, which decides how concentrated content weight vector should be.

*Vineeth N B. (IIT-H) §9.4 Other Attention Models*

*Slide 7/30*
```

# DL4CV_Week09_Part04.pdf - Page 12

```markdown
# Neural Turing Machines: Attention

- Content weight vector is combined with attention vector in previous time step, using a scalar parameter \( g_t \):

  \[
  w_t^g = g_t w_t^c + (1 - g_t) w_{t-1}
  \]

- To allow controller to shift to other rows, circular convolution of resultant vector with a kernel \( s_t(.) \) is performed:

  \[
  \tilde{w}_t(i) = \sum_{j=1}^{R} w_t^g(j) s_t(i - j)
  \]

- Finally, attention distribution is sharpened using a scalar \( \gamma_t \geq 1 \):

  \[
  w_t(i) = \frac{\tilde{w}_t(i)^{\gamma_t}}{\sum_{j=1}^{R} \tilde{w}_t(j)^{\gamma_t}}
  \]

![Vineeth N B (IITH)](https://example.com/image.png)

§9.4 Other Attention Models

8 / 30
```

# DL4CV_Week09_Part04.pdf - Page 13

```markdown
# Neural Turing Machines: Attention

## Summary of attention vector generation:

### Previous State
- \( W_{t-1} \)
- \( M_t \)

### Controller Outputs
- \( k_t \)
- \( \beta_t \)
- \( g_t \)
- \( s_t \)
- \( \gamma_t \)

### Diagram
![Neural Turing Machine Attention Mechanism](image_url)

### Process
1. **Content Addressing**
   - Inputs: \( k_t \), \( \beta_t \), \( g_t \), \( s_t \), \( \gamma_t \)
   - Outputs: \( w_i^c \)

2. **Interpolation**
   - Input: \( w_i^c \)
   - Output: \( w_i^g \)

3. **Convolutional Shift**
   - Input: \( w_i^g \)
   - Output: \( w_i \)

4. **Sharpening**
   - Input: \( w_i \)
   - Output: \( w_t \)

### References
- Vineeth N B (IIIT-H)
- §9.4 Other Attention Models

---

Page 9 / 30
```

# DL4CV_Week09_Part04.pdf - Page 14

 accuracy and formatting conventions are prioritized.

```markdown
# DRAW: Deep Recurrent Attentive Writer²

- **Humans learn to draw figures in a sequential manner**

  - First, we draw outlines and then iteratively add more and more detail

- **Deep Recurrent Attentive Writer (DRAW) is a VAE (a kind of autoencoder, which we will see soon) which aims to mimic this sequential drawing approach**

![Image Placeholder](image.png)

_²Gregor et al., DRAW: A Recurrent Neural Network For Image Generation, ICML 2015_

*Vineeth N B (IIIT-H)*

*§9.4 Other Attention Models*

---

Page 10 / 30
```

# DL4CV_Week09_Part04.pdf - Page 15

```markdown
# DRAW: Deep Recurrent Attentive Writer

- An image is generated sequentially instead of being generated at once
- To achieve this, **DRAW** uses a modified Variational Autoencoder (VAE) where both encoder and decoder are RNNs
- Attention mechanism is used (shown as red boxes) to focus and draw on a small area of an image in each time step

![DRAW Process](image_url)

*Vineeth N B. (IIIT-H) §9.4 Other Attention Models*

*Time →*
```

# DL4CV_Week09_Part04.pdf - Page 16

```markdown
# DRAW: Implementation

![DRAW Diagram](image_placeholder.png)

- **Encoder and decoder are RNNs**

- **Image is iteratively drawn on a canvas matrix \( c \) in \( T \) time steps**

    ```math
    Q(z_t|x, z_{1:t-1})
    ```

- **Consists of read and write heads (inspired by NTMs) to enable focusing on a small portion of image**

    - **Encoding (inference)**

        ```math
        h^{enc}_{t-1} \rightarrow \text{encoder RNN} \rightarrow \text{read}
        ```

    - **Decoding (generative model)**

        ```math
        Q(z_{t+1}|x, z_{1:t}) \rightarrow \text{sample} \rightarrow \text{decoder RNN} \rightarrow c_t \rightarrow \text{write}
        ```

        ```math
        h^{dec}_{t-1} \rightarrow \text{decoder RNN} \rightarrow \text{sample}
        ```

        ```math
        Q(z_{t+1}|x, z_{1:t}) \rightarrow \text{sample}
        ```

        ```math
        Q(z_{t+1}|x, z_{1:t}) \rightarrow \text{sample} \rightarrow \text{decoder RNN} \rightarrow c_t \rightarrow \text{write}
        ```

        ```math
        c_t \rightarrow \text{write} \rightarrow c_{t-1} \rightarrow \dots \rightarrow c_1 \rightarrow \boldsymbol{\sigma} \rightarrow P(x|z_{1:T})
        ```

    - **Read and write heads inspired by NTMs**

        ```math
        h^{enc}_{t-1} \rightarrow \text{encoder RNN} \rightarrow \text{read}
        ```

        ```math
        h^{dec}_{t-1} \rightarrow \text{decoder RNN} \rightarrow \text{write}
        ```

    - **Iterative drawing process**

        ```math
        Q(z_t|x, z_{1:t-1}) \rightarrow \text{sample} \rightarrow \text{decoder RNN} \rightarrow c_t \rightarrow \text{write}
        ```

    - **Final image generation**

        ```math
        c_T \rightarrow \boldsymbol{\sigma} \rightarrow P(x|z_{1:T})
        ```

*Source: Vineeth N B (IIIT-H) §9.4 Other Attention Models*

*Slide 12 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 17

```markdown
# DRAW: Encoding

## Encoder takes four inputs:

```plaintext
c_{t-1} -> write -> c_t -> write -> ... -> c_T -> σ -> P(x|z_{1:T})

h_{t-1}^{dec} -> decoder RNN -> z_t
h_{t-1}^{enc} -> encoder RNN -> z_{t+1}
```

- **decoding (generative model)**
  - `Q(z_{t+1}|x, z_{1:t})`
    - `sample`
    - `z_{t+1}`
    - `encoder RNN`
    - `read`
    - `x`

- **encoding (inference)**
  - `Q(z_t|x, z_{1:t-1})`
    - `sample`
    - `z_t`
    - `decoder RNN`
    - `write`
    - `c_{t-1}`
```

![NPTEL](image-placeholder.png)

Vineeth N B. (IIT-H)

§9.4 Other Attention Models

13 / 30
```

# DL4CV_Week09_Part04.pdf - Page 18

```markdown
# DRAW: Encoding

![Diagram](image_url)

- Encoder takes four inputs:
  - input image \( x \)

## Diagram Explanation

- **Encoding (inference):**
  - \( h_{t-1}^{enc} \)
    - **read**
    - encoder RNN
    - \( x \)
  - \( h_{t}^{enc} \)
  - \( Q(z_t | x, z_{1:t-1}) \)
  - **sample**
  - \( z_t \)
  - encoder RNN
  - \( Q(z_{t+1} | x, z_{1:t}) \)
  - **sample**
  - \( z_{t+1} \)
  - decoder RNN
  - \( h_{t}^{dec} \)

- **Decoding (generative model):**
  - \( h_{t}^{dec} \)
  - decoder RNN
  - \( z_{t+1} \)
  - **write**
  - \( c_t \)
  - \( c_{t-1} \)
  - ... \( c_T \)
  - \( \boldsymbol{\sigma} \)
  - \( P(x | z_{1:T}) \)

## Slide Details

- Presented by: Vineeth N B (IIIT-H)
- Section: §9.4 Other Attention Models
- Slide Number: 13 / 30
```

# DL4CV_Week09_Part04.pdf - Page 19



```markdown
# DRAW: Encoding

![Diagram](image.png)

## Encoder takes four inputs:
- Input image \(x\)
- Residual image \(\hat{x}_t\)

```math
c_{t-1} \rightarrow \text{write} \rightarrow c_t \rightarrow \text{write} \rightarrow \ldots \rightarrow c_T
```

### Decoding (generative model)
```math
\overline{H} \rightarrow P(x|z_1:T)
```
### Encoding (inference)

```math
h_{t-1}^{enc} \rightarrow \text{encoder RNN}
```

- \(z_t\) sample
- \(Q(z_t|x, z_1:t-1)\)

#### Decoder RNN
```math
h_{t-1}^{dec} \rightarrow \text{decoder RNN}
```

- \(z_{t+1}\)
- \(Q(z_{t+1}|x, z_1:t)\)

#### Encoder RNN
```math
x \rightarrow \text{read}
```

- \(h_{t-1}^{enc}\)
- \(Q(z_t|x, z_1:t-1)\)

## Vineeth N B. (IIIT-H)

### §9.4 Other Attention Models

---

Page 13 / 30

NPTel
```

# DL4CV_Week09_Part04.pdf - Page 20

 and o

```markdown
# DRAW: Encoding

## Encoder takes four inputs:
- input image \( x \)
- residual image \( \hat{x}_t \)
- encoder hidden state \( h_{t-1}^{enc} \)

### Decoding (generative model)
```{=latex}
\begin{array}{c}
c_{t-1} \xrightarrow{\text{write}} c_t \xrightarrow{\text{write}} \cdots \xrightarrow{\text{write}} c_T \xrightarrow{\sigma} P(x \mid z_1:T)
\end{array}
```
- Decoder RNN
- \( z_t \)
- Sample
```{=latex}
Q(z_t \mid x, z_t: t-1)
```
- Encoder RNN
- Read

### Encoding (inference)
```{=latex}
\begin{array}{c}
h_{t-1}^{enc} \xrightarrow{\text{encoder RNN}} h_t^{enc}
\end{array}
```
- Sample
```{=latex}
Q(z_{t+1} \mid x, z_1:t)
```
- Decoder RNN
- \( z_{t+1} \)
- Read

![NPTEL](https://via.placeholder.com/150)

_Reference: Vineeth N B. (IIT-H)_
```
```

# DL4CV_Week09_Part04.pdf - Page 22

 on the provided image.

```markdown
# DRAW: Encoding

## Encoder takes four inputs:
- input image \( x \)
- residual image \( \hat{x}_t \)
- encoder hidden state \( h_{t-1}^{enc} \)
- decoder hidden state \( h_{t-1}^{dec} \) at previous time step

![Diagram](image_url)

\[ \hat{x}_t = x - \sigma(c_{t-1}) \]

\[ r_t = read(x_t, \hat{x}_t, h_{t-1}^{dec}) \]

\[ h_t^{enc} = RNN^{enc}(h_{t-1}^{enc}, r_t, h_{t-1}^{dec}) \]

where \( \sigma \) is sigmoid function and \( [x, y] \) denotes concatenation of two vectors \( x \) and \( y \).

```
```

# DL4CV_Week09_Part04.pdf - Page 23

:

```markdown
# DRAW: Sampling and Decoding

![DRAW Diagram](image_url)

**Vector** $z_t$ is sampled from latent distribution $Q$ as follows:

$$
z_t \sim Q \left( Z_t \mid h_t^{enc} \right)
$$

(decoding (generative model))

(we will see more of this later)

**Decoder**:

$$
h_t^{dec} = RNN_N^{dec} \left( h_{t-1}^{dec}, z_t \right)
$$

$$
c_t = c_{t-1} + write \left( h_t^{dec} \right)
$$

*Vineeth N B. (IIT-H)*

§9.4 Other Attention Models

14 / 30
```

**Note**: The placeholder `image_url` should be replaced with the actual URL of the image or a suitable filename if the image is to be included directly in the markdown file.

# DL4CV_Week09_Part04.pdf - Page 24

# DRAW: Attention

## Read and write functions used to attend to (focus on) certain regions in image

![Attention Mechanism Diagram](image_url)

- **\( g_Y \)**: A function that operates on the Y-axis.
- **\( \delta \)**: Represents the offset or adjustment applied to the region of interest.
- **\( g_X \)**: A function that operates on the X-axis.

![Examples of Attention Mechanisms](image_url)

- Top-left image: Attention applied to a specific region in the image focusing on the highlighted area.
- Top-right image: A smaller region within the image highlighted for attention.
- Bottom-left image: Another example of focused attention on a different region.
- Bottom-right image: yet another example of attention focusing on a particular area.

---

**Vineeth N B (IIT-H)**

**§9.4. Other Attention Models**

---

*Page 15 / 30*

# DL4CV_Week09_Part04.pdf - Page 25

```markdown
# DRAW: Attention

![DRAW Attention Image](link-to-image)

- **Read and write functions used to attend**
  - Used to (focus on) certain regions in image

- **Since picking one particular region of image (hard attention) is not differentiable**
  - DRAW uses an array of 2D Gaussian filters, which gives out a patch
  - Patch of smoothly varying location and zoom

![Diagram](link-to-diagram)

## Variables

- **gY**
- **gX**
- **δ**

## Visual Representation

![Image 1](link-to-image-1)
![Image 2](link-to-image-2)
![Image 3](link-to-image-3)

## Explanation

- **Read and write functions**: These functions are designed to selectively focus on specific areas within an image.
- **Smooth Attention**: DRAW uses a more flexible approach by employing an array of 2D Gaussian filters. This method allows for a smooth variation in location and zoom, making the process differentiable and more adaptable to different image regions.

---

_Vineeth N B. (IIT-H)_

## Other Attention Models

### Section 9.4

---

_15 / 30_
```

# DL4CV_Week09_Part04.pdf - Page 26

: 

# DRAW: Attention

![DRAW Attention Image](image-url)

- **Read and write functions used to attend to (focus on) certain regions in image**

![Attention Mechanism](image-url)

- Since picking one particular region of image (hard attention) is not differentiable, DRAW uses an array of 2D Gaussian filters, which gives out a patch of smoothly varying location and zoom

![DRAW Mechanism Steps](image-url)

- **Attention mechanism is done in 2 steps:**
  - Predict filter parameters
  - Apply Gaussian filters over image

**Vineeth N B (IIT-H)**

**§9.4 Other Attention Models**

**15 / 30**

# DL4CV_Week09_Part04.pdf - Page 27

```markdown
# DRAW: Attention

![Attention Diagram](image_placeholder)

**Vineeth N B (IIT-H)**

## 9.4 Other Attention Models

The attention mechanism involves the use of Gaussian filters characterized by several key parameters:

1. **Grid center coordinates: \(g_X, g_Y\)**
2. **Stride \(\delta\)**
3. **Variance of Gaussian \(\sigma^2\)**
4. **Intensity \(\gamma\)**

### Diagram Explanation

In the diagram:

- The blue box represents the grid cell coordinates \(g_X\) and \(g_Y\).
- The parameter \(\delta\) denotes the stride of the Gaussian filter.
- The green squares show different Gaussian filters with varying center coordinates and parameters.

Each Gaussian filter is positioned at the grid coordinates \((g_X, g_Y)\) and characterized by the stride \(\delta\), variance \(\sigma^2\), and intensity \(\gamma\).

**Note:** The images next to the grid cells likely represent visualizations of the Gaussian filters at different grid points.

```

# DL4CV_Week09_Part04.pdf - Page 28



```markdown
# DRAW: Attention

![DRAW: Attention Diagram](image_url)

- **N' x N Gaussian filters parametrized by:**
  1. Grid center coordinates: $g_X, g_Y$
  2. Stride $\delta$
  3. Variance of Gaussian $\sigma^2$
  4. Intensity $\gamma$

- **How to learn these parameters?**

### Image Descriptions
- **Top Image**: Green highlighted area with grid coordinates and Gaussian filter.
- **Middle Image**: Another highlighted area with different Gaussian filter parameters.
- **Bottom Image**: Highlighted area with another set of parameters.

**Source**: Vineeth N B (IIIT-H)

**Section**: §9.4. Other Attention Models

**Slide Number**: 16 / 30
```

# DL4CV_Week09_Part04.pdf - Page 29



```markdown
# DRAW: Attention

![DRAW Attention Diagram](../path/to/diagram-image.png)

- **N x N Gaussian filters parametrized by:**
  1. Grid center coordinates: $g_X, g_Y$
  2. Stride $\delta$
  3. Variance of Gaussian $\sigma^2$
  4. Intensity $\gamma$

- **How to learn these parameters?**
  A single fully connected layer transforms decoder output as follows:

  \[
  [\tilde{g}_X, \tilde{g}_Y, \log \tilde{\sigma}^2, \log \tilde{\delta}, \tilde{\gamma}] = W(H^{dec})
  \]

*Vineeth N B. (IIT-H)*
*9.4 Other Attention Models*
*16 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 30

 may take some time due to the complexity of the content.

```markdown
# DRAW: Attention

![DRAW Attention Diagram](image-url)

- **For an $A \times B$ input image, the grid location and stride are determined:**

  \[
  g_X = \frac{A + 1}{2}(g_X + 1)
  \]

  \[
  g_Y = \frac{B + 1}{2}(g_Y + 1)
  \]

  \[
  \delta = \frac{\max(A, B) - 1}{N - 1}
  \]

- These values determine the mean location of the filter at row \(i\) and column \(j\):

  \[
  \mu_X^i = g_X + (i - N/2 - 0.5)\delta
  \]

  \[
  \mu_X^j = g_Y + (j - N/2 - 0.5)\delta
  \]

*Vineeth N B. (IIT-H) §9.4 Other Attention Models 17 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 31

:

# DRAW: Attention

## Using generated attention parameters, horizontal and vertical filter bank matrices \( F_X \) and \( F_Y \) (of dimensions \( N \times A \) and \( N \times B \) respectively) defined as follows.

\[
F_X[i, a] = \frac{1}{Z_X} \exp \left( \frac{(a - \mu_X^i)^2}{2\sigma^2} \right)
\]

\[
F_Y[j, b] = \frac{1}{Z_Y} \exp \left( \frac{(b - \mu_Y^j)^2}{2\sigma^2} \right)
\]

where \((i, j)\) is a point in the attention patch, \((a, b)\) is a point in input image and \(Z_X, Z_Y\) are normalization constants

![Diagram](image_url)

```markdown
Vineeth N B (IIIT-H)
§9.4 Other Attention Models
18 / 30
```

# DL4CV_Week09_Part04.pdf - Page 32



```markdown
# DRAW: Reading and Writing with Attention

- **Read function** implemented as concatenation of Gaussian filtered input image and residual image:

  \[
  \text{read}(x, \hat{x}_t, h_{t-1}^{dec}) = \gamma \left[ F_Y x F_X^T, F_Y \hat{x} F_X^T \right]
  \]

![NPTEL](https://example.com/nptel_logo.png)

*Vineeth N B (IIT-H)*

## 9.4 Other Attention Models

Page 19 / 30
```

Note: The URL for the NPTEL image is a placeholder and should be replaced with the actual URL if available.

# DL4CV_Week09_Part04.pdf - Page 33

```markdown
# DRAW: Reading and Writing with Attention

- **Read function** implemented as concatenation of Gaussian filtered input image and residual image:

  \[
  \text{read}(x, \hat{x}_t, h_{t-1}^{dec}) = \gamma[F_Y x F_X^T, F_Y \hat{x}_t F_X^T]
  \]

- Separate set of attention parameters \(\gamma\), \(\tilde{F}_X\) and \(\tilde{F}_Y\) generated to be used for **write** operation:

  \[
  w_t = W(h_t^{dec})
  \]

  \[
  \text{write}(h_t^{dec}) = \frac{1}{\gamma} \tilde{F}_Y^T w_t \tilde{F}_X
  \]

  where \(w_t\) is writing patch emitted by \(h_t^{dec}\)
```

# DL4CV_Week09_Part04.pdf - Page 34



```markdown
# DRAW: Reading and Writing with Attention

- **Read function** implemented as concatenation of Gaussian filtered input image and residual image:

    ```math
    \text{read}(x, \tilde{x}_t, h_{t-1}^{dec}) = \gamma[F_Y x F_X^T, F_Y \tilde{x} F_X^T]
    ```

- Separate set of attention parameters \(\tilde{\gamma}, \tilde{F}_X\) and \(\tilde{F}_Y\) generated to be used for **write operation**:

    ```math
    w_t = W(h_t^{dec})
    ```

    ```math
    \text{write}(h_t^{dec}) = \frac{1}{\tilde{\gamma}} F_Y^T w_t \tilde{F}_X
    ```

    where \(w_t\) is writing patch emitted by \(h_t^{dec}\)

- Note that order of transposition is reversed and intensity is inverted

_Vineeth N B (IIIT-H)_
_§9.4 Other Attention Models_
_Page 19 / 30_
```

This markdown format ensures the scientific content is accurately and properly presented, with clear and consistent formatting.

# DL4CV_Week09_Part04.pdf - Page 35

```markdown
# DRAW: Deep Recurrent Attentive Writer

- An image is generated sequentially instead of being generated at once

- To achieve this, DRAW uses a modified Variational Autoencoder (VAE) where both encoder and decoder are RNNs

- Attention mechanism is used (shown as red boxes) to focus and draw on a small area of an image in each time step

![DRAW Visualization](image_url)

*Vineeth N B. (IIT-H) §9.4 Other Attention Models*

---

**Notes:**

1. Each point explains a key aspect of the DRAW model.
2. The image is a critical part of the presentation, showing how the model uses attention mechanisms to draw sequentially.
3. The references and other details are included at the bottom.

```

# DL4CV_Week09_Part04.pdf - Page 36

 and use ```markdown``` at the end of the markdown content to indicate completion.

```markdown
# Spatial Transformer Networks

## Image Processing Steps

### Input Images
![Input Images](image1.png)

### Initial Transformations
![Transformations](image2.png)

### Further Processing
![Further Processing](image3.png)

### Output Images
![Output Images](image4.png)

## Key Points
- **CNNs and Spatial Invariance**: 
  - CNNs lack spatial invariance.
  - Max-pooling offers limited spatial invariance only in deeper network layers.

- **Spatial Transformers**:
  - Differentiable modules that can be inserted into a CNN.
  - Provides dynamic spatial invariance.

## References
- Jaderberg et al., Spatial Transformer Networks, NeurlPS 2015
- Vineeth N B (IIT-H)

## Additional Information
- §9.4 Other Attention Models

---

{% endmarkdown %}
```

# DL4CV_Week09_Part04.pdf - Page 37



```markdown
# Spatial Transformer Networks: Example

## Step-by-step Process on MNIST Dataset

### (a) Initial Images
- Images from the MNIST dataset with random distortions applied.

### (b) Transformation Step
- Sample images after applying transformations using Spatial Transformer Networks.

### (c) Transformed Images
- Resultant images after transformations.

### (d) Final Outputs
- Final outputs of the transformed images, maintaining the original digit structure despite transformations.

### Image Sequence

1. **Top Row:**
   - Initial image: ![Initial Image](image1.png)
   - Transformed image: ![Transformed Image](image2.png)
   - Resulting image: ![Resulting Image](image3.png)
   - Final output: **7**

2. **Middle Row:**
   - Initial image: ![Initial Image](image4.png)
   - Transformed image: ![Transformed Image](image5.png)
   - Resulting image: ![Resulting Image](image6.png)
   - Final output: **5**

3. **Bottom Row:**
   - Initial image: ![Initial Image](image7.png)
   - Transformed image: ![Transformed Image](image8.png)
   - Resulting image: ![Resulting Image](image9.png)
   - Final output: **6**

### Additional Notes
- The transformation process involves applying spatial distortions to the initial images to enhance the robustness and generalization capabilities of the neural network.
- The final outputs show the efficacy of Spatial Transformer Networks in maintaining the integrity of the digit structure despite applied distortions.

#### Presented by:
- **Vineeth N B (IIIT-H)**

#### Reference:
- §9.4. Other Attention Models

---

_This markdown representation ensures accurate and structured conversion of the provided scientific slide content into a readable and presentable format._
```

# DL4CV_Week09_Part04.pdf - Page 38



```markdown
# Spatial Transformer Networks: Example

## Steps

### (a) Images from MNIST dataset, on which random distortions are applied

- First row: Initial image of digit `7`
- Second row: Initial image of digit `5`
- Third row: Initial image of digit `6`

### (b) Spatial transformer automatically localizes useful parts of image

- First row: Transformed image of digit `7`
- Second row: Transformed image of digit `5`
- Third row: Transformed image of digit `6`

### (c) Resulting images after transformation

- First row: Processed image of digit `7`
- Second row: Processed image of digit `5`
- Third row: Processed image of digit `6`

### (d) Final output

- First row: Final image of digit `7`
- Second row: Final image of digit `5`
- Third row: Final image of digit `6`

## References

- Vineeth N B. (IIT-H)
- §9.4 Other Attention Models
- Slide 22 / 30
```

```markdown
# Spatial Transformer Networks: Example

## Steps

1. **Initial Images from MNIST dataset with random distortions applied**
    - **First Row**: Initial image of digit `7`
    - **Second Row**: Initial image of digit `5`
    - **Third Row**: Initial image of digit `6`

2. **Spatial Transformer Localizes Useful Parts of Image**
    - **First Row**: Transformed image of digit `7`
    - **Second Row**: Transformed image of digit `5`
    - **Third Row**: Transformed image of digit `6`

3. **Resulting Images After Transformation**
    - **First Row**: Processed image of digit `7`
    - **Second Row**: Processed image of digit `5`
    - **Third Row**: Processed image of digit `6`

4. **Final Output**
    - **First Row**: Final image of digit `7`
    - **Second Row**: Final image of digit `5`
    - **Third Row**: Final image of digit `6`

## References

- Vineeth N B. (IIT-H)
- §9.4 Other Attention Models
- Slide 22 / 30
```

# DL4CV_Week09_Part04.pdf - Page 39

 the OCR result and ensure the scientific integrity of the content. 

---

# Spatial Transformer Networks: Example

## Steps in the Process

1. **Input (a)**: Images from MNIST dataset, on which random distortions are applied
   - Example images: 
      - ![Image 1](data:image/png;base64,...) (7)
      - ![Image 2](data:image/png;base64,...) (5)
      - ![Image 3](data:image/png;base64,...) (6)

2. **Localization (b)**: Spatial transformer automatically localizes useful parts of the image
   - Example output:
      - ![Localization 1](data:image/png;base64,...) (highlighted and localized parts of the image)

3. **Transformation (c)**: Spatial transformer applies suitable transformation to obtain untransformed image
   - Example output:
      - ![Transformation 1](data:image/png;base64,...) (untransformed image)

4. **Result (d)**: Final result after applying transformation
   - Example results:
      - ![Result 1](data:image/png;base64,...) (7)
      - ![Result 2](data:image/png;base64,...) (5)
      - ![Result 3](data:image/png;base64,...) (6)

## Additional Notes

- The images shown are from the MNIST dataset, which is a common dataset used in machine learning for digit recognition.
- The process demonstrates how spatial transformer networks can handle image distortions and ensure accurate localization and transformation leading to correct digit recognition.
- The transformations are essential for improving the performance of machine learning models, especially when dealing with image data.

## References

- Vineeth N B (IIT-H)
- §9.4 Other Attention Models

---

This markdown format ensures that the content from the scientific slide is accurately and clearly presented, preserving formatting and visual elements where possible.

# DL4CV_Week09_Part04.pdf - Page 40

 the raw text as it is.

```markdown
# Spatial Transformer Networks: Example

## Steps:

### (a)
- **Images from MNIST dataset**, on which random distortions are applied

### (b)
- Spatial transformer automatically localizes useful parts of image

### (c)
- Spatial transformer applies suitable transformation to obtain untransformed image

### (d)
- CNN able to classify digits correctly now

## Images:

![Image 1](image1.png)
![Image 2](image2.png)
![Image 3](image3.png)

## Description:

- **Initial Distorted Images:** Displayed in column (a)
- **Localized Images:** Spatial transformer identifies and highlights key parts in column (b)
- **Transformed Images:** Resulting images after transformation in column (c)
- **Final Classified Images:** Correct classification of digits post-transformation in column (d)

## Reference:
- Vineeth N B. (IIT-H)
- §9.4. Other Attention Models
- Slide 22 / 30
```

# DL4CV_Week09_Part04.pdf - Page 41



```markdown
# Spatial Transformer Networks: Architecture

- **Three main parts:**
  1. Localization Network;
  2. Grid Generator;
  3. Sampler

![Spatial Transformer Network Architecture](image_url)

- **Components:**
  - **U**: Input data
  - **Localization Network**: Computes the parameters (θ) for the transformation
  - **Grid Generator**: Generates the grid (G) based on the computed parameters (θ)
  - **Tθ(G)**: Transformed grid
  - **Sampler**: Samples the input data (U) based on the transformed grid (Tθ(G))
  - **V**: Output data

*Source:* Vineeth N B (IIT-H) §9.4 Other Attention Models 23 / 30
```

# DL4CV_Week09_Part04.pdf - Page 42



```markdown
# Spatial Transformer Networks: Architecture

- **Three main parts:**
  - (1) Localization Network;
  - (2) Grid Generator;
  - (3) Sampler

![Spatial Transformer Network Diagram](image.png)

- **Takes an input feature map** \( U \) **and transforms it into an output feature map** \( V \)

_Notes provided in the image:_
- Vineeth N B (IIIT-H)
- §9.4 Other Attention Models
- 23 / 30
```

**Note**: Replace `"image.png"` with the actual image placeholder if needed. Ensure the image is captured accurately in the markdown file. Ensure all scientific terms and symbols are accurately represented.

# DL4CV_Week09_Part04.pdf - Page 43

```markdown
# Spatial Transformer Networks: Architecture

- Three main parts: (1) Localization Network; (2) Grid Generator; (3) Sampler

![Spatial Transformer Network Diagram](image_placeholder.png)

- Takes an input feature map \( U \) and transforms it into an output feature map \( V \)
- The three modules are designed to be differentiable, hence can be inserted in any CNN

*Vineeth N B (IIIT-H)*

*§9.4 Other Attention Models*

*23 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 44

```markdown
# Spatial Transformer Networks: Localization Network

- Takes an input feature map \( U \in \mathbb{R}^{H \times W \times C} \) and outputs \( \theta \), parameters of transformation
  i.e., \( \theta = f_{loc}(U) \)

![Diagram](diagram_placeholder.png)

## Diagram Details

- **Input (\( U \))**: The input feature map \( U \) is fed into the system.
- **Localisation Network**: This network processes the input to generate transformation parameters \( \theta \).
- **Grid Generator**: Uses \( \theta \) to generate a grid \( G \).
- **Sampler**: Applies the transformation \( T_\theta(G) \) to the input \( U \) producing the output \( V \).
- **Output (\( V \))**: The transformed feature map \( V \).

---

**Notes:**
- This diagram illustrates the Localization Network used in Spatial Transformer Networks.
- The localization network determines the parameters of the transformation.
- The grid generator and sampler together apply the transformation to the input feature map.

---

**References:**
- Vineeth N B (IIT-H) §9.4 Other Attention Models
- Slide number: 24 / 30
```


# DL4CV_Week09_Part04.pdf - Page 45



```markdown
# Spatial Transformer Networks: Localization Network

- Takes an input feature map \( U \in \mathbb{R}^{H \times W \times C} \) and outputs \( \theta \), parameters of transformation
  i.e., \( \theta = f_{loc}(U) \).

![Spatial Transformer Diagram](image_url)

- Localization network generally has multiple hidden layers, and an output layer equal to number of transformation parameters; e.g. for an affine transformation, \( \theta \) is 6-dimensional

*Vineeth N B. (IIT-H) §9.4: Other Attention Models 24 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 46

```markdown
# Spatial Transformer Networks: Grid Generator

- We assume output pixels lie on a regular grid of dimensions \(H' \times W' \times C\)

![NPTEL](https://example.com/nptel-logo.png)

*Vineeth N B (IIT-H) §9.4 Other Attention Models*

---

Page 25 / 30
```

Ensure the image linked is correct or replace it with a placeholder if necessary. This markdown format maintains the structure and scientific integrity of the original content.

# DL4CV_Week09_Part04.pdf - Page 47



```markdown
# Spatial Transformer Networks: Grid Generator

- We assume output pixels lie on a regular grid of dimensions \(H' \times W' \times C\)

- For a point \((x_i^s, y_i^s)\) in input grid and a point \((x_i^t, y_i^t)\) in output grid, parametrized transformation defined as follows:

\[
\begin{pmatrix}
x_i^s \\
y_i^s
\end{pmatrix}
=
T_\theta(G_i)
=
\begin{bmatrix}
\theta_{11} & \theta_{12} & \theta_{13} \\
\theta_{21} & \theta_{22} & \theta_{23}
\end{bmatrix}
\begin{pmatrix}
x_i^t \\
y_i^t \\
1
\end{pmatrix}
\]

![NPTEL](https://example.com/image.png)

*Vineeth N B (IIT-H) §9.4 Other Attention Models*
```

**Note**: The placeholder for the image URL should be replaced with the actual URL if available. Ensure that the scientific content is accurately transcribed and formatted.

# DL4CV_Week09_Part04.pdf - Page 48



```markdown
# Spatial Transformer Networks: Grid Generator

- We assume output pixels lie on a regular grid of dimensions $H' \times W' \times C$

- For a point $(x^s_i, y^s_i)$ in input grid and a point $(x^t_i, y^t_i)$ in output grid, parametrized transformation defined as follows:

  \[
  \begin{pmatrix}
  x^s_i \\
  y^s_i
  \end{pmatrix}
  =
  T_\theta(G_i)
  =
  \begin{bmatrix}
  \theta_{11} & \theta_{12} & \theta_{13} \\
  \theta_{21} & \theta_{22} & \theta_{23}
  \end{bmatrix}
  \begin{pmatrix}
  x^t_i \\
  y^t_i \\
  1
  \end{pmatrix}
  \]

- Idea is to now find which points in $U$ to sample, to generate $V$

*Vineeth N B (IIT-H) §9.4 Other Attention Models 25 / 30*
```

Note: The placeholder for the image with the text "NPTEl" needs to be replaced with the actual image or a description if the OCR cannot capture it directly.

# DL4CV_Week09_Part04.pdf - Page 49

 it properly.

```markdown
# Spatial Transformer Networks: Grid Generator Example

- **Figure (a)** shows an identity transformation where each pixel in target \(\bar{V}\) corresponds to same location in source \(\bar{U}\)

![Figure (a)](image_url_a)

- **Figure (b)** shows a parametrized affine transformation which transforms a distorted "9" to a canonical shape of a "9" which a typical CNN is aware of

![Figure (b)](image_url_b)

_Vineeth N B. (IIIT-H)_

_§9.4. Other Attention Models_

_26 / 30_
```
```

# DL4CV_Week09_Part04.pdf - Page 50



```markdown
# Spatial Transformer Networks: Sampler

- We have seen how to generate a sampling grid; how do we make the sampler at each grid point, differentiable?

![NPTEL Logo](image_url)

Vineeth N B (IIT-H) §9.4 Other Attention Models 27 / 30
```

Note: Replace `image_url` with the actual URL or placeholder for the image if available.

# DL4CV_Week09_Part04.pdf - Page 51

 is forbidden to edit or modify the OCR content in any way.

---

# Spatial Transformer Networks: Sampler

- We have seen how to generate a sampling grid; how do we make the sampler at each grid point, differentiable?
- The sampler is formulated as:

![Diagram of Sampling Process](image-url)

**Sampling kernel** is the center of the process, influenced by **kernel parameters**. 

Target feature value at `i` in channel `c`:

\[ V_i^c = \sum_{n} \sum_{m} I_{nm}^{c} k(x_i^s - m; \Phi_x) k(y_i^s - n; \Phi_y) \]

Input feature value at location `(n,m)` in channel `c`:

\[ \forall i \in [1 \dots H'W'] \forall c \in [1 \dots C] \]

Sampling coordinates play a crucial role in this process.

**Note**: Packing all necessary details from the provided image into the markdown.

Vineeth N B (IIT-H) §9.4 Other Attention Models 27 / 30

# DL4CV_Week09_Part04.pdf - Page 52

```markdown
# Spatial Transformer Networks: Sampler

An example of a bilinear sampling kernel:

\[ V_i^c = \sum_{n=1}^{H} \sum_{m=1}^{W} U_{nm}^c \max(0, 1 - |x_i^s - m|) \max(0, 1 - |y_i^s - n|) \]

**How to calculate the partial derivative of this transformation?**
  
![NPTEL](https://example.com/nptel-logo.png)

Vineeth N B. (IIT-H) §9.4. Other Attention Models 28 / 30
```

# DL4CV_Week09_Part04.pdf - Page 53

 on the provided scientific text or slides and convert the extracted content into a detailed markdown format.

```markdown
# Spatial Transformer Networks: Sampler

An example of a bilinear sampling kernel:

$$
V_i^c = \sum_{n}^{H} \sum_{m}^{W} U_{nm}^c \max(0, 1 - |x_i^s - m|) \max(0, 1 - |y_i^s - n|)
$$

How to calculate the partial derivative of this transformation?

$$
\frac{\partial V_i^c}{\partial U_{nm}^c} = \sum_{n}^{H} \sum_{m}^{W} \max(0, 1 - |x_i^s - m|) \max(0, 1 - |y_i^s - n|)
$$

$$
\frac{\partial V_i^c}{\partial x_i^s} = \sum_{n}^{H} \sum_{m}^{W} U_{nm}^c \max(0, 1 - |y_i^s - n|) \begin{cases}
0 & \text{if } |m - x_i^s| \geq 1 \\
1 & \text{if } m \geq x_i^s \\
-1 & \text{if } m < x_i^s
\end{cases}
```

*Vineeth N B (IIT-H) §9.4 Other Attention Models 28 / 30*
```

# DL4CV_Week09_Part04.pdf - Page 54



```markdown
# Homework

## Readings

- [Lilian Weng, Attention? Attention](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
- [Jonathan Hui, Deep Recurrent Attentive Writer](https://arxiv.org/abs/1809.07743)
- [Sik Ho-Tsang, Spatial Transformer Networks Review](https://arxiv.org/abs/1704.02075)

---

**Vineeth N B (IIT-H)**

**9.4 Other Attention Models**

---

*Page 29 of 30*
```

# DL4CV_Week09_Part04.pdf - Page 55

```markdown
# References

- **Alex Graves, Greg Wayne, and Ivo Danihelka.** "Neural Turing Machines". *In: CoRR abs/1410.5401* (2014). arXiv: 1410.5401.

- **Karol Gregor et al.** "DRAW: A Recurrent Neural Network For Image Generation". *In: ed. by Francis Bach and David Blei. Vol. 37. Proceedings of Machine Learning Research*. Lille, France: PMLR, 2015, pp. 1462–1471.

- **Max Jaderberg et al.** "Spatial Transformer Networks". *In: Advances in Neural Information Processing Systems 28*. Ed. by C. Cortes et al. Curran Associates, Inc., 2015, pp. 2017–2025.
```

