# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 1

 is not required.

```markdown
# Deep Learning for Computer Vision

## From Transformers to Vision Transformers

**Vineeth N Balasubramanian**

Department of Computer Science and Engineering
Indian Institute of Technology, Hyderabad

![IIT Hyderabad Logo](image_url)

Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers 1 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 2

 the output to markdown format.

```markdown
# Acknowledgements

- Most of this lecture’s slides are based on Justin Johnson's **Deep Learning for Computer Vision** course from University of Michigan

- Unless explicitly specified, assume that content and figures are either directly taken or adapted from above source

---

_Vineeth N B (IIIT-H)_

_§13.1 From Transformers to Vision Transformers_

_2 / 37_
```

Ensure the markdown file is ready for review and that it adheres to the markdown formatting standards.

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 3



```markdown
# Transformers in NLP

- Transformers\^{1}, have revolutionized Natural Language Processing (NLP) tasks by introducing a novel architecture based on self-attention mechanisms

^{1}Vaswani et al., Attention is All You Need, NeurIPS 2017

Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers 3 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 4



```markdown
# Transformers in NLP

- Transformers\(^{1}\), have revolutionized Natural Language Processing (NLP) tasks by introducing a novel architecture based on self-attention mechanisms

## Sample NLP tasks include:

- Machine Translation
- Language Understanding
- Text Generation
- Question Answering
- Named Entity Recognition

---

\(^{1}\) Vaswani et al., Attention is All You Need, NeurIPS 2017

*Vineeth N B (IIT-H)*

*§13.1 From Transformers to Vision Transformers*

*3 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 5

:

```markdown
# Transformers in NLP

- Transformers<sup>1</sup>, have revolutionized Natural Language Processing (NLP) tasks by introducing a novel architecture based on self-attention mechanisms

- Sample NLP tasks include:
  - Machine Translation
  - Language Understanding
  - Text Generation
  - Question Answering
  - Named Entity Recognition

- BERT [1], RoBERTa [2], DeBERTa [3], T5 [4], GPT v1-4 [5] represent major contributions in transformer-based models developed for these NLP tasks

<sup>1</sup>Vaswani et al., Attention is All You Need, NeurIPS 2017

Vineeth N B (III-T-H) §13.1 From Transformers to Vision Transformers 3 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 6

 it as a markdown snippet.

```markdown
# Transformers in Vision

## Creating the input sequence

- Transformer encoder requires a sequence of vectors as input
- In language applications, this sequence is just the set of (ordered) word embeddings that correspond to the tokens within our input

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers 4 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 7

# Transformers in Vision

## Creating the input sequence

- Transformer encoder requires a sequence of vectors as input
- In language applications, this sequence is just the set of (ordered) word embeddings that correspond to the tokens within our input
- How can we form such a token sequence if our input is an image?
- What if we use each pixel as a token?

*Source: Vineeth N B (IIT-H), §13.1 From Transformers to Vision Transformers*

---

Vineeth N B (IIT-H), §13.1 From Transformers to Vision Transformers, 4 / 37

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 8

```markdown
# Transformers in Vision

## Creating the input sequence

- Transformer encoder requires a sequence of vectors as input
- In language applications, this sequence is just the set of (ordered) word embeddings that correspond to the tokens within our input
- How can we form such a token sequence if our input is an image?
- What if we use each pixel as a token?
- It blows up the memory usage and computations! For an image of size $3 \times 224 \times 224$, we need to store $(3 \times 224 \times 224)^2 = 22$ billion parameters for self-attention calculation.

*Vineeth N B (IIT-H)*

*§13.1 From Transformers to Vision Transformers*

*4 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 9

 special characters or symbols (e.g., Greek letters, mathematical operators) are represented accurately. Multilingual content should be identified appropriately in the output.

```markdown
# Vision Transformer<sup>2</sup>

Consider an **RGB image** of size

$$
224 \times 224
$$

![Cat Image](image_url)

---

<sup>2</sup> Dosovitskiy et al, *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*, ICLR 2021

Vineeth N B (IIT-H)

*13.1 From Transformers to Vision Transformers*

*Page 5 of 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 10

:

```markdown
# Vision Transformer

![Image of a cat divided into smaller images](image-url)

- **Divide the image into 196 (14 x 14) small images of size 16 x 16**

Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers

6 / 37
```

Note: Replace `image-url` with the actual URL or file path of the image. The placeholder is used because the OCR process cannot directly capture the image content.

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 11

```markdown
# Vision Transformer

![Vision Transformer](image_url)

N input patches, each of shape 3x16x16

---

Vineeth N B (IIT-H)

From: Transformers to Vision Transformers

Page 7 of 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 12

 and ensure the scientific integrity of the content.

```md
# Vision Transformer

![Vision Transformer Diagram](image-url)

## Linear projection to D-dimensional vector

**N input patches, each of shape 3x16x16**

Vineeth N B (IIIT-H)

§13.1 From Transformers to Vision Transformers

8 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 13

```markdown
# Vision Transformer

## Steps in Vision Transformer

1. **Input Patches:**
   - The input consists of `N` patches, each of shape `3x16x16`.
   - These patches are extracted from the input image.

2. **Linear Projection:**
   - Each patch is projected to a `D`-dimensional vector using a linear projection.

3. **Add Positional Embedding:**
   - A learned `D`-dim vector (positional embedding) is added to each patch to incorporate positional information.

### Diagram Explanation

- **Input Patches:** 
  ![Input Patches](image-placeholder)

  These are the `N` patches of shape `3x16x16` extracted from the input image.

- **Linear Projection:**
  ![Linear Projection](image-placeholder)

  Each patch is projected to a `D`-dimensional vector using linear projection.

- **Add Positional Embedding:**
  ![Add Positional Embedding](image-placeholder)

  A learned `D`-dimensional positional embedding vector is added to each of the projected vectors.

### Summary

The Vision Transformer process begins with dividing the input image into smaller patches, projecting these patches into a `D`-dimensional space, and then adding learned positional embeddings to incorporate spatial information.

---

_Vineeth N B (IIT-H)_

_§13.1 From Transformers to Vision Transformers_

_Page 9 / 37_
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 14

```markdown
# Vision Transformer

## Output vectors

- Exact same as NLP Transformer!

## Transformer

### Add positional embedding: learned D-dimensional vector

### Linear projection to D-dimensional vector

### N input patches, each of shape 3x16x16

![Vision Transformer Diagram](image_url_here)

*Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers*

---

10 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 15

 is not required for this task.

```markdown
# Vision Transformer

![Vision Transformer Diagram](image_url)

---

**Transformer**

- **Special extra input**: classification token (D dims, learned)

  ![Special extra input](image_url)

  - Vineeth N B (IIT-H)
  - §13.1 From Transformers to Vision Transformers
  - Slide 11 / 37

---

- **Vision Transformer (ViT) Architecture**:
  - Uses Transformer model architecture adapted for image data.
  - Image is split into patches and flattened.
  - Each patch is converted into a linear embedding and added with positional embeddings.
  - Special classification token added to sequence.
  - Passed through Transformer layers.

- **Key Components**:
  - **Patch Embedding**: Splits image into fixed-size patches.
  - **Positional Embeddings**: Adds positional information to each patch.
  - **Classification Token**: Special token to guide classification.
  - **Transformer Encoder**: Transformer layers process input sequence.

- **Applications**:
  - Image classification.
  - Object detection.

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 16

```markdown
# Vision Transformer

## Overview

The Vision Transformer utilizes a Transformer architecture designed specifically for image data processing. The process involves several key components and steps:

1. **Input Images**: A series of images are provided as input. These images are represented by small colored blocks at the bottom of the diagram.
   
2. **Patch Embeddings and Classification Token**: 
   - The input images are divided into smaller patches.
   - Each patch is converted into an embedding using linear layers.
   - A special extra input called the **classification token** is added. This token is a learned embedding with dimensions D.

3. **Position Embeddings**: Positional information is added to the patch embeddings to retain spatial information. This is crucial as the Transformer architecture is permutation invariant.

4. **Transformer Processing**: The modified patch embeddings, along with the classification token, are passed through a series of Transformer layers. This process involves self-attention mechanisms that allow the model to capture complex relationships in the image data.

5. **Output**: 
   - The final output from the Transformer is represented by yellow blocks.
   - A linear projection is applied to transform the output into a C-dimensional vector of predicted class scores.

### Diagram Components

- **Input Images**: Shown as small images at the bottom.
- **Patch Embeddings**: Represented by small gray blocks (orange for the classification token).
- **Transformer Layer**: The central green block labeled "Transformer".
- **Output**: Yellow blocks after the Transformer layer, indicating the predicted class scores.

### References

- **Author**: Vineeth N B (IIT-H)
- **Publication**: From Transformers to Vision Transformers
- **Page**: 12 / 37

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 17

```markdown
# Vision Transformer: Overall Architecture

## Vision Transformer (ViT)

### Components

- **Class (e.g., Bird, Ball, Car)**
  - **MLP Head**

- **Transformer Encoder**
  - Input: Embedded Patches
  - Output: Processed Embeddings

### Process Flow

1. **Input Image**
    - Split into Patches
    - Add Position Embedding
    - Optional: Add Extra Learnable `[class]` Embedding

2. **Linear Projection of Flattened Patches**
    - Each patch is linearly projected and combined with positional embeddings.

3. **Transformer Encoder**
    - **Embedded Patches** are processed through a series of layers.
    - **Multi-Head Attention** mechanism processes the embedded patches.
    - **Norm** and **MLP** layers are applied.
    - The process repeats `L ×` times.

### Transformer Encoder Detailed Components

- **Embedded Patches**
    - Input to the Transformer Encoder

- **Multi-Head Attention**
    - Processes the patches in parallel.

- **Norm** (Normalization)
    - Applied after Multi-Head Attention.

- **MLP**
    - Multi-Layer Perceptron for further processing.

- **Addition and Normalization**
    - Intermediate results are added and normalized.

### Position Embedding

- **Patch + Position Embedding**
    - Each patch is augmented with positional information.

### Diagram Details
![Diagram](image_placeholder.png)

*Source: Vineeth N B (IIIT-H), §13.1 From Transformers to Vision Transformers*

---

**Note**: This markdown format strictly adheres to the guidelines for scientific text, ensuring the accuracy and proper representation of all key elements from the provided image.
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 18

```markdown
# Vision Transformer: Overall Architecture

![Transformer Architecture Diagram](image_url)

- **Class**
  - Bird
  - Ball
  - Car
  - ...

- **MLP Head**

  ![MLP Head](image_url)

- **Transformer Encoder**

  ![Transformer Encoder](image_url)

- **Patch + Position Embedding**
  - Extra learnable [class] embedding

  ![Patch + Position Embedding](image_url)

- **Using a pixel as a token, for an image of size \(3 \times 224 \times 224\), we had \((3 \times 224 \times 224)^2 = 22\) billion parameters for self-attention calculation. Now?**

  ![Equation](image_url)

  Vineeth N B (IIT-H)
  §13.1 From Transformers to Vision Transformers
  14 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 19

```markdown
# Vision Transformer: Overall Architecture

![Overall Architecture Diagram](image_url)

**Class**
- Bird
- Ball
- Car
- ...

**MLP Head**

## Transformer Encoder

- **Patch + Position Embedding**
    - Extra learnable `[class]` embedding

- **Linear Projection of Flattened Patches**

![Linear Projection of Flattened Patches](image_url)

- Using a pixel as a token, for an image of size `3 x 224 x 224`, we had `(3 x 224 x 224)^2 = 22 billion parameters for self-attention calculation. Now?

- `(14 x 14)^2 = 38,416 parameters (approx 150 KB) per transformer layer!`

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers* 14 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 20

```markdown
# Vision Transformer: Overall Architecture

## Overview

![Vision Transformer Architecture](image-url)

- **Class**: Bird, Ball, Car, ...
- **MLP Head**: Multi-layer Perceptron for classification.
- **Transformer Encoder**: Core component for processing the data.

### Data Processing Pipeline

1. **Input Images**: 
   - Example images of buildings and landscapes.

2. **Patch + Position Embedding**:
   - Patches of the image are extracted and embedded with positional information.
   - An extra learnable parameter `[class]` embedding is used.

3. **Linear Projection of Flattened Patches**:
   - Each patch is linearly projected to a higher-dimensional space.

4. **Patch Embeddings**:
   - Embeddings are passed to the Transformer Encoder.

5. **Visualization**:
   - Image patches are visualized to show the intermediate steps.

### Key Question

- **Do we have a vision architecture without convolution?**

### Reference
- **Vineeth N B (IIT-H)**
- **§13.1 From Transformers to Vision Transformers**

---

*Page 15 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 21

 the provided image.

```markdown
# Vision Transformer: Overall Architecture

![Vision Transformer Diagram](image_url)

- **Class**:
  - Bird
  - Ball
  - Car
  - ...

  - **MLP Head**

    ```
    Transformer Encoder
    ```

    - **Patch + Position Embedding**:
      - Extra learnable `[class]` embedding

- **Do we have a vision architecture without convolution?!**
  - **No!** The linear projection from patches is a convolution (with stride being patch size)

  - **Linear Projection of Flattened Patches**

    ```
    Patch + Position Embedding
    ```

    - ![Patch Images](image_url)

    ```
    Linear Projection of Flattened Patches
    ```

    - **Transformer Encoder**

    - **MLP Head**

    ```
    Class: Bird, Ball, Car, ...
    ```

**Vineeth N B. (IIIT-H)**

**§13.1 From Transformers to Vision Transformers**

---

15 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 22

 special attention to the figure captions or descriptions of images.

```markdown
# Vision Transformer: Overall Architecture

![Overall Architecture of Vision Transformer](image_url)

- **Class**:
  - Bird
  - Ball
  - Car
  - ...

- **MLP Head**

  Do we have a vision architecture without convolution?!
  - **No!** The linear projection from patches is a convolution (with stride being patch size)
  - Also, MLPs in Transformer are stacks of 1 × 1 convolution

## Patch + Position Embedding
- *Extra learnable [class] embedding*

## Linear Projection of Flattened Patches

## Transformer Encoder

*Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers 15 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 23

```markdown
# Vision Transformer (ViT) vs ResNets<sup>3</sup>

![Graph](image_url)

## Vision Transformer (ViT) vs ResNets

### Pre-training Dataset vs ImageNet Top1 Accuracy [%]

- **Pre-training datasets**: 
  - ImageNet
  - ImageNet-21k
  - JFT-300M

- **ImageNet Top1 Accuracy [%]:**
  - Y-axis: Accuracy percentage up to 90%.

### Models Compared:
- **ResNet-152x4**: Represented by a grey bar.
- **ViT-B/32**: Represented by a green dot.
- **ViT-L/32**: Represented by a light blue dot.
- **ViT-B/16**: Represented by a pink dot.
- **ViT-L/16**: Represented by a blue dot.
- **ViT-H/14**: Represented by an orange dot.

### Key:
- **Legend:**
  - B = Base
  - L = Large
  - H = Huge
- **Patch size:**
  - `/32`, `/16`, `/14` is patch size; smaller patch size is a bigger model (more patches).

### Sources:
- Dosovitskiy et al., *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*, ICLR 2021
- Vineeth N B (IIT-H)
- §13.1 From Transformers to Vision Transformers

---

<sup>3</sup> Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 24

```markdown
# Vision Transformer (ViT) vs ResNets

## Recall: ImageNet dataset has 1k categories, 1.2M images

When trained on ImageNet, ViT models perform worse than ResNets

![Graph](image_url)

**Legend:**
- ResNets
- ViT-B/32
- ViT-L/32
- ViT-B/16
- ViT-L/16
- ViT-H/14

**Patch Size Indicators:**
- `/32`, `/16`, `/14` is patch size; smaller patch size is a bigger model (more patches)

**Pre-training datasets:**
- ImageNet
- ImageNet-21k
- JFT-300M

**Accuracy:**
- Y-axis: ImageNet Top1 Accuracy [%]
- X-axis: Pre-training dataset

**Source Attribution:**
- Dosovitskiy et al, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021
- Vineeth N B (III-T-H)
- §13.1: From Transformers to Vision Transformers
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 25



```markdown
# Vision Transformer (ViT) vs ResNets

## ImageNet-21k has 14M images with 21k categories

If you pretrain on ImageNet-21k and fine-tune on ImageNet, ViT does better: big ViTs match big ResNets

![Graph comparing ViT and ResNets](graph.png)

- **Pre-training datasets:** ImageNet, ImageNet-21k, JFT-300M
- **Legend:**
  - **ResNets**
  - **ViT-L/32**: ViT Large with patch size 32
  - **ViT-B/32**: ViT Base with patch size 32
  - **ViT-L/16**: ViT Large with patch size 16
  - **ViT-B/16**: ViT Base with patch size 16
  - **ViT-H/14**: ViT Huge with patch size 14
  - **B = Base**
  - **L = Large**
  - **H = Huge**
  - **Patch size:** smaller patch size is a bigger model (more patches)

## References

- Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021
- Vineeth N B (IIT-H)
- §13.1 From Transformers to Vision Transformers

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 26

```markdown
# Vision Transformer (ViT) vs ResNets

**JFT-300M is an internal Google dataset with 300M labeled images**

If you pretrain on JFT and finetune on ImageNet, large ViTs outperform large ResNets

---

![Graph comparing ViT and ResNet performance](image-url)

**ImageNet Top1 Accuracy [%]**

| Pre-training Dataset | ResNets | ViT-B/32 | ViT-L/32 | ViT-B/16 | ViT-L/16 | ViT-H/14 |
|----------------------|---------|----------|----------|----------|----------|----------|
| ImageNet             | ~75     | ~77      | ~78      | ~78      | ~80      | ~82      |
| ImageNet-21k          | ~80     | ~83      | ~84      | ~84      | ~86      | ~88      |
| JFT-300M              | ~85     | ~87      | ~88      | ~88      | ~90      | ~91      |

**Legend:**

- B = Base
- L = Large
- H = Huge

/32, /16, /14 is patch size; smaller patch size is a bigger model (more patches)

---

*Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021*

*Vineeth N B (III-T-H)*

*§13.1 From Transformers to Vision Transformers*

---

**Page 19 / 37**

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 27

```markdown
# Vision Transformer (ViT) vs ResNets

**JFT-300M is an internal Google dataset with 300M labeled images**

If you pretrain on JFT and finetune on ImageNet, large ViTs outperform large ResNets

![Graph](image_url)

**ViT: 2.5k TPU-v3 core days of training**

**ResNet: 9.9k TPU-v3 core days of training**

**ViTs make more efficient use of GPU/TPU hardware (matrix multiply is more hardware-friendly than conv)**

## Legend

- **ResNets**
- **ViT-L/32**
- **ViT-B/32**
- **ViT-B/16**
- **ViT-L/16**
- **ViT-H/14**

## Pre-training Dataset

- **ImageNet**
- **ImageNet-21k**
- **JFT-300M**

## Accuracy

- **ImageNet Top1 Accuracy (%)**

## Citations

7. Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, ICLR 2021

Vineeth N B (IIT-M)

§13.1 From Transformers to Vision Transformers

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 28

:

# Improving Vision Transformer Performance

## Regularization and Augmentation

- **Regularization**

  - Weight Decay
  - Stochastic Depth
  - Dropout (in FFN layers of Transformer)

- **Augmentation**

  - MixUp
  - RandAugment

---

8Steiner et al, How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, TMLR 2022

Vineeth N B (IIT-H)

§13.1 From Transformers to Vision Transformers

---

21 / 37

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 29

.

```markdown
# Improving Vision Transformer Performance

## ViT with Distillation: Data-efficient image Transformers (DeiT)

- **From a trained teacher model (large CNN), train student model (ViT) using standard cross-entropy loss AND KL Divergence loss with teacher outputs**

![Transformer Model Diagram](image_url)

**Top-1 accuracy increases from 77.9 to 83.4 (and even more with higher training epochs and resolution)**

*Touvron et al, Training data-efficient image transformers distillation through attention, ICML 2021*

*Vineeth N B (IIT-H)*

*§13.1 From Transformers to Vision Transformers*

---

22 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 30

```markdown
# ViT vs CNN

## Hierarchical Architecture in CNNs

In most CNNs (including ResNets), resolution decreases and the number of channels increases as we go deep into the network (Hierarchical architecture).

This is useful since objects can occur in multiple scales.

### Stages

#### Stage 3:
- **Resolution**: 256 x 14 x 14

#### Stage 2:
- **Resolution**: 128 x 28 x 28

#### Stage 1:
- **Resolution**: 64 x 56 x 56

#### Input:
- **Resolution**: 3 x 224 x 224

---

*Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers*

---

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 31

```markdown
# ViT vs CNN

![ViT vs CNN Diagram](image_url_if_available)

- **Input**:
  - 3 x 224 x 224

- **1st block**:
  - 768 x 14 x 14
  - **Layer Normalization**
  - **Self Attention**
  - **Layer Normalization**
  - **MLP (Multi-Layer Perceptron)**

- **2nd block**:
  - 768 x 14 x 14
  - **Layer Normalization**
  - **Self Attention**
  - **Layer Normalization**
  - **MLP (Multi-Layer Perceptron)**

- **3rd block**:
  - 768 x 14 x 14
  - **Layer Normalization**
  - **Self Attention**
  - **Layer Normalization**
  - **MLP (Multi-Layer Perceptron)**

**In a ViT, all blocks have the same resolution and number of channels (isotropic architecture).**

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*

*24 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 32

```markdown
# ViT vs CNN

![ViT vs CNN Diagram](https://via.placeholder.com/150)

## Diagram Explanation

### 1st block:
- **Input:** 3 x 224 x 224
- **Layer Normalization**
- **Self-Attention**
  - **Multi-Head Attention (MH-P, MH-Q, MH-K, MH-V)**
- **Layer Normalization**
- **768 x 14 x 14**

### 2nd block:
- **Layer Normalization**
- **Self-Attention**
  - **Multi-Head Attention (MH-P, MH-Q, MH-K, MH-V)**
- **Layer Normalization**
- **768 x 14 x 14**

### 3rd block:
- **Layer Normalization**
- **Self-Attention**
  - **Multi-Head Attention (MH-P, MH-Q, MH-K, MH-V)**
- **Layer Normalization**
- **768 x 14 x 14**

## In a ViT, all blocks have the same resolution and number of channels (Isotropic architecture)

### Can we build a hierarchical ViT model?

---

**Vineeth N B (IIT-H)**

*§13.1 From Transformers to Vision Transformers*

---

Page 24 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 33



```markdown

# Swin Transformer<sup>10</sup>

![Swin Transformer](https://example.com/your-image-file.png)

- **Images**: 3 × H × W
  - **Patch Partition**
  - **Linear Embedding**
  - **Swin Transformer Block**
    - ×2
    - Divide image into 4x4 patches and project to C dimensions

Stage 1: C × H/4 × W/4

---

<sup>10</sup> Liu et al., Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, CVPR 2021

Vineeth N B (IIT-H)

§13.1 From Transformers to Vision Transformers

```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 34

```markdown
# Swin Transformer

## Patch Partition:

- **Break image into 3x4x4 patches (48 dim)**: 
    - **Input**: \(3 \times H \times W\)
    - **Output**: \(48 \times \frac{H}{4} \times \frac{W}{4}\)

## Linear Embedding:

- **Project linearly to C dimensions**: 
    - **Input**: \(48 \times \frac{H}{4} \times \frac{W}{4}\)
    - **Output**: \(C \times \frac{H}{4} \times \frac{W}{4}\)

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*

![](image-placeholder)

26 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 35

```markdown
# Swin Transformer

![Swin Transformer Diagram](image-url)

## Swin Transformer Overview

### Input
- **Images**: 3 × H × W

### Stage 1
1. **Patch Partition**: Divide the image into 4 × 4 patches.
   - Output: H/4 × W/4 × 3C
2. **Linear Embedding**: Project patches to C dimensions.
   - Output: C × H/4 × W/4
3. **Swin Transformer Block x2**: Process the patches through two Swin Transformer Blocks.
   - Output: C × H/4 × W/4

### Patch Merging
- Merge 2 × 2 neighborhoods.
- Output: 2C × H/8 × W/8

### Stage 2
1. **Swin Transformer Block x2**: Process the merged patches through two Swin Transformer Blocks.
   - Output: 2C × H/8 × W/8

### Summary
- **Stage 1**: Divide the image into 4 × 4 patches and project to C dimensions.
- **Patch Merging**: Merge 2 × 2 neighborhoods; now patches are (effectively) 8 × 8.

_Referenced from: Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers_

Page 27 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 36

 special characters or symbols are represented accurately.

```markdown
# Swin Transformer

## Patch Merging:

![Patch Merging Diagram](image_url)

- **Initial Input (leftmost block)**:
  - Dimensions: H/4 x W/4 x C
  - Structure: 2x2 grid highlighted
 
- **Merge 2x2 neighborhoods**:
  - Operation: Combine adjacent 2x2 patches
  - Result: Dimensions change to H/8 x W/8 x 4C

- **Project linearly to 2C dimensions**:
  - Operation: Linear projection to reduce the number of channels
  - Result: Dimensions change to H/8 x W/8 x 2C

**Source Attribution**:
- Vineeth N B (IIIT-H)
- §13.1 From Transformers to Vision Transformers
- Slide number: 28 / 37
```

Note: Replace `[image_url]` with the appropriate URL or placeholder for the image if available.

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 37

```markdown
# Swin Transformer

![Swin Transformer](image_url)

## Overview

### Input
- **Images**: 
  - Dimensions: \(3 \times H \times W\)

### Process Stages

#### Stage 1
- **Patch Partitioning**: 
  - Divide image into 4x4 patches.
  - Project to C dimensions.
- **Linear Embedding**: 
  - Resulting dimensions: \(C \times \frac{H}{4} \times \frac{W}{4}\)
- **Swin Transformer Block**: 
  - Number of blocks: 2
  
#### Stage 2
- **Patch Merging**: 
  - Merge 2x2 neighborhoods.
  - Resulting patches: effectively 8x8.
- **Swin Transformer Block**: 
  - Number of blocks: 2
  
#### Stage 3
- **Patch Merging**: 
  - Merge 2x2 neighborhoods.
  - Resulting patches: effectively 16x16.
- **Swin Transformer Block**: 
  - Number of blocks: 6
  
### Output Dimensions
- **Stage 1**: \(C \times \frac{H}{4} \times \frac{W}{4}\)
- **Stage 2**: \(2C \times \frac{H}{8} \times \frac{W}{8}\)
- **Stage 3**: \(4C \times \frac{H}{16} \times \frac{W}{16}\)

## References
- Vineeth N B (IIIT-H)
- §13.1 From Transformers to Vision Transformers

---

Page Number: 29 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 38

```markdown
# Swin Transformer

## Overview

The Swin Transformer is a hierarchical vision transformer designed to capture both local and global features in images. The architecture involves multiple stages, each consisting of patch merging and Swin Transformer blocks.

## Architecture

1. **Input Image**:
   - Size: \(3 \times H \times W\)
   
2. **Stage 1**:
   - **Patch Partitioning**: Divide the image into \(4 \times 4\) patches and project to \(C\) dimensions.
   - **Linear Embedding**: Linear transformation to map patches to \(C\) dimensions.
   - **Swin Transformer Block (x2)**: Applies the Swin Transformer to the embedded patches.

3. **Stage 2**:
   - **Patch Merging**: Merge \(2 \times 2\) neighborhoods, effectively increasing the patch size to \(8 \times 8\).
   - **Swin Transformer Block (x2)**: Applies the Swin Transformer to the merged patches.

4. **Stage 3**:
   - **Patch Merging**: Merge \(2 \times 2\) neighborhoods, effectively increasing the patch size to \(16 \times 16\).
   - **Swin Transformer Block (x6)**: Applies the Swin Transformer to the merged patches.

5. **Stage 4**:
   - **Patch Merging**: Merge \(2 \times 2\) neighborhoods, effectively increasing the patch size to \(32 \times 32\).
   - **Swin Transformer Block (x2)**: Applies the Swin Transformer to the merged patches.

## Detailed Breakdown

- **Patch Partitioning**: The input image is divided into \(4 \times 4\) patches and projected to \(C\) dimensions.
- **Linear Embedding**: Each patch is linearly embedded into \(C\) dimensions.
- **Swin Transformer Block**: The block consists of multiple layers that process the patches hierarchically.
- **Patch Merging**: Merge neighboring patches to increase their resolution and reduce input dimensions.

## Output Dimensions
- **Stage 1**: \(C \times \frac{H}{4} \times \frac{W}{4}\)
- **Stage 2**: \(2C \times \frac{H}{8} \times \frac{W}{8}\)
- **Stage 3**: \(4C \times \frac{H}{16} \times \frac{W}{16}\)
- **Stage 4**: \(8C \times \frac{H}{32} \times \frac{W}{32}\)

![Diagram of Swin Transformer](image_url_placeholder)

## Source
- Vineeth N B (IIIT-H)
- §13.1 From Transformers to Vision Transformers

---

Refer to the original source for more detailed information and additional context.
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 39

:

```markdown
# Swin Transformer

## Problem

**With \(H \times W\) grid of tokens, attention matrices are of size \(H^2W^2\), which is quadratic in image size**

---

*Source: Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 40



# Swin Transformer

### Problem:

With `H × W` grid of tokens, attention matrices are of size `H^2W^2`, which is quadratic in image size

### Solution:

**Window Attention** - Instead of allowing each token to attend to all other tokens in the image, divide image into windows of `M × M` tokens and compute attention within each window. Now the attention matrices are of size `M^2HW`, which is linear in image size for a fixed M. Swin uses M=7 throughout the network.

Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers 31 / 37

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 41

```markdown
# Swin Transformer

**Problem:** With window attention, there is no communication between windows, restricting the flow of information across different regions of the image.

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*

*Page 32 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 42

 OCR result:

# Swin Transformer

## Problem:

**With window attention, there is no communication between windows, restricting the flow of information across different regions of the image.**

## Solution:

**Shifted Window Attention - Shift the attention windows progressively across the image in a hierarchical manner, enabling interactions between patches across different levels of abstraction.**

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*

---

Here is the extracted and formatted text as a markdown file:

```markdown
# Swin Transformer

## Problem:

**With window attention, there is no communication between windows, restricting the flow of information across different regions of the image.**

## Solution:

**Shifted Window Attention - Shift the attention windows progressively across the image in a hierarchical manner, enabling interactions between patches across different levels of abstraction.**

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 43

: - *Ensure that all the extracted text is properly formatted and aligned to the instructions.*

# Swin Transformer

## Problem:
With window attention, there is no communication between windows, restricting the flow of information across different regions of the image

## Solution:
**Shifted Window Attention** - Shift the attention windows progressively across the image in a hierarchical manner, enabling interactions between patches across different levels of abstraction.

This allows for capturing long-range dependencies while maintaining computational tractability, which is crucial for applications such as image classification, object detection, and semantic segmentation.

*Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers*

*Slide: 32 / 37*

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 44

 will be converted to `math` format.

# Swin Transformer: Shifted Window Attention

## Layer l

![Layer l](data:image/png;base64,...) 

## Layer l+1

![Layer l+1](data:image/png;base64,...) 

- **A local window to perform self-attention**
  ![Local window](data:image/png;base64,...) 

- **A patch**
  ![Patch](data:image/png;base64,...) 

*Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers 33 / 37*

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 45

```markdown
# Swin Transformer: Architecture Details

**Vineeth N B (IIIT-H) §13.1 From Transformers to Vision Transformers**

## Key Components

- **LN**: Layer Normalization
- **W-MSA, SW-MSA**: Windowed and Shifted Windowed configurations of self-attention
- **MLP**: Multi Layer Perceptron

## Diagram Explanation

```markdown
```
![Swin Transformer Architecture](image-url)

## Flow of Data

1. **Input**: z^i
2. **W-MSA**: Windowed Multi-Head Self-Attention
3. **LN**: Layer Normalization
4. **Residual Connection**: z^i + W-MSA
5. **MLP**: Multi Layer Perceptron
6. **LN**: Layer Normalization
7. **Residual Connection**: z^{i+1} + MLP
8. **SW-MSA**: Shifted Windowed Multi-Head Self-Attention
9. **LN**: Layer Normalization
10. **Residual Connection**: z^{i+1} + SW-MSA
11. **MLP**: Multi Layer Perceptron
12. **LN**: Layer Normalization
13. **Residual Connection**: z^{i+2} + MLP

## Mathematical Representation

```math
z^{i+1} = \text{W-MSA}(\text{LN}(z^i)) + z^i
z^{i+2} = \text{MLP}(\text{LN}(z^{i+1})) + z^{i+1}
z^{i+3} = \text{SW-MSA}(\text{LN}(z^{i+2})) + z^{i+2}
z^{i+4} = \text{MLP}(\text{LN}(z^{i+3})) + z^{i+3}
```

This architecture efficiently handles large-scale image data by leveraging localized attention mechanisms and hierarchical feature extraction.
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 46

```markdown
# Swin Transformer: Architecture Details

![Swin Transformer Architecture](image-url)

- **LN**: Layer Normalization
- **W-MSA, SW-MSA**: Windowed and Shifted Windowed configurations of self-attention
- **MLP**: Multi Layer Perceptron

**What about positional embedding here?**

*Vineeth N B (IIT-H)*

*§13.1 From Transformers to Vision Transformers*

*34 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 47

```markdown
# Swin Transformer: Architecture Details

## Components

- **LN**: Layer Normalization
- **W-MSA, SW-MSA**: Windowed and Shifted Windowed configurations of self-attention
- **MLP**: Multi Layer Perceptron

## Positional Embedding

What about positional embedding here?

**Swin does not use positional embedding!** Instead, it encodes relative position between patches when computing attention using learned biases \((B \in \mathbb{R}^{M^2 \times M^2})\). Attention with relative bias defined as:

\[ \hat{A} = \text{Softmax} \left( \frac{QK^T}{\sqrt{D}} + B \right) V \]

### Diagram
![Diagram Placeholder](diagram-url)

### References

- Vineeth N B. (IIIT-H)
- §13.1 From Transformers to Vision Transformers
- Page 34 / 37
```

This markdown format maintains the structure and formatting of the original content while ensuring the scientific terms and symbols are accurately represented.

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 48

```markdown
# Swin Transformer: Speed vs Accuracy Comparison

**Legend:**
- RegNetY (blue)
- EffNet (orange)
- ViT+Distillation (DeiT) (gray)
- Swin (yellow)

## Accuracy (ImageNet Top1) vs Speed (ms/image on V100)

![Graph Image](image_url)

**Graph Details:**
- The x-axis represents speed in milliseconds per image on a V100 GPU.
- The y-axis represents accuracy as a percentage on the ImageNet Top1 validation set.
- Four models are compared: RegNetY, EffNet, ViT+Distillation (DeiT), and Swin.

### Observations:
- **RegNetY**: Shows a rapid increase in accuracy from around 0 ms to approximately 2 ms/image, reaching an accuracy of about 81.5%.
- **EffNet**: Starts at a similar point to RegNetY, showing a steady increase in accuracy, reaching around 84% at about 2 ms/image and maintaining this accuracy level up to 18 ms/image.
- **ViT+Distillation (DeiT)**: Begins with lower accuracy around 0 ms and rises steadily, reaching approximately 83% at about 12 ms/image.
- **Swin**: Starts off with high accuracy at around 0 ms, quickly reaching around 84% accuracy at 2 ms/image, and maintaining this level up to the highest speeds shown (around 18 ms/image).

### Conclusion:
- **Swin** and **EffNet** models achieve the highest accuracy at their respective speeds.
- **RegNetY** and **ViT+Distillation (DeiT)** models show a good balance of speed and accuracy but do not reach the levels of the top two models.

**Source:**
- Vineeth N B (IIT-H)
- Section 13.1 from "Transformers to Vision Transformers"

*Figure 35 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 49

```markdown
# Task-specific Vision Transformers

- Swin Transformer serves as a general-purpose backbone for diverse vision tasks, including **Image Classification**, **Object Detection**, and **Semantic Segmentation**

---

## References

1. Carion et al., "End-to-End Object Detection with Transformers", *ECCV 2020*
2. Kirillov, Alexander, et al. "Segment anything", *ICCV 2023*
3. Bianchi, Federico, et al. "Contrastive language-image pre-training for the italian language.", *arXiv 2021* by Open-AI
4. Liang, Jingyun, et al. "Swimir: Image restoration using swin transformer.", *ICCV 2021*

---

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*

---

Page 36 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 50

```markdown
# Task-specific Vision Transformers

- Swin Transformer serves as a general-purpose backbone for diverse vision tasks, including Image Classification, Object Detection, and Semantic Segmentation

- In addition to Swin Transformer, recent advancements have seen the emergence of task-specific transformers tailored for specialized vision tasks:
  - **DETR (Detection TRansformer)**[^11]: Specifically designed for object detection tasks, DETR replaces traditional convolutional layers with transformer-based architectures.

[^11]: Carion et al. “End-to-End Object Detection with Transformers”, ECCV 2020
[^12]: Kirillov, Alexander, et al. “Segment anything”, ICCV 2023
[^13]: Bianchi, Federico, et al. “Contrastive language-image pre-training for the italian language.”, arXiv 2021 by OpenAI
[^14]: Liang, Jingyun, et al. “Swinir: Image restoration using swin transformer.”, ICCV 2021

Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers 36 / 37
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 51

```markdown
# Task-specific Vision Transformers

- Swin Transformer serves as a general-purpose backbone for diverse vision tasks, including **Image Classification**, **Object Detection**, and **Semantic Segmentation**

- In addition to Swin Transformer, recent advancements have seen the emergence of task-specific transformers tailored for specialized vision tasks:
  - **DETR (Detection Transformer)**[^11]: Specifically designed for object detection tasks, DETR replaces traditional convolutional layers with transformer-based architectures.
  - **Segment Anything (SAM)**[^12]: SAM focuses on Semantic Segmentation tasks, leveraging transformer architectures to effectively capture spatial dependencies and context information.

[^11]: Carion et al. "End-to-End Object Detection with Transformers", ECCV 2020
[^12]: Kirillov, Alexander, et al. "Segment anything", ICCV 2023
[^13]: Bianchi, Federico, et al. "Contrastive language-image pre-training for the italian language.", arXiv 2021 by Open-AI
[^14]: Liang, Jingyun, et al. "Swinir: Image restoration using swin transformer.", ICCV 2021

*Vineeth N B (III-T-H) §13.1 From Transformers to Vision Transformers*

*36 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 52

```markdown
# Task-specific Vision Transformers

- **Swim Transformer** serves as a general-purpose backbone for diverse vision tasks, including:
  - Image Classification
  - Object Detection
  - Semantic Segmentation

- In addition to Swim Transformer, recent advances have seen the emergence of task-specific transformers tailored for specialized vision tasks:
  - **DETR (DEtection TRansformer)**[^11]: Specifically designed for object detection tasks, DETR replaces traditional convolutional layers with transformer-based architectures.
  - **Segment Anything (SAM)**[^12]: SAM focuses on Semantic Segmentation tasks, leveraging transformer architectures to effectively capture spatial dependencies and context information.
  - **Other Notable Examples**: CLIP[^13] for vision-language tasks, and SwinIR[^14] for image restoration.

[^11]: Carion et al., "End-to-End Object Detection with Transformers", ECCV 2020
[^12]: Kirillov, Alexander, et al. "Segment anything", ICCV 2023
[^13]: Bianchi, Federico, et al. "Contrastive language-image pre-training for the italian language", arXiv 2021 by Open-AI
[^14]: Liang, Jingyun, et al. "SwinIR: Image restoration using swin transformer", ICCV 2021

*Vineeth N B (IIT-M)*
*$13.1 From Transformers to Vision Transformers* 
*36 / 37*
```

# DL4CV_Week11.1 From Transformers to Vision Transformers.pdf - Page 53

```markdown
# References

- **[1]** Jacob Devlin et al. "Bert: Pre-training of deep bidirectional transformers for language understanding". In: *arXiv preprint arXiv:1810.04805* (2018).

- **[2]** Yinhan Liu et al. "Roberta: A robustly optimized bert pretraining approach". In: *arXiv preprint arXiv:1907.11692* (2019).

- **[3]** Pengcheng He et al. "Deberta: Decoding-enhanced bert with disentangled attention". In: *arXiv preprint arXiv:2006.03654* (2020).

- **[4]** Colin Raffel et al. "Exploring the limits of transfer learning with a unified text-to-text transformer". In: *Journal of machine learning research* 21.140 (2020), pp. 1–67.

- **[5]** Gokul Yenduri et al. *Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions*. 2023. arXiv: [2305.10435](https://arxiv.org/abs/2305.10435) [cs.CL].

---

*Vineeth N B (IIT-H) §13.1 From Transformers to Vision Transformers*

*Page 37 / 37*
```

