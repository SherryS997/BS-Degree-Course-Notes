# DL4CV_Week05_Part02.pdf - Page 1

```markdown
# Deep Learning for Computer Vision

## Backpropagation in CNNs

### Vineeth N Balasubramanian

**Department of Computer Science and Engineering**

**Indian Institute of Technology, Hyderabad**

---

*Vineeth N B (IIT-H)*

*§5.2 Backprop in CNNs*

---

![IIT Logo](https://via.placeholder.com/150)

---

## Introduction

In this section, we will discuss the backbone of deep learning models: **Backpropagation**.

Backpropagation, short for "backward propagation of errors," is an algorithm used in artificial neural networks for supervised learning of the weights of the network by calculation of the gradient of the loss function. It is a method to minimize errors by adjusting the weights of the connections between neurons.

### How Backpropagation Works

1. **Forward Pass**: 
   - Input data is passed forward through the network to generate an output.
   - The output is compared to the actual value to calculate the error.

2. **Backward Pass**: 
   - The error is propagated backward through the network.
   - The weights are adjusted to minimize the error.

### Mathematical Formulation

Given a network with layers:
- **Input Layer**: \( x \)
- **Hidden Layers**: \( h_1, h_2, \ldots, h_n \)
- **Output Layer**: \( y \)

The error at the output layer is calculated as:
\[ E = \frac{1}{2} (y - \hat{y})^2 \]

Where \( y \) is the actual output and \( \hat{y} \) is the predicted output.

The gradients of the weights are calculated using the chain rule of differentiation:
\[ \frac{\partial E}{\partial w} = \delta \cdot \frac{\partial f}{\partial w} \]

Where:
- \( \delta \) is the error term for the layer.
- \( f \) is the activation function.

### Backpropagation Step-by-Step

1. **Initialize Weights**:
   - Randomly initialize the weights of the connections between neurons.

2. **Forward Pass**:
   - Compute the output of each layer using the activation function.
   - Compute the error at the output layer.

3. **Backward Pass**:
   - Compute the error term for the output layer.
   - Backpropagate the error to each hidden layer.
   - Compute the gradients of the weights and adjust them using gradient descent:
     \[ w = w - \eta \frac{\partial E}{\partial w} \]
   - Where \( \eta \) is the learning rate.

4. **Repeat**:
   - Repeat steps 2-3 for a number of iterations (epochs) until the error is minimized.

### Challenges

- **Vanishing Gradient Problem**: Gradients can become very small, causing slow convergence or getting stuck.
- **Exploding Gradient Problem**: Gradients can become very large, causing instability.
- **Local Minima**: The algorithm can get stuck in local minima.

### Solutions

- **Advanced Activation Functions**: Use functions like ReLU to mitigate vanishing gradients.
- **Learning Rate Adjustment**: Use adaptive learning rates like Adam or RMSprop.
- **Batch Normalization**: Normalize the outputs of layers to stabilize and speed up training.

### Conclusion

Backpropagation is a critical component in training deep learning models. Understanding its intricacies and challenges can help in designing more efficient and effective neural networks.

---

1 / 16
```

# DL4CV_Week05_Part02.pdf - Page 2

```markdown
# Exercises

Given a \(32 \times 32 \times 3\) image and 6 filters of size \(5 \times 5 \times 3\), what will be the dimension of the output volume when a stride of 1 and a padding of 0 is considered?

**Recall:** \(W_2 = \frac{W_1 - F + 2P}{S} + 1\), \(H_2 = \frac{H_1 - F + 2P}{S} + 1\). Hence, dimension of output volume =

\[28 \times 28 \times 6\]

![NPTEL Logo](https://example.com/nptel-logo.png)

Vineeth N B. (IIT-H) §5.2 Backprop in CNNs 2 / 16
```

# DL4CV_Week05_Part02.pdf - Page 3

```markdown
# Acknowledgements

- This lecture’s content is largely based on a similar lecture by Dhruv Batra at Georgia Tech (with a few adaptations)

![NPTEL Logo](image_placeholder)

---

Vineeth N B (IIT-H)

§5.2 Backprop in CNNs

3 / 16
```

# DL4CV_Week05_Part02.pdf - Page 4

```markdown
# Backpropagation in Convolutional Layers: Assumptions

- For simplicity, we consider a grayscale image i.e., number of input channels \( C = 1 \).

- Also, we consider the number of convolutional filters (which is also the number of output channels) to be 1.

![NPTEL Logo](image_url)

*Vineeth N B (IIT-H)*

*Section 5.2 Backprop in CNNs*

*Slide 4 of 16*
```

# DL4CV_Week05_Part02.pdf - Page 5

```markdown
# Convolution Operation

- Consider a single convolutional filter \( W^{K_1 \times K_2} \) applied to an image \( X^{N_1 \times N_2} \) resulting in an output \( Y^{M_1 \times M_2} \).

  ![Convolution Operation Diagram](image-url)

  - Let an element of output \( Y[i, j] \) be written as (note that we are not centering the convolution at a pixel here, but placing the filter at a corner of the window rather - this is only for convenience and simplicity of notation):

    \[
    Y[i, j] = \sum_{a=0}^{K_1-1} \sum_{b=0}^{K_2-1} X[i-a, j-b] W[a, b]
    \]

_Vineeth N B. (IIT-H)_

_§5.2 Backprop in CNNs_

_5 / 16_
```

# DL4CV_Week05_Part02.pdf - Page 6

```markdown
# Backpropagation in Convolutional Layer

![NPTEL Logo](https://example.com/nptel_logo.png)

- Given a loss function \( L \) used to train the CNN, for our convolutional layer, we need to calculate two gradients:
  1. \( \frac{\partial L}{\partial W} \) with respect to the weights, for weight update
  2. \( \frac{\partial L}{\partial X} \) with respect to the input, for further backprop to previous layers

_Vineeth N B (IIT-H)_

---

## 5.2 Backprop in CNNs

- Sections: S5.2 Backprop in CNNs
- Page: 6 / 16
```

# DL4CV_Week05_Part02.pdf - Page 7

```markdown
# Backpropagation in Convolutional Layer

Let's start with \(\partial L / \partial W\), gradient w.r.t weights:

- Consider the gradient of the loss function with respect to a single weight in the convolutional filter (this can be generalized to other weights): \(\partial L / \partial W[a', b']\)

- How many pixels in the output (in the next layer, \(Y\)) does this weight affect?

![NPTEL](image_url)

*Vineeth N B (IIT-H)*
## Section 5.2: Backprop in CNNs

Page 7 / 16
```

# DL4CV_Week05_Part02.pdf - Page 8

```markdown
# Backpropagation in Convolutional Layer

Let's start with \(\partial L / \partial W\), gradient w.r.t weights:

- Consider the gradient of the loss function with respect to a single weight in the convolutional filter (this can be generalized to other weights): \(\partial L / \partial W[a', b']\)

  \[
  \frac{\partial L}{\partial W[a', b']}
  \]

- How many pixels in the output (in the next layer, \(Y\)) does this weight affect?

- It affects every pixel in \(Y\) because:
  - Each pixel in the output corresponds to one position of the filter overlapping the input
  - Every pixel in the output is a weighted sum of a part of the input image

_Vineeth N B (IIT-H)_

\(\S 5.2\) Backprop in CNNs

_7 / 16_
```

# DL4CV_Week05_Part02.pdf - Page 9

```markdown
# Backpropagation in Convolutional Layer

- We assume \(\frac{\partial L}{\partial Y}\) is known since we compute gradients backward from the last layer.
- Hence, \(\frac{\partial L}{\partial W[a', b']}\) can be written as (summing all gradients coming from each pixel in the output):

\[
\frac{\partial L}{\partial W[a', b']} = \sum_{i=0}^{M_1-1} \sum_{j=0}^{M_2-1} \left( \frac{\partial L}{\partial Y[i, j]} \right) \frac{\partial Y[i, j]}{\partial W[a', b']}
\]

  - \(\frac{\partial L}{\partial Y[i, j]}\) is known
  - \(\frac{\partial Y[i, j]}{\partial W[a', b']}\) is not known yet

- To expand this expression, let's compute \(\frac{\partial Y[i, j]}{\partial W[a', b']}\).

*Vineeth N B (IIIT-H)*
*§5.2 Backprop in CNNs*
*8 / 16*
```

# DL4CV_Week05_Part02.pdf - Page 10

```markdown
# Backpropagation in Convolutional Layer

**Computing** \(\partial Y[i,j] / \partial W[a', b']\):

- We have (by definition of convolution):

\[ Y[i, j] = \sum_{a=0}^{K_1-1} \sum_{b=0}^{K_2-1} X[i - a, j - b] W[a, b] \]

- So, we can compute \(\partial Y[i, j] / \partial W[a', b']\) as:

\[ \frac{\partial Y[i, j]}{\partial W[a', b']} = \frac{\partial \left( \sum_{a=0}^{K_1-1} \sum_{b=0}^{K_2-1} X[i - a, j - b] W[a, b] \right)}{\partial W[a', b']} \]

\[ = \frac{\partial (W[a', b'] X[i - a', j - b'])}{\partial W[a', b']} = X[i - a', j - b'] \]

*Vineeth N B (IIIT-H) §5.2 Backprop in CNNs 9 / 16*
```

# DL4CV_Week05_Part02.pdf - Page 11

```markdown
# Backpropagation in Convolutional Layer

- We can hence write the gradient of loss function w.r.t weights as:

  \[
  \frac{\partial L}{\partial W[a', b']} = \sum_{i=0}^{M_1 - 1} \sum_{j=0}^{M_2 - 1} \frac{\partial L}{\partial Y[i, j]} X[i - a', j - b']
  \]

  \[
  = X \ast \frac{\partial L}{\partial Y}
  \]

- This is a convolutional operation, which is nice!

![Vineeth N B (IIT-H)](https://example.com/image)

## 5.2 Backpropagation in CNNs

![Section](https://example.com/image)

---

*Page 10 / 16*
```

This markdown format maintains the structure and formatting of the original scientific material, ensuring that all mathematical expressions, headings, and notes are accurately captured.

# DL4CV_Week05_Part02.pdf - Page 12

```markdown
# Backpropagation in Convolutional Layer

Let’s now compute \(\partial L / \partial X\), gradient w.r.t input:

- We consider a single input pixel \(X[i', j']\). Which output pixels does it affect?
- That depends on the size of the convolutional filter:

  ![Convolution Operation](image_url)

  The dotted region in the output represents the output pixels affected by \(X[i', j']\). Let’s call the region \(P\).

  Vineeth N B. (IIT-H) §5.2 Backprop in CNNs 11 / 16
```

# DL4CV_Week05_Part02.pdf - Page 13

# Backpropagation in Convolutional Layer

- Applying chain rule, we have:

  \[
  \frac{\partial L}{\partial X[i', j']} = \sum_{p \in P} \frac{\partial L}{\partial Y[p]} \frac{\partial Y[p]}{\partial X[i', j']}
  \]

  - From the figure in previous slide, we can mathematically define the region \(P\):

    \[
    \frac{\partial L}{\partial X[i', j']} = \sum_{a=0}^{K_1 - 1} \sum_{b=0}^{K_2 - 1} \frac{\partial L}{\partial Y[i' + a, j' + b]} \frac{\partial Y[i' + a, j' + b]}{\partial X[i', j']}
    \]

  - In the next slides, we calculate \(\frac{\partial Y[i' + a, j' + b]}{\partial X[i', j']}\).

*Vineeth N B (IIT-H)*

§5.2 Backprop in CNNs

12 / 16

# DL4CV_Week05_Part02.pdf - Page 14

```markdown
# Backpropagation in Convolutional Layer

- We already have:

  \[
  Y[i', j'] = \sum_{a=0}^{K_1-1} \sum_{b=0}^{K_2-1} X[i' - a, j' - b] W[a, b]
  \]

- We can rewrite it as:

  \[
  Y[i' + a, j' + b] = \sum_{a=0}^{K_1-1} \sum_{b=0}^{K_2-1} X[i', j'] W[a, b]
  \]

- So the derivative can be calculated as:

  \[
  \frac{\partial Y[i' + a, j' + b]}{\partial X[i', j']} = W[a, b]
  \]

*Vineeth N B (IIT-H) §5.2 Backprop in CNNs 13 / 16*
```

# DL4CV_Week05_Part02.pdf - Page 15

```markdown
# Backpropagation in Convolutional Layer

- The final expression for gradient of the loss function w.r.t an input pixel can be written as:

  \[
  \frac{\partial L}{\partial X[i', j']} = \sum_{a=0}^{K_1-1} \sum_{b=0}^{K_2-1} \frac{\partial L}{\partial Y[i' + a, j' + b]} W[a, b]
  \]

  \[
  = \frac{\partial L}{\partial Y} * flip_{180}(W)
  \]

- Thus, the final expression is a convolution operation with a flipped version of the filter.

![Image of convolution operation](image_url)

Vineeth N B. (IIT-H) §5.2 Backprop in CNNs 14 / 16
```

# DL4CV_Week05_Part02.pdf - Page 16

```markdown
# Backpropagation in Pooling Layers

- There are no weights to learn, only have to propagate gradients through

![NPTel Logo](image_url)

*Vineeth N B (IIT-H)*

## 5.2 Backprop in CNNs

Page 15 / 16
```

# DL4CV_Week05_Part02.pdf - Page 17

```markdown
# Backpropagation in Pooling Layers

- There are no weights to learn, only have to propagate gradients through

- In max-pooling, backpropagated gradient is assigned only to the **winning pixel** i.e., the one which had maximum value in the pooling block; this can be kept track of in the forward pass

- In average pooling, the backpropagated gradient is divided by the area of the pooling block (K × K) and equally assigned to all pixels in the block

![Forward Propagation Diagram](image-url)

## Forward propagation

Single depth slice

| x  | 1 | 1 | 2 | 4 |
|---|---|---|---|---|
|   | 5 | 6 | 7 | 8 |
|   | 3 | 2 | 1 | 0 |
|   | 1 | 2 | 3 | 4 |

max pool with 2x2 filters and stride 2

|   | 6 | 8 |
|---|---|---|
|   | 3 | 4 |

Backpropagation

|   | 6 | 8 |
|---|---|---|
|   | 3 | 4 |

![Backpropagation Diagram](image-url)

|   | 0 | 0 | 0 | 0 |
|---|---|---|---|---|
|   | 0 | dout | 0 | dout |
|   | dout | 0 | 0 | 0 |
|   | 0 | 0 | 0 | dout |

Vineeth N B. (IIT-H) §5.2 Backprop in CNNs

15 / 16
```

# DL4CV_Week05_Part02.pdf - Page 18

```markdown
# Homework

## Readings

- [Lecture 5 Notes, Dhruv Batra, ECE 6504: Deep Learning for Perception](#)
- [Jefkine, Backpropagation in CNNs](#)

---

Vineeth N B (IIT-H) §5.2 Backprop in CNNs

Page 16 / 16
```

