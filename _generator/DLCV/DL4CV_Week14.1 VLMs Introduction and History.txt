# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 1

 is not required for this task.

```markdown
# Deep Learning for Computer Vision

## Vision-Language Models: Introduction and History

### Vineeth N Balasubramanian

**Department of Computer Science and Engineering**

**Indian Institute of Technology, Hyderabad**

![IIT Hyderabad Logo](image-url)

Vineeth N B (IIT-H)

§14.1 Vision-Language Models: Introduction and History
```

(The above markdown template should be filled with the extracted content from the provided scientific slides or text.)

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 2

```markdown
# Computer Vision: Existing paradigms

## Computer Vision tasks

- Object Classification
- Object Detection
- Object Segmentation
- Instant Segmentation
- Semantic Segmentation

---

Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History 2 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 3

 the input image.

```markdown
# Computer Vision: Existing paradigms

## Computer Vision tasks

- Object Classification
- Object Detection
- Object Segmentation
- Instant Segmentation
- Semantic Segmentation

## Limitations: the missing elements

- Existing Computer vision techniques output bounding boxes, images, classes, etc.
- They lack communication through language and text, which humans are good at.

_Vineeth N B (IIIT-H) §14.1 Vision-Language Models: Introduction and History 2 / 15_
```

This markdown format maintains the structure and content integrity of the original scientific presentation slide, ensuring all elements are correctly formatted and easily readable.

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 4

```markdown
# Computer Vision tasks roping in language

![Image Captioning](image_captioning.png)
**Figure 1: Image Captioning**

![Visual Question Answering](visual_question_answering.png)
**Figure 2: Visual Question Answering**

---

1. **Vinyals, Oriol et al.** “Show and tell: A neural image caption generator.” 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014): 3156-3164.

2. **Agrawal, Aishwarya et al.** “VQA: Visual Question Answering.” International Journal of Computer Vision 123 (2015): 4 - 31.

---

*Vineeth N B (IIT-H)*

§14.1 Vision-Language Models: Introduction and History

---

*Page 3 / 15*
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 5

```markdown
# Computer Vision tasks roping in language

## Figure 3: Text-to-image generation

![Text-to-image generation](image_path)

A small gray bird with white and dark gray wingbars and white breast

This beautiful little bird has a white breast and very intriguing red eyes

![Text-to-image generation](image_path)

## Figure 4: Text-to-Video retrieval

![Text-to-Video retrieval](image_path)

"Animated comic scene of guy cutting up food for dinner"

![Text-to-Video retrieval](image_path)

"a woman holding a ribbon"

## References

[^2]: Gu, Shuyang, et al. "Vector quantized diffusion model for text-to-image synthesis." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2022.

[^3]: Sirnam, Swetha et al. "Preserving Modality Structure Improves Multi-Modal Learning." 2023 IEEE/CVF International Conference on Computer Vision (ICCV) (2023): 21936-21946.

Vineeth N B (IIIT-H) §14.1 Vision-Language Models: Introduction and History

Page 4 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 6

 the provided content into a detailed markdown format.

```markdown
# Natural Language Processing (NLP)

## NLP tasks

- Search engines
- Spam filtering
- Machine translation
- Sentiment Analysis

*Vineeth N B (IIT-H)*

*14.1 Vision-Language Models: Introduction and History*

*5 / 15*
```

Note: This conversion assumes that a direct visual text extraction into markdown is intended. If additional context or deeper structural analysis is necessary, further adjustments can be made.

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 7

 the output should be in markdown format.

```markdown
# Natural language Processing (NLP)

## NLP tasks

- Search engines
- Spam filtering
- Machine translation
- Sentiment Analysis

## Limitations: the missing elements

- Unable to decode visual cues.
- Linguistic Ambiguities.
- Verifying interpretations against real-world visual references.
- Exhibit Flair of text analytics and generation they fall short in decoding visual cues.

*Vineeth N B (IIT-H)*

*§14.1 Vision-Language Models: Introduction and History*
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 8



```markdown
# Language Models

- **Understand and generate text**

*Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History*

---

Page 6 / 15
```

```markdown
# Language Models

- **Understand and generate text**

*Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History*

---

Page 6 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 9

```markdown
# Language Models

- **Understand and generate text**
- **Learn from raw text**

_Vineeth N B (IIT-H)_

## §14.1 Vision-Language Models: Introduction and History

---

Page 6 / 15
```

In this markdown format, the content is structured accurately with proper emphasis on the scientific terminology and format. The section titles and headings have been correctly formatted, and the items are presented in a bulleted list for clarity.

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 10

```markdown
# Language Models

- **Understand and generate text**
- **Learn from raw text**
- **Transformer architecture**

*Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History*

---

Note: The above content represents the structured layout and content extracted from the provided image/slide. Additional context or specific details about the content of "Vision-Language Models: Introduction and History" were not provided in the original image, so specific content details have been omitted.
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 11

```markdown
# Large Language Models<sup>3</sup>

- Pre-trained on large datasets

---

<sup>3</sup> *Beginner’s Guide to Large Language Models* — by Digitate — Medium

Vineeth N B (IIT-H)

Section 14.1 Vision-Language Models: Introduction and History

---

Page 7 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 12

```markdown
# Large Language Models

- Pre-trained on large datasets
- They have a large number of parameters

---

**Beginner's Guide to Large Language Models** — by Digitate — Medium

Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History

---

7 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 13

```markdown
# Large Language Models

- Pre-trained on large datasets
- They have a large number of parameters
- Datasets used to train LLMs are Common Crawl (60%), WebText2 (22%), Books1 (8%), Books2 (8%), Wikipedia (3%)

---

*Beginner's Guide to Large Language Models* — by Digitate — Medium

*Vineeth N B (IIT-H)*

*§14.1 Vision-Language Models: Introduction and History*

---

Page 7 of 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 14

:

```markdown
# LLMs

- **LaMDA**: Developed by google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!

---

**Beginner's Guide to Large Language Models** — by Digitate — Medium

Vineeth N B (IIT-H)

**§14.1 Vision-Language Models: Introduction and History**
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 15

```markdown
# LLMs

- **LaMDA**: Developed by Google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!

- **LLaMA**: Developed by Meta, a relatively small model (7B parameters) yet accurate as compared to GPT-3.

---

_Beginner's Guide to Large Language Models_ — by Digitate — Medium

*Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History*

8 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 16

```markdown
# LLMs

- **LaMDA**: Developed by Google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!

- **LLaMA**: Developed by Meta, a relatively small model (7B parameters) yet accurate as compared to GPT3.

- **BLOOM**: Open source and multilingual model, trained on data from 46 natural languages and 13 programming languages.

---

*Beginner's Guide to Large Language Models* — by Digitate — Medium

Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History

---

```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 17

```markdown
# LLMs

## LLMs

- **LaMDA**: Developed by google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!
- **LLaMA**: Developed by Meta, a relatively small model (7B parameters) yet accurate as compared to GPT3.
- **BLOOM**: Open source and multilingual model, trained data from 46 natural languages and 13 programming languages.
- **Galactica**: Developed by Meta, can store, combine, and reason about scientific knowledge.

---

_Beginner's Guide to Large Language Models — by Digitate — Medium_
_Vineeth N B (IIT-H)_
§14.1 Vision-Language Models: Introduction and History_

```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 19

```markdown
# LLMs

- **LaMDA**: Developed by Google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!

- **LLaMA**: Developed by Meta, a relatively small model (7B parameters) yet accurate as compared to GPT3.

- **BLOOM**: Open source and multilingual model, trained data from 46 natural languages and 13 programming languages.

- **Galactica**: Developed by Meta, can store, combine, and reason about scientific knowledge.

- **Codex**: Model that powers GitHub Copilot. Proficient in more than a dozen programming languages. Codex can now interpret simple commands in natural language and execute them.

- **PaLM-E**: Developed by Google, a LLM focused on robot sensor data.

*Beginner's Guide to Large Language Models* — by Digitate — Medium

*Vineeth N B (IIT-H)*

§14.1 Vision-Language Models: Introduction and History

8 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 20



```markdown
# LLMs

- **LaMDA**: Developed by google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!
- **LLaMA**: Developed by Meta, a relatively small model (7B parameters) yet accurate as compared to GPT3.
- **BLOOM**: open source and multilingual model, trained data from 46 natural languages and 13 programming languages.
- **Galactica**: Developed by Meta, can store, combine, and reason about scientific knowledge.
- **Codex**: model that powers GitHub Copilot. Proficient in more than a dozen programming languages. Codex can now interpret simple commands in natural language and execute them.
- **PaLM-E**: Developed by google, a LLM focused on robot sensor data.
- **Chinchilla**: Developed by Deepmind, considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning.

_Beginner's Guide to Large Language Models — by Digitate — Medium_

_Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History_

![Image](https://example.com/image)
```

```markdown
# LLMs

- **LaMDA**: Developed by Google, trained on 1.56 trillion words of public dialog data. It powers the BARD chatbot!
- **LLaMA**: Developed by Meta, a relatively small model (7B parameters) yet accurate as compared to GPT3.
- **BLOOM**: Open-source and multilingual model, trained on data from 46 natural languages and 13 programming languages.
- **Galactica**: Developed by Meta, can store, combine, and reason about scientific knowledge.
- **Codex**: Model that powers GitHub Copilot. Proficient in more than a dozen programming languages. Codex can now interpret simple commands in natural language and execute them.
- **PaLM-E**: Developed by Google, a LLM focused on robot sensor data.
- **Chinchilla**: Developed by DeepMind, considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning.

_Beginner's Guide to Large Language Models — by Digitate — Medium_

_Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History_
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 21

```markdown
# Generative Pretrained Transformer (GPT)

## Target Output:
- Life
- is
- a
- work
- in
- progress

### Input line shifted by one token

```plaintext
GPT : Generative Layers
```
- Life
- is
- a
- work
- in
- progress

## Input at every step
- s1
- s2
- s3
- s4
- s5
- s6
- s7

---

*Beginner's Guide to Large Language Models* — by Digitate — Medium

*Vineeth N B (IIT-H)*

*§14.1 Vision-Language Models: Introduction and History*

---

Page 9 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 22

```markdown
# Training aspects of GPT<sup>6</sup>

## Self-supervised

The model learns from raw sentences with a target sequence shifted by one token, enabling it to grasp word relationships for accurate output prediction.

---

_Beginner's Guide to Large Language Models_ — by Digitate — Medium

Vineeth N B (IIT-H)

§14.1 Vision-Language Models: Introduction and History

---

10 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 23



```markdown
# Training aspects of GPT<sup>6</sup>

## Self-supervised

The model learns from raw sentences with a target sequence shifted by one token, enabling it to grasp word relationships for accurate output prediction.

## Auto-regressive

Words derive context from all preceding words. Each generated token is added to the input sequence, forming the input for the next step.

---

*Beginner's Guide to Large Language Models* — by Digitate — Medium

*Vineeth N B (IIT-H)* §14.1 Vision-Language Models: Introduction and History

---

Page 10 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 25

```markdown
# Applications and Limitations of LLMs

## Applications

- Code generation
- Content generation tools
- Copywriting
- Conversational tools
- Educational tools

---

_Beginner's Guide to Large Language Models — by Digitate — Medium_

_Vineeth N B (IIT-H)_

§14.1 Vision-Language Models: Introduction and History

---

11 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 26

 the markdown format and ensure the scientific integrity of the content.

```markdown
# Applications and Limitations of LLMs

## Applications

- Code generation
- Content generation tools
- Copywriting
- Conversational tools
- Educational tools

## Limitations

- LLMs are large
- LLMs can have bias
- LLMs can hallucinate

*Beginner's Guide to Large Language Models — by Digitate — Medium*

*Vineeth N B (IIT-H)*

*§14.1 Vision-Language Models: Introduction and History*
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 27

```markdown
# Vision Language Models

- Vision systems are fundamental to understanding our world; however, humans are good in communicating through language.

---

![Awasis](https://arxiv.org/abs/2307.13721)

**Awasis, Muhammad et al.** "Foundational Models Defining a New Era in Vision: A Survey and Outlook." *ArXiv abs/2307.13721* (2023): n. pag.

Vineeth N B. “IIT-H.” §14.1 Vision-Language Models: Introduction and History.

---

12 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 28



```markdown
# Vision Language Models

- Vision systems are fundamental to understanding our world, however, humans are good in communicating through language
- Complex relations between objects and their locations can be better described in human language (text)

---

*Reference:*
  
8 Awais, Muhammad et al. “Foundational Models Defining a New Era in Vision: A Survey and Outlook.” *ArXiv* abs/2307.13721 (2023): n. pag.
  
*Source:*
  
Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 29

```markdown
# Vision Language Models

- Vision systems are fundamental to understanding our world, however, humans are good in communicating through language
- Complex relations between objects and their locations can be better described in human language (text)
- Visual-Language models bridge the gap between vision and language understanding both

---

*[^8]: Awais, Muhammad et al. "Foundational Models Defining a New Era in Vision: A Survey and Outlook." ArXiv abs/2307.13721 (2023): n. pag.

Vineeth N B (IIIT-H) §14.1 Vision-Language Models: Introduction and History 12 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 30

:

```markdown
# Vision Language Models

- Vision systems are fundamental to understanding our world; however, humans are good in communicating through language
- Complex relations between objects and their locations can be better described in human language (text)
- Visual-Language models bridge the gap between vision and language understanding both
- The output of VLM can be modified through human-provided prompts, e.g.
  - segmenting a particular object by providing a bounding box,
  - having interactive dialogues by asking questions about an image or video scene
  - manipulating the robot’s behavior through language instructions

8Awais, Muhammad et al. "Foundational Models Defining a New Era in Vision: A Survey and Outlook," ArXiv abs/2307.13721 (2023): n. pag.

Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History 12 / 15
```

```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 31

```markdown
# Visual Language Tasks

- [ ] Image retrieval from natural language text

*Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History 13 / 15*
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 32

```markdown
# Visual Language Tasks

- **Image retrieval from natural language text**
- **Phrase grounding**, i.e., performing object detection from an input image and natural text (example: A young person swings a bat)

*Vineeth N B (IIIT-H) §14.1 Vision-Language Models: Introduction and History 13 / 15*
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 33

 the provided image is:

```
Visual Language Tasks

- Image retrieval from natural language text
- Phrase grounding, i.e., performing object detection from an input image and natural text (example: A young person swings a bat)
- Visual question answering, i.e., finding answers from an input image and a question in natural language
```

Here is the converted markdown with proper formatting:

```markdown
## Visual Language Tasks

- Image retrieval from natural language text
- Phrase grounding, i.e., performing object detection from an input image and natural text (example: A young person swings a bat)
- Visual question answering, i.e., finding answers from an input image and a question in natural language
```

```
Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History 13 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 34

 it carefully as it pertains to the field of vision-language models.

```markdown
# Visual Language Tasks

- **Image retrieval from natural language text**
- **Phrase grounding**, i.e., performing object detection from an input image and natural text (example: A young person swings a bat)
- **Visual question answering**, i.e., finding answers from an input image and a question in natural language
- **Generate a caption for a given image**

_Vineeth N B (IIIT-H) §14.1 Vision-Language Models: Introduction and History_

*13 / 15*
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 35

```markdown
# Visual Language Tasks

- **Image retrieval from natural language text**
- **Phrase grounding**, i.e., performing object detection from an input image and natural text (example: **A young person swings a bat**)
- **Visual question answering**, i.e., finding answers from an input image and a question in natural language
- **Generate a caption** for a given image
- **Detection of hate speech from social media content involving both images and text modalities**

*Vineeth N B (IIIT-H) §14.1 Vision-Language Models: Introduction and History*

13 / 15
```

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 36

 and ensure the 
### Glimpse of this week's topics

### Contrastive Language Image Pre-training (CLIP) - The Madness!

#### (1) Contrastive pre-training

![Contrastive pre-training](attachment:contrastive_pre_training.png)

#### (2) Create dataset; classifier from label text

![Create dataset; classifier from label text](attachment:create_dataset_classifier_label_text.png)

#### (3) Use for zero-shot prediction

![Use for zero-shot prediction](attachment:use_zero_shot_prediction.png)

9 Radford, Alec et al. "Learning Transferable Visual Models From Natural Language Supervision." *International Conference on Machine Learning* (2021).

Vineeth N B (IIT-H) §14.1 Vision-Language Models: Introduction and History

---

**Note**:
- Ensure the images are correctly referenced or replace them with descriptive text if not available.
- The reference to Radford et al. (2021) should include a proper citation format if it's a part of a larger document or presentation.
- The OCR step might require manual adjustments to ensure the content is accurate and correctly formatted.

# DL4CV_Week12.1 VLMs Introduction and History.pdf - Page 37

 the provided image into a markdown formatted text.

```markdown
# Glimpse of this week's topics

## GPT-4

### Bootstrapping Language-Image Pretraining(BLIP)

![BLIP Diagram](http://example.com/blip_diagram.png)

10

- Li, Junnan et al. "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation." International Conference on Machine Learning (2022).
- Achiam, OpenAI Josh et al. "GPT-4 Technical Report." (2023).

Vimeeth N B (IIT-H)

**§14.1 Vision-Language Models: Introduction and History**

---

![GPT-4 Image](http://example.com/gpt4_image.png)

**User:**

*What is GPT-4?*

**GPT-4:**

GPT-4 is a "Lightning Cable" adapter with three prongs. It can be used to charge a device, such as a smartphone or tablet. The adapter has a USB port, which allows it to connect to a computer or other device. The cable is white, with a blue tip.

**User:**

*Where can I buy a Lightning Cable?*

**GPT-4:**

You can buy a Lightning Cable at a store or online. You can also find them at electronics stores or other retailers.

**User:**

*Can I use a Lightning Cable to charge my phone?*

**GPT-4:**

Yes, you can use a Lightning Cable to charge your phone. You can also use it to connect your phone to a computer or other device.

**User:**

*What is a Lightning Cable?*

**GPT-4:**

A Lightning Cable is a type of cable that is used to connect a device to a computer or other device. It has a USB port, which allows it to connect to a computer or other device.

**User:**

*What is a Lightning Cable used for?*

**GPT-4:**

A Lightning Cable is used to charge a device, such as a smartphone or tablet. It can also be used to connect a device to a computer or other device.

**User:**

*What is a Lightning Cable made of?*

**GPT-4:**

A Lightning Cable is made of plastic and metal. It is designed to be durable and long-lasting.

**User:**

*How do I use a Lightning Cable?*

**GPT-4:**

To use a Lightning Cable, you need to plug it into the charging port of your device. Then, you need to plug the other end of the cable into a power source, such as a wall outlet or a computer.

**User:**

*What is a Lightning Cable?*

**GPT-4:**

A Lightning Cable is a type of cable that is used to connect a device to a computer or other device. It has a USB port, which allows it to connect to a computer or other device.

**User:**

*What is a Lightning Cable used for?*

**GPT-4:**

A Lightning Cable is used to charge a device, such as a smartphone or tablet. It can also be used to connect a device to a computer or other device.
```

