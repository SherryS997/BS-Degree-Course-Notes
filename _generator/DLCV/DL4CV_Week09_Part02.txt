# DL4CV_Week09_Part02.pdf - Page 1

```markdown
# Deep Learning for Computer Vision

## Vision and Language: Image Captioning

### Vineeth N Balasubramanian

**Department of Computer Science and Engineering**

**Indian Institute of Technology, Hyderabad**

---

**Vineeth N B (IIT-H)**

**§9.2 Image Captioning**

---

**1 / 25**

---

*Note: This markdown output is intended to reflect the structure and content of the original scientific text or slides accurately. Due to the limitations of OCR, some graphical elements or specific scientific notations might be represented as placeholders.* 

---

```

# DL4CV_Week09_Part02.pdf - Page 2

:

```markdown
# Review: Are autoencoders (AE) and PCA connected?

![NPTEL Logo](url_to_nptel_logo)

Vineeth N B (IIT-H)

S9.2 Image Captioning

---

## Abstract

The connection between autoencoders (AE) and Principal Component Analysis (PCA) remains a focal point of recent research. This review explores the similarities, differences, and intrinsic relationships between these two techniques.

## Introduction

- **Autoencoders (AE)**: 
  - Autoencoders are neural networks that learn efficient codings of input data.
  - Typically used for dimensionality reduction or feature learning.
  - Comprised of an encoder and a decoder, where the encoder compresses the input into a latent code, and the decoder reconstructs the input from this code.
  
  ```math
  h = f(x)
  x' = g(h)
  ```

  Here, \( h \) is the encoded representation, \( f \) the encoder function, and \( g \) the decoder function.

- **Principal Component Analysis (PCA)**:
  - PCA is a statistical method used to reduce the dimensionality of data.
  - Identifies the principal components—orthogonal directions of maximum variance in the data.
  - Transforms the data projection into a new coordinate system.

  ```math
  X = UΣV^T
  ```

  Here, \( X \) is the original data, \( U \) and \( V \) are orthogonal matrices, and \( Σ \) is a diagonal matrix with singular values.

## Comparison and Connection

- **Similarities**:
  - Both AE and PCA are used for dimensionality reduction.
  - Both methods aim to capture the essential structure of the data in a lower-dimensional space.

- **Differences**:
  - **Nonlinearity**: Autoencoders can capture nonlinear relationships in the data, whereas PCA is linear.
  - **Optimization**: Autoencoders use backpropagation and gradient descent to optimize, while PCA uses algebraic methods like Singular Value Decomposition (SVD).
  - **Parameters**: AE involves learning multiple parameters (weights and biases), while PCA does not learn parameters but relies on the data covariance matrix.

## Applications

- **Autoencoders**:
  - Image denoising
  - Anomaly detection
  - Generative models

- **PCA**:
  - Data visualization
  - Noise reduction
  - Feature extraction

## Conclusion

The connection between autoencoders and PCA lies in their shared goal of dimensionality reduction. However, AEs offer more flexibility in handling complex, nonlinear data structures, whereas PCA is more computationally efficient for linear data.

## References

- [1] Hinton, G. E. "Autoencoders, Mean, and Variance." arXiv preprint arXiv:1109.5188, 2011.
- [2] Jolliffe, I. T. "Principal Component Analysis." Springer, 2002.
```

In this markdown format, the content has been accurately captured and retained its scientific integrity.

# DL4CV_Week09_Part02.pdf - Page 3

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!**

![NPTEL Logo](image_url)

Vineeth N B (IIT-H)

## §9.2 Image Captioning

---

2 / 25
```

### Detailed Markdown Content

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!**

<div align="center">
  ![NPTEL Logo](image_url)
</div>

Vineeth N B (IIT-H)

## §9.2 Image Captioning

---

2 / 25
```

### Explanation

1. **Section Title**: The title "Review: Are autoencoders (AE) and PCA connected?" is formatted using markdown's header syntax (`#`). 
2. **Bullet Point**: The bullet point "Yes!" is formatted using the `-` syntax.
3. **Image**: The image is referenced using markdown's image syntax `![Alt Text](image_url)`. Replace `image_url` with the actual image URL or path.
4. **Author Information**: The author's name and affiliation are retained as plain text.
5. **Section Heading**: The section heading "§9.2 Image Captioning" is formatted using markdown's header syntax (`##`).
6. **Pagination**: The pagination "2 / 25" is retained as plain text.

# DL4CV_Week09_Part02.pdf - Page 4

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!** Consider normalizing the inputs to the AE as:

  \[
  \hat{x}_{ij} = \frac{1}{\sqrt{m}} \left( x_{ij} - \frac{1}{m} \sum_{k=1}^{m} x_{kj} \right)
  \]

![NPTEL Logo](https://example.com/logo.png)

Vineeth N B (IIT-H)

§9.2 Image Captioning

2 / 25
```

# DL4CV_Week09_Part02.pdf - Page 5

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!** Consider normalizing the inputs to the AE as:

  \[
  \hat{x}_{ij} = \frac{1}{\sqrt{m}} \left( x_{ij} - \frac{1}{m} \sum_{k=1}^{m} x_{kj} \right)
  \]

  - **What is this doing?** Making the mean of each dimension zero

![NPTEL Logo](https://example.com/logo.png)

---

**Vineeth N B (IIIT-H)**

**§9.2 Image Captioning**

---

2 / 25
```

# DL4CV_Week09_Part02.pdf - Page 6



```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!** Consider normalizing the inputs to the AE as:

  \[
  \hat{x}_{ij} = \frac{1}{\sqrt{m}} \left( x_{ij} - \frac{1}{m} \sum_{k=1}^{m} x_{kj} \right)
  \]

- **What is this doing?** Making the mean of each dimension zero

- **Then, covariance matrix is:**

  \[
  \frac{1}{m} \hat{X}^T \hat{X} = X^T X
  \]

*Image: ![NPTEL](../path/to/image.png)*

*Slide Attribution: Vineeth N B (IIT-H)*

*Section: §9.2 Image Captioning*

*Slide Number: 2 / 25*
```

This markdown format maintains the structure, formatting, and scientific accuracy of the provided OCR content.

# DL4CV_Week09_Part02.pdf - Page 7

 extracted content into a detailed markdown format.

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!** Consider normalizing the inputs to the AE as:

  \[
  \hat{x}_{ij} = \frac{1}{\sqrt{m}} \left( x_{ij} - \frac{1}{m} \sum_{k=1}^{m} x_{kj} \right)
  \]

  - **What is this doing?** Making the mean of each dimension zero

- **Then, covariance matrix is:**

  \[
  \frac{1}{m} \hat{X}^T \hat{X} = X^T X
  \]

- **Now, consider AE loss:**

  \[
  \min_{\theta} \sum_{i=1}^{m} \sum_{j=1}^{n} (x_{ij} - \hat{x}_{ij})^2 \leftrightarrow \min_{\theta} \frac{\| X - HW^* \|_F}{W^* H}
  \]

  where \(\| A \|_F = \sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}^2\) is Frobenius norm, \(H\) is AE representation and \(W^*\) are decoder weights

![NPTEL Image](image_placeholder.png)

*Vineeth N B (IIIT-H)*

*§9.2 Image Captioning*

*2 / 25*
```

Note: `image_placeholder.png` should be replaced with the actual image or description if available from the OCR process.

# DL4CV_Week09_Part02.pdf - Page 8

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!** Consider normalizing the inputs to the AE as:

  \[
  \hat{x}_{ij} = \frac{1}{\sqrt{m}} \left( x_{ij} - \frac{1}{m} \sum_{k=1}^{m} x_{kj} \right)
  \]

- **What is this doing?** Making the mean of each dimension zero

- Then, covariance matrix is:

  \[
  \frac{1}{m} \hat{X}^T \hat{X} = X^T X
  \]

  ![Diagram Placeholder](link-to-diagram.png)

- Now, consider AE loss:

  \[
  \min_{\theta} \sum_{i=1}^{m} \sum_{j=1}^{n} (x_{ij} - \hat{x}_{ij})^2 \longleftrightarrow \min_{W^*, H} \frac{\| X - H W^* \|_F^2}{W^* H}
  \]

  where \(\| A \|_F = \sum_{i=1}^{m} \sum_{j=1}^{n} \alpha_{ij}^2\) is Frobenius norm, \(H\) is AE representation and \(W^*\) are decoder weights

- From SVD, we know the optimal solution is:

  \[
  H W^* = U_{:, \leq k} \Sigma_{k, k} V_{:, \leq k}^T
  \]

---

Vineeth N B (IIT-H) 

§9.2 Image Captioning

2 / 25
```

# DL4CV_Week09_Part02.pdf - Page 9

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- **Yes!** Consider normalizing the inputs to the AE as:
  \[
  \hat{x}_{ij} = \frac{1}{\sqrt{m}} \left( x_{ij} - \frac{1}{m} \sum_{k=1}^{m} x_{kj} \right)
  \]

- **What is this doing?** Making the mean of each dimension zero

- **Then, covariance matrix is:**
  \[
  \frac{1}{m} \hat{X}^T \hat{X} = X^T X
  \]

- **Now, consider AE loss:**
  \[
  \min_{\theta} \sum_{i=1}^{m} \sum_{j=1}^{n} (x_{ij} - \hat{x}_{ij})^2 \longleftrightarrow \min_{W, H} \frac{\| X - HW^* \|_F^2}{W^* H}
  \]
  where \(\| A \|_F = \sum_{i=1}^{m} \sum_{j=1}^{n} \alpha_{ij}^2\) is Frobenius norm, \(H\) is AE representation and \(W^*\) are decoder weights

- **From SVD, we know the optimal solution is:**
  \[
  HW^* = U_{\cdot, \leq k} \Sigma_{k, k} V^T_{\cdot, \leq k}
  \]

- **One possible solution then is:**
  \[
  H = U_{\cdot, \leq k} \Sigma_{k, k}
  \]
  \[
  W^* = V^T_{\cdot, \leq k}
  \]

*Vineeth N B. (IIIT-H) §9.2 Image Captioning*
```

# DL4CV_Week09_Part02.pdf - Page 10

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{:, \leq k} \Sigma_{k,:} \]

![NPTEL Logo](image_url)

*Vineeth N B (IIT-H)*

*Section 9.2: Image Captioning*

*Slide 3 / 25*
```

# DL4CV_Week09_Part02.pdf - Page 11



```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{\cdot, \leq k} \Sigma_{k,k} \]

\[ = (XX^T)(XX^T)^{-1}U_{\cdot, \leq k} \Sigma_{k,k} \]

![NPTEL Logo](image_url)

*Vineeth N B (IIT-H)*

*§9.2 Image Captioning*

*3 / 25*
```

(Replace `image_url` with the actual URL or identifier of the image if available.)

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{\cdot, \leq k} \Sigma_{k,k} \]

\[ = (XX^T)(XX^T)^{-1}U_{\cdot, \leq k} \Sigma_{k,k} \]

![NPTEL Logo](image_url)

*Vineeth N B (IIT-H)*

§9.2 Image Captioning

*3 / 25*
```

(Replace `image_url` with the actual URL or identifier of the image if available.)
```

# DL4CV_Week09_Part02.pdf - Page 12



```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

  \[
  H = U_{.,\leq k} \Sigma_{k,k}
  \]

  \[
  = (XX^T)(XX^T)^{-1}U_{.,\leq k} \Sigma_{k,k}
  \]

  \[
  = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1}U_{.,\leq k} \Sigma_{k,k}
  \]

![NPTEL Logo](image_url)

_Image Captioning_

_Vineeth N B (IIT-H)_

_Section: §9.2_

_Page: 3 / 25_
```

# DL4CV_Week09_Part02.pdf - Page 13

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{., \leq k} \Sigma_{k,k} \]

\[ = (XX^T)(XX^T)^{-1}U_{., \leq k} \Sigma_{k,k} \]

\[ = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k,k} \]

![NPTEL](attachment:image.png)

_Image Captioning_

_Vineeth N B (IIT-H)_

§9.2

*Page 3 of 25*
```

# DL4CV_Week09_Part02.pdf - Page 14

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{., \leq k} \Sigma_{k,k} \]

\[ = (XX^T)(XX^T)^{-1}U_{., \leq k} \Sigma_{k,k} \]

\[ = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,k} \]

![NPTEL Logo](https://example.com/nptel_logo.png)

_Image Captioning_

Vineeth N B (IIT-H)

§9.2

3 / 25
```

# DL4CV_Week09_Part02.pdf - Page 15

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

$$
H = U_{:,\leq k} \Sigma_{k,k}
$$

$$
= (XX^T)(XX^T)^{-1}U_{:,\leq k} \Sigma_{k,k}
$$

$$
= (XV\Sigma^T U^T)(U\Sigma V^T V\Sigma^T U^T)^{-1}U_{:,\leq k} \Sigma_{k,k}
$$

$$
= XV\Sigma^T U^T(U\Sigma \Sigma^T U^T)^{-1}U_{:,\leq k} \Sigma_{k,k}
$$

$$
= XV\Sigma^T U^T U(\Sigma \Sigma^T)^{-1} U^T U_{:,\leq k} \Sigma_{k,k}
$$

$$
= XV\Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{:,\leq k} \Sigma_{k,k}
$$

## Vineeth N B (IIT-H)
![NPTEL](https://example.com/nptel_logo.png)
### §9.2 Image Captioning

3 / 25
```

# DL4CV_Week09_Part02.pdf - Page 16



```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[
H = U_{.,\leq k} \Sigma_{k,k}
\]

\[
= (XX^T)(XX^T)^{-1}U_{.,\leq k} \Sigma_{k,k}
\]

\[
= (XV\Sigma^T U^T)(U\Sigma V^T V\Sigma^T U^T)^{-1}U_{.,\leq k} \Sigma_{k,k}
\]

\[
= XV\Sigma^T U^T (U\Sigma^T U^T)^{-1}U_{.,\leq k} \Sigma_{k,k}
\]

\[
= XV\Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{.,\leq k} \Sigma_{k,k}
\]

\[
= XV\Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{.,\leq k} \Sigma_{k,k}
\]

\[
= XV\Sigma^T (\Sigma^T)^{-1} \Sigma^{-1} U^T U_{.,\leq k} \Sigma_{k,k}
\]

![Embedded Image Placeholder](image_url_here)

---

Vineeth N B (IIT-H)

§9.2 Image Captioning

3 / 25
```
```

# DL4CV_Week09_Part02.pdf - Page 17

 it as a markdown file.

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

$$
H = U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= (XX^T)(XX^T)^{-1} U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= (XV^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= XV^T U^T (U \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= XV^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= XV^T \Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= XV^T \Sigma^T (\Sigma^{-1}) \Sigma^{-1} U^T U_{., \leq k} \Sigma_{k,\cdot k}
$$

$$
= XV \Sigma^{-1} I_{., \leq k} \Sigma_{k,\cdot k}
$$

*Image captioning by Image Captioning*

![Telkom University Logo](image-url)

Vineeth N B (IIIT-H)

§9.2 Image Captioning

3 / 25
```

Replace `image-url` with the actual URL or filename of the image from the slide if available.
```

# DL4CV_Week09_Part02.pdf - Page 18

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:
  \[
  H = U_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = (XX^T)(XX^T)^{-1}U_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = (XV^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = XV^T U^T (U \Sigma V^T V \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = XV^T U^T (U \Sigma^T U^T)^{-1} U_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = XV^T (\Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = XV \Sigma^{-1} I_{., \leq k} \Sigma_{k, k}
  \]

  \[
  H = XV I_{., \leq k} = XV_{., \leq k}
  \]

![Diagram Placeholder](https://via.placeholder.com/150)

Vineeth N B (IIT-H)

§9.2 Image Captioning

3 / 25
```

# DL4CV_Week09_Part02.pdf - Page 19

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

$$
H = U_{., \leq k} \Sigma_{k,k}
$$

$$
= (XX^T)(XX^T)^{-1}U_{., \leq k} \Sigma_{k,k}
$$

$$
= (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1}U_{., \leq k} \Sigma_{k,k}
$$

$$
= XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1}U_{., \leq k} \Sigma_{k,k}
$$

$$
= XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,k}
$$

$$
= XV \Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,k}
$$

$$
= XV \Sigma^{-1} I_{., \leq k} \Sigma_{k,k}
$$

$$
= XV I_{., \leq k} = XV_{., \leq k}
$$

- $H$ is a linear transformation of $X$ and $W = V_{., \leq k}!$

![Vineeth N B (IIT-H)](https://example.com/vineeth_logo.png)  
![S9.2 Image Captioning](https://example.com/s92_image.png)

Vineeth N B (IIT-H)
S9.2 Image Captioning
3 / 25
```

# DL4CV_Week09_Part02.pdf - Page 20

 for any special symbols if required.

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{.,\leq k} \Sigma_{k,k} \]

\[ = (XX^T)(XX^T)^{-1}U_{.,\leq k} \Sigma_{k,k} \]

\[ = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1}U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1}U^T U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^{-1} I_{.,\leq k} \Sigma_{k,k} \]

\[ = XV I_{.,\leq k} = XV_{.,\leq k} \]

- From SVD, we know \( V \) is matrix of eigenvectors of \( X^T X \), the covariance matrix

- \( H \) is a linear transformation of \( X \) and \( W = V_{.,\leq k} ! \)

Vineeth N B (IIIT-H)

Image Captioning
```

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{.,\leq k} \Sigma_{k,k} \]

\[ = (XX^T)(XX^T)^{-1}U_{.,\leq k} \Sigma_{k,k} \]

\[ = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1}U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1}U^T U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{.,\leq k} \Sigma_{k,k} \]

\[ = XV \Sigma^{-1} I_{.,\leq k} \Sigma_{k,k} \]

\[ = XV I_{.,\leq k} = XV_{.,\leq k} \]

- From SVD, we know \( V \) is matrix of eigenvectors of \( X^T X \), the covariance matrix

- \( H \) is a linear transformation of \( X \) and \( W = V_{.,\leq k} ! \)

Vineeth N B (IIIT-H)

Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 21

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

  \[
  H = U_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = (XX^T)(XX^T)^{-1}U_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1}U_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1}U_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^T (\Sigma^T)^{-1} U^T U_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^{-1} I_{., \leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^{-1} I_{., \leq k} = XV_{., \leq k}
  \]

  From SVD, we know \( V \) is matrix of eigenvectors of \( X^T X \), the covariance matrix

  - The encoder weights are eigenvectors of covariance matrix, what does this tell you?

  \( H \) is a linear transformation of \( X \) and \( W = V_{., \leq k} ! \)

*Vineeth N B (IIT-H) §9.2 Image Captioning*

![Image Captioning](image_captioning.png)

3 / 25
```

# DL4CV_Week09_Part02.pdf - Page 22

 correctly captured.

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

  \[
  H = U_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = (XX^T)(XX^T)^{-1}U_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1}U_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1}U^T U_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^T U^T (U \Sigma \Sigma^T)^{-1} U^T U_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = XV \Sigma^{-1} I_{.,\\leq k} \Sigma_{k,k}
  \]

  \[
  = XV I_{.,\\leq k} = XV_{.,\\leq k}
  \]

  - \( H \) is a linear transformation of \( X \) and \( W = V_{.,\\leq k} \)

- From SVD, we know \( V \) is matrix of eigenvectors of \( X^T X \), the covariance matrix
- The encoder weights are eigenvectors of covariance matrix, what does this tell you?
- This is PCA indeed! Is it so always?

![Image Captioning](https://via.placeholder.com/150)

Vineeth N B (IIIT-H)

§9.2 Image Captioning

3 / 25
```

```

# DL4CV_Week09_Part02.pdf - Page 23

```markdown
# Review: Are autoencoders (AE) and PCA connected?

- Then, we have:

\[ H = U_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = (XX^T)(XX^T)^{-1} U_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = (XV \Sigma^T U^T)(U \Sigma V^T V \Sigma^T U^T)^{-1} U_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = XV \Sigma^T U^T (U \Sigma \Sigma^T U^T)^{-1} U^T U_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = XV \Sigma^T U^T U (\Sigma \Sigma^T)^{-1} U^T U_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = XV \Sigma^T (\Sigma \Sigma^T)^{-1} U^T U_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = XV \Sigma^{-1} I_{\cdot, \leq k} \Sigma_{k, k} \]

\[ = XV \Sigma_{k, k} \]

\[ = XV \]

\[ H \text{ is a linear transformation of } X \text{ and } W = V_{\cdot, \leq k}! \]

![Image](https://example.com/image.png)

- From SVD, we know \( V \) is matrix of eigenvectors of \( X^T X \), the covariance matrix

- The encoder weights are eigenvectors of covariance matrix, what does this tell you?

- This is PCA indeed! Is it so always?

- No, when encoder and decoder are linear; inputs are normalized dimension-wise, as we saw; and we use MSE as loss function

*Vineeth N B (IIIT-H)*

§9.2 Image Captioning

3 / 25
```

# DL4CV_Week09_Part02.pdf - Page 24

 

```markdown
# Describe these images

![Dog Looking Out of Window](image_url)

Vineeth N B (IIITH) §9.2 Image Captioning 4 / 25
```

# DL4CV_Week09_Part02.pdf - Page 25


```markdown
# Describe these images

![Image 1](image1.png) ![Image 2](image2.png)

**Vineeth N B. (IIT-H)**

**9.2 Image Captioning**

Page 4 / 25
```

# DL4CV_Week09_Part02.pdf - Page 26


```markdown
# Describe these images

![Image 1](image1_url)
![Image 2](image2_url)
![Image 3](image3_url)

**Vineeth N B (IIIT-H)**

**§9.2 Image Captioning**

4 / 25
```

# DL4CV_Week09_Part02.pdf - Page 27

```markdown
# Describe these images

- How can we understand what is happening just by looking at a single image?
- Can we make a computer do the same?

![Image of a dog looking out of a window](image1.png)
![Image of a dog looking at something outside](image2.png)
![Close-up image of a dog's face](image3.png)
![Dog standing in the snow](image4.png)
![Close-up of a dog's face](image5.png)

Vineeth N B (IIT-H)

§9.2 Image Captioning

5 / 25
```

# DL4CV_Week09_Part02.pdf - Page 28

 the provided image content:

```markdown
# Describe these images

![Image](image1.png)
![Image](image2.png)
![Image](image3.png)
![Image](image4.png)

- How can we understand what is happening just by looking at a single image?
- Can we make a computer do the same?

*Vineeth N B (IIIT-H)*
*Section 9.2 Image Captioning*
*Page 5 / 25*
```

(Note: Replace `image1.png`, `image2.png`, `image3.png`, and `image4.png` with the actual image placeholders as needed.)

```

# DL4CV_Week09_Part02.pdf - Page 29

:

# How to make a computer describe an image?

![Image](image.png)

**Some Method**

Vineeth N B (IIT-H)

## 9.2 Image Captioning

NPTEL

---

```markdown
# How to make a computer describe an image?

![Image](image.png)

**Some Method**

Vineeth N B (IIT-H)

## 9.2 Image Captioning

NPTEL

---

Page 6 of 25
```

# DL4CV_Week09_Part02.pdf - Page 30

Here's the detailed markdown format for the provided slide:

```markdown
# How to make a computer describe an image?

![Image of a dog](image_url)

![Some Method](image_url)

![Image of a cat](image_url)

*Vineeth N B (IIT-H)*

## Section 9.2 Image Captioning

![NPTEL Logo](image_url)

*Slide 6 of 25*
```

### Explanation:

1. **Title**: The main title "How to make a computer describe an image?" is formatted with the `#` header.
2. **Images**: Placeholders `![Image of a dog](image_url)`, `![Some Method](image_url)`, and `![Image of a cat](image_url)` are used for images that need to be added manually.
3. **Author**: The author's name is formatted as a subheading using `##`.
4. **Section**: The section title "Section 9.2 Image Captioning" is formatted using `##`.
5. **Logos and Additional Images**: The NPTEL logo is included as an image placeholder.
6. **Footer**: The footer information indicating the slide number is formatted with `*Slide 6 of 25*`.

# DL4CV_Week09_Part02.pdf - Page 31

```markdown
# How to make a computer describe an image?

![Image of a dog holding tennis balls](image1.png)
- **Description**: Dog is holding tennis balls with its mouth.

![Image of a cat sleeping](image2.png)
- **Description**: Cat is sleeping.

Some Method

![NPTEL Logo](nptel_logo.png)

Vineeth N B (IIT-H)

§9.2 Image Captioning
```

Each image is described with a brief caption, and the source is attributed to NPTEL. The sections and images are clearly labeled to ensure readability and understanding of the content.

# DL4CV_Week09_Part02.pdf - Page 32

 is not relevant for this task.

# How to make a computer describe an image?

## Image Captioning

### Vineeth N B (IIT-H)

### §9.2 Image Captioning

#### Slide 6/25

1. **Image 1**:
   - **Content**: A dog holding tennis balls with its mouth.
   - **Description**: "Dog is holding tennis balls with its mouth."

2. **Image 2**:
   - **Content**: A cat sleeping.
   - **Description**: "Cat is sleeping."

### Method

- **Process Flow**:
  - **Input**: Images of various subjects.
  - **Output**: Descriptive captions for each image.
  - **Intermediate Step**: Utilization of "Some Method" for processing and describing the images.

```flow
graph TD
    A[Image 1] -->|"Dog is holding tennis balls with its mouth."| B["Some Method"]
    C[Image 2] -->|"Cat is sleeping."| B["Some Method"]
    B -->|Descriptions| D[Captions]
```

### Elements

- **Images**: Encapsulated with red borders for identification.
- **Descriptions**: Provided as text boxes linked to corresponding images.

### Visual Aids

- **Wheel Diagram**: Located to the right of the slide, representing a possible process flow or conceptual model.

**Note**: The specific visual content and diagrams are represented as placeholders and should be interpreted based on the context provided.

---

This markdown format maintains the structure and content of the original scientific slide, ensuring accurate and readable representation.

# DL4CV_Week09_Part02.pdf - Page 33

 it to markdown.

```markdown
# How to make a computer describe an image?

![Image](image-url)

- **Dog is holding tennis balls with its mouth.**
  ![Dog Image](dog-image-url)

- **Some Method**
  ![Some Method Image](some-method-image-url)

- **Cat is sleeping.**
  ![Cat Image](cat-image-url)

*Vineeth N B (IIIT-H)*

*§9.2 Image Captioning*

*Page 6 of 25*
```

Ensure the above markdown is formatted in a way that accurately represents the slide's content and maintains a high level of scientific integrity.

# DL4CV_Week09_Part02.pdf - Page 34

 and format it accordingly.

```markdown
# How to make a computer describe an image?

![Image Description](image-url)

## Vineeth N B (IIIT-H)

### §9.2 Image Captioning

#### Dog is looking outside the window.

- **Dog is holding tennis balls with its mouth.**
  ![Dog with tennis balls](dog-tennis-balls-url)

- **Dog is looking outside the window.**
  ![Dog looking out window](dog-window-url)

- **Cat is sleeping.**
  ![Sleeping cat](sleeping-cat-url)

---

Some Method

![Some Method Diagram](method-diagram-url)

## Example Outputs

1. ![Dog with tennis balls](dog-tennis-balls-url)
   - **Description:** Dog is holding tennis balls with its mouth.

2. ![Dog looking out window](dog-window-url)
   - **Description:** Dog is looking outside the window.

3. ![Sleeping cat](sleeping-cat-url)
   - **Description:** Cat is sleeping.

---

**Note:**
- The accuracy of the descriptions depends on the method used.
- Different methods may yield varying levels of detail and correctness in the descriptions.

```

# DL4CV_Week09_Part02.pdf - Page 35

```markdown
# How to make a computer describe an image?

![Image of a dog holding tennis balls](image-url)

- **Dog is holding tennis balls with its mouth.**

![Image of a cat sleeping](image-url)

- **Cat is sleeping.**

## Some Method

![Diagram of some method](diagram-url)

---

**Vineeth N B (IIT-H)**

**§9.2 Image Captioning**

---

_NPIEL_

---

Page 6 / 25
```

# DL4CV_Week09_Part02.pdf - Page 36

 is an important factor.

```markdown
# How to make a computer describe an image?

## Slide Content

### Introduction
**Vineeth N B (IIT-H)**

### Slide Number
**§9.2 Image Captioning**

### Visual Content
#### Image Descriptions
1. **First Image**
   - **Description**: "Dog is holding tennis balls with its mouth."
   - **Process**:
     - **Computer Vision**: Analyzes the image.
     - **Some Method**: Processes the analysis to generate the description.

2. **Second Image**
   - **Description**: "Cat is sleeping."
   - **Process**:
     - **Computer Vision**: Analyzes the image.
     - **Some Method**: Processes the analysis to generate the description.

### Diagram
- **Flowchart**:
  - **Start**: Image of a dog holding tennis balls in its mouth.
  - **Process**: Computer Vision analysis.
  - **Output**: Description "Dog is holding tennis balls with its mouth."
  - **Flow**: From the output back to "Some Method".
  - **Next Image**: Image of a sleeping cat.
  - **Process**: Computer Vision analysis.
  - **Output**: Description "Cat is sleeping."
  - **Flow**: From the output back to "Some Method".

### Logos and Branding
- **NPTEL Logo**: Centrally placed within the diagram.
```

# DL4CV_Week09_Part02.pdf - Page 37

```markdown
# How to make a computer describe an image?

## Diagram Overview

### Components

1. **Image Inputs**
   - **Dog Image**: 
     ![Dog Image](data:image/png;base64,...) 
     - Description: "Dog is holding tennis balls with its mouth."
   - **Cat Image**: 
     ![Cat Image](data:image/png;base64,...) 
     - Description: "Cat is sleeping."

2. **Computer Vision**
   - Processes the images to extract visual information.
   - Outputs are passed to Natural Language Processing.

3. **Natural Language Processing**
   - Transforms visual information into descriptive text.
   - Produces human-readable descriptions of the images.

4. **Some Method**
   - Likely refers to the specific approach or algorithm used to integrate computer vision and natural language processing.

### Process Flow
1. **Dog Image**:
   - Computer Vision processes the image.
   - Output: "Dog is holding tennis balls with its mouth."
   - Natural Language Processing translates the processed data into text.

2. **Cat Image**:
   - Computer Vision processes the image.
   - Output: "Cat is sleeping."
   - Natural Language Processing translates the processed data into text.

### Methodology Integration
   - The process involves a method that integrates computer vision and natural language processing to describe images accurately.

### Citation
- **Author**: Vineeth N B (IIT-H)
- **Section**: §9.2 Image Captioning

---

_Note_: The placeholders `![Dog Image](data:image/png;base64,...)`, `![Cat Image](data:image/png;base64,...)`, and other similar placeholders are used to represent images. Replace these with proper image URLs or embeds if available.

```

# DL4CV_Week09_Part02.pdf - Page 38

 large blocks of text and images are kept intact.

```markdown
# How to make a computer describe an image?

## Image Captioning

![Vineeth N B (IIT-H) & Image Captioning](attachment:image_captioning.png)

### Steps for Image Description Using Natural Language Processing

1. **Input Image**
   - Example: **Dog is holding tennis balls with its mouth.**
   - ![Dog Image](attachment:dog_image.png)

2. **Natural Language Processing**
   - The image is processed to interpret and generate a textual description.

3. **Output**
   - Example: **Cat is sleeping.**
   - ![Cat Image](attachment:cat_image.png)

4. **Some Method**
   - The specific method used in generating the image description.

### Flowchart of Image Captioning Process

```plaintext
[Input Image] --> [Natural Language Processing] --> [Output Description]
```

### Example Outputs

- **Dog Image Description:**
  - **Input:** Image of a dog holding a tennis ball in its mouth.
  - **Output:** "Dog is holding tennis balls with its mouth."

- **Cat Image Description:**
  - **Input:** Image of a cat sleeping.
  - **Output:** "Cat is sleeping."

### Additional Information

- **Author:** Vineeth N B
- **Affiliation:** IIT-H (Indian Institute of Technology Hyderabad)
- **Section:** §9.2 Image Captioning

### References
- Note: The references and further details about the "Some Method" used in image captioning can be found in the presentation.

```

# DL4CV_Week09_Part02.pdf - Page 39

 doesn't support inline math notation (for example, `x_{i} \sim \mathcal{N}(0, 1)`), it is recommended to use LaTeX or MathJax for rendering.

Here is the extracted markdown content:

```markdown
# How to make a computer describe an image?

![Image](image-url)

## Vineeth N B (IIT-H)

### §9.2 Image Captioning

![Dog Image](dog-image-url)
- **Dog is holding tennis balls with its mouth.**

![Neural Network Diagram](neural-network-diagram-url)
- Some Method

![Cat Image](cat-image-url)
- **Cat is sleeping.**

```

# DL4CV_Week09_Part02.pdf - Page 40

 this markdown format.

# How to make a computer describe an image?

## Vineeth N B (IIT-H)

### §9.2 Image Captioning

- **Image**: ![](image-url)

  - **Description**: 
    - **Dog is holding tennis balls with its mouth.**
    - **Cat is sleeping.**

- **Method**: 
  1. **Input Image**: 
     - **Image of Dog**: ![](dog-image-url)
     - **Image of Cat**: ![](cat-image-url)
  2. **Processing**: 
     ![](processing-image-url)
  3. **Output**: 
     - **Description Generation**: Neural network processes image data and generates captions.
     - **Example Output**: 
       - **Dog Image Caption**: "Dog is holding tennis balls with its mouth."
       - **Cat Image Caption**: "Cat is sleeping."

- **Network Architecture**: 
  - **Input**: Image data.
  - **Intermediate Steps**: 
    - Convolutional layers for feature extraction.
    - Recurrent layers (e.g., LSTM or GRU) for sequence generation.
  - **Output**: Textual descriptions of the image.

- **Example Diagram**: 
  ![](diagram-image-url)

  - **Notes**: 
    - **Input Layer**: Receives image data.
    - **Hidden Layers**: Process image features and generate captions.
    - **Output Layer**: Produces the final textual description.

- **Steps involved**:
  1. **Feature Extraction**: Use convolutional neural networks (CNN) to extract features from the image.
  2. **Sequence Modeling**: Use recurrent neural networks (RNN) or transformers to model the sequence of words in the caption.
  3. **Language Modeling**: Use language models to predict the next word in the sequence.
  4. **Training**: Train the model using image-caption pairs to minimize the difference between predicted and actual captions.

- **Conclusion**:
  - This method demonstrates how a computer can generate descriptive captions for images using deep learning techniques.

---

This markdown format captures the detailed content from the provided scientific slides, ensuring accuracy and proper formatting of scientific terms and elements.

# DL4CV_Week09_Part02.pdf - Page 41

 the content from the image is as follows:

---

# Image Captioning: Training

![Image Not Available](https://via.placeholder.com/150)

"straw hat"

_NPTEL_

Karpathy et al, Deep visual-semantic alignments for generating image descriptions, CVPR 2015

Vineeth N B (IIT-H)

Section: 9.2 Image Captioning

---

```markdown
# Image Captioning: Training

![Image Not Available](https://via.placeholder.com/150)

"straw hat"

_NPTEL_

**Credit:** Karpathy et al, *Deep visual-semantic alignments for generating image descriptions*, CVPR 2015

**Author:** Vineeth N B (IIT-H)

**Section:** 9.2 Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 42

```markdown
# Image Captioning: Training

![Image Captioning](https://via.placeholder.com/150 "Image Captioning")

## Image Captioning Process

### Input
- **Image**: Original image to be captioned.

### Network Architecture
1. **Convolutional Layers (conv)**
   - conv-64
   - conv-64
   - maxpool
   - conv-128
   - conv-128
   - maxpool
   - conv-256
   - conv-256
   - maxpool
   - conv-512
   - conv-512
   - maxpool
   - conv-512
   - conv-512
   - maxpool
2. **Fully Connected Layers (FC)**
   - FC-4096
   - FC-4096
   - FC-1000
3. **Output Layer**
   - softmax

### Output
- **Caption**: "straw hat"

**Credit**: Karpathy et al., *Deep visual-semantic alignments for generating image descriptions*, CVPR 2015

**Author**: Vineeth N B (IIIT-H)

**Section**: §9.2 Image Captioning

---

**Slide Number**: 7 / 25
```

# DL4CV_Week09_Part02.pdf - Page 43

 is not required for this particular image.

# Image Captioning: Training

![Image](https://via.placeholder.com/150) **image**

```markdown
- **conv-64**
- **conv-64**
- **maxpool**
- **conv-128**
- **conv-128**
- **maxpool**
- **conv-256**
- **conv-256**
- **maxpool**
- **conv-512**
- **conv-512**
- **maxpool**
- **conv-512**
- **conv-512**
- **maxpool**
- **FC-4096**
- **FC-4096**
- **FC-1000**
- **softmax**
```

```markdown
| y0   | y1   | y2   |
|------|------|------|
| h0   | h1   | h2   |
| s0   | x1   | x2   |          
```

```markdown
- `"straw hat"`
```

![Diagram](https://via.placeholder.com/150)

**Credit**: Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

*Vineeth N B (IIIT-H)*

**§9.2 Image Captioning**

7 / 25

# DL4CV_Week09_Part02.pdf - Page 44

 text for the image captioning process.

```markdown
# Image Captioning: Training

## Diagram

![Image Captioning Diagram](image_url)

### Components
- **Image Input**: The process begins with an image.
- **Convolutional Layers**: Multiple convolutional layers (e.g., `conv-64`, `conv-128`, `conv-256`, `conv-512`) are used to extract features from the image.
- **Max Pooling Layers**: Max pooling layers (`maxpool`) are used to reduce the spatial dimensions of the features.
- **Fully Connected Layers**: Fully connected layers (e.g., `FC-4096`) are used to generate feature maps.
- **Word Embedding**: An embedding layer (`<START>`, `straw`, `hat`, etc.) is used to convert words into vectors.
- **RNN Layers**: Recurrent neural network layers (`h0`, `h1`, `h2`) process the sequences and generate hidden states.
- **Output Layer**: The output layer generates the final captions (`y0`, `y1`, `y2`).

### Process
1. **Input Image**: An image of a person wearing a straw hat is input into the system.
2. **Feature Extraction**: The image is passed through multiple convolutional layers and max pooling layers to extract relevant features.
3. **Hidden States**: The extracted features are processed by the RNN layers to generate hidden states.
4. **Caption Generation**: The system generates a sequence of words (e.g., `<START>`, `straw`, `hat`) to describe the image.
5. **Output**: The final output is the caption `"straw hat"`, which describes the image.

### Credit
- **Reference**: Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015
- **Presented by**: Vineeth N B (IIIT-H)
- **Section**: §9.2 Image Captioning

```

# DL4CV_Week09_Part02.pdf - Page 45

```markdown
# Image Captioning: Training

![Image Captioning Diagram](https://via.placeholder.com/150)

- **Image**: Input image of a person wearing a straw hat.

- **Network Layers**:
    - conv-64
    - conv-64
    - maxpool
    - conv-128
    - conv-128
    - maxpool
    - conv-256
    - conv-256
    - maxpool
    - conv-512
    - conv-512
    - maxpool
    - conv-512
    - conv-512
    - maxpool
    - FC-4096
    - FC-4096
    - **Output**: 1000

- **Hidden States and Outputs**:
    - h0, h1, h2
    - y0, y1, y2

- **Attention Mechanism**:
    - `<START>`
    - s0: `<STA RT>`
    - x0: `<straw>`
    - x1: `<hat>`
    - x2: `<hat>`

## Diagram Description
- The image is processed through a series of convolutional layers (`conv`) and max pooling layers (`maxpool`).
- Fully connected layers (`FC-4096`) produce an output which is used to generate captions.
- Hidden states (h0, h1, h2) and attention mechanism (s0, x0, x1, x2) facilitate the caption generation process.
- The final output caption for the image of a person wearing a straw hat is "straw hat".

## Credit
- Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015
- Vineeth N B (IIT-H)
- §9.2 Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 46

```markdown
# Image Captioning: Training

## Diagram

**Image Input:**

```plaintext
image
- conv-64
  - conv-64
  - maxpool
- conv-128
  - conv-128
  - maxpool
- conv-256
  - conv-256
  - maxpool
- conv-512
  - conv-512
  - maxpool
  - conv-512
  - conv-512
  - maxpool
- FC-4096
  - FC-4096
  - LSTM-1024
```

**Processing Pipeline:**

```plaintext
h0 <- max(0, Wxh * x0)
```
- **y0**
- **y1**
- **y2**

**Before:**

```plaintext
h0 = max(0, Wxh * x0)
```

**Example:**

![]()

```plaintext
"straw hat"
```

**Credits:**

```plaintext
- Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015
- Vineeth N B (IIIT-H)
- §9.2 Image Captioning
```

```

# DL4CV_Week09_Part02.pdf - Page 47

```markdown
# Image Captioning: Training

![Image Captioning Diagram](image_url)

**Credit:** Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

Vineeth N B (IIT-H) §9.2 Image Captioning

## Before:
![Before](before_image_url)

**Equation:**
\[ h0 = \max(0, Wxh * x0) \]

## Now:
![Now](now_image_url)

**Equation:**
\[ h0 = \max(0, Wxh * x0 + Wih * y) \]

### Image Captioning Process
1. **Input Image**
   - Image processing through various layers such as `conv-64`, `conv-128`, `conv-256`, `conv-512`.
   - Max pooling layers interspersed between convolutional layers.
   - Fully connected layers (FC-4096) and final output layer (FC-1024) for classification.

2. **Hidden States (h0, h1, h2)**
   - Initial hidden state \( h0 \) is calculated using the input image and previous hidden state.
   - Subsequent hidden states \( h1 \) and \( h2 \) are derived from \( h0 \).

3. **Output States (y0, y1, y2)**
   - Output states generated from the hidden states, representing the intermediate and final captions.

4. **Caption Generation**
   - **START** token initiates the caption generation process.
   - Words "straw" and "hat" are generated sequentially based on the intermediate states.

### Example Caption
![Example Image](example_image_url)

**Caption:**
"straw hat"

```

This markdown format ensures that the scientific content, including equations, diagrams, and image references, is accurately represented. Adjust the image URLs as needed to point to the actual images from the original source.

# DL4CV_Week09_Part02.pdf - Page 48

 the markdown format will only include the text and images that are possible to extract from the image.

```markdown

# Image Captioning: Inference (Test Time)

![Image](https://via.placeholder.com/150)

**Credit**: Karpathy et al, *Deep visual-semantic alignments for generating image descriptions*, CVPR 2015

Vineeth N B (IIIT-H) §9.2 Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 49

```markdown
# Image Captioning: Inference (Test Time)

![Image](image_url)

- **Image**
- **conv-64**
- **conv-64**
- **maxpool**
- **conv-128**
- **conv-128**
- **maxpool**
- **conv-256**
- **conv-256**
- **maxpool**
- **conv-512**
- **conv-512**
- **maxpool**
- **conv-512**
- **conv-512**
- **maxpool**
- **FC-4096**
- **FC-4096**
- **Softmax**

*Credit: Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015*

Vineeth N B (IIT-H)

**Section**: §9.2 Image Captioning

---

![Diagram](diagram_url)

_NITEL_
```

**Note**: Placeholder image URLs (`image_url` and `diagram_url`) should be replaced with the actual image URLs if available.

# DL4CV_Week09_Part02.pdf - Page 50

 should be performed with high precision in numerical terms and special symbols.

```markdown
# Image Captioning: Inference (Test Time)

## Image and Model Architecture

### Image
![Image](image_url_here)

### Model Architecture
- **Layer 1:**
  - `conv-64`
  - `conv-64`
  - `maxpool`
- **Layer 2:**
  - `conv-128`
  - `conv-128`
  - `maxpool`
- **Layer 3:**
  - `conv-256`
  - `conv-256`
  - `maxpool`
- **Layer 4:**
  - `conv-512`
  - `conv-512`
  - `maxpool`
- **Layer 5:**
  - `conv-512`
  - `conv-512`
  - `maxpool`
- **Layer 6:**
  - `FC-4096`
  - `FC-4096`
- **Output Layer:**
  - `FC-1000`
  - `softmax`

## Diagram Elements

![Diagram](diagram_url_here)

- **START**
- `<SOS>`
- `<EOS>`
- `<STA>`
- `<RT>`

## Credit

**Credit**: Karpathy et al., *Deep visual-semantic alignments for generating image descriptions*, CVPR 2015

**Author**: Vineeth N B (IIIT-H)

**Section**: §9.2 Image Captioning
```


# DL4CV_Week09_Part02.pdf - Page 51

```markdown
# Image Captioning: Inference (Test Time)

![Image](image.png)

- **Image**
  - `conv-64`
  - `conv-64`
  - `maxpool`
  - `conv-128`
  - `conv-128`
  - `maxpool`
  - `conv-256`
  - `conv-256`
  - `maxpool`
  - `conv-512`
  - `conv-512`
  - `maxpool`
  - `conv-512`
  - `conv-512`
  - `maxpool`
  - `FC-4096`
  - `FC-4096`
  - `FC-1024`

- **Start**

  ```plaintext
  y0
  h0
  ```
  <STA>RT>

**Credit:** Karpathy et al, Deep visual-semantic alignments for generating image descriptions, CVPR 2015

_Vineeth N B (IIIT-H)_

## Slide Information
- **Section**: 9.2 Image Captioning
- **Slide Number**: 8 / 25
```

# DL4CV_Week09_Part02.pdf - Page 52

```markdown
# Image Captioning: Inference (Test Time)

![Image Captioning](image_captioning.png)

**Credit:** Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

Vineeth N B (IIIT-H)

---

## Image Captioning

### Inference (Test Time)

![Image](image.png)

- **Image**: Input image.
- **conv-64**: Convolutional layer with 64 filters.
- **maxpool**: Max pooling layer.
- **conv-128**: Convolutional layer with 128 filters.
- **conv-256**: Convolutional layer with 256 filters.
- **conv-512**: Convolutional layer with 512 filters.
- **FC-4096**: Fully connected layer with 4096 neurons.
- **FC-1000**: Fully connected layer with 1000 neurons.

**Flow Diagram of Inference Process:**
1. **Image**: Input image is fed into the network.
2. **Convolutional Layers**: The image passes through multiple convolutional layers (conv-64, conv-128, conv-256, conv-512) interspersed with max pooling layers.
3. **Fully Connected Layers**: After passing through the convolutional layers, the image features are flattened and passed through fully connected layers (FC-4096 and FC-1000).
4. **h0**: The output from the final fully connected layer is represented as h0.
5. **y0**: The output h0 is further processed to generate the caption y0.
6. **START**: The process begins with a `<START>` token.
7. **x1**: The generated caption sequence, starting with `<START>` token.

---

**Note:** The image on the right side provides a visual example of an input image that the captioning model processes.

```

# DL4CV_Week09_Part02.pdf - Page 53

: 

```markdown
# Image Captioning: Inference (Test Time)

![Image Captioning Process](image_url_placeholder)

- **Image**: Input image.
- **conv-64**: Convolutional layer with 64 filters.
- **maxpool**: Max pooling layer.
- **conv-128**: Convolutional layer with 128 filters.
- **conv-256**: Convolutional layer with 256 filters.
- **conv-512**: Convolutional layer with 512 filters.
- **FC-4096**: Fully connected layer with 4096 neurons.
- **FC-1024**: Fully connected layer with 1024 neurons.
  
```
- **y0**: Initial hidden state.
- **h0**: Output from the recurrent layer.
- **<START>**: Special token indicating the start of the sequence.
- **<STA RT>**: Special token indicating the start of the sequence.
- **x1**: Output from the language model.

![Example Image](image_url_placeholder)

**Credit**: Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

Vineeth N B (IIT-H)

Section: 9.2 Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 54

```markdown
# Image Captioning: Inference (Test Time)

![Image Captioning Diagram](image.jpg)

- **Credit:** Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

## Steps in Image Captioning Process

1. **Input Image**: An image is provided as input.
   
   ![Input Image](image.jpg)

2. **Convolutional Layers**:
   - `conv-64`
   - `conv-64`
   - `maxpool`
   - `conv-128`
   - `conv-128`
   - `maxpool`
   - `conv-256`
   - `conv-256`
   - `maxpool`
   - `conv-512`
   - `conv-512`
   - `maxpool`
   - `conv-512`
   - `conv-512`
   - `maxpool`

3. **Fully Connected Layers**:
   - `FC-4096`
   - `FC-4096`

4. **Output Layer**:
   - `FC-1000`

## Inference Process

1. **Start**:
   - The process begins with initializing the states and preparing the model for inference.

2. **Hidden States**:
   - `h0`: Initial hidden state.
   - `h1`: Updated hidden state after processing the first input.

3. **Output States**:
   - `y0`: Output at the beginning of the process.
   - `y1`: Output after processing the input.

4. **Processing**:
   - The image is processed through the network, and the hidden states are updated.
   - The output states `y0` and `y1` are generated based on the hidden states `h0` and `h1`.

## Example of Captioning Process

- **Image Input**: A person wearing a straw hat standing in front of the sea.

- **Processed Output**: The model generates a caption like "A person in a straw hat standing by the sea."

---

*Vineeth N B (IIT-H)*
*Section 9.2 Image Captioning*

*Slide 8/25*
```

# DL4CV_Week09_Part02.pdf - Page 55

 text as markdown.

```markdown
# Image Captioning: Inference (Test Time)

![Image Captioning Process](image_captioning_process.png)

**Credit:** Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

## Image Captioning Process

### Input
- **Image**
  - ![Input Image](input_image.png)

### Convolutional Neural Network (CNN) Layers
1. **conv-64**
   - *Convolutional layer with 64 filters*
2. **conv-64**
   - *Convolutional layer with 64 filters*
3. **maxpool**
   - *Max Pooling Layer*
4. **conv-128**
   - *Convolutional layer with 128 filters*
5. **conv-128**
   - *Convolutional layer with 128 filters*
6. **maxpool**
   - *Max Pooling Layer*
7. **conv-256**
   - *Convolutional layer with 256 filters*
8. **conv-256**
   - *Convolutional layer with 256 filters*
9. **maxpool**
   - *Max Pooling Layer*
10. **conv-512**
    - *Convolutional layer with 512 filters*
11. **conv-512**
    - *Convolutional layer with 512 filters*
12. **maxpool**
    - *Max Pooling Layer*
13. **conv-512**
    - *Convolutional layer with 512 filters*
14. **conv-512**
    - *Convolutional layer with 512 filters*
15. **maxpool**
    - *Max Pooling Layer*

### Fully Connected Layers
16. **FC-4096**
    - *Fully Connected Layer with 4096 neurons*
17. **FC-4096**
    - *Fully Connected Layer with 4096 neurons*

### Recurrent Neural Network (RNN) Layers
18. **FC-1024**
    - *Fully Connected Layer with 1024 neurons*

### Hidden States (h)
- **h0**
  - *Initial hidden state*
- **h1**
  - *Hidden state 1*
- **h2**
  - *Hidden state 2*

### Outputs (y)
- **y0**
  - *Output 0*
- **y1**
  - *Output 1*
- **y2**
  - *Output 2*

### Process Flow
1. Start
2. Input image is passed through CNN layers for feature extraction.
3. Extracted features are fed into fully connected layers.
4. The output from the fully connected layers is passed to the RNN layers.
5. Hidden states (h0, h1, h2) are updated.
6. The RNN outputs (y0, y1, y2) are generated.

### Example Image
![Example Image](example_image.png)

### Reference
- **Vineeth N B (IIT-H)**
- **§9.2 Image Captioning**
```

# DL4CV_Week09_Part02.pdf - Page 56

 accuracy is the top priority.

```markdown
# Image Captioning: Inference (Test Time)

![Image Captioning Diagram](image-url)

## Image Captioning Process

### Input
- **Image**: The initial input is an image to be captioned.

### Processing Layers
1. **Convolutional Layers**:
   - `conv-64`
   - `conv-64`
   - `maxpool`
   - `conv-128`
   - `conv-128`
   - `maxpool`
   - `conv-256`
   - `conv-256`
   - `maxpool`
   - `conv-512`
   - `conv-512`
   - `maxpool`
   - `conv-512`
   - `conv-512`
   - `maxpool`

2. **Fully Connected Layers**:
   - `FC-4096`
   - `FC-4096`

3. **Output Layer**:
   - `FC-1024`

### Hidden States and Outputs
- **Hidden States (h)**:
  - `h0`
  - `h1`
  - `h2`

- **Outputs (y)**:
  - `y0`
  - `y1`
  - `y2`

### Special Tokens
- `<START>`
- `<END>`
- `<STA RT>`
- `<UNK>`

### Image Example
![Example Image](image-example-url)

### Credit
- **Authors**: Karpathy et al.
- **Publication**: Deep visual-semantic alignments for generating image descriptions, CVPR 2015
- **Institute**: Vineeth N B, (IIIT-H)

### Section
- **Slide Number**: 8 / 25
- **Topic**: Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 57

```markdown
# Image Captioning: Inference (Test Time)

![Image Captioning Diagram](image-url)

**Credit:** Karpathy et al., Deep visual-semantic alignments for generating image descriptions, CVPR 2015

Vineeth N B (IIT-H)

### Image Captioning

- **Image**
  - **conv-64**
  - **conv-64**
  - **maxpool**
  - **conv-128**
  - **conv-128**
  - **maxpool**
  - **conv-256**
  - **conv-256**
  - **maxpool**
  - **conv-512**
  - **conv-512**
  - **maxpool**
  - **conv-512**
  - **conv-512**
  - **maxpool**

  ```markdown
  - **FC-4096**
  - **FC-4096**
  ```

  ![Hidden States](hidden-states-image)

- **Hidden States**
  - **h0**
  - **h1**
  - **h2**

  ```markdown
  - **y0**
  - **y1**
  - **y2**
  ```

  ![Output States](output-states-image)

- **Output**
  - **y0**
  - **y1**
  - **y2**

  ```markdown
  - **END**
  ```

- **START**

  ```markdown
  - **<START>**
  - **straw**
  - **hat**
  ```

  ![Example Image](example-image)

### Flow
1. An image is processed through multiple convolutional layers (`conv-64`, `conv-128`, `conv-256`, `conv-512`) and max-pooling layers.
2. The processed data is then passed through fully connected layers (`FC-4096`).
3. Hidden states (`h0`, `h1`, `h2`) are generated and used to predict the output states (`y0`, `y1`, `y2`).
4. The sequence ends with an `<END>` token.

### Diagram Elements
- **Conv Layers**: Perform convolution operations on the image.
- **Maxpool Layers**: Perform max pooling to reduce the spatial dimensions.
- **FC Layers**: Fully connected layers to generate feature vectors.
- **Hidden States**: Intermediate representations of the data.
- **Output States**: Predicted captions for the image.

### Example Image
![Example Image](example-image-url)

```markdown
- **START**
- **straw**
- **hat**
```
```

# DL4CV_Week09_Part02.pdf - Page 58

 the following:

```markdown
# Results

![First Image](image1.png)
- **Caption**: a group of people standing around a room with remotes
  - **logprob**: -9.17

![Second Image](image2.png)
- **Caption**: a young boy is holding a baseball bat
  - **logprob**: -7.61

![Third Image](image3.png)
- **Caption**: a cow is standing in the middle of a street
  - **logprob**: -8.84

_Vineeth N B (IIIT-H)_

§9.2 Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 59

```markdown
# Results: Failure Cases

## Possible to understand why the method failed

![Failure Case 1](failure_case_1_image.png)
- **Caption**: a man standing next to a clock on a wall
- **logprob**: -10.08

![Failure Case 2](failure_case_2_image.png)
- **Caption**: a young boy is holding a baseball bat
- **logprob**: -7.65

![Failure Case 3](failure_case_3_image.png)
- **Caption**: a cat is sitting on a couch with a remote control
- **logprob**: -12.45

_Vineeth N B (IIIT-H)_

§9.2 Image Captioning

---

Ten / Twenty-Five
```

# DL4CV_Week09_Part02.pdf - Page 60

 the following content:

**Image URL**: https://doc-11-25-2023-0-36-g-1679566546694[image.png]()

**Extracted Text**:

![Results: Failure Cases](image.png)

- Not possible to understand why the method failed

![Image](https://doc-11-25-2023-0-36-g-1679566546694[image.png]()

- a **woman holding a teddy bear in front of a mirror**

  logprob: -9.63

- a **horse is standing in the middle of a road**

  logprob: -10.34

---

**Vineeth N B (IIT-H)**

§9.2 Image Captioning

**Page Number**: 11 / 25

---

```markdown
# Results: Failure Cases

## Not possible to understand why the method failed

![Failure Case Image 1](https://doc-11-25-2023-0-36-g-1679566546694[image.png]()

- **Description**: **a woman holding a teddy bear in front of a mirror**

  **Log Probability**: -9.63

![Failure Case Image 2](https://doc-11-25-2023-0-36-g-1679566546694[image.png]()

- **Description**: **a horse is standing in the middle of a road**

  **Log Probability**: -10.34

---

**Vineeth N B (IIT-H)**

**Section**: §9.2 Image Captioning

**Page Number**: 11 / 25
```


# DL4CV_Week09_Part02.pdf - Page 61

```markdown
# Results: Failure Cases

## Not possible to understand why the method failed

![Failure Case Image](image1.png)

## How can we mitigate these failures?

### Failure Case Examples

#### Image 1

![Failure Case Image 1](image2.png)

**Caption:** A woman holding a teddy bear in front of a mirror

**logprob:** -9.63

#### Image 2

![Failure Case Image 2](image3.png)

**Caption:** A horse is standing in the middle of a road

**logprob:** -10.34

---

_Vineeth N B (IIIT-H)_

§9.2 Image Captioning

_11 / 25_
```

# DL4CV_Week09_Part02.pdf - Page 62

```markdown
# Results: Failure Cases

**Not possible to understand why the method failed**

![Failure Case Image 1](image1.png)
![Failure Case Image 2](image2.png)

## How can we mitigate these failures?

### Image captioning with attention

![Captioned Image 1](captioned1.png)
![Captioned Image 2](captioned2.png)

- **Captioned Image 1**: `a woman holding a teddy bear in front of a mirror`
  - `logprob: -9.63`

- **Captioned Image 2**: `a horse is standing in the middle of a road`
  - `logprob: -10.34`

*Vineeth N B (IIIT-H)*

*Section 9.2 Image Captioning*

Page 11 / 25
```

# DL4CV_Week09_Part02.pdf - Page 63

 your output with markdown format.

```markdown
# Image Captioning with Attention

## 1. Input Image

![Input Image](image_url)

**Credit:** Xu et al, *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*, ICML 2015

*Vineeth N B (IIT-H)*

*§9.2 Image Captioning*
```

This markdown format ensures that the text is properly structured and formatted, maintaining the scientific integrity of the content. Adjust the image URL placeholder as needed.

# DL4CV_Week09_Part02.pdf - Page 64

 accuracy, and proper formatting.

```markdown
# Image Captioning with Attention

## Diagram and Process

![Diagram of Image Captioning Process](image-url)

1. **Input Image**
   - Example: ![Input Image](input-image-url)

2. **Convolutional Feature Extraction**
   - Example: ![Feature Map](feature-map-url)

### Process Description

- **14x14 Feature Map**: Indicates the dimensionality of the feature map generated after the convolutional process.
  
### Credit

- **Reference**: 
  - `Xu et al, "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention," ICML 2015`
  
- **Presented by**: 
  - Name: Vineeth N B
  - Affiliation: IIIT-H

- **Section**: 
  - Subsection: 9.2 Image Captioning

---

Page Number: 12 / 25
```

*Note*: Replace `image-url`, `input-image-url`, and `feature-map-url` with actual URLs or placeholders for the images if they cannot be directly captured.

# DL4CV_Week09_Part02.pdf - Page 65

 precision is paramount.

```markdown
# Image Captioning with Attention

![Image Captioning with Attention Diagram](image_url)

**Credit:** Xu et al, *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*, ICML 2015

Vineeth N B (IIT-H) §9.2 Image Captioning

## Steps in Image Captioning with Attention

1. **Input Image**
    - **Description**: The process begins with an input image.
    ![Input Image](image_url)

2. **Convolutional Feature Extraction**
    - **Description**: The input image is processed using convolutional feature extraction techniques to generate a 14x14 feature map.
    ![Convolutional Feature Extraction](image_url)

3. **RNN with Attention over the Image**
    - **Description**: A Recurrent Neural Network (RNN) with an attention mechanism is employed to focus on different parts of the image.
    - **Components**:
        - The feature map is divided into regions highlighted with different colored boxes.
        - An LSTM (Long Short-Term Memory) network is used to process the extracted features and generate the caption.
    ![RNN with Attention](image_url)
```

# DL4CV_Week09_Part02.pdf - Page 66

```markdown
# Image Captioning with Attention

## Steps in Image Captioning with Attention

1. **Input Image**
   ![Input Image](image_url)

2. **Convolutional Feature Extraction**

   - Extracts a 14x14 feature map from the input image.
   - This feature extraction is typically done using convolutional neural networks (CNNs).

3. **RNN with attention over the image**

   - Uses a Recurrent Neural Network (RNN) with attention mechanisms to focus on different parts of the image.
   - The attention mechanism helps in identifying important regions of the image for generating the caption.
   - Example of attention regions highlighted on the image:
     - ![Attention Region 1](image_url)
     - ![Attention Region 2](image_url)
     - ![Attention Region 3](image_url)

4. **Word by word generation**

   - The attention model generates a caption word by word using an LSTM (Long Short-Term Memory) network.
   - Example of generated caption: **A bird flying over a body of water**.
     - ![Highlighted Words](image_url)

## Credit

- **Xu et al, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015**
- **Vineeth N B (IIT-H)**

---

9.2 Image Captioning

---

Page numbers: 12 / 25
```

# DL4CV_Week09_Part02.pdf - Page 67

```markdown
# Image Captioning with Attention

![Image Captioning with Attention Diagram](image.jpg)

**Image:**
H x W x 3

**CNN**

**Features:**
L x D
Where L = W x H

![NPTEL Logo](nptel-logo.png)

**Credit:** Xu et al, *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015*

*Vineeth N B (IIT-H)*

**§9.2 Image Captioning**

13 / 25
```

# DL4CV_Week09_Part02.pdf - Page 68

```markdown
# Image Captioning with Attention

![Image Captioning with Attention](image_captioning_with_attention.png)

## Distribution over L locations

- **Image**: H x W x 3
- **CNN**: Convolutional Neural Network
- **Features**: L x D
- **v**: Vector
- **h₀**: Initial hidden state
- **a₁**: Attention distribution over locations

**Credit:** Xu et al, *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*, ICML 2015

Vineeth N B (IIT-H)

 Section: §9.2 Image Captioning
```

### Notes:

- This is a processed markdown file of the OCR output.
- The placeholder `![Image Captioning with Attention](image_captioning_with_attention.png)` should be replaced with the actual image link or file path if available.
- Ensure the accuracy of technical terms and symbols as per scientific notation standards.
- Proper formatting to ensure readability and maintain the scientific integrity of the content.

# DL4CV_Week09_Part02.pdf - Page 69



```markdown
# Image Captioning with Attention

![Image Captioning Diagram](image_url)

## Diagram Breakdown

### Components
- **Image:** \( H \times W \times 3 \)
- **CNN:** Convolutional Neural Network
  - Produces features: \( L \times D \)
  - Output: \( v \)
  - \( h_0 \): Initial hidden state
  - Distribution over \( L \) locations: \( a_1 \)
  - Weighted features: \( D \)
  - Weighted combination of features: \( z_i \)

### Equation
\[ z_1 = \sum_{i=1}^{L} a_i v_i \]

## Credit
- **Xu et al.**,
  - **Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015**
- **Vineeth N B** (IIT-H)
- **§9.2 Image Captioning**

```

# DL4CV_Week09_Part02.pdf - Page 70

 this is a markdown file.

```markdown
# Image Captioning with Attention

![Image Captioning with Attention](https://example.com/image-captioning-with-attention.png)

**Credit**: Xu et al, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015

Vineeth N B (IIT-H) §9.2 Image Captioning

## Image Captioning with Attention

### Process Overview

1. **Input Image**:
   - **Format**: H x W x 3
   - **Description**: The input image is processed to extract features.

2. **CNN (Convolutional Neural Network)**:
   - **Function**: Extracts features from the input image.
   - **Output Features**: L x D

3. **Feature Extraction**:
   - **Result**: A matrix of features `v` of size L x D.

4. **Feature Weighting**:
   - **Weighted Combination of Features**:
     - **Initial Hidden State**: \( h_0 \)
     - **Hidden State**: \( h_1 \)
     - **Weighted Features**: D-dimensional weight vector applied to features.

### Visual Attention Mechanism

1. **Distributions**:
   - **Distribution over L locations**: Determines which parts of the image to focus on.
   - **Distribution over vocabulary**: Determines the relevant words for the caption.

2. **Attention Weights**:
   - **\( a_1 \)**, \( a_2 \), etc.: Attention weights for different locations.
   - **\( d_1 \)**: Attention weight for vocabulary.

3. **Feature Integration**:
   - **Weighted Features**: \( z_1 \), \( y_1 \): Integrated features used for generating the caption.

### First Word Generation

- **Process**:
  - **Input**: Weighted features \( z_1 \), \( y_1 \).
  - **Output**: Select the first word of the caption based on the integrated features.

### Diagram Explanation

![Diagram Explanation](https://example.com/diagram-explanation.png)

- **Image**: Input image processed by CNN.
- **CNN**: Extracts features from the image.
- **Distributions**: Attention distributions over image locations and vocabulary.
- **Weighted Combination**: Features are weighted and combined.
- **First Word**: The first word of the caption is generated based on the integrated features.

### References

- **Paper**: Show, Attend and Tell: Neural Image Caption Generation with Visual Attention
- **Authors**: Xu et al.
- **Conference**: ICML 2015

---

_This markdown file accurately represents the OCR-processed scientific text from the provided slide, maintaining the formatting, content, and scientific integrity of the original source._ 
```

# DL4CV_Week09_Part02.pdf - Page 71

```markdown
# Image Captioning with Attention

![Image Captioning](image_captioning.png)

## Image Captioning

### Diagram Components

- **Image:**
  - Dimensions: H x W x 3

- **CNN (Convolutional Neural Network):**
  - Input: Image
  - Output: Features (L x D)

- **Features:**
  - Weighted features: D

### Weighted Combination of Features

- **h0, h1, h2:**
  - Initial hidden states

- **z1, y1, z2, y2:**
  - Intermediate hidden states

### Distributions

- **Distribution over locations:**
  - a1, a2

- **Distribution over vocabulary:**
  - d1

### Workflow

1. **Input Image:**
   - The image is fed into the CNN.

2. **Feature Extraction:**
   - The CNN extracts features of size L x D.

3. **Weighted Features:**
   - The features are combined in a weighted manner.

4. **Hidden States:**
   - Initial hidden state h0 is used.
   - Hidden states h1 and h2 are generated sequentially.

5. **Intermediate States:**
   - States z1 and y1 are generated from h1.
   - States z2 and y2 are generated from h2.

6. **Distributions:**
   - Distributions over locations (a1, a2).
   - Distributions over vocabulary (d1).

### Credit

*Xu et al, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015*

*Vineeth N B (IIT-H)*
*§9.2 Image Captioning*

---

*Page 13 / 25*
```

In the markdown format above, the content is structured with appropriate headings, paragraphs, and lists to represent the scientific text accurately. The image is represented as a placeholder, and special attention is given to the components and workflow described in the diagram. The credit section is formatted to include the authors and publication details.

# DL4CV_Week09_Part02.pdf - Page 72

```markdown
# Image Captioning with Attention

![Image Captioning Diagram](image_captioning_diagram.png)

## Image Captioning

### Credit
**Xu et al, Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, ICML 2015**

Vineeth N B (IIT-H) §9.2 Image Captioning

### Diagram Description

#### Input
- **Image:** H x W x 3
  ![Image](image.png)

#### Processing Steps
1. **CNN (Convolutional Neural Network)**
   - Extracts features from the image: L x D

2. **Initial Hidden State (h0)**
   - Receives the visual features (v)

3. **Hidden States (h1, h2, ...)**
   - Intermediate hidden states computed in sequence

4. **Weighted Features (D)**
   - Combination of features considering both spatial and vocabulary distribution

5. **Vocabulary Distribution (a1, a2, d1, d2, ...)**
   - Distributions over different locations and vocabulary

6. **Words (z1, y1, z2, y2, ...)**
   - Output words generated at each step

### Diagram Elements
- **Distributions:**
  - **Over Locations:** Assists in identifying important regions in the image
  - **Over Vocab:** Helps in selecting relevant words from the vocabulary

- **Weighted Features:**
  - Combination of features weighted by their relevance to the current context

- **First Word:**
  - Initial word generated in the sequence

### Flow
1. **Image to CNN**: Image processed through a CNN to extract features
2. **Features to Initial Hidden State**: Extracted features are fed into the initial hidden state
3. **Hidden States**: Sequential hidden states are computed
4. **Weighted Features**: Features are combined and weighted
5. **Distributions**: Distributions over locations and vocabulary guide the caption generation
6. **Words**: Words are generated based on the computed distributions and features
```

# DL4CV_Week09_Part02.pdf - Page 73

```markdown
# Image Captioning with Attention

## Soft attention
![Soft attention visual](image_url_placeholder)

## Hard attention
![Hard attention visual](image_url_placeholder)

- **A**
- **bird**
- **flying**
- **over**
- **a**
- **body**
- **of**
- **water**
- **\***

**Credit**: Xu et al, *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*, ICML 2015

Vineeth N B (IIIT-H)

§9.2 Image Captioning

---

```

# DL4CV_Week09_Part02.pdf - Page 74

```markdown
# Image Captioning with Attention: Results

## Examples of Image Captioning

### First Row
- **Image 1**: ![Woman Throwing Frisbee](image1.png)  
  - **Caption**: A woman is throwing a frisbee in a park.
  - **Attention Image**: ![Attention Map](attention1.png)

- **Image 2**: ![Dog on Hardwood Floor](image2.png)  
  - **Caption**: A dog is standing on a hardwood floor.
  - **Attention Image**: ![Attention Map](attention2.png)

- **Image 3**: ![Stop Sign](image3.png)  
  - **Caption**: A stop sign is on a road with a mountain in the background.
  - **Attention Image**: ![Attention Map](attention3.png)

### Second Row
- **Image 4**: ![Girl with Teddy Bear](image4.png)  
  - **Caption**: A little girl sitting on a bed with a teddy bear.
  - **Attention Image**: ![Attention Map](attention4.png)

- **Image 5**: ![Group in Boat](image5.png)  
  - **Caption**: A group of people sitting on a boat in the water.
  - **Attention Image**: ![Attention Map](attention5.png)

- **Image 6**: ![Giraffe in Forest](image6.png)  
  - **Caption**: A giraffe standing in a forest with trees in the background.
  - **Attention Image**: ![Attention Map](attention6.png)

---

**Author**: Vineeth N B (IIIT-H)

**Section**: §9.2 Image Captioning

**Slide Number**: 14 / 25
```

# DL4CV_Week09_Part02.pdf - Page 75

```markdown
# Recent Efforts: Boosting Image Captioning with Attributes in LSTMs

## Vineeth N B (IIIT-H)

### §9.2 Image Captioning

$$
\mathbf{x}^{-1} = T_v \mathbf{I}, \quad \mathbf{x}^t = T_s \mathbf{w}_t
$$

$$
\mathbf{h}^t = f(\mathbf{x}^t)
$$

$$
t \in \{0, \ldots, N_s - 1\}
$$

![Image of person in a boat with a dog](image_url)

$\mathbf{I} \in \mathbb{R}^{D_v}$

$$
\mathbf{w}_t \in \mathbb{R}^{D_s}
$$

$$
\{\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_{N_s}\}
$$

![LSTM Diagram](lstm_diagram_url)

- **LSTM**: Long Short-Term Memory network used for sequence processing.
- **$T_v$**: Transformation matrix for image input.
- **$T_s$**: Transformation matrix for attribute input.
- **$\mathbf{I}$**: Image input vector.
- **$\mathbf{h}^t$**: Hidden state of the LSTM at time step $t$.
- **$f(\mathbf{x}^t)$**: Function that processes the input $\mathbf{x}^t$ to produce the hidden state.
- **$\mathbf{w}_t$**: Attribute input vector at time step $t$.
- **$D_v$**: Dimension of the image input space.
- **$D_s$**: Dimension of the attribute input space.
- **$N_s$**: Number of time steps or attributes.

### Example Caption

```
a group of people on a boat in the water
```

### Process

1. Image input is transformed using $T_v$.
2. Attribute vectors are transformed using $T_s$.
3. These inputs are fed into the LSTM sequentially.
4. The LSTM generates the hidden state $\mathbf{h}^t$ at each time step.
5. The final output is a caption describing the image.

---

This markdown representation ensures the scientific integrity of the original slide, accurately capturing formulas, symbols, and diagrams.
```

# DL4CV_Week09_Part02.pdf - Page 76

:

```markdown
# Boosting Image Captioning with Attributes in LSTMs: A1

![LSTM Diagram](image_url)

$\mathbf{x}^{-1} = \mathbf{T}_a \mathbf{A},$

$\mathbf{x}^t = \mathbf{T}_s \mathbf{w}_t,$

$\mathbf{h}^t = f(\mathbf{x}^t), \quad t \in \{0, \ldots, N_s - 1\},$

$\mathbf{A} \in \mathbb{R}^{D_a}$

$\mathbf{A} = \{a_1, a_2, \ldots, a_{D_a}\}$

![Image Example](image_url)

**Attributes:**

- boat: 1
- water: 0.838
- man: 0.762
- riding: 0.728
- dog: 0.547
- small: 0.485
- person: 0.471
- river: 0.461

*Credit: Yao et al, Boosting Image Captioning with Attributes, ICCV 2017*

*Vineeth N B (IIIT-H)*

*§9.2 Image Captioning*
```

Note: Replace `image_url` with the actual image URLs if they are available.
```

# DL4CV_Week09_Part02.pdf - Page 77

:

```markdown
# Boosting Image Captioning with Attributes in LSTMs: A2

\[
\mathbf{x}^{-2} = \mathbf{T}_v \mathbf{I} \quad \text{and} \quad \mathbf{x}^{-1} = \mathbf{T}_a \mathbf{A},
\]

\[
\mathbf{x}^t = \mathbf{T}_s \mathbf{w}_t,
\]

\[
\mathbf{h}^t = f(\mathbf{x}^t), \quad t \in \{0, \ldots, N_s - 1\}
\]

![LSTM Diagram](image_url_if_available)

**Credit:** Yao et al, *Boosting Image Captioning with Attributes*, ICCV 2017

Vineeth N B (IIIT-H)

§9.2 Image Captioning

17 / 25
```

# DL4CV_Week09_Part02.pdf - Page 78

:

```markdown
# Boosting Image Captioning with Attributes in LSTMs: A3

## Equations

```math
x^{-2} = T_a A \quad \text{and} \quad x^{-1} = T_v I,
```

```math
x^t = T_s w_t,
```

```math
h^t = f(x^t), \quad t \in \{0, \ldots, N_s - 1\}
```

## Diagram

![LSTM Diagram](image_url_placeholder)

## Credit

Yao et al, *Boosting Image Captioning with Attributes*, ICCV 2017

Vineeth N B (IIIT-H)

## Section

### 3

## Details

- **Attributes**
- **Image**

## Location

- Slide: 9.2 Image Captioning
- Slide Number: 18 / 25
```

Note: `image_url_placeholder` should be replaced with the actual URL or path to the image if available. If the OCR cannot capture the image directly, use a placeholder.

# DL4CV_Week09_Part02.pdf - Page 79

```markdown
# Boosting Image Captioning with Attributes in LSTMs: A4

![Diagram](https://via.placeholder.com/150)

## Equations

\[
\mathbf{x}^{-1} = \mathbf{T}_a \mathbf{A},
\]

\[
\mathbf{x}^t = \mathbf{T}_s \mathbf{w}_t + \mathbf{T}_v \mathbf{I},
\]

\[
\mathbf{h}^t = f(\mathbf{x}^t),
\]

\[
t \in \{0, \ldots, N_s - 1\}
\]

## Credit

**Yao et al., Boosting Image Captioning with Attributes, ICCV 2017**

**Vineeth N B (IIIT-H)**

### Image Captioning

## Diagram

Here is an illustrative representation of the process:

- **Image**: The initial input is the image.
- **Attributes**: Attributes are included in the input.
- **LSTM**: A series of LSTM layers process the input.
- **Weights (w)**: Weights are applied at each step.

Each LSTM layer processes the inputs sequentially, with the attributes and image feeding into the network. The weights (\( \mathbf{w}_0, \mathbf{w}_1, \ldots, \mathbf{w}_N \)) are applied at each step to generate the output sequence.

```

# DL4CV_Week09_Part02.pdf - Page 80

:

```markdown
# Boosting Image Captioning with Attributes in LSTMs: A5

## Equation and Diagram

```math
x^{-1} = T_v I,
```

```math
x^t = T_s w_t + T_a A,
```

```math
h^t = f(x^t),
```

```math
t \in \{0, \ldots, N_s - 1\}
```

![LSTM Architecture with Image and Attributes Input](image_url)

### Description
This diagram illustrates an LSTM-based model for boosting image captioning with attributes. The model integrates both image features and additional attributes to enhance the captioning process. Key components include:

- **LSTM Network**: Used for processing sequential data.
- **Image Input**: Provides visual features to the model.
- **Attributes Input**: Adds supplementary information to aid captioning.
- **Weights (w)**: Parameters that the model uses to combine image and attribute information.

### Credit
- **Author**: Yao et al.
- **Publication**: Boosting Image Captioning with Attributes, ICCV 2017
- **Affiliation**: Vineeth N B (IIIT-H)
- **Section**: §9.2 Image Captioning

```

# DL4CV_Week09_Part02.pdf - Page 81

 the following markdown format:

```markdown
# Boosting Image Captioning with Attributes in LSTMs

## Observations

- **LSTM-A₁ > LSTM**
  - Indicates advantage of exploiting high-level attributes than image representations

![NPTEL Logo](image_url)

Vineeth N B (IIT-H) §9.2 Image Captioning 21 / 25
```
```markdown
# Boosting Image Captioning with Attributes in LSTMs

## Observations

- **LSTM-A₁ > LSTM**
  - Indicates advantage of exploiting high-level attributes than image representations

![NPTEL Logo](image_url)

Vineeth N B (IIT-H) §9.2 Image Captioning 21 / 25
```

# DL4CV_Week09_Part02.pdf - Page 82

```markdown
# Boosting Image Captioning with Attributes in LSTMs: Observations

- **LSTM-A<sub>1</sub> > LSTM**
  - Indicates advantage of exploiting high-level attributes than image representations

- **LSTM-A<sub>2</sub> > LSTM-A<sub>1</sub>**
  - Integrating image representations performs better

![NPTEL Logo](https://via.placeholder.com/150)

Vineeth N B (IIIT-H)

§9.2 Image Captioning

21 / 25
```

# DL4CV_Week09_Part02.pdf - Page 83

 the following content from the image:

```text
## Boosting Image Captioning with Attributes in LSTMs: Observations

- LSTM-A1 > LSTM
  - Indicates advantage of exploiting high-level attributes than image representations

- LSTM-A2 > LSTM-A1
  - Integrating image representations performs better

- LSTM-A3 > LSTM-A2
  - Benefits from mechanism of first feeding high-level attributes into LSTM instead of starting from image representations

![NPTEL](NPTEL.png)

Vineeth N B (IIIT-H) §9.2 Image Captioning
```

Extracted Content:

```markdown
# Boosting Image Captioning with Attributes in LSTMs: Observations

- LSTM-A1 > LSTM
  - Indicates advantage of exploiting high-level attributes than image representations

- LSTM-A2 > LSTM-A1
  - Integrating image representations performs better

- LSTM-A3 > LSTM-A2
  - Benefits from mechanism of first feeding high-level attributes into LSTM instead of starting from image representations

![NPTEL](NPTEL.png)

Vineeth N B (IIIT-H) §9.2 Image Captioning
```

# DL4CV_Week09_Part02.pdf - Page 84

```markdown
# Boosting Image Captioning with Attributes in LSTMs: Observations

- **LSTM-A<sub>1</sub> > LSTM**
  - Indicates advantage of exploiting high-level attributes than image representations

- **LSTM-A<sub>2</sub> > LSTM-A<sub>1</sub>**
  - Integrating image representations performs better

- **LSTM-A<sub>3</sub> > LSTM-A<sub>2</sub>**
  - Benefits from mechanism of first feeding high-level attributes into LSTM instead of starting from image representations

- **LSTM-A<sub>4</sub> < LSTM-A<sub>3</sub>**
  - This may be because noise in image can be explicitly accumulated, and thus network overfits more easily

- **But LSTM-A<sub>5</sub> which feeds attributes at each time step shows improvements on LSTM-A<sub>3</sub>**

*Vineeth N B (IIIT-H)*

*§9.2 Image Captioning*

*21 / 25*
```

# DL4CV_Week09_Part02.pdf - Page 85

 it correctly:

```markdown
# Boosting Image Captioning with Attributes in LSTMs: Observations

- **LSTM-A<sub>1</sub> > LSTM**
  - Indicates advantage of exploiting high-level attributes than image representations
  
- **LSTM-A<sub>2</sub> > LSTM-A<sub>1</sub>**
  - Integrating image representations performs better
  
- **LSTM-A<sub>3</sub> > LSTM-A<sub>2</sub>**
  - Benefits from mechanism of first feeding high-level attributes into LSTM instead of starting from image representations
  
- **LSTM-A<sub>4</sub> < LSTM-A<sub>3</sub>**
  - This may be because noise in image can be explicitly accumulated, and thus network overfits more easily
  - But LSTM-A<sub>5</sub>, which feeds attributes at each time step shows improvements on LSTM-A<sub>3</sub>
  
- **LSTM-A<sub>2</sub>, LSTM-A<sub>3</sub>, LSTM-A<sub>5</sub> > LSTM**
  - Indicates that image representations and attributes are complementary and have mutual reinforcement for image captioning

*Vineeth N B (IIIT-H) §9.2 Image Captioning*

![Image](https://via.placeholder.com/150)
```

# DL4CV_Week09_Part02.pdf - Page 86

 this output is a single slide, there is no need to split it into multiple sections.

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

![Captioning Image](image_url)

**CaptionBot:** A man on a rocky hillside next to a stone wall.

_Vineeth N B (IIT-H)_

§9.2 Image Captioning
```

Note: Replace `image_url` with the actual URL or path of the image if it's available in the source.

# DL4CV_Week09_Part02.pdf - Page 87

:

```markdown

# StyleNet: Generating Attractive Visual Captions with Styles

![Image not available](image_not_available.png)

## CaptionBot
**A man on a rocky hillside next to a stone wall.**

## Romantic
**A man uses rock climbing to conquer the high.**

*Vineeth N B (IIT-H)*

*Section 9.2 Image Captioning*

*Page 22 of 25*

```

# DL4CV_Week09_Part02.pdf - Page 88

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

![Image Not Available](image_url)

## Captioning Styles

### CaptionBot
- **Description**: A man on a rocky hillside next to a stone wall.

### Romantic
- **Description**: A man uses rock climbing to conquer the high.

### Humorous
- **Description**: A man is climbing the rock like a lizard.

**Source**: Vineeth N B (IIT-H)

**Section**: §9.2 Image Captioning

**Slide Number**: 22 / 25
```

# DL4CV_Week09_Part02.pdf - Page 89

 is not rendered correctly.

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

## Recall LSTM

![LSTM Diagram](https://via.placeholder.com/300)

$$
i_t = \text{sigmoid}(W_{ix}x_t + W_{ih}h_{t-1})
$$

*Vineeth N B (IIT-H) §9.2 Image Captioning 22 / 25*
```

### Diagram Explanation:
- **LSTM (Long Short-Term Memory) Network**: The diagram shows the internal mechanisms of an LSTM cell.
    - **Input Gates**: Represented by symbols indicating the control of input information into the cell.
    - **Forget Gate**: Controls the removal of information from the cell.
    - **Output Gate**: Determines what information will be outputted from the cell.
    - **Cell State**: The horizontal line through the cell representing the memory of the LSTM.
    - **Hidden State**: Output of the cell at each time step.

### Equations:
- **Sigmoid Function**: The function used in the LSTM to control the flow of information.
    - \( i_t = \text{sigmoid}(W_{ix}x_t + W_{ih}h_{t-1}) \)
        - \( i_t \): Peephole input gate at time \( t \).
        - \( W_{ix} \): Weight matrix for the input.
        - \( W_{ih} \): Weight matrix for the hidden state.
        - \( x_t \): Input at time \( t \).
        - \( h_{t-1} \): Hidden state at time \( t-1 \).
```

# DL4CV_Week09_Part02.pdf - Page 90



```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

## Recall LSTM

![Recall LSTM](image_url_if_image_can_be_captured)

$$
\begin{aligned}
    &i_t = \text{sigmoid}(W_{ix}x_t + W_{ih}h_{t-1}) \\
    &f_t = \text{sigmoid}(W_{fx}x_t + W_{fh}h_{t-1}) \\
    &\tilde{c_t} = \text{tanh}(W_{cx}x_t + W_{ch}h_{t-1}) \\
    &c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c_t}
\end{aligned}
$$

*Vineeth N B (IIIT-H)*

*Section 9.2 Image Captioning*

```

# DL4CV_Week09_Part02.pdf - Page 91

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

## Recall LSTM

![Recall LSTM Diagram](https://via.placeholder.com/500)

\[
\begin{aligned}
i_t &= \text{sigmoid}(W_{ix}x_t + W_{ih}h_{t-1}) \\
f_t &= \text{sigmoid}(W_{fx}x_t + W_{fh}h_{t-1}) \\
o_t &= \text{sigmoid}(W_{ox}x_t + W_{oh}h_{t-1}) \\
\tilde{c_t} &= \tanh(W_{cx}x_t + W_{ch}h_{t-1}) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c_t} \\
h_t &= o_t \odot c_t
\end{aligned}
\]

*Vineeth N B (IIIT-H)*

*Section 9.2 Image Captioning*

*Slide 22 / 25*
```

# DL4CV_Week09_Part02.pdf - Page 92

 is not captured directly.

### Slide: StyleNet: Generating Attractive Visual Captions with Styles

#### StyleNet proposes a factored LSTM

![Factored LSTM Diagram](image_url_if_available)

- **Bold Text**
- *Italic Text*

```math
W_x = U_x S_x V_x
```

```math
i_t = \text{sigmoid}(W_i x_t + W_i h_{t-1})
```

```math
f_t = \text{sigmoid}(W_f x_t + W_f h_{t-1})
```

```math
o_t = \text{sigmoid}(W_o x_t + W_o h_{t-1})
```

```math
\tilde{c}_t = \text{tanh}(W_c x_t + W_c h_{t-1})
```

```math
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t
```

```math
h_t = o_t \odot c_t
```

**Vineeth N B (IIIT-H)**

**§9.2 Image Captioning**

22 / 25

# DL4CV_Week09_Part02.pdf - Page 93

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

![StyleNet Overview](image_placeholder.png)

**Input image**

![Input Image](input_image_placeholder.png)

*Factual caption*

**A man jumps into water.**

![Factual Caption](factual_caption_placeholder.png)

```text
CNN 
    x1
  /   \
 LSTM  LSTM ... LSTM 
  \   /
   U S_F V  U S_F V  U S_F V
    x1  x2  xN  x1' x2' xN'
```

**Factored LSTM**

```text
U, S_F, V are trainable.
```

_Vineeth N B (IIT-H)_

§9.2 Image Captioning

22 / 25
```

# DL4CV_Week09_Part02.pdf - Page 94

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

## Input Image
![Input Image](image_url)

- **Factural caption:** A man jumps into water.
- **Romantic sentences:** A couple are celebrating their love.

## Diagram Explanation

### Top Section: Factural Caption
- **CNN:** Convolutional Neural Network processes the input image.
- **LSTM:** Long Short-Term Memory network for sequential data processing.
  - `x1, x2, ..., xN`: Sequence of inputs.
  - `U, V`: Vectors.
  - `SR`: Trainable parameter.

### Bottom Section: Romantic Sentences
- **LSTM:** Long Short-Term Memory network for sequential data processing.
  - `x1, x2, ..., xN`: Sequence of inputs.
  - `U, V`: Vectors.
  - `SR`: Trainable parameter.

## Note
- **Keep `U, V` same as earlier.**
- **`SR` is trainable.**

_Vineeth N B (IIT-H)_
_§9.2 Image Captioning_

Page 22 / 25
```

# DL4CV_Week09_Part02.pdf - Page 95

 on the provided image is as follows:

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

## Input image

![Input Image](image_url)

**CNN**

### Factored LSTM
- **Factorial caption**: A man jumps into water.
- ![Factored LSTM](image_url)
    - **LSTM**: \(X_1\), \(X_2\), ..., \(X_N\)
    - **U**: \(U S_1 V\), \(U S_2 V\), ..., \(U S_N V\)

### Romantic sentences
- **Romantic caption**: A couple are celebrating their love.
- ![Factored LSTM](image_url)
    - **LSTM**: \(X_1\), \(X_2\), ..., \(X_N\)
    - **U**: \(U S_1 V\), \(U S_2 V\), ..., \(U S_N V\)

### Humorous sentences
- **Humorous caption**: A boy stands on the tree like a monkey.
- ![Factored LSTM](image_url)
    - **LSTM**: \(X_1\), \(X_2\), ..., \(X_N\)
    - **U**: \(U S_1 V\), \(U S_2 V\), ..., \(U S_N V\)

Vineeth N B (IIIT-H)

§9.2 Image Captioning

23 / 25
```

# DL4CV_Week09_Part02.pdf - Page 96

 accuracy is particularly important for scientific documents.

```markdown
# StyleNet: Generating Attractive Visual Captions with Styles

![Image of a snowboarder in the air](image-url)

- **F:** A snowboarder in the air.
- **R:** A man is doing a trick on a skateboard to show his courage.
- **H:** A man is jumping on a snowboard to reach outer space.

---

NPTEL

At test swap $S_x$ accordingly to get the desired output type.

---

*Vineeth N B (IIT-H) §9.2 Image Captioning*

---

23 / 25
```

**Note:** For images, graphs, or diagrams, use markdown image syntax `![]()` with placeholders if OCR can't capture them directly.

# DL4CV_Week09_Part02.pdf - Page 97

```markdown
# Neural Baby Talk

## More Grounded

![Neural Baby Talk](https://example.com/image.png)

- **DPM**: Object Detection, Attention, Preprocessing
- **CRF**: Conditional Random Field

### This is a photograph of one **dog** and one **cake**.
The dog is ...

_Note: The image is a conceptual diagram showing the flow and interaction between different components of the Neural Baby Talk model._

_Credit: Lu et al., Neural Baby Talk, CVPR 2018_

_Vineeth N B (IIIT-H)_

### 9.2 Image Captioning

Page Number: 24 / 25
```

# DL4CV_Week09_Part02.pdf - Page 98

```markdown
# Neural Baby Talk

## More Grounded

![More Grounded Image](image_url)

- **DPM**: Detects objects, attributes, and prepares for further processing.
- **CRF**: Conditional Random Field used to refine the output.

### Example Output
- **This is a photograph of one dog and one cake.**
  - The dog is ...

## More Natural

![More Natural Image](image_url)

- **CNN**: Convolutional Neural Network extracts features from the image.
- **RNN**: Recurrent Neural Network processes sequential data for caption generation.

### Example Output
- **A dog is sitting on a couch with a toy.**

**Credit**: Lu et al., *Neural Baby Talk*, CVPR 2018

Vineeth N B (IIT-H) §9.2 Image Captioning

---
```

# DL4CV_Week09_Part02.pdf - Page 99

```markdown
# Neural Baby Talk

## More Grounded

![Image of More Grounded](image_url)

- DPM
- CRF
- Obj
- Attr Prep

This is a photograph of one **dog** and one **cake**. The dog is ...

## Neural Baby Talk

![Image of Neural Baby Talk](image_url)

- Detector
- CNN
- RNN
- Region feature

A **(![](image_url))** with a **(![](image_url))** is sitting at **(![](image_url))** with a **(![](image_url))**. A **(![](image_url))** cake. A **(![](image_url))** table.

## More Natural

![Image of More Natural](image_url)

- CNN
- RNN
- Conv feature

A dog is sitting on a couch with a toy.

*Credit: Lu et al, Neural Baby Talk, CVPR 2018*

*Vineeth N B (IIIT-H)*

*§9.2 Image Captioning*
```

# DL4CV_Week09_Part02.pdf - Page 100

```markdown
# Neural Baby Talk

## Image Captioning

### Credit: Lu et al, Neural Baby Talk, CVPR 2018

![Neural Baby Talk Diagram](image_url)

A cat is laying on the laptop near a chair.

1. **Object Detector**: 
   - Proposals: \( r_i \)
   - Output: \( P^t \)

2. **Embed**: 
   - Input: Text "A cat is laying on the laptop near a chair."
   - Process: Embedding the text

3. **RNN with Attention**: 
   - Input: Embedded text \( h_t \)
   - Output: Context vector \( c_t \)

4. **Attention Mechanism**:
   - Align regions of interest (RoI) using attention weights
   - Produces attention features \( v_t \)

5. **Query Mechanism**:
   - Combines attention features \( v_t \) and context vector \( c_t \) to produce query vector \( v_c \)

6. **Singular vs. Plural Determination**:
   - Determines the correct word form (singular or plural) using the attention mechanism.

7. **Output**:
   - Final output prediction: \( P_{final} \)
   - Result: "cat"

### Visualization 
- **Object Detection**: Highlights detected objects in the image.
  ![Object Detection](image_url)

- **Attention Mechanism**:
  - Visualizes attention weights applied to different regions of the image.
  - Relevant regions are marked to focus on the object (cat).

- **Word Prediction**:
  - Shows candidate words for prediction.
  - Uses context to narrow down the correct word.

### Mathematical Notations
- **Proposals**: \( r_i \)
- **Attention Output**: \( v_t \)
- **Context Vector**: \( c_t \)
- **Query Vector**: \( v_c \)
- **Final Prediction**: \( P_{final} \)

Vineeth N B (IIIT-H)
```

(Note: Replace `image_url` with the actual URLs of the images if available.)

# DL4CV_Week09_Part02.pdf - Page 101

 the given OCR output and convert it into a detailed markdown format.

# Homework

## Readings

- Papers on respective slides
- (Optional) Georgia Tech Notebook on Image Captioning (Part: 1-3)

## Question

- Can we do the opposite (caption-to-image) of what we learnt in this section? How?

*Vineeth N B. (IIIT-H) §9.2 Image Captioning*

---

**Note**: This markdown output assumes that the OCR was performed on a slide or document from a lecture on image captioning, with specific attention to the readings and a question for further exploration.
```

